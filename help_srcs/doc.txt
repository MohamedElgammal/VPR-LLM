/BUILDING.md
--------------------------------------
# Building VTR


## Setting up Your Environment

If you cloned the repository you will need to set up the git submodules (if you downloaded and extracted a release, you can skip this step):

    git submodule init
    git submodule update

VTR requires several system packages.  From the top-level directory, run the following script to install the required packages on a modern Debian or Ubuntu system:

    ./install_apt_packages.sh


You will also need several Python packages.  You can optionally install and activate a Python virtual environment so that you do not need to modify your system Python installation:

    make env
    source .venv/bin/activate

Then to install the Python packages:

    pip install -r requirements.txt

**Note:** If you chose to install the Python virtual environment, you will need to remember to activate it on any new terminal window you use, before you can run the VTR flow or regressions tests (`source .venv/bin/activate`).

## Building

From the top-level, run:

    make

   which will build all the required tools.

The complete VTR flow has been tested on 64-bit Linux systems.
The flow should work in other platforms (32-bit Linux, Windows with cygwin) but this is untested.

*Full information about building VTR, including setting up required system packages and Python packages, can be found in [Optional Build Information](doc/src/vtr/optional_build_info.md) page.*

Please [let us know](doc/src/contact.md) your experience with building VTR so that we can improve the experience for others.

The tools included official VTR releases have been tested for compatibility.
If you download a different version of those tools, then those versions may not be mutually compatible with the VTR release.

## Verifying Installation

To verfiy that VTR has been installed correctly run::

    ./vtr_flow/scripts/run_vtr_task.py ./vtr_flow/tasks/regression_tests/vtr_reg_basic/basic_timing

The expected output is::
    
    k6_N10_mem32K_40nm/single_ff            OK
    k6_N10_mem32K_40nm/single_ff            OK
    k6_N10_mem32K_40nm/single_wire          OK
    k6_N10_mem32K_40nm/single_wire          OK
    k6_N10_mem32K_40nm/diffeq1              OK
    k6_N10_mem32K_40nm/diffeq1              OK
    k6_N10_mem32K_40nm/ch_intrinsics                OK
    k6_N10_mem32K_40nm/ch_intrinsics                OK




/CHANGELOG.md
--------------------------------------
# VTR Change Log
<!-- 
This file documents user-facing changes between releases of the VTR
project. The goal is to concicely communicate to end users what is new
or has changed in a particular release. It should *NOT* just be a dump
of the commit log, as that is far too detailed. Most code re-factoring
does not warrant a change log entry unless it has a significant impact
on the end users (e.g. substantial performance improvements).

Each release's change log should include headings (where releveant) with
bullet points listing what was: 
  - added           (new feature)
  - changed         (change to existing feature behaviour)
  - fixed           (bug fix)
  - deprecated      (features planned for future removal)
  - removed         (previous features which have been removed)

Changes which have landed in the master/trunk but not been released
should be included in the 'Unreleased' section and moved to the releveant
releases' section when released.

In the case of release candidates (e.g. v8.0.0-rc1) the current
set of unreleased changes should be moved under that heading. Any
subsequent fixes to the release candidate would be placed under
'Unreleased', eventually moving into the next release candidate's
heading (e.g. v8.0.0-rc2) when created. Note this means the change log for
subsequent release candidates (e.g. rc2) would only contain new changes
not included in previous release candidates (e.g. rc1).  When the final
(non-release candidate) release is made (e.g. v8.0.0) the change log
should contain all the relevant changes compared to the last non-release
candidate release (e.g. v7.0.0). That is, it should be the concatenation
of the unreleased and any previous release candidates change logs.
-->

_Note that changes from release candidates (e.g. v8.0.0-rc1, v8.0.0-rc2) are included/repeated in the final release (e.g. v8.0.0) change log._

## Unreleased
_The following are changes which have been implemented in the VTR master branch but have not yet been included in an official release._

### Added

### Changed

### Fixed

### Deprecated

### Removed


## v9.0.0 - 2024-12-23

### Added
  * Support for Advanced Architectures:
    * 3D FPGA and RAD architectures.
    * Architectures with hard Networks-on-Chip (NoCs).
    * Distinct horizontal and vertical channel widths and types.
    * Diagonal routing wires and other complex wire shapes (L-shaped, T-shaped, ....).
  
  * New Benchmark Suites:
    * Koios: A deep-learning-focused benchmark suite with various design sizes.
    * Hermes: Benchmarks utilizing hard NoCs.
    * TitanNew: Large benchmarks targeting the Stratix 10 architecture.

  * Commercial FPGAs Architecture Captures:
    * Intel’s Stratix 10 FPGA architecture.
    * AMD’s 7-series FPGA architecture.

  * Parmys Logic Synthesis Flow:
    * Better Verilog language coverage
    * More efficient hard block mapping

  * VPR Graphics Visualizations:
    * New interface for improved usability and underlying graphics rewritten using EZGL/GTK to allow more UI widgets.
    * Algorithm breakpoint visualizations for placement and routing algorithm debugging.
    * User-guided (manual) placement optimization features.
    * Enabled a live connection for client graphical application to VTR engines through sockets (server mode).
    * Interactive timing path analysis (IPA) client using server mode.
   
  * Performance Enhancements:
    * Parallel router for faster inter-cluster routing or flat routing.

  * Re-clustering API to modify packing decisions during the flow.
  * Support for floorplanning and placement constraints.
  * Unified intra- and inter-cluster (flat) routing.
  * Comprehensive web-based VTR utilities and API documentation.
  
### Changed
  * The default values of many command line options (e.g. inner_num is 0.5 instead of 1.0)
  * Changes to placement engine
    * Smart centroid initial placement algorithm.
    * Multiple smart placement directed moves.
    * Reinforcement learning-based placement algorithm.
  * Changes to routing engine
    * Faster lookahead creation.
    * More accurate lookahead for large blocks.
    * More efficient heap and pruning strategies.
    * max `pres_fac` capped to avoid possible numeric issues.
    
    
### Fixed
  * Many algorithmic and coding bugs are fixed in this release
   
### Removed
  * Breadth-first (non-timing-driven) router.
  * Non-linear congestion placement cost.

## v8.0.0 - 2020-03-24

### Added
 * Support for arbitrary FPGA device grids/floorplans
 * Support for clustered blocks with width > 1
 * Customizable connection-block and switch-blocks patterns (controlled from FPGA architecture file)
 * Fan-out dependent routing mux delays
 * VPR can generate/load a routing architecture (routing resource graph) in XML format
 * VPR can load routing from a `.route` file
 * VPR can performing analysis (STA/Power/Area) independently from optimization (via `vpr --analysis`)
 * VPR supports netlist primitives with multiple clocks
 * VPR can perform hold-time (minimum delay) timing analysis
 * Minimum delays can be annotated in the FPGA architecture file
 * Flow supports formal verification of circuit implementation against input netlist
 * Support for generating FASM to drive bitstream generators
 * Routing predictor which predicts and aborts impossible routings early (saves significant run-time during minimum channel width search)
 * Support for minimum routable channel width 'hints' (reduces minimum channel width search run-time if accurate)
 * Improved VPR debugging/verbosity controls
 * VPR can perform basic netlist cleaning (e.g. sweeping dangling logic)
 * VPR graphics visualizations:
   * Critical path during placement/routing
   * Cluster pin utilization heatmap
   * Routing utilization heatmap
   * Routing resource cost heatmaps
   * Placement macros
 * VPR can route constant nets
 * VPR can route clock nets
 * VPR can load netlists in extended BLIF (eBLIF) format
 * Support for generating post-placement timing reports
 * Improved router 'map' lookahead which adapts to routing architecture structure
 * Script to upgrade legacy architecture files (`vtr_flow/scripts/upgrade_arch.py`)
 * Support for Fc overrides which depend on both pin and target wire segment type
 * Support for non-configurable switches (shorts, inline-buffers) used to model structures like clock-trees and non-linear wires (e.g. 'L' or 'T' shapes)
 * Various other features since VTR 7

### Changed
 * VPR will exit with code 1 on errors (something went wrong), and code 2 when unable to implement a circuit (e.g. unroutable)
 * VPR now gives more complete help about command-line options (`vpr -h`)
 * Improved a wide variety of error messages
 * Improved STA timing reports (more details, clearer format)
 * VPR now uses Tatum as its STA engine
 * VPR now detects missmatched architecture (.xml) and implementation (.net/.place/.route) files more robustly
 * Improved router run-time and quality through incremental re-routing and improved handling of high-fanout nets
 * The timing edges within each netlist primitive must now be specified in the <models> section of the architecture file
 * All interconnect tags must have unique names in the architecture file
 * Connection block input pin switch must now be specified in <switchlist> section of the architecture file
 * Renamed switch types buffered/pass_trans to more descriptive tristate/pass_gate in architecture file
 * Require longline segment types to have no switchblock/connectionblock specification
 * Improve naming (true/false -> none/full/instance) and give more control over block pin equivalnce specifications
 * VPR will produce a .route file even if the routing is illegal (aids debugging), however analysis results will not be produced unless `vpr --analsysis` is specified
 * VPR long arguments are now always prefixed by two dashes (e.g. `--route`) while short single-letter arguments are prefixed by a single dash (e.g. `-h`)
 * Improved logic optimization through using a recent 2018 version of ABC and new synthesis script
 * Significantly improved implementation quality (~14% smaller minimum routable channel widths, 32-42% reduced wirelength, 7-10% lower critical path delay)
 * Significantly reduced run-time (~5.5-6.3x faster) and memory usage (~3.3-5x lower)
 * Support for non-contiguous track numbers in externally loaded RR graphs
 * Improved placer quality (reduced cost round-off)
 * Various other changes since VTR 7

### Fixed
 * FPGA Architecture file tags can be in arbitary orders
 * SDC command arguments can be in arbitary orders
 * Numerous other fixes since VTR 7

### Removed
 * Classic VPR timing analyzer
 * IO channel distribution section of architecture file

### Deprecated
 * VPR's breadth-first router (use the timing-driven router, which provides supperiour QoR and Run-time)

### Docker Image
 * A docker image is available for VTR 8.0 release on mohamedelgammal/vtr8:latest. You can run it using the following commands:
```
$ sudo docker pull mohamedelgammal/vtr8:latest
$ sudo docker run -it mohamedelgammal/vtr8:latest
```
 
## v8.0.0-rc2 - 2019-08-01

### Changed
 * Support for non-contiguous track numbers in externally loaded RR graphs
 * Improved placer quality (reduced cost round-off)

## v8.0.0-rc1 - 2019-06-13

### Added
 * Support for arbitrary FPGA device grids/floorplans
 * Support for clustered blocks with width > 1
 * Customizable connection-block and switch-blocks patterns (controlled from FPGA architecture file)
 * Fan-out dependent routing mux delays
 * VPR can generate/load a routing architecture (routing resource graph) in XML format
 * VPR can load routing from a `.route` file
 * VPR can performing analysis (STA/Power/Area) independently from optimization (via `vpr --analysis`)
 * VPR supports netlist primitives with multiple clocks
 * VPR can perform hold-time (minimum delay) timing analysis
 * Minimum delays can be annotated in the FPGA architecture file
 * Flow supports formal verification of circuit implementation against input netlist
 * Support for generating FASM to drive bitstream generators
 * Routing predictor which predicts and aborts impossible routings early (saves significant run-time during minimum channel width search)
 * Support for minimum routable channel width 'hints' (reduces minimum channel width search run-time if accurate)
 * Improved VPR debugging/verbosity controls
 * VPR can perform basic netlist cleaning (e.g. sweeping dangling logic)
 * VPR graphics visualizations:
   * Critical path during placement/routing
   * Cluster pin utilization heatmap
   * Routing utilization heatmap
   * Routing resource cost heatmaps
   * Placement macros
 * VPR can route constant nets
 * VPR can route clock nets
 * VPR can load netlists in extended BLIF (eBLIF) format
 * Support for generating post-placement timing reports
 * Improved router 'map' lookahead which adapts to routing architecture structure
 * Script to upgrade legacy architecture files (`vtr_flow/scripts/upgrade_arch.py`)
 * Support for Fc overrides which depend on both pin and target wire segment type
 * Support for non-configurable switches (shorts, inline-buffers) used to model structures like clock-trees and non-linear wires (e.g. 'L' or 'T' shapes)
 * Various other features since VTR 7

### Changed
 * VPR will exit with code 1 on errors (something went wrong), and code 2 when unable to implement a circuit (e.g. unroutable)
 * VPR now gives more complete help about command-line options (`vpr -h`)
 * Improved a wide variety of error messages
 * Improved STA timing reports (more details, clearer format)
 * VPR now uses Tatum as its STA engine
 * VPR now detects missmatched architecture (.xml) and implementation (.net/.place/.route) files more robustly
 * Improved router run-time and quality through incremental re-routing and improved handling of high-fanout nets
 * The timing edges within each netlist primitive must now be specified in the <models> section of the architecture file
 * All interconnect tags must have unique names in the architecture file
 * Connection block input pin switch must now be specified in <switchlist> section of the architecture file
 * Renamed switch types buffered/pass_trans to more descriptive tristate/pass_gate in architecture file
 * Require longline segment types to have no switchblock/connectionblock specification
 * Improve naming (true/false -> none/full/instance) and give more control over block pin equivalnce specifications
 * VPR will produce a .route file even if the routing is illegal (aids debugging), however analysis results will not be produced unless `vpr --analsysis` is specified
 * VPR long arguments are now always prefixed by two dashes (e.g. `--route`) while short single-letter arguments are prefixed by a single dash (e.g. `-h`)
 * Improved logic optimization through using a recent 2018 version of ABC and new synthesis script
 * Significantly improved implementation quality (~14% smaller minimum routable channel widths, 32-42% reduced wirelength, 7-10% lower critical path delay)
 * Significantly reduced run-time (~5.5-6.3x faster) and memory usage (~3.3-5x lower)
 * Various other changes since VTR 7

### Fixed
 * FPGA Architecture file tags can be in arbitary orders
 * SDC command arguments can be in arbitary orders
 * Numerous other fixes since VTR 7

### Deprecated

### Removed
 * Classic VPR timing analyzer
 * IO channel distribution section of architecture file



/CONTRIBUTING.md
--------------------------------------
# Contribution Guidelines

Thanks for considering contributing to VTR!
Here are some helpful guidelines to follow.

## Common Scenarios

### I have a question
If you have questions about VTR take a look at our [Support Resources](SUPPORT.md).

If the answer to your question wasn't in the documentation (and you think it should have been), consider [enhancing the documentation](#enhancing-documentation).
That way someone (perhaps your future self!) will be able to quickly find the answer in the future.


### I found a bug!
While we strive to make VTR reliable and robust, bugs are inevitable in large-scale software projects.

Please file a [detailed bug report](#filling-bug-reports).
This ensures we know about the problem and can work towards fixing it.


### It would be great if VTR supported ...
VTR has many features and is highly flexible.
Make sure you've checkout out all our [Support Resources](SUPPORT.md) to see if VTR already supports what you want.

If VTR does not support your use case, consider [filling an enhancement](#filling-enhancement-requests).

### I have a bug-fix/feature I'd like to include in VTR
Great! Submitting bug-fixes and features is a great way to improve VTR.
See the guidlines for [submitting code](#submitting-code-to-vtr).

## The Details

### Enhancing Documentation
Enhancing documentation is a great way to start contributing to VTR.

You can edit the [documentation](https://docs.verilogtorouting.org) directly by clicking the `Edit on GitHub` link of the relevant page, or by editing the re-structured text (`.rst`) files under `doc/src`.

Generally it is best to make small incremental changes.
If you are considering larger changes its best to discuss them first (e.g. file a [bug](#filling-bug-reports) or [enhancement](#filling-enhancement-requests)).

Once you've made your enhancements [open a pull request](#making-pull-requests) to get your changes considered for inclusion in the documentation.

### How do I build the documentation?
The documentation can be built by using the command `make html` in the `$VTR_ROOT/doc` directory and you can view it in a web browser by loading the file at `$VTR_ROOT/_build/html/index.html`. More information on building
the documentation can be found on the [README on GitHub](https://github.com/verilog-to-routing/vtr-verilog-to-routing/tree/master/doc).

### Filling Bug Reports
First, search for [existing issues](https://github.com/verilog-to-routing/vtr-verilog-to-routing/issues?&=) to see if the bug has already been reported.

If no bug exists you will need to collect key pieces of information.
This information helps us to quickly reproduce (and hopefully fix) the issue:

* What behaviour you expect

    How you think VTR should be working.

* What behaviour you are seeing

    What VTR actually does on your system.

* Detailed steps to re-produce the bug

    *This is key to getting your bug fixed.*

    Provided *detailed steps* to reproduce the bug, including the exact commands to reproduce the bug.
    Attach all relevant files (e.g. FPGA architecture files, benchmark circuits, log files).

    If we can't re-produce the issue it is very difficult to fix.

* Context about what you are trying to achieve

    Sometimes VTR does things in a different way than you expect.
    Telling us what you are trying to accomplish helps us to come up with better real-world solutions.

* Details about your environment

    Tell us what version of VTR you are using (e.g. the output of `vpr --version`), which Operating System and compiler you are using, or any other relevant information about where or how you are building/running VTR.

Once you've gathered all the information [open an Issue](https://github.com/verilog-to-routing/vtr-verilog-to-routing/issues/new?template=bug_report.md) on our issue tracker.

If you know how to fix the issue, or already have it coded-up, please also consider [submitting the fix](#submitting-code-to-vtr).
This is likely the fastest way to get bugs fixed!

### Filling Enhancement Requests
First, search [existing issues](https://github.com/verilog-to-routing/vtr-verilog-to-routing/issues) to see if your enhancement request overlaps with an existing Issue.

If not feature request exists you will need to describe your enhancement:

* New behaviour

    How your proposed enhancement will work (from a user's perspective).

* Contrast with current behaviour

    How will your enhancement differ from the current behaviour (from a user's perspective).

* Potential Implementation

    Describe (if you have some idea) how the proposed enhancement would be implemented.

* Context

    What is the broader goal you are trying to accomplish? How does this enhancement help?
    This allows us to understand why this enhancement is beneficial, and come up with the best real-world solution.

**VTR developers have limited time and resources, and will not be able to address all feature requests.**
Typically, simple enhancements, and those which are broadly useful to a wide group of users get higher priority.

Features which are not generally useful, or useful to only a small group of users will tend to get lower priority.
(Of course [coding the enhancement yourself](#submitting-code-to-vtr) is an easy way to bypass this challenge).

Once you've gathered all the information [open an Issue](https://github.com/verilog-to-routing/vtr-verilog-to-routing/issues/new?template=feature_request.md) on our issue tracker.

### Submitting Code to VTR
VTR welcomes external contributions.

In general changes that are narrowly focused (e.g. small bug fixes) are easier to review and include in the code base.

Large changes, such as substantial new features or significant code-refactoring are more challenging to review.
It is probably best to file an [enhancement](#filling-enhancement-requests) first to discuss your approach.

Additionally, new features which are generally useful are much easier to justify adding to the code base, whereas features useful in only a few specialized cases are more difficult to justify.

Once your fix/enahcement is ready to go, [start a pull request](#making-pull-requests).

### Making Pull Requests
It is assumed that by opening a pull request to VTR you have permission to do so, and the changes are under the relevant [License](LICENSE.md).
VTR does not require a Contributor License Agreement (CLA) or formal Developer Certificate of Origin (DCO) for contributions.

Each pull request should describe it's motivation and context (linking to a relevant Issue for non-trivial changes).

Code-changes should also describe:

* The type of change (e.g. bug-fix, feature)

* How it has been tested

* What tests have been added

    All new features must have tests added which exercise the new features.
    This ensures any future changes which break your feature will be detected.
    It is also best to add tests when fixing bugs, for the same reason

    See [Adding Tests](README.developers.md#adding-tests) for details on how to create new regression tests.
    If you aren't sure what tests are needed, ask a maintainer.

* How the feature has been documented

    Any new user-facing features should be documented in the public documentation, which is in `.rst` format under `doc/src`, and served at https://docs.verilogtorouting.org

Once everything is ready [create a pull request](https://github.com/verilog-to-routing/vtr-verilog-to-routing/pulls).

**Tips for Pull Requests**
The following are general tips for making your pull requests easy to review (and hence more likely to be merged):

* Keep changes small

    Large change sets are difficult and time-consuming to review.
    If a change set is becoming too large, consider splitting it into smaller pieces; you'll probably want to [file an issue](#filling-enhancement-requests) to discuss things first.

* Do one thing only

    All the changes and commits in your pull request should be relevant to the bug/feature it addresses.
    There should be no unrelated changes (e.g. adding IDE files, re-formatting unchanged code).

    Unrelated changes make it difficult to accept a pull request, since it does more than what the pull request described.

* Match existing code style
    When modifying existing code, try match the existing coding style.
    This helps to keep the code consistent and reduces noise in the pull request (e.g. by avoiding re-formatting changes), which makes it easier to review and more likely to be merged.



/LICENSE.md
--------------------------------------
# VTR License

The software package "VTR" includes the software tools ODIN II, ABC, and VPR as
well as additional benchmarks, documentation, libraries and scripts. The authors
of the various components of VTR retain their ownership of their tools.

* Unless otherwise noted (in particular ABC, the benchmark circuits and some libraries),
all software, documents, and scripts in VTR, follows the standard MIT license described
[here](http://www.opensource.org/licenses/mit-license.php) copied below for
your convenience:

> The MIT License (MIT)
>
> Copyright 2012 VTR Developers
>
> Permission is hereby granted, free of charge, to any person obtaining a copy of
> this software and associated documentation files (the "Software"), to deal in
> the Software without restriction, including without limitation the rights to
> use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
> of the Software, and to permit persons to whom the Software is furnished to do
> so, subject to the following conditions:
>
> The above copyright notice and this permission notice shall be included in all
> copies or substantial portions of the Software.
>
> THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
> IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
> FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
> AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
> LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
> OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
> SOFTWARE.

* Terms and conditions for ABC is found
[here](http://www.eecs.berkeley.edu/~alanmi/abc/copyright.htm) copied below
for your convenience:

> Copyright (c) The Regents of the University of California. All rights reserved.
>
> Permission is hereby granted, without written agreement and without license or
> royalty fees, to use, copy, modify, and distribute this software and its
> documentation for any purpose, provided that the above copyright notice and the
> following two paragraphs appear in all copies of this software.
>
> IN NO EVENT SHALL THE UNIVERSITY OF CALIFORNIA BE LIABLE TO ANY PARTY FOR
> DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF
> THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF THE UNIVERSITY OF
> CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
>
> THE UNIVERSITY OF CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING,
> BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
> A PARTICULAR PURPOSE. THE SOFTWARE PROVIDED HEREUNDER IS ON AN "AS IS" BASIS,
> AND THE UNIVERSITY OF CALIFORNIA HAS NO OBLIGATION TO PROVIDE MAINTENANCE,
> SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.

The benchmark circuits are all open source but each have their own
individual terms and conditions which are listed in the source code of each
benchmark.

Subject to these conditions, the software is provided free of charge to all
interested parties.

If you do decide to use this tool, please reference our work as references are
important in academia.

Donations in the form of research grants to promote further research and
development on the tools will be gladly accepted, either anonymously or with
attribution on our future publications.




/README.developers.md
--------------------------------------
# Commit Procedures

For general guidance on contributing to VTR see [Submitting Code to VTR](CONTRIBUTING.md#submitting-code-to-vtr).

The actual machanics of submitting code are outlined below.

However they differ slightly depending on whether you are:
 * an **internal developer** (i.e. you have commit access to the main VTR repository at `github.com/verilog-to-routing/vtr-verilog-to-routing`) or, 
 * an (**external developer**) (i.e. no commit access).

The overall approach is similar, but we call out the differences below.

1. Setup a local repository on your development machine.

    a. **External Developers**

    * Create a 'fork' of the VTR repository.

        Usually this is done on GitHub, giving you a copy of the VTR repository (i.e. `github.com/<username>/vtr-verilog-to-routing`, where `<username>` is your GitHub username) to which you have commit rights.
        See [About forks](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-forks) in the GitHub documentation.

    * Clone your 'fork' onto your local machine.
    
        For example, `git clone git@github.com:<username>/vtr-verilog-to-routing.git`, where `<username>` is your GitHub username.

    b. **Internal Developers**

    * Clone the main VTR repository onto your local machine.
    
        For example, `git clone git@github.com:verilog-to-routing/vtr-verilog-to-routing.git`.

2. Move into the cloned repository.

    For example, `cd vtr-verilog-to-routing`.

3. Create a *branch*, based off of `master` to work on.

    For example, `git checkout -b my_awesome_branch master`, where `my_awesome_branch` is some helpful (and descriptive) name you give you're branch.
    *Please try to pick descriptive branch names!*

4. Make your changes to the VTR code base.

5. Test your changes to ensure they work as intended and have not broken other features.

    At the bare minimum it is recommended to run:
    ```
    make                                                #Rebuild the code
    ./run_reg_test.py vtr_reg_basic vtr_reg_strong      #Run tests
    ```

    See [Running Tests](#running-tests) for more details.

    Also note that additional [code formatting](#code-formatting) checks, and tests will be run when you open a Pull Request.

6. Commit your changes (i.e. `git add` followed by `git commit`).

    *Please try to use good commit messages!*

    See [Commit Messages and Structure](#commit-messages-and-structure) for details.

7. Push the changes to GitHub.

    For example, `git push origin my_awesome_branch`.

    a. **External Developers**

    Your code changes will now exist in your branch (e.g. `my_awesome_branch`) within your fork (e.g. `github.com/<username>/vtr-verilog-to-routing/tree/my_awesome_branch`, where `<username>` is your GitHub username)

    b. **Internal Developers**

    Your code changes will now exist in your branch (e.g. `my_awesome_branch`) within the main VTR repository (i.e. `github.com/verilog-to-routing/vtr-verilog-to-routing/tree/my_awesome_branch`)

8. Create a Pull Request (PR) to request your changes be merged into VTR.

    * Navigate to your branch on GitHub

        a. **External Developers**

        Navigate to your branch within your fork on GitHub (e.g. `https://github.com/<username/vtr-verilog-to-routing/tree/my_awesome_branch`, where `<username>` is your GitHub username, and `my_awesome_branch` is your branch name).

        b. **Internal Developers**

        Navigate to your branch on GitHub (e.g. `https://github.com/verilog-to-routing/vtr-verilog-to-routing/tree/my_awesome_branch`, where `my_awesome_branch` is your branch name).

    * Select the `New pull request` button.

        a. **External Developers**

        If prompted, select `verilog-to-routing/vtr-verilog-to-routing` as the base repository.

# Commit Messages and Structure

## Commit Messages

Commit messages are an important part of understanding the code base and its history.
It is therefore *extremely* important to provide the following information in the commit message:

* What is being changed?
* Why is this change occurring?

The diff of changes included with the commit provides the details of what is actually changed, so only a high-level description of what is being done is needed.
However a code diff provides *no* insight into **why** the change is being made, so this extremely helpful context can only be encoded in the commit message.

The preferred convention in VTR is to structure commit messages as follows:
```
Header line: explain the commit in one line (use the imperative)

More detailed explanatory text. Explain the problem that this commit
is solving. Focus on why you are making this change as opposed to how
(the code explains that). Are there side effects or other unintuitive
consequences of this change? Here's the place to explain them.

If necessary. Wrap lines at some reasonable point (e.g. 74 characters,
or so) In some contexts, the header line is treated as the subject
of the commit and the rest of the text as the body. The blank line
separating the summary from the body is critical (unless you omit
the body entirely); various tools like `log`, `shortlog` and `rebase`
can get confused if you run the two together.

Further paragraphs come after blank lines.

 - Bullet points are okay, too

 - Typically a hyphen or asterisk is used for the bullet, preceded
   by a single space, with blank lines in between, but conventions
   vary here

You can also put issue tracker references at the bottom like this:

Fixes: #123
See also: #456, #789
```
(based off of [here](https://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html), and [here](https://github.com/torvalds/subsurface-for-dirk/blob/master/README.md#contributing)).

Commit messages do not always need to be long, so use your judgement.
More complex or involved changes with wider ranging implications likely deserve longer commit messages than fixing a simple typo.

It is often helpful to phrase the first line of a commit as an imperative/command written as if to tell the repository what to do (e.g. `Update netlist data structure comments`, `Add tests for feature XYZ`, `Fix bug which ...`).

To provide quick context, some VTR developers also tag the first line with the main part of the code base effected, some common ones include:
* `vpr:` for the VPR place and route tool (`vpr/`)
* `flow:` VTR flow architectures, scripts, tests, ... (`vtr_flow/`)
* `archfpga:` for FPGA architecture library (`libs/libarchfpga`)
* `vtrutil:` for common VTR utilities (`libs/libvtrutil`)
* `doc:` Documentation (`doc/`, `*.md`, ...)
* `infra:` Infrastructure (CI, `.github/`, ...)


## Commit Structure
Generally, you should strive to keep commits atomic (i.e. they do one logical change to the code).
This often means keeping commits small and focused in what they change.
Of course, a large number of miniscule commits is also unhelpful (overwhelming and difficult to see the structure), and sometimes things can only be done in large changes -- so use your judgement.
A reasonable rule of thumb is to try and ensure VTR will still compile after each commit.

For those familiar with history re-writing features in git (e.g. rebase) you can sometimes use these to clean-up your commit history after the fact.
However these should only be done on private branches, and never directly on `master`.

# Code Formatting

Some parts of the VTR code base (e.g. VPR, libarchfpga, libvtrutil) have C/C++ code formatting requirements which are checked automatically by regression tests.
If your code changes are not compliant with the formatting, you can run:
```shell
make format
```
from the root of the VTR source tree.
This will automatically reformat your code to be compliant with formatting requirements (this requires the `clang-format` tool to be available on your system).

Python code must also be compliant with the formatting.  To format Python code, you can run:
```shell
make format-py
```
from the root of the VTR source tree (this requires the `black` tool to be available on your system).

## Large Scale Reformatting

For large scale reformatting (should only be performed by VTR maintainers) the script `dev/autoformat.py` can be used to reformat the C/C++ code and commit it as 'VTR Robot', which  keeps the revision history clearer and records metadata about reformatting commits (which allows `git hyper-blame` to skip such commits).  The `--python` option can be used for large scale formatting of Python code.

## Python Linting

Python files are automatically checked using `pylint` to ensure they follow established Python conventions.  You can run `pylint` on the entire repository by running `./dev/pylint_check.py`.  Certain files which were created before we adopted Python lint checking are grandfathered and are not checked.  To check *all* files, provide the `--check_grandfathered` argument.  You can also manually check individual files using `./dev/pylint_check.py <path_to_file1> <path_to_file2> ...`.

# Running Tests

VTR has a variety of tests which are used to check for correctness, performance and Quality of Result (QoR).

## Tests
There are 4 main regression testing suites:

### vtr_reg_basic

~1 minute serial

**Goal:** Fast functionality check

**Feature Coverage:** Low

**Benchmarks:** A few small and simple circuits

**Architectures:** A few simple architectures

This regression test is *not* suitable for evaluating QoR or performance.
Its primary purpose is to make sure the various tools do not crash/fail in the basic VTR flow.

QoR checks in this regression test are primarily 'canary' checks to catch gross degradations in QoR.
Occasionally, code changes can cause QoR failures (e.g. due to CAD noise -- particularly on small benchmarks); usually such failures are not a concern if the QoR differences are small.

### vtr_reg_strong

~20 minutes serial, ~15 minutes with `-j4`

**Goal:** Broad functionality check

**Feature Coverage:** High

**Benchmarks:** A few small circuits, with some special benchmarks to exercise specific features

**Architectures:** A variety of architectures, including special architectures to exercise specific features

This regression test is *not* suitable for evaluating QoR or performance.
Its primary purpose is try and achieve high functionality coverage.

QoR checks in this regression test are primarily 'canary' checks to catch gross degradations in QoR.
Occasionally, changes can cause QoR failures (e.g. due to CAD noise -- particularly on small benchmarks); usually such failures are not a concern if the QoR differences are small.
    
### vtr_reg_nightly_test1-N

**Goal:** Most QoR and Performance evaluation 

**Feature Coverage:** Medium

**Architectures:** A wider variety of architectures

**Benchmarks:** Small-large size, diverse. Includes: 

* VTR benchmarks
* Titan benchmarks except gaussian_blur (which has the longest run time)
* Koios benchmarks
* Various special benchmarks and tests for functionality 

   QoR checks in these regression suites are aimed at evaluating quality and run-time of the VTR flow.
   As a result any QoR failures are a concern and should be investigated and understood.
   
   Note:
   
   These suites comprise a single large suite, `vtr_reg_nightly` and should be run together to test nightly level regression. They are mostly similar in benchmark coverage interms of size and diversity however each suite tests some unique benchmarks in addition to the VTR benchmarks. Each vtr_reg_nightly<N> suite runs on a different server (in parallel), so by having N such test suites we speed up CI by a factor of N. Currently the runtime of each suite is capped at 6 hours, so if the runtime exceeds six hours a new vtr_reg_nightly suite (i.e. N+1) should be created.
    

### vtr_reg_weekly

~42 hours with `-j4`

**Goal:** Full QoR and Performance evaluation.

**Feature Coverage:** Medium

**Benchmarks:** Medium-Large size, diverse. Includes:

* VTR benchmarks
* Titan23 benchmarks, including gaussian_blur

**Architectures:** A wide variety of architectures

   QoR checks in this regression are aimed at evaluating quality and run-time of the VTR flow.
   As a result any QoR failures are a concern and should be investigated and understood.

These can be run with `run_reg_test.py`:
```shell
#From the VTR root directory
$ ./run_reg_test.py vtr_reg_basic
$ ./run_reg_test.py vtr_reg_strong
```

The *nightly* and *weekly* regressions require the Titan, ISPD, and Symbiflow benchmarks
which can be integrated into your VTR tree with:
```shell
$ make get_titan_benchmarks
$ make get_ispd_benchmarks
$ make get_symbiflow_benchmarks
```
They can then be run using `run_reg_test.py`:
```shell
$ ./run_reg_test.py vtr_reg_nightly_test1 
$ ./run_reg_test.py vtr_reg_nightly_test2 
$ ./run_reg_test.py vtr_reg_nightly_test3 
$ ./run_reg_test.py vtr_reg_weekly
```

To speed-up things up, individual sub-tests can be run in parallel using the `-j` option:
```shell
#Run up to 4 tests in parallel
$ ./run_reg_test.py vtr_reg_strong -j4
```

You can also run multiple regression tests together:
```shell
#Run both the basic and strong regression, with up to 4 tests in parallel
$ ./run_reg_test.py vtr_reg_basic vtr_reg_strong -j4
```
## Running in a large cluster using SLURM
For the very large runs, you can submit your runs on a large cluster. A template of submission script to 
a Slurm-managed cluster can be found under vtr_flow/tasks/slurm/

## Continuous integration (CI)
For the following tests, you can use remote servers instead of running them locally. Once the changes are pushed into the 
remote repository, or a PR is created, the [Test Workflow](https://github.com/verilog-to-routing/vtr-verilog-to-routing/blob/master/.github/workflows/test.yml)
will be triggered. Many tests are included in the workflow, including:
* [vtr_reg_nightly_test1-N](#vtr_reg_nightly_test1-N)
* [vtr_reg_strong](#vtr_reg_strong)
* [vtr_reg_basic](#vtr_reg_basic)
* odin_reg_strong
* parmys_reg_basic

instructions on how to gather QoR results of CI runs can be found [here](#example-extracting-qor-data-from-ci-runs).

#### Re-run CI Tests
In the case that you want to re-run the CI tests, due to certain issues such as infrastructure failure,
go to the "Action" tab and find your workflow under Test Workflow.
Select the test which you want to re-run. There is a re-run button on the top-right corner of the newly appeared window.
![Rerun CI Test](https://raw.githubusercontent.com/verilog-to-routing/vtr-verilog-to-routing/master/doc/src/dev/eval_qor/re_run_tests.png)

**Attention** If the previous run is not finished, you will not be able to re-run the CI tests. To circumvent this limitation,
there are two options:
1. Cancel the workflow. After a few minutes, you would be able to re-run the workflow
   ![Rerun CI Test](https://raw.githubusercontent.com/verilog-to-routing/vtr-verilog-to-routing/master/doc/src/dev/eval_qor/cancel_workflow.png)
2. Wait until the workflow finishes, then re-run the failed jobs



## Odin Functionality Tests

Odin has its own set of tests to verify the correctness of its synthesis results:

* `odin_reg_basic`: ~2 minutes serial
* `odin_reg_strong`: ~6 minutes serial

These can be run with:
```shell
#From the VTR root directory
$ ./run_reg_test.py odin_reg_basic
$ ./run_reg_test.py odin_reg_strong
```
and should be used when making changes to Odin.

## Unit Tests

VTR also has a limited set of unit tests, which can be run with:

```shell
#From the VTR root directory
$ make && make test
```

This will run `test_vtrutil`, `test_vpr`, `test_fasm`, and `test_archfpga`. Each test suite is added in their CMake
files.

### Running Individual Testers

To run one of the four testers listed above on its own, navigate to the appropriate folder:

| Test            | Directory                          |
|-----------------|------------------------------------|
| `test_archfpga` | `$VTR_ROOT/build/libs/libarchfpga` |
| `test_vtrutil`  | `$VTR_ROOT/build/libs/libvtrutil`  |
| `test_fasm`     | `$VTR_ROOT/build/utils/fasm`       |
| `test_vpr`      | `$VTR_ROOT/build/vpr`              |

To see tester options, run it with `-h`:

```shell
# Using test_vpr as an example
# From $VTR_ROOT/build/vpr
$ ./test_vpr -h
```

To see the names of each unit test, use `--list-tests`:

```shell
# From $VTR_ROOT/build/vpr
$ ./test_vpr --list-tests
```

The output should look similar to this:

```shell
All available test cases:
  test_route_flow
      [vpr_noc_bfs_routing]
  test_find_block_with_matching_name
      [vpr_clustered_netlist]
  connection_router
      [vpr]
  binary_heap
      [vpr]
  edge_groups_create_sets
      [vpr]
  read_interchange_models
      [vpr]
      
... # many more test cases

52 test cases
```

To run specific unit tests, pass them as arguments. For example:

```shell
# From $VTR_ROOT/build/vpr
$ ./test_vpr test_route_flow connection_router
```

# Evaluating Quality of Result (QoR) Changes
VTR uses highly tuned and optimized algorithms and data structures.
Changes which effect these can have significant impacts on the quality of VTR's design implementations (timing, area etc.) and VTR's run-time/memory usage.
Such changes need to be evaluated carefully before they are pushed/merged to ensure no quality degradation occurs.

If you are unsure of what level of QoR evaluation is necessary for your changes, please ask a VTR developer for guidance.

## General QoR Evaluation Principles
The goal of performing a QoR evaluation is to measure precisely the impact of a set of code/architecture/benchmark changes on both the quality of VTR's design implementation (i.e. the result of VTR's optimizations), and on tool run-time and memory usage.

This process is made more challenging by the fact that many of VTR's optimization algorithms are based on heuristics (some of which depend on randomization).
This means that VTR's implementation results are dependent upon:
 * The initial conditions (e.g. input architecture & netlist, random number generator seed), and
 * The precise optimization algorithms used.

The result is that a minor change to either of these can can make the measured QoR change.
This effect can be viewed as an intrinsic 'noise' or 'variance' to any QoR measurement for a particular architecture/benchmark/algorithm combination.

There are typically two key methods used to measure the 'true' QoR:

1. Averaging metrics across multiple architectures and benchmark circuits.

2. Averaging metrics multiple runs of the same architecture and benchmark, but using different random number generator seeds

    This is a further variance reduction technique, although it can be very CPU-time intensive.
    A typical example would be to sweep an entire benchmark set across 3 or 5 different seeds.

In practice any algorithm changes will likely cause improvements on some architecture/benchmark combinations, and degradations on others.
As a result we primarily focus on the *average* behaviour of a change to evaluate its impact.
However extreme outlier behaviour on particular circuits is also important, since it may indicate bugs or other unexpected behaviour.

### Key QoR Metrics

The following are key QoR metrics which should be used to evaluate the impact of changes in VTR.

Implementation Quality Metrics:

| Metric                          | Meaning                                                                      | Sensitivity |
|---------------------------------|------------------------------------------------------------------------------|-------------|
| num_pre_packed_blocks           | Number of primitive netlist blocks (after tech. mapping, before packing)     | Low         |
| num_post_packed_blocks          | Number of Clustered Blocks (after packing)                                   | Medium      |
| device_grid_tiles               | FPGA size in grid tiles                                                      | Low-Medium  |
| min_chan_width                  | The minimum routable channel width                                           | Medium\*    |
| crit_path_routed_wirelength     | The routed wirelength at the relaxed channel width                           | Medium      |
| NoC_agg_bandwidth\**            | The total link bandwidth utilized by all traffic flows                       | Low         |
| NoC_latency\**                  | The total time of traffic flow data transfer (summed over all traffic flows) | Low         |
| NoC_latency_constraints_cost\** | Total number of traffic flows that meet their latency constraints            | Low         |

\* By default, VPR attempts to find the minimum routable channel width; it then performs routing at a relaxed (e.g. 1.3x minimum) channel width. At minimum channel width routing congestion can distort the true timing/wirelength characteristics. Combined with the fact that most FPGA architectures are built with an abundance of routing, post-routing metrics are usually only evaluated at the relaxed channel width.

\** NoC-related metrics are only reported when --noc option is enabled.

Run-time/Memory Usage Metrics:

| Metric                      | Meaning                                                                        | Sensitivity |
|-----------------------------|--------------------------------------------------------------------------------|-------------|
| vtr_flow_elapsed_time       | Wall-clock time to complete the VTR flow                                       | Low         |
| pack_time                   | Wall-clock time VPR spent during packing                                       | Low         |
| place_time                  | Wall-clock time VPR spent during placement                                     | Low         |
| min_chan_width_route_time   | Wall-clock time VPR spent during routing at the minimum routable channel width | High\*      |
| crit_path_route_time        | Wall-clock time VPR spent during routing at the relaxed channel width          | Low         |
| max_vpr_mem                 | Maximum memory used by VPR (in kilobytes)                                      | Low         |

\*  Note that the minimum channel width route time is chaotic and can be highly variable (e.g. 10x variation is not unusual). Minimum channel width routing performs a binary search to find the minimum channel width. Since route time is highly dependent on congestion, run-time is highly dependent on the precise channel widths searched (which may change due to perturbations).

In practice you will likely want to consider additional and more detailed metrics, particularly those directly related to the changes you are making.
For example, if your change related to hold-time optimization you would want to include hold-time related metrics such as `hold_TNS` (hold total negative slack) and `hold_WNS` (hold worst negative slack).
If your change related to packing, you would want to report additional packing-related metrics, such as the number of clusters formed by each block type (e.g. numbers of CLBs, RAMs, DSPs, IOs).

### Benchmark Selection

An important factor in performing any QoR evaluation is the benchmark set selected.
In order to draw reasonably general conclusions about the impact of a change we desire two characteristics of the benchmark set:

1. It includes a large number of benchmarks which are representative of the application domains of interest.

    This ensures we don't over-tune to a specific benchmark or application domain.

2. It should include benchmarks of large sizes.

    This ensures we can optimize and scale to large problem spaces.

In practice (for various reasons) satisfying both of these goals simultaneously is challenging.
The key goal here is to ensure the benchmark set is not unreasonably biased in some manner (e.g. benchmarks which are too small, benchmarks too skewed to a particular application domain).

### Fairly measuring tool run-time
Accurately and fairly measuring the run-time of computer programs is challenging in practice.
A variety of factors effect run-time including:

* Operating System
* System load (e.g. other programs running)
* Variance in hardware performance (e.g. different CPUs on different machines, CPU frequency scaling)

To make reasonably 'fair' run-time comparisons it is important to isolate the change as much as possible from other factors.
This involves keeping as much of the experimental environment identical as possible including:

1. Target benchmarks
2. Target architecture
3. Code base (e.g. VTR revision)
4. CAD parameters
5. Computer system (e.g. CPU model, CPU frequency/power scaling, OS version)
6. Compiler version

## Collecting QoR Measurements
The first step is to collect QoR metrics on your selected benchmark set.

You need at least two sets of QoR measurements:
1. The baseline QoR (i.e. unmodified VTR).
2. The modified QoR (i.e. VTR with your changes).

The following tests can be run locally by running the given commands on the local machine. In addition, since CI tests are run whenever
changes are pushed to the remote repository, one can use the CI test results to measure the impact
of his/her changes. The instructions to gather CI tests' results are [here](./README.developers.md#Example:-CI-Tests-QoR-Measurement).

Note that it is important to generate both sets of QoR measurements on the same computing infrastructure to ensure a fair run-time comparison.

The following examples show how a single set of QoR measurements can be produced using the VTR flow infrastructure.

### Example: VTR Benchmarks QoR Measurement

The VTR benchmarks are a group of benchmark circuits distributed with the VTR project.
The are provided as synthesizable verilog and can be re-mapped to VTR supported architectures.
They consist mostly of small to medium sized circuits from a mix of application domains.
They are used primarily to evaluate the VTR's optimization quality in an architecture exploration/evaluation setting (e.g. determining minimum channel widths).

A typical approach to evaluating an algorithm change would be to run `vtr_reg_qor_chain` task from the nightly regression test:

```shell
#From the VTR root
$ cd vtr_flow/tasks

#Run the VTR benchmarks
$ ../scripts/run_vtr_task.py regression_tests/vtr_reg_nightly_test3/vtr_reg_qor_chain

#Several hours later... they complete

#Parse the results
$ ../scripts/python_libs/vtr/parse_vtr_task.py regression_tests/vtr_reg_nightly_test3/vtr_reg_qor_chain

#The run directory should now contain a summary parse_results.txt file
$ head -5 vtr_reg_nightly_test3/vtr_reg_qor_chain/latest/parse_results.txt
arch                                  	circuit           	script_params	vpr_revision 	vpr_status	error	num_pre_packed_nets	num_pre_packed_blocks	num_post_packed_nets	num_post_packed_blocks	device_width	device_height	num_clb	num_io	num_outputs	num_memoriesnum_mult	placed_wirelength_est	placed_CPD_est	placed_setup_TNS_est	placed_setup_WNS_est	min_chan_width	routed_wirelength	min_chan_width_route_success_iteration	crit_path_routed_wirelength	crit_path_route_success_iteration	critical_path_delay	setup_TNS	setup_WNS	hold_TNS	hold_WNS	logic_block_area_total	logic_block_area_used	min_chan_width_routing_area_total	min_chan_width_routing_area_per_tile	crit_path_routing_area_total	crit_path_routing_area_per_tile	odin_synth_time	abc_synth_time	abc_cec_time	abc_sec_time	ace_time	pack_time	place_time	min_chan_width_route_time	crit_path_route_time	vtr_flow_elapsed_time	max_vpr_mem	max_odin_mem	max_abc_mem
k6_frac_N10_frac_chain_mem32K_40nm.xml	bgm.v             	common       	9f591f6-dirty	success   	     	26431              	24575                	14738               	2258                  	53          	53           	1958   	257   	32         	0           11      	871090               	18.5121       	-13652.6            	-18.5121            	84            	328781           	32                                    	297718                     	18                               	20.4406            	-15027.8 	-20.4406 	0       	0       	1.70873e+08           	1.09883e+08          	1.63166e+07                      	5595.54                             	2.07456e+07                 	7114.41                        	11.16          	1.03          	-1          	-1          	-1      	141.53   	108.26    	142.42                   	15.63               	652.17               	1329712    	528868      	146796
k6_frac_N10_frac_chain_mem32K_40nm.xml	blob_merge.v      	common       	9f591f6-dirty	success   	     	14163              	11407                	3445                	700                   	30          	30           	564    	36    	100        	0           0       	113369               	13.4111       	-2338.12            	-13.4111            	64            	80075            	18                                    	75615                      	23                               	15.3479            	-2659.17 	-15.3479 	0       	0       	4.8774e+07            	3.03962e+07          	3.87092e+06                      	4301.02                             	4.83441e+06                 	5371.56                        	0.46           	0.17          	-1          	-1          	-1      	67.89    	11.30     	47.60                    	3.48                	198.58               	307756     	48148       	58104
k6_frac_N10_frac_chain_mem32K_40nm.xml	boundtop.v        	common       	9f591f6-dirty	success   	     	1071               	1141                 	595                 	389                   	13          	13           	55     	142   	192        	0           0       	5360                 	3.2524        	-466.039            	-3.2524             	34            	4534             	15                                    	3767                       	12                               	3.96224            	-559.389 	-3.96224 	0       	0       	6.63067e+06           	2.96417e+06          	353000.                          	2088.76                             	434699.                     	2572.18                        	0.29           	0.11          	-1          	-1          	-1      	2.55     	0.82      	2.10                     	0.15                	7.24                 	87552      	38484       	37384
k6_frac_N10_frac_chain_mem32K_40nm.xml	ch_intrinsics.v   	common       	9f591f6-dirty	success   	     	363                	493                  	270                 	247                   	10          	10           	17     	99    	130        	1           0       	1792                 	1.86527       	-194.602            	-1.86527            	46            	1562             	13                                    	1438                       	20                               	2.4542             	-226.033 	-2.4542  	0       	0       	3.92691e+06           	1.4642e+06           	259806.                          	2598.06                             	333135.                     	3331.35                        	0.03           	0.01          	-1          	-1          	-1      	0.46     	0.31      	0.94                     	0.09                	2.59                 	62684      	8672        	32940
```

### Example: Titan Benchmarks QoR Measurement

The [Titan benchmarks](https://docs.verilogtorouting.org/en/latest/vtr/benchmarks/#titan-benchmarks) are a group of large benchmark circuits from a wide range of applications, which are compatible with the VTR project.
The are typically used as post-technology mapped netlists which have been pre-synthesized with Quartus.
They are substantially larger and more realistic than the VTR benchmarks, but can only target specifically compatible architectures.
They are used primarily to evaluate the optimization quality and scalability of VTR's CAD algorithms while targeting a fixed architecture (e.g. at a fixed channel width).

A typical approach to evaluating an algorithm change would be to run `titan_quick_qor` task from the nightly regression test:
#### [Running and Integrating the Titan Benchmarks with VTR](https://docs.verilogtorouting.org/en/latest/tutorials/titan_benchmarks/)
```shell
#From the VTR root

#Download and integrate the Titan benchmarks into the VTR source tree
$ make get_titan_benchmarks

#Move to the task directory
$ cd vtr_flow/tasks

#Run the Titan benchmarks
$ ../scripts/run_vtr_task.py regression_tests/vtr_reg_nightly_test2/titan_quick_qor

#Several days later... they complete

#Parse the results
$ ../scripts/python_libs/vtr/parse_vtr_task.py regression_tests/vtr_reg_nightly_test2/titan_quick_qor

#The run directory should now contain a summary parse_results.txt file
$ head -5 vtr_reg_nightly_test2/titan_quick_qor/latest/parse_results.txt
arch                     	circuit                                 	vpr_revision	vpr_status	error	num_pre_packed_nets	num_pre_packed_blocks	num_post_packed_nets	num_post_packed_blocks	device_width	device_height	num_clb	num_io	num_outputs	num_memoriesnum_mult	placed_wirelength_est	placed_CPD_est	placed_setup_TNS_est	placed_setup_WNS_est	routed_wirelength	crit_path_route_success_iteration	logic_block_area_total	logic_block_area_used	routing_area_total	routing_area_per_tile	critical_path_delay	setup_TNS   setup_WNS	hold_TNS	hold_WNS	pack_time	place_time	crit_path_route_time	max_vpr_mem	max_odin_mem	max_abc_mem
stratixiv_arch.timing.xml	neuron_stratixiv_arch_timing.blif       	0208312     	success   	     	119888             	86875                	51408               	3370                  	128         	95           	-1     	42    	35         	-1          -1      	3985635              	8.70971       	-234032             	-8.70971            	1086419          	20                               	0                     	0                    	2.66512e+08       	21917.1              	9.64877            	-262034     -9.64877 	0       	0       	127.92   	218.48    	259.96              	5133800    	-1          	-1
stratixiv_arch.timing.xml	sparcT1_core_stratixiv_arch_timing.blif 	0208312     	success   	     	92813              	91974                	54564               	4170                  	77          	57           	-1     	173   	137        	-1          -1      	3213593              	7.87734       	-534295             	-7.87734            	1527941          	43                               	0                     	0                    	9.64428e+07       	21973.8              	9.06977            	-625483     -9.06977 	0       	0       	327.38   	338.65    	364.46              	3690032    	-1          	-1
stratixiv_arch.timing.xml	stereo_vision_stratixiv_arch_timing.blif	0208312     	success   	     	127088             	94088                	62912               	3776                  	128         	95           	-1     	326   	681        	-1          -1      	4875541              	8.77339       	-166097             	-8.77339            	998408           	16                               	0                     	0                    	2.66512e+08       	21917.1              	9.36528            	-187552     -9.36528 	0       	0       	110.03   	214.16    	189.83              	5048580    	-1          	-1
stratixiv_arch.timing.xml	cholesky_mc_stratixiv_arch_timing.blif  	0208312     	success   	     	140214             	108592               	67410               	5444                  	121         	90           	-1     	111   	151        	-1          -1      	5221059              	8.16972       	-454610             	-8.16972            	1518597          	15                               	0                     	0                    	2.38657e+08       	21915.3              	9.34704            	-531231     -9.34704 	0       	0       	211.12   	364.32    	490.24              	6356252    	-1          	-1
```

### Example: NoC Benchmarks QoR Measurements
NoC benchmarks currently include synthetic and MLP benchmarks. Synthetic benchmarks have various NoC traffic patters,
bandwidth utilization, and latency requirements. High-quality NoC router placement solutions for these benchmarks are
known. By comparing the known solutions with NoC router placement results, the developer can evaluate the sanity of 
the NoC router placement algorithm. MLP benchmarks are the only realistic netlists included in this benchmark set.

Based on the number of NoC routers in a synthetic benchmark, it is run on one of two different architectures. All MLP
benchmarks are run on an FPGA architecture with 16 NoC routers. Post-technology mapped netlists (blif files)
for synthetic benchmarks are added to the VTR project. However, MLP blif files are very large and should be downloaded
separately.

Since NoC benchmarks target different FPGA architectures, they are run as different circuits. A typical way to run all
NoC benchmarks is to run a task list and gather QoR data form different tasks:

#### Running and Integrating the NoC Benchmarks with VTR
```shell
#From the VTR root

#Download and integrate NoC MLP benchmarks into the VTR source tree
$ make get_noc_mlp_benchmarks

#Move to the task directory
$ cd vtr_flow

#Run the VTR benchmarks
$ scripts/run_vtr_task.py -l tasks/noc_qor/task_list.txt

#Several days later... they complete

#NoC benchmarks are run as several different tasks. Therefore, QoR results should be gathered from multiple directories,
#one for each task.
$ head -5 tasks/noc_qor/large_complex_synthetic/latest/parse_results.txt
$ head -5 tasks/noc_qor/large_simple_synthetic/latest/parse_results.txt
$ head -5 tasks/noc_qor/small_complex_synthetic/latest/parse_results.txt
$ head -5 tasks/noc_qor/small_simple_synthetic/latest/parse_results.txt
$ head -5 tasks/noc_qor/MLP/latest/parse_results.txt
```

### Example: Koios Benchmarks QoR Measurement

The [Koios benchmarks](https://github.com/verilog-to-routing/vtr-verilog-to-routing/tree/master/vtr_flow/benchmarks/verilog/koios) are a group of Deep Learning benchmark circuits distributed with the VTR project.
The are provided as synthesizable verilog and can be re-mapped to VTR supported architectures. They consist mostly of medium to large sized circuits from Deep Learning (DL).
They can be used for FPGA architecture exploration for DL and also for tuning CAD tools.

A typical approach to evaluating an algorithm change would be to run `koios_medium` (or `koios_medium_no_hb`) tasks from the nightly regression test (vtr_reg_nightly_test4), the `koios_large` (or `koios_large_no_hb`) and the `koios_proxy` (or `koios_proxy_no_hb`) tasks from the weekly regression test (vtr_reg_weekly). The nightly test contains smaller benchmarks, whereas the large designs are in the weekly regression test. To measure QoR for the entire benchmark suite, both nightly and weekly tests should be run and the results should be concatenated.

As 3 of the `koios_large` circuits require special settings due to having long DSP chains, they are split in separate tasks as follows:
  * `bwave_like.float.large.v` and `bwave_like.fixed.large.v` are in `vtr_reg_weekly/koios_bwave_large` task
  * `dla_like.large.v` is in `vtr_reg_weekly/koios_dla_large` task

For evaluating an algorithm change in the Odin frontend, run `koios_medium` (or `koios_medium_no_hb`) tasks from the nightly regression test (vtr_reg_nightly_test4_odin) and the `koios_large_odin` (or `koios_large_no_hb_odin`) tasks from the weekly regression test (vtr_reg_weekly).

The `koios_medium`, `koios_large`, and `koios_proxy` regression tasks run these benchmarks with complex_dsp functionality enabled, whereas `koios_medium_no_hb`, `koios_large_no_hb` and `koios_proxy_no_hb` regression tasks run these benchmarks without complex_dsp functionality. Normally, only the `koios_medium`, `koios_large`, and `koios_proxy` tasks should be enough for QoR.

The `koios_sv` and `koios_sv_no_hb` tasks utilize the System-Verilog parser in the Parmys frontend.

The following table provides details on available Koios settings in VTR flow:
| Suite         |Test Description      | Target | Complex DSP Features   | Config file   | Frontend   | Parser   |
|---------------|----------------------|---------------|---------------|---------------|---------------|---------------|
| Nightly       | Medium designs     | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_nightly_test4/koios_medium | Parmys | |
| Nightly       | Medium designs     | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml |          | vtr_reg_nightly_test4/koios_medium_no_hb | Parmys | |
| Nightly       | Medium designs     | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_nightly_test4_odin/koios_medium | Odin | |
| Nightly       | Medium designs     | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml |          | vtr_reg_nightly_test4_odin/koios_medium_no_hb | Odin | |
| Weekly        | Large designs      | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_weekly/koios_large | Parmys | |
| Weekly        | Large designs      | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_weekly/koios_dla_large | Parmys | |
| Weekly        | Large designs      | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_weekly/koios_bwave_large | Parmys | |
| Weekly        | Large designs      | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml |          | vtr_reg_weekly/koios_large_no_hb | Parmys | |
| Weekly        | Large designs      | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_weekly/koios_large_odin | Odin | |
| Weekly        | Large designs      | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml |          | vtr_reg_weekly/koios_large_no_hb_odin | Odin | |
| Weekly        | Proxy designs      | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_weekly/koios_proxy | Parmys | |
| Weekly        | Proxy designs      | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml |          | vtr_reg_weekly/koios_proxy_no_hb | Parmys | |
| Weekly        | deepfreeze designs | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_weekly/koios_sv | Parmys | System-Verilog |
| Weekly        | deepfreeze designs | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml |          | vtr_reg_weekly/koios_sv_no_hb | Parmys | System-Verilog |

For more information refer to the [Koios benchmark home page](vtr_flow/benchmarks/verilog/koios/README.md).

To make running all the koios benchmarks easier, especially with thos circuits scattered between different tasks, there is an overall task list that runs all the 40 circuits of Koios as follows (this will run all the circuits with complex DSP functionality enabled. If you want to disable the complex DSP, edit the file to point to the `koios_*_no_hb` tasks):

```shell
$ ../scripts/run_vtr_task.py -l koios_task_list.txt 

#Several hours later... they complete
#

If you want to run a subset of the koios benchmarks or run them without hard DSP blocks, you can run lower-level 'koios' tasks as follows:

```shell
#From the VTR root
$ cd vtr_flow/tasks

#Choose any config file from the table above and run the Koios benchmarks, for example:
$ ../scripts/run_vtr_task.py regression_tests/vtr_reg_nightly_test4/koios_medium &
$ ../scripts/run_vtr_task.py regression_tests/vtr_reg_weekly/koios_large &
$ ../scripts/run_vtr_task.py regression_tests/vtr_reg_weekly/koios_proxy &
$ ../scripts/run_vtr_task.py regression_tests/vtr_reg_weekly/koios_sv &

#Disable hard blocks (hard_mem and complex_dsp macros) to verify memory and generic hard blocks inference:
$ ../scripts/run_vtr_task.py regression_tests/vtr_reg_nightly_test4/koios_medium_no_hb &
$ ../scripts/run_vtr_task.py regression_tests/vtr_reg_weekly/koios_large_no_hb &
$ ../scripts/run_vtr_task.py regression_tests/vtr_reg_weekly/koios_proxy_no_hb &
$ ../scripts/run_vtr_task.py regression_tests/vtr_reg_weekly/koios_sv_no_hb &

#Several hours later... they complete

#The run directory should now contain a summary parse_results.txt file
$ head -5 vtr_reg_nightly_test4/koios_medium/<latest_run_dir>/parse_results.txt
arch	  circuit	  script_params	  vtr_flow_elapsed_time	  vtr_max_mem_stage	  vtr_max_mem	  error	  odin_synth_time	  max_odin_mem	  parmys_synth_time	  max_parmys_mem	  abc_depth	  abc_synth_time	  abc_cec_time	  abc_sec_time	  max_abc_mem	  ace_time	  max_ace_mem	  num_clb	  num_io	  num_memories	  num_mult	  vpr_status	  vpr_revision	  vpr_build_info	  vpr_compiler	  vpr_compiled	  hostname	  rundir	  max_vpr_mem	  num_primary_inputs	  num_primary_outputs	  num_pre_packed_nets	  num_pre_packed_blocks	  num_netlist_clocks	  num_post_packed_nets	  num_post_packed_blocks	  device_width	  device_height	  device_grid_tiles	  device_limiting_resources	  device_name	  pack_mem	  pack_time	  placed_wirelength_est	  place_mem	  place_time	  place_quench_time	  placed_CPD_est	  placed_setup_TNS_est	  placed_setup_WNS_est	  placed_geomean_nonvirtual_intradomain_critical_path_delay_est	  place_delay_matrix_lookup_time	  place_quench_timing_analysis_time	  place_quench_sta_time	  place_total_timing_analysis_time	  place_total_sta_time	  min_chan_width	  routed_wirelength	  min_chan_width_route_success_iteration	  logic_block_area_total	  logic_block_area_used	  min_chan_width_routing_area_total	  min_chan_width_routing_area_per_tile	  min_chan_width_route_time	  min_chan_width_total_timing_analysis_time	  min_chan_width_total_sta_time	  crit_path_routed_wirelength	  crit_path_route_success_iteration	  crit_path_total_nets_routed	  crit_path_total_connections_routed	  crit_path_total_heap_pushes	  crit_path_total_heap_pops	  critical_path_delay	  geomean_nonvirtual_intradomain_critical_path_delay	  setup_TNS	  setup_WNS	  hold_TNS	  hold_WNS	  crit_path_routing_area_total	  crit_path_routing_area_per_tile	  router_lookahead_computation_time	  crit_path_route_time	  crit_path_total_timing_analysis_time	  crit_path_total_sta_time	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  tpu_like.small.os.v	  common	  677.72	  vpr	  2.29 GiB	  	  -1	  -1	  19.40	  195276	  5	  99.61	  -1	  -1	  109760	  -1	  -1	  492	  355	  32	  -1	  success	  327aa1d-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.10.35-v8 x86_64	  2023-02-09T16:01:10	  gh-actions-runner-vtr-auto-spawned87	  /root/vtr-verilog-to-routing/vtr-verilog-to-routing	  2400616	  355	  289	  25429	  18444	  2	  12313	  1433	  136	  136	  18496	  dsp_top	  auto	  208.3 MiB	  14.61	  359754	  2344.4 MiB	  16.75	  0.18	  5.12303	  -82671.4	  -5.12303	  2.1842	  6.09	  0.0412666	  0.0368158	  6.35102	  5.65512	  -1	  394367	  16	  5.92627e+08	  8.53857e+07	  4.08527e+08	  22087.3	  4.50	  8.69097	  7.85207	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  tpu_like.small.ws.v	  common	  722.22	  vpr	  2.30 GiB	  	  -1	  -1	  23.09	  242848	  5	  72.60	  -1	  -1	  117236	  -1	  -1	  686	  357	  58	  -1	  success	  327aa1d-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.10.35-v8 x86_64	  2023-02-09T16:01:10	  gh-actions-runner-vtr-auto-spawned87	  /root/vtr-verilog-to-routing/vtr-verilog-to-routing	  2415672	  357	  289	  25686	  20353	  2	  12799	  1656	  136	  136	  18496	  dsp_top	  auto	  233.3 MiB	  98.40	  226648	  2359.1 MiB	  20.07	  0.17	  8.31923	  -74283.8	  -8.31923	  2.78336	  6.05	  0.0420585	  0.0356747	  6.53862	  5.54952	  -1	  293644	  13	  5.92627e+08	  9.4632e+07	  4.08527e+08	  22087.3	  4.58	  8.69976	  7.55132	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  dla_like.small.v	  common	  2800.18	  vpr	  1.75 GiB	  	  -1	  -1	  94.38	  736748	  6	  754.09	  -1	  -1	  389988	  -1	  -1	  3895	  206	  132	  -1	  success	  327aa1d-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.10.35-v8 x86_64	  2023-02-09T16:01:10	  gh-actions-runner-vtr-auto-spawned87	  /root/vtr-verilog-to-routing/vtr-verilog-to-routing	  1840088	  206	  13	  165036	  139551	  1	  69732	  4358	  88	  88	  7744	  dsp_top	  auto	  1052.4 MiB	  1692.76	  601396	  1606.1 MiB	  88.48	  0.64	  5.30279	  -150931	  -5.30279	  5.30279	  1.96	  0.131322	  0.104184	  16.7561	  13.7761	  -1	  876475	  15	  2.4541e+08	  1.55281e+08	  1.69370e+08	  21871.2	  14.42	  24.7943	  21.0377	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  bnn.v	  common	  797.74	  vpr	  2.01 GiB	  	  -1	  -1	  84.28	  729308	  3	  56.57	  -1	  -1	  411036	  -1	  -1	  6190	  260	  0	  -1	  success	  327aa1d-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.10.35-v8 x86_64	  2023-02-09T16:01:10	  gh-actions-runner-vtr-auto-spawned87	  /root/vtr-verilog-to-routing/vtr-verilog-to-routing	  2106860	  260	  122	  206251	  154342	  1	  87361	  6635	  87	  87	  7569	  clb	  auto	  1300.8 MiB	  202.79	  910701	  1723.3 MiB	  174.17	  1.12	  6.77966	  -140235	  -6.77966	  6.77966	  1.97	  0.198989	  0.175034	  29.926	  24.7241	  -1	  1199797	  17	  2.37162e+08	  1.88714e+08	  1.65965e+08	  21927.0	  20.72	  41.872	  35.326	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 

$ head -5 vtr_reg_weekly/koios_large/<latest_run_dir>/parse_results.txt
arch	  circuit	  script_params	  vtr_flow_elapsed_time	  vtr_max_mem_stage	  vtr_max_mem	  error	  odin_synth_time	  max_odin_mem	  parmys_synth_time	  max_parmys_mem	  abc_depth	  abc_synth_time	  abc_cec_time	  abc_sec_time	  max_abc_mem	  ace_time	  max_ace_mem	  num_clb	  num_io	  num_memories	  num_mult	  vpr_status	  vpr_revision	  vpr_build_info	  vpr_compiler	  vpr_compiled	  hostname	  rundir	  max_vpr_mem	  num_primary_inputs	  num_primary_outputs	  num_pre_packed_nets	  num_pre_packed_blocks	  num_netlist_clocks	  num_post_packed_nets	  num_post_packed_blocks	  device_width	  device_height	  device_grid_tiles	  device_limiting_resources	  device_name	  pack_mem	  pack_time	  placed_wirelength_est	  total_swap	  accepted_swap	  rejected_swap	  aborted_swap	  place_mem	  place_time	  place_quench_time	  placed_CPD_est	  placed_setup_TNS_est	  placed_setup_WNS_est	  placed_geomean_nonvirtual_intradomain_critical_path_delay_est	  place_delay_matrix_lookup_time	  place_quench_timing_analysis_time	  place_quench_sta_time	  place_total_timing_analysis_time	  place_total_sta_time	  min_chan_width	  routed_wirelength	  min_chan_width_route_success_iteration	  logic_block_area_total	  logic_block_area_used	  min_chan_width_routing_area_total	  min_chan_width_routing_area_per_tile	  min_chan_width_route_time	  min_chan_width_total_timing_analysis_time	  min_chan_width_total_sta_time	  crit_path_num_rr_graph_nodes	  crit_path_num_rr_graph_edges	  crit_path_collapsed_nodes	  crit_path_routed_wirelength	  crit_path_route_success_iteration	  crit_path_total_nets_routed	  crit_path_total_connections_routed	  crit_path_total_heap_pushes	  crit_path_total_heap_pops	  critical_path_delay	  geomean_nonvirtual_intradomain_critical_path_delay	  setup_TNS	  setup_WNS	  hold_TNS	  hold_WNS	  crit_path_routing_area_total	  crit_path_routing_area_per_tile	  router_lookahead_computation_time	  crit_path_route_time	  crit_path_create_rr_graph_time	  crit_path_create_intra_cluster_rr_graph_time	  crit_path_tile_lookahead_computation_time	  crit_path_router_lookahead_computation_time	  crit_path_total_timing_analysis_time	  crit_path_total_sta_time	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  lenet.v	  common	  6320.39	  parmys	  6.81 GiB	  	  -1	  -1	  2279.37	  7141128	  8	  3659.89	  -1	  -1	  229600	  -1	  -1	  1215	  3	  0	  -1	  success	  9c0df2e-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	  2023-12-03T14:49:57	  mustang	  /homes/vtr-verilog-to-routing	  406996	  3	  73	  29130	  23346	  1	  13644	  1292	  40	  40	  1600	  clb	  auto	  246.6 MiB	  64.06	  136280	  627318	  185500	  408250	  33568	  357.7 MiB	  81.14	  0.66	  8.27929	  -16089.3	  -8.27929	  8.27929	  1.10	  0.16804	  0.146992	  16.9432	  13.6451	  -1	  224227	  19	  4.87982e+07	  3.41577e+07	  3.42310e+07	  21394.3	  19.75	  26.6756	  21.8374	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  clstm_like.small.v	  common	  11605.17	  vpr	  3.24 GiB	  	  -1	  -1	  669.16	  1080564	  4	  7868.39	  -1	  -1	  606244	  -1	  -1	  7733	  652	  237	  -1	  success	  9c0df2e-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	  2023-12-03T14:49:57	  mustang	  /homes/vtr-verilog-to-routing	  3400468	  652	  290	  299247	  274102	  1	  72966	  9121	  120	  120	  14400	  dsp_top	  auto	  1946.1 MiB	  741.62	  1061263	  13535473	  5677109	  7516142	  342222	  3001.0 MiB	  915.91	  6.25	  6.0577	  -397722	  -6.0577	  6.0577	  16.74	  1.09797	  0.908781	  169.318	  135.356	  -1	  1289121	  17	  4.60155e+08	  3.01448e+08	  3.17281e+08	  22033.4	  108.23	  234.326	  190.185	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  clstm_like.medium.v	  common	  42560.88	  vpr	  6.35 GiB	  	  -1	  -1	  1060.82	  2104648	  4	  35779.24	  -1	  -1	  1168924	  -1	  -1	  15289	  652	  458	  -1	  success	  9c0df2e-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	  2023-12-03T14:49:57	  mustang	  /homes/vtr-verilog-to-routing	  6658128	  652	  578	  587833	  538751	  1	  142046	  17388	  168	  168	  28224	  dsp_top	  auto	  3792.2 MiB	  1334.50	  2402446	  32440572	  13681743	  17973716	  785113	  5856.8 MiB	  1927.66	  10.89	  6.9964	  -921673	  -6.9964	  6.9964	  34.97	  2.51671	  1.97649	  373.17	  302.896	  -1	  2735850	  16	  9.07771e+08	  5.93977e+08	  6.21411e+08	  22017.1	  228.75	  493.742	  407.089	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  clstm_like.large.v	  common	  79534.09	  vpr	  9.24 GiB	  	  -1	  -1	  1581.99	  3213072	  4	  69583.96	  -1	  -1	  1763048	  -1	  -1	  22846	  652	  679	  -1	  success	  9c0df2e-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	  2023-12-03T14:49:57	  mustang	  /homes/vtr-verilog-to-routing	  9688232	  652	  866	  876458	  803425	  1	  211260	  25656	  200	  200	  40000	  dsp_top	  auto	  5580.4 MiB	  2073.77	  4237568	  55245338	  23267923	  30805131	  1172284	  8437.3 MiB	  2868.84	  15.36	  8.07111	  -1.60215e+06	  -8.07111	  8.07111	  54.87	  2.67554	  2.06921	  438.894	  351.141	  -1	  4656710	  14	  1.28987e+09	  8.86534e+08	  8.79343e+08	  21983.6	  469.61	  576.631	  470.505	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 

$ head -5 vtr_reg_weekly/koios_proxy/<latest_run_dir>/parse_results.txt
arch	circuit	script_params	vtr_flow_elapsed_time	vtr_max_mem_stage	vtr_max_mem	error	odin_synth_time	max_odin_mem	parmys_synth_time	max_parmys_mem	abc_depth	abc_synth_time	abc_cec_time	abc_sec_time	max_abc_mem	ace_time	max_ace_mem	num_clb	num_io	num_memories	num_mult	vpr_status	vpr_revision	vpr_build_info	vpr_compiler	vpr_compiled	hostname	rundir	max_vpr_mem	num_primary_inputs	num_primary_outputs	num_pre_packed_nets	num_pre_packed_blocks	num_netlist_clocks	num_post_packed_nets	num_post_packed_blocks	device_width	device_height	device_grid_tiles	device_limiting_resources	device_name	pack_mem	pack_time	placed_wirelength_est	total_swap	accepted_swap	rejected_swap	aborted_swap	place_mem	place_time	place_quench_time	placed_CPD_est	placed_setup_TNS_est	placed_setup_WNS_est	placed_geomean_nonvirtual_intradomain_critical_path_delay_est	place_delay_matrix_lookup_time	place_quench_timing_analysis_time	place_quench_sta_time	place_total_timing_analysis_time	place_total_sta_time	min_chan_width	routed_wirelength	min_chan_width_route_success_iteration	logic_block_area_total	logic_block_area_used	min_chan_width_routing_area_total	min_chan_width_routing_area_per_tile	min_chan_width_route_time	min_chan_width_total_timing_analysis_time	min_chan_width_total_sta_time	crit_path_num_rr_graph_nodes	crit_path_num_rr_graph_edges	crit_path_collapsed_nodes	crit_path_routed_wirelength	crit_path_route_success_iteration	crit_path_total_nets_routed	crit_path_total_connections_routed	crit_path_total_heap_pushes	crit_path_total_heap_pops	critical_path_delay	geomean_nonvirtual_intradomain_critical_path_delay	setup_TNS	setup_WNS	hold_TNS	hold_WNS	crit_path_routing_area_total	crit_path_routing_area_per_tile	router_lookahead_computation_time	crit_path_route_time	crit_path_create_rr_graph_time	crit_path_create_intra_cluster_rr_graph_time	crit_path_tile_lookahead_computation_time	crit_path_router_lookahead_computation_time	crit_path_total_timing_analysis_time	crit_path_total_sta_time	
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	proxy.1.v	common	30535.22	vpr	9.48 GiB		-1	-1	1652.38	3799616	7	2393.26	-1	-1	771680	-1	-1	5817	938	845	-1	success	909f29c-dirty	release IPO VTR_ASSERT_LEVEL=2	GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	2023-12-08T17:55:38	mustang	/homes/vtr-verilog-to-routing	9940848	938	175	262404	208705	1	137273	8816	264	264	69696	dsp_top	auto	1962.1 MiB	17465.99	3242084	14209964	6064078	7558347	587539	9707.9 MiB	2269.49	11.20	8.49902	-576590	-8.49902	8.49902	120.99	1.65144	1.34401	319.238	263.953	-1	4269357	15	2.25492e+09	5.42827e+08	1.53035e+09	21957.6	291.49	414.451	348.422	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	proxy.2.v	common	49383.26	parmys	7.46 GiB		-1	-1	6711.91	7820216	8	22879.15	-1	-1	1478720	-1	-1	8948	318	1105	-1	success	909f29c-dirty	release IPO VTR_ASSERT_LEVEL=2	GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	2023-12-08T17:55:38	mustang	/homes/vtr-verilog-to-routing	6046424	318	256	373725	328044	1	148054	10957	188	188	35344	memory	auto	2466.3 MiB	15021.62	2653372	16311253	6713874	9344147	253232	5904.7 MiB	1439.25	8.76	7.35195	-768561	-7.35195	7.35195	47.97	1.45054	1.22978	225.237	181.257	-1	3431386	18	1.1352e+09	4.85551e+08	7.77871e+08	22008.6	262.44	314.625	258.401	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	proxy.3.v	common	19852.09	vpr	4.44 GiB		-1	-1	2415.20	2344724	9	11508.95	-1	-1	604164	-1	-1	9318	732	846	-1	success	909f29c-dirty	release IPO VTR_ASSERT_LEVEL=2	GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	2023-12-08T17:55:38	mustang	/homes/vtr-verilog-to-routing	4650536	732	304	284977	256401	1	127990	11307	164	164	26896	memory	auto	2050.2 MiB	1517.07	1834702	15487251	6133696	9051915	301640	4541.5 MiB	1750.28	13.38	9.89252	-499927	-9.89252	9.89252	33.45	1.83357	1.60237	215.923	175.904	-1	2500777	18	8.6211e+08	4.03628e+08	5.92859e+08	22042.6	191.91	301.651	247.975	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	

$ head -5 vtr_reg_weekly/koios_sv/<latest_run_dir>/parse_results.txt
arch	  circuit	  script_params	  vtr_flow_elapsed_time	  vtr_max_mem_stage	  vtr_max_mem	  error	  odin_synth_time	  max_odin_mem	  parmys_synth_time	  max_parmys_mem	  abc_depth	  abc_synth_time	  abc_cec_time	  abc_sec_time	  max_abc_mem	  ace_time	  max_ace_mem	  num_clb	  num_io	  num_memories	  num_mult	  vpr_status	  vpr_revision	  vpr_build_info	  vpr_compiler	  vpr_compiled	  hostname	  rundir	  max_vpr_mem	  num_primary_inputs	  num_primary_outputs	  num_pre_packed_nets	  num_pre_packed_blocks	  num_netlist_clocks	  num_post_packed_nets	  num_post_packed_blocks	  device_width	  device_height	  device_grid_tiles	  device_limiting_resources	  device_name	  pack_mem	  pack_time	  placed_wirelength_est	  total_swap	  accepted_swap	  rejected_swap	  aborted_swap	  place_mem	  place_time	  place_quench_time	  placed_CPD_est	  placed_setup_TNS_est	  placed_setup_WNS_est	  placed_geomean_nonvirtual_intradomain_critical_path_delay_est	  place_delay_matrix_lookup_time	  place_quench_timing_analysis_time	  place_quench_sta_time	  place_total_timing_analysis_time	  place_total_sta_time	  min_chan_width	  routed_wirelength	  min_chan_width_route_success_iteration	  logic_block_area_total	  logic_block_area_used	  min_chan_width_routing_area_total	  min_chan_width_routing_area_per_tile	  min_chan_width_route_time	  min_chan_width_total_timing_analysis_time	  min_chan_width_total_sta_time	  crit_path_num_rr_graph_nodes	  crit_path_num_rr_graph_edges	  crit_path_collapsed_nodes	  crit_path_routed_wirelength	  crit_path_route_success_iteration	  crit_path_total_nets_routed	  crit_path_total_connections_routed	  crit_path_total_heap_pushes	  crit_path_total_heap_pops	  critical_path_delay	  geomean_nonvirtual_intradomain_critical_path_delay	  setup_TNS	  setup_WNS	  hold_TNS	  hold_WNS	  crit_path_routing_area_total	  crit_path_routing_area_per_tile	  router_lookahead_computation_time	  crit_path_route_time	  crit_path_create_rr_graph_time	  crit_path_create_intra_cluster_rr_graph_time	  crit_path_tile_lookahead_computation_time	  crit_path_router_lookahead_computation_time	  crit_path_total_timing_analysis_time	  crit_path_total_sta_time	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  deepfreeze.style1.sv	  common	  22714.73	  vpr	  4.09 GiB	  	  -1	  -1	  949.56	  2651192	  3	  16835.50	  -1	  -1	  1290132	  -1	  -1	  12293	  27	  396	  -1	  success	  377bca3-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	  2023-12-13T17:58:15	  mustang	  /homes/sv-deep	  4288252	  27	  513	  420409	  319910	  1	  173122	  13274	  122	  122	  14884	  clb	  auto	  2706.3 MiB	  2229.92	  358719	  32218159	  15492330	  11108513	  5617316	  3575.6 MiB	  1036.24	  4.96	  4.77742	  -203483	  -4.77742	  4.77742	  16.43	  1.44734	  1.24291	  322.276	  265.06	  -1	  525106	  18	  4.7523e+08	  4.08959e+08	  3.28149e+08	  22047.1	  89.42	  403.175	  333.904	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  deepfreeze.style2.sv	  common	  24680.43	  vpr	  14.80 GiB	  	  -1	  -1	  827.06	  2325884	  3	  11919.13	  -1	  -1	  1064952	  -1	  -1	  8475	  6	  140	  -1	  success	  377bca3-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	  2023-12-13T17:58:15	  mustang	  /homes/sv-deep	  15515036	  6	  513	  281129	  194945	  1	  142714	  10896	  338	  338	  114244	  dsp_top	  auto	  2163.1 MiB	  2308.76	  1873008	  23434650	  9090338	  12891091	  1453221	  15151.4 MiB	  1246.22	  10.86	  11.0869	  -410426	  -11.0869	  11.0869	  189.96	  1.47102	  1.33008	  298.642	  263.028	  -1	  2267430	  14	  3.68993e+09	  7.02925e+08	  2.50989e+09	  21969.6	  104.21	  368.851	  326.754	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  deepfreeze.style3.sv	  common	  9459.64	  parmys	  2.59 GiB	  	  -1	  -1	  1046.45	  2716236	  3	  5554.19	  -1	  -1	  1151548	  -1	  -1	  4951	  27	  115	  -1	  success	  377bca3-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	  2023-12-13T17:58:15	  mustang	  /homes/sv-deep	  2669896	  27	  513	  162561	  120322	  1	  71039	  5820	  120	  120	  14400	  dsp_top	  auto	  1254.2 MiB	  874.69	  253375	  9948140	  4723336	  3618748	  1606056	  2607.3 MiB	  379.75	  1.99	  5.71612	  -91795.4	  -5.71612	  5.71612	  14.90	  0.558622	  0.482091	  114.978	  97.3208	  -1	  365131	  15	  4.60155e+08	  2.08293e+08	  3.17281e+08	  22033.4	  34.50	  143.778	  122.884	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 

$ head -5 vtr_reg_nightly_test4/koios_medium_no_hb/<latest_run_dir>/parse_results.txt
arch	circuit	script_params	vtr_flow_elapsed_time	vtr_max_mem_stage	vtr_max_mem	error	odin_synth_time	max_odin_mem	parmys_synth_time	max_parmys_mem	abc_depth	abc_synth_time	abc_cec_time	abc_sec_time	max_abc_mem	ace_time	max_ace_mem	num_clb	num_io	num_memories	num_mult	vpr_status	vpr_revision	vpr_build_info	vpr_compiler	vpr_compiled	hostname	rundir	max_vpr_mem	num_primary_inputs	num_primary_outputs	num_pre_packed_nets	num_pre_packed_blocks	num_netlist_clocks	num_post_packed_nets	num_post_packed_blocks	device_width	device_height	device_grid_tiles	device_limiting_resources	device_name	pack_mem	pack_time	placed_wirelength_est	total_swap	accepted_swap	rejected_swap	aborted_swap	place_mem	place_time	place_quench_time	placed_CPD_est	placed_setup_TNS_est	placed_setup_WNS_est	placed_geomean_nonvirtual_intradomain_critical_path_delay_est	place_delay_matrix_lookup_time	place_quench_timing_analysis_time	place_quench_sta_time	place_total_timing_analysis_time	place_total_sta_time	min_chan_width	routed_wirelength	min_chan_width_route_success_iteration	logic_block_area_total	logic_block_area_used	min_chan_width_routing_area_total	min_chan_width_routing_area_per_tile	min_chan_width_route_time	min_chan_width_total_timing_analysis_time	min_chan_width_total_sta_time	crit_path_num_rr_graph_nodes	crit_path_num_rr_graph_edges	crit_path_collapsed_nodes	crit_path_routed_wirelength	crit_path_route_success_iteration	crit_path_total_nets_routed	crit_path_total_connections_routed	crit_path_total_heap_pushes	crit_path_total_heap_pops	critical_path_delay	geomean_nonvirtual_intradomain_critical_path_delay	setup_TNS	setup_WNS	hold_TNS	hold_WNS	crit_path_routing_area_total	crit_path_routing_area_per_tile	router_lookahead_computation_time	crit_path_route_time	crit_path_create_rr_graph_time	crit_path_create_intra_cluster_rr_graph_time	crit_path_tile_lookahead_computation_time	crit_path_router_lookahead_computation_time	crit_path_total_timing_analysis_time	crit_path_total_sta_time	
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	tpu_like.small.os.v	common	2297.73	vpr	2.39 GiB		-1	-1	67.66	248916	5	386.57	-1	-1	139588	-1	-1	1092	355	32	-1	success	9550a0d	release IPO VTR_ASSERT_LEVEL=2	GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	2023-12-12T17:44:41	mustang	/homes/koios	2505488	355	289	47792	39479	2	22463	2033	136	136	18496	dsp_top	auto	315.6 MiB	829.80	417547	2035967	800879	1110613	124475	2446.8 MiB	59.61	0.36	7.56032	-98878.8	-7.56032	2.65337	18.45	0.123782	0.101211	21.3991	17.4955	-1	526122	14	5.92627e+08	1.02128e+08	4.08527e+08	22087.3	15.74	27.6882	23.1868	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	tpu_like.small.ws.v	common	2034.94	vpr	2.43 GiB		-1	-1	56.02	302204	5	517.89	-1	-1	139816	-1	-1	1447	357	58	-1	success	9550a0d	release IPO VTR_ASSERT_LEVEL=2	GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	2023-12-12T17:44:41	mustang	/homes/koios	2549132	357	289	56236	49095	2	21896	2417	136	136	18496	dsp_top	auto	393.4 MiB	344.10	429105	2548015	930606	1466225	151184	2489.4 MiB	85.48	0.50	7.79199	-137248	-7.79199	2.69372	18.37	0.163784	0.137256	28.7844	22.9255	-1	558155	17	5.92627e+08	1.15867e+08	4.08527e+08	22087.3	23.93	38.6761	31.6913	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	dla_like.small.v	common	8355.37	vpr	1.83 GiB		-1	-1	172.77	753612	6	2243.64	-1	-1	412976	-1	-1	4119	206	132	-1	success	9550a0d	release IPO VTR_ASSERT_LEVEL=2	GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	2023-12-12T17:44:41	mustang	/homes/koios	1920604	206	13	177171	148374	1	74857	4582	88	88	7744	dsp_top	auto	1112.1 MiB	5121.00	676743	4607543	1735144	2771118	101281	1657.7 MiB	309.31	2.26	6.5785	-161896	-6.5785	6.5785	6.26	0.492287	0.382534	63.1824	50.6687	-1	975264	23	2.4541e+08	1.61532e+08	1.69370e+08	21871.2	57.11	95.977	78.7754	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	bnn.v	common	1618.20	vpr	2.03 GiB		-1	-1	148.99	734288	3	121.88	-1	-1	410764	-1	-1	6192	260	0	-1	success	9550a0d	release IPO VTR_ASSERT_LEVEL=2	GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	2023-12-12T17:44:41	mustang	/homes/koios	2131528	260	122	206267	154358	1	87325	6637	87	87	7569	clb	auto	1304.8 MiB	399.50	897507	7862107	3019050	4332770	510287	1741.6 MiB	424.98	3.12	6.46586	-141256	-6.46586	6.46586	5.97	0.627132	0.490712	79.1961	63.5977	-1	1180668	18	2.37162e+08	1.8877e+08	1.65965e+08	21927.0	60.49	113.428	92.6149	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	

$ head -5 vtr_reg_weekly/koios_large_no_hb/<latest_run_dir>/parse_results.txt
arch	  circuit	  script_params	  vtr_flow_elapsed_time	  vtr_max_mem_stage	  vtr_max_mem	  error	  odin_synth_time	  max_odin_mem	  parmys_synth_time	  max_parmys_mem	  abc_depth	  abc_synth_time	  abc_cec_time	  abc_sec_time	  max_abc_mem	  ace_time	  max_ace_mem	  num_clb	  num_io	  num_memories	  num_mult	  vpr_status	  vpr_revision	  vpr_build_info	  vpr_compiler	  vpr_compiled	  hostname	  rundir	  max_vpr_mem	  num_primary_inputs	  num_primary_outputs	  num_pre_packed_nets	  num_pre_packed_blocks	  num_netlist_clocks	  num_post_packed_nets	  num_post_packed_blocks	  device_width	  device_height	  device_grid_tiles	  device_limiting_resources	  device_name	  pack_mem	  pack_time	  placed_wirelength_est	  total_swap	  accepted_swap	  rejected_swap	  aborted_swap	  place_mem	  place_time	  place_quench_time	  placed_CPD_est	  placed_setup_TNS_est	  placed_setup_WNS_est	  placed_geomean_nonvirtual_intradomain_critical_path_delay_est	  place_delay_matrix_lookup_time	  place_quench_timing_analysis_time	  place_quench_sta_time	  place_total_timing_analysis_time	  place_total_sta_time	  min_chan_width	  routed_wirelength	  min_chan_width_route_success_iteration	  logic_block_area_total	  logic_block_area_used	  min_chan_width_routing_area_total	  min_chan_width_routing_area_per_tile	  min_chan_width_route_time	  min_chan_width_total_timing_analysis_time	  min_chan_width_total_sta_time	  crit_path_num_rr_graph_nodes	  crit_path_num_rr_graph_edges	  crit_path_collapsed_nodes	  crit_path_routed_wirelength	  crit_path_route_success_iteration	  crit_path_total_nets_routed	  crit_path_total_connections_routed	  crit_path_total_heap_pushes	  crit_path_total_heap_pops	  critical_path_delay	  geomean_nonvirtual_intradomain_critical_path_delay	  setup_TNS	  setup_WNS	  hold_TNS	  hold_WNS	  crit_path_routing_area_total	  crit_path_routing_area_per_tile	  router_lookahead_computation_time	  crit_path_route_time	  crit_path_create_rr_graph_time	  crit_path_create_intra_cluster_rr_graph_time	  crit_path_tile_lookahead_computation_time	  crit_path_router_lookahead_computation_time	  crit_path_total_timing_analysis_time	  crit_path_total_sta_time	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  lenet.v	  common	  6512.03	  parmys	  6.81 GiB	  	  -1	  -1	  2803.15	  7141204	  8	  3272.22	  -1	  -1	  229632	  -1	  -1	  1215	  3	  0	  -1	  success	  9c0df2e-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	  2023-12-03T14:49:57	  mustang	  /homes/vtr-verilog-to-routing	  406888	  3	  73	  29130	  23346	  1	  13644	  1292	  40	  40	  1600	  clb	  auto	  246.5 MiB	  63.14	  136280	  627318	  185500	  408250	  33568	  357.6 MiB	  85.00	  0.86	  8.27929	  -16089.3	  -8.27929	  8.27929	  1.13	  0.12917	  0.113598	  13.8302	  11.3301	  -1	  224227	  19	  4.87982e+07	  3.41577e+07	  3.42310e+07	  21394.3	  19.69	  22.8327	  18.7232	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  clstm_like.small.v	  common	  17199.48	  vpr	  3.24 GiB	  	  -1	  -1	  583.78	  1084852	  4	  13572.40	  -1	  -1	  606412	  -1	  -1	  7731	  652	  237	  -1	  success	  9c0df2e-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	  2023-12-03T14:49:57	  mustang	  /homes/vtr-verilog-to-routing	  3400564	  652	  290	  299239	  274094	  1	  72874	  9119	  120	  120	  14400	  dsp_top	  auto	  1946.4 MiB	  725.17	  1086525	  13721951	  5750436	  7628104	  343411	  3000.6 MiB	  920.88	  5.92	  6.3706	  -404576	  -6.3706	  6.3706	  16.00	  1.30631	  1.07494	  208.425	  167.37	  -1	  1308179	  19	  4.60155e+08	  3.01393e+08	  3.17281e+08	  22033.4	  125.07	  285.633	  232.404	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  clstm_like.medium.v	  common	  44836.58	  vpr	  6.35 GiB	  	  -1	  -1	  1206.67	  2108616	  4	  37270.70	  -1	  -1	  1168924	  -1	  -1	  15290	  652	  460	  -1	  success	  9c0df2e-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	  2023-12-03T14:49:57	  mustang	  /homes/vtr-verilog-to-routing	  6654212	  652	  578	  587830	  538748	  1	  142127	  17391	  168	  168	  28224	  dsp_top	  auto	  3784.4 MiB	  1272.33	  2541145	  33348915	  14048448	  18476269	  824198	  5852.2 MiB	  2378.39	  15.56	  6.83162	  -1.04508e+06	  -6.83162	  6.83162	  36.38	  2.58887	  2.22298	  379.541	  301.913	  -1	  2865108	  16	  9.07771e+08	  5.9428e+08	  6.21411e+08	  22017.1	  283.80	  506.773	  410.065	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  clstm_like.large.v	  common	  79425.36	  vpr	  9.26 GiB	  	  -1	  -1	  1997.66	  3183680	  4	  68911.59	  -1	  -1	  1763240	  -1	  -1	  22848	  652	  682	  -1	  success	  9c0df2e-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	  2023-12-03T14:49:57	  mustang	  /homes/vtr-verilog-to-routing	  9708760	  652	  866	  876471	  803438	  1	  211268	  25661	  200	  200	  40000	  dsp_top	  auto	  5596.5 MiB	  2037.93	  4249390	  55259651	  23005638	  31099607	  1154406	  8453.4 MiB	  2762.94	  28.11	  7.65321	  -1.56393e+06	  -7.65321	  7.65321	  50.04	  2.65623	  2.07346	  405.053	  322.505	  -1	  4619796	  15	  1.28987e+09	  8.87003e+08	  8.79343e+08	  21983.6	  963.02	  568.098	  461.604	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 
 
$ head -5 vtr_reg_weekly/koios_proxy_no_hb/<latest_run_dir>/parse_results.txt
arch	circuit	script_params	vtr_flow_elapsed_time	vtr_max_mem_stage	vtr_max_mem	error	odin_synth_time	max_odin_mem	parmys_synth_time	max_parmys_mem	abc_depth	abc_synth_time	abc_cec_time	abc_sec_time	max_abc_mem	ace_time	max_ace_mem	num_clb	num_io	num_memories	num_mult	vpr_status	vpr_revision	vpr_build_info	vpr_compiler	vpr_compiled	hostname	rundir	max_vpr_mem	num_primary_inputs	num_primary_outputs	num_pre_packed_nets	num_pre_packed_blocks	num_netlist_clocks	num_post_packed_nets	num_post_packed_blocks	device_width	device_height	device_grid_tiles	device_limiting_resources	device_name	pack_mem	pack_time	placed_wirelength_est	total_swap	accepted_swap	rejected_swap	aborted_swap	place_mem	place_time	place_quench_time	placed_CPD_est	placed_setup_TNS_est	placed_setup_WNS_est	placed_geomean_nonvirtual_intradomain_critical_path_delay_est	place_delay_matrix_lookup_time	place_quench_timing_analysis_time	place_quench_sta_time	place_total_timing_analysis_time	place_total_sta_time	min_chan_width	routed_wirelength	min_chan_width_route_success_iteration	logic_block_area_total	logic_block_area_used	min_chan_width_routing_area_total	min_chan_width_routing_area_per_tile	min_chan_width_route_time	min_chan_width_total_timing_analysis_time	min_chan_width_total_sta_time	crit_path_num_rr_graph_nodes	crit_path_num_rr_graph_edges	crit_path_collapsed_nodes	crit_path_routed_wirelength	crit_path_route_success_iteration	crit_path_total_nets_routed	crit_path_total_connections_routed	crit_path_total_heap_pushes	crit_path_total_heap_pops	critical_path_delay	geomean_nonvirtual_intradomain_critical_path_delay	setup_TNS	setup_WNS	hold_TNS	hold_WNS	crit_path_routing_area_total	crit_path_routing_area_per_tile	router_lookahead_computation_time	crit_path_route_time	crit_path_create_rr_graph_time	crit_path_create_intra_cluster_rr_graph_time	crit_path_tile_lookahead_computation_time	crit_path_router_lookahead_computation_time	crit_path_total_timing_analysis_time	crit_path_total_sta_time	
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	proxy.1.v	common	30535.22	vpr	9.48 GiB		-1	-1	1652.38	3799616	7	2393.26	-1	-1	771680	-1	-1	5817	938	845	-1	success	909f29c-dirty	release IPO VTR_ASSERT_LEVEL=2	GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	2023-12-08T17:55:38	mustang	/homes/vtr-verilog-to-routing	9940848	938	175	262404	208705	1	137273	8816	264	264	69696	dsp_top	auto	1962.1 MiB	17465.99	3242084	14209964	6064078	7558347	587539	9707.9 MiB	2269.49	11.20	8.49902	-576590	-8.49902	8.49902	120.99	1.65144	1.34401	319.238	263.953	-1	4269357	15	2.25492e+09	5.42827e+08	1.53035e+09	21957.6	291.49	414.451	348.422	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	proxy.2.v	common	49383.26	parmys	7.46 GiB		-1	-1	6711.91	7820216	8	22879.15	-1	-1	1478720	-1	-1	8948	318	1105	-1	success	909f29c-dirty	release IPO VTR_ASSERT_LEVEL=2	GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	2023-12-08T17:55:38	mustang	/homes/vtr-verilog-to-routing	6046424	318	256	373725	328044	1	148054	10957	188	188	35344	memory	auto	2466.3 MiB	15021.62	2653372	16311253	6713874	9344147	253232	5904.7 MiB	1439.25	8.76	7.35195	-768561	-7.35195	7.35195	47.97	1.45054	1.22978	225.237	181.257	-1	3431386	18	1.1352e+09	4.85551e+08	7.77871e+08	22008.6	262.44	314.625	258.401	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	proxy.3.v	common	19852.09	vpr	4.44 GiB		-1	-1	2415.20	2344724	9	11508.95	-1	-1	604164	-1	-1	9318	732	846	-1	success	909f29c-dirty	release IPO VTR_ASSERT_LEVEL=2	GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	2023-12-08T17:55:38	mustang	/homes/vtr-verilog-to-routing	4650536	732	304	284977	256401	1	127990	11307	164	164	26896	memory	auto	2050.2 MiB	1517.07	1834702	15487251	6133696	9051915	301640	4541.5 MiB	1750.28	13.38	9.89252	-499927	-9.89252	9.89252	33.45	1.83357	1.60237	215.923	175.904	-1	2500777	18	8.6211e+08	4.03628e+08	5.92859e+08	22042.6	191.91	301.651	247.975	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	proxy.4.v	common	54152.82	parmys	8.16 GiB		-1	-1	5711.77	8560300	7	7695.81	-1	-1	1228588	-1	-1	7685	546	1085	-1	success	909f29c-dirty	release IPO VTR_ASSERT_LEVEL=2	GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	2023-12-08T17:55:38	mustang	/homes/vtr-verilog-to-routing	7638244	546	1846	328200	285098	1	145315	11924	222	222	49284	dsp_top	auto	2318.8 MiB	34102.96	3359643	20028032	8510897	11052028	465107	7459.2 MiB	2454.78	12.61	9.3047	-839575	-9.3047	9.3047	72.17	2.37032	2.07569	353.073	294.754	-1	4470327	15	1.58612e+09	5.57186e+08	1.08358e+09	21986.5	321.00	457.912	387.485	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	-1	

$ head -5 vtr_reg_weekly/koios_sv_no_hb/<latest_run_dir>/parse_results.txt
arch	  circuit	  script_params	  vtr_flow_elapsed_time	  vtr_max_mem_stage	  vtr_max_mem	  error	  odin_synth_time	  max_odin_mem	  parmys_synth_time	  max_parmys_mem	  abc_depth	  abc_synth_time	  abc_cec_time	  abc_sec_time	  max_abc_mem	  ace_time	  max_ace_mem	  num_clb	  num_io	  num_memories	  num_mult	  vpr_status	  vpr_revision	  vpr_build_info	  vpr_compiler	  vpr_compiled	  hostname	  rundir	  max_vpr_mem	  num_primary_inputs	  num_primary_outputs	  num_pre_packed_nets	  num_pre_packed_blocks	  num_netlist_clocks	  num_post_packed_nets	  num_post_packed_blocks	  device_width	  device_height	  device_grid_tiles	  device_limiting_resources	  device_name	  pack_mem	  pack_time	  placed_wirelength_est	  total_swap	  accepted_swap	  rejected_swap	  aborted_swap	  place_mem	  place_time	  place_quench_time	  placed_CPD_est	  placed_setup_TNS_est	  placed_setup_WNS_est	  placed_geomean_nonvirtual_intradomain_critical_path_delay_est	  place_delay_matrix_lookup_time	  place_quench_timing_analysis_time	  place_quench_sta_time	  place_total_timing_analysis_time	  place_total_sta_time	  min_chan_width	  routed_wirelength	  min_chan_width_route_success_iteration	  logic_block_area_total	  logic_block_area_used	  min_chan_width_routing_area_total	  min_chan_width_routing_area_per_tile	  min_chan_width_route_time	  min_chan_width_total_timing_analysis_time	  min_chan_width_total_sta_time	  crit_path_num_rr_graph_nodes	  crit_path_num_rr_graph_edges	  crit_path_collapsed_nodes	  crit_path_routed_wirelength	  crit_path_route_success_iteration	  crit_path_total_nets_routed	  crit_path_total_connections_routed	  crit_path_total_heap_pushes	  crit_path_total_heap_pops	  critical_path_delay	  geomean_nonvirtual_intradomain_critical_path_delay	  setup_TNS	  setup_WNS	  hold_TNS	  hold_WNS	  crit_path_routing_area_total	  crit_path_routing_area_per_tile	  router_lookahead_computation_time	  crit_path_route_time	  crit_path_create_rr_graph_time	  crit_path_create_intra_cluster_rr_graph_time	  crit_path_tile_lookahead_computation_time	  crit_path_router_lookahead_computation_time	  crit_path_total_timing_analysis_time	  crit_path_total_sta_time	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  deepfreeze.style1.sv	  common	  47967.94	  vpr	  10.31 GiB	  	  -1	  -1	  1750.70	  3477528	  3	  33798.52	  -1	  -1	  1967140	  -1	  -1	  20253	  27	  1843	  -1	  success	  377bca3-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	  2023-12-13T17:58:15	  mustang	  /homes/sv-deep	  10811692	  27	  513	  778797	  600279	  1	  384107	  23186	  244	  244	  59536	  memory	  auto	  4968.5 MiB	  3724.68	  4867625	  48601541	  21188063	  25604799	  1808679	  10366.4 MiB	  3892.48	  41.19	  8.46401	  -1.13947e+06	  -8.46401	  8.46401	  82.35	  2.83854	  2.28574	  443.492	  355.56	  -1	  5791588	  17	  1.92066e+09	  9.58441e+08	  1.30834e+09	  21975.7	  419.89	  594.451	  484.887	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  deepfreeze.style2.sv	  common	  48524.73	  vpr	  8.29 GiB	  	  -1	  -1	  1440.31	  3118316	  3	  35219.69	  -1	  -1	  1725016	  -1	  -1	  22674	  27	  1231	  -1	  success	  377bca3-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	  2023-12-13T17:58:15	  mustang	  /homes/sv-deep	  8696204	  27	  513	  757966	  564979	  1	  371413	  24999	  196	  196	  38416	  memory	  auto	  4726.6 MiB	  2712.89	  5184470	  52271336	  22299033	  27769653	  2202650	  7642.4 MiB	  5209.27	  55.51	  9.75062	  -937734	  -9.75062	  9.75062	  50.02	  2.30465	  1.94566	  366.253	  293.69	  -1	  6516523	  17	  1.23531e+09	  9.4276e+08	  8.45266e+08	  22003.0	  925.98	  493.024	  402.412	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 
k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml	  deepfreeze.style3.sv	  common	  41631.02	  vpr	  15.22 GiB	  	  -1	  -1	  1622.97	  3431784	  3	  24896.76	  -1	  -1	  1856148	  -1	  -1	  20779	  27	  3333	  -1	  success	  377bca3-dirty	  release IPO VTR_ASSERT_LEVEL=2	  GNU 9.4.0 on Linux-5.4.0-148-generic x86_64	  2023-12-13T17:58:15	  mustang	  /homes/sv-deep	  15958564	  27	  513	  703297	  547641	  1	  350325	  24854	  324	  324	  104976	  memory	  auto	  4656.9 MiB	  3861.23	  5201129	  61655974	  26414908	  31818866	  3422200	  15584.5 MiB	  3575.85	  19.40	  9.71561	  -1.53645e+06	  -9.71561	  9.71561	  179.24	  2.62795	  2.23108	  484.893	  395.834	  -1	  6173057	  19	  3.39753e+09	  1.08992e+09	  2.30538e+09	  21961.0	  377.21	  640.096	  530.51	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	  -1	 

```

### Example: Extracting QoR Data from CI Runs

Instead of running tests/designs locally to generate QoR data, you can also extract the QoR data from any of the standard
test runs performed automatically by CI on a pull request. To get the QoR results of the above tests, go to the "Action" 
tab. On the menu on the left, choose "Test" and select your workflow. If running the tests is done, scroll down and click 
on "artifact". This would download the results for all CI tests.

1. Go to "Action" tab
![Action Button](https://raw.githubusercontent.com/verilog-to-routing/vtr-verilog-to-routing/master/doc/src/dev/eval_qor/action_button.png)
2. Select "Test" and choose your workflow
![Test Button](https://raw.githubusercontent.com/verilog-to-routing/vtr-verilog-to-routing/master/doc/src/dev/eval_qor/test.png)
3. Scroll down and download "artifact"
![Artifact](https://raw.githubusercontent.com/verilog-to-routing/vtr-verilog-to-routing/master/doc/src/dev/eval_qor/artifact.png)

Assume that we want to get the QoR results for "vtr_reg_nightly_test3". In the artifact, there is a file named 
"qor_results_vtr_reg_nightly_test3.tar.gz." Unzip this file, and a new directory named "vtr_flow" is created. Go to 
"vtr_flow/tasks/regression_tests/vtr_reg_nightly_test3." In this directory, you can find a directory for each benchmark
contained in this test suite (vtr_reg_nightly_test3.) In the directory for each sub-test, there is another directory
named *run001*. Two files are here: *qor_results.txt*, and *parse_results.txt*. QoR results for all circuits tested in this
benchmark are stored in these files.
Using these parsed results, you can do a detailed QoR comparison using the instructions given [here](#comparing-qor-measurements).
![Parse File Dir](https://raw.githubusercontent.com/verilog-to-routing/vtr-verilog-to-routing/master/doc/src/dev/eval_qor/parse_result_dir.png)



## Comparing QoR Measurements
Once you have two (or more) sets of QoR measurements they now need to be compared.

A general method is as follows:
1. Normalize all metrics to the values in the baseline measurements (this makes the relative changes easy to evaluate)
2. Produce tables for each set of QoR measurements showing the per-benchmark relative values for each metric
3. Calculate the GEOMEAN over all benchmarks for each normalized metric
4. Produce a summary table showing the Metric Geomeans for each set of QoR measurements

### QoR Comparison Gotchas
There are a variety of 'gotchas' you need to avoid to ensure fair comparisons:
* GEOMEAN's must be over the same set of benchmarks .
    A common issue is that a benchmark failed to complete for some reason, and it's metric values are missing

* Run-times need to be collected on the same compute infrastructure at the same system load (ideally unloaded).

### Example QoR Comparison
Suppose we've make a change to VTR, and we now want to evaluate the change.
As described above we produce QoR measurements for both the VTR baseline, and our modified version.

We then have the following (hypothetical) QoR Metrics.

**Baseline QoR Metrics:**

| arch                                   | circuit            | num_pre_packed_blocks | num_post_packed_blocks | device_grid_tiles | min_chan_width | crit_path_routed_wirelength | critical_path_delay | vtr_flow_elapsed_time | pack_time | place_time | min_chan_width_route_time | crit_path_route_time | max_vpr_mem |
|----------------------------------------|--------------------|-----------------------|------------------------|-------------------|----------------|-----------------------------|---------------------|-----------------------|-----------|------------|---------------------------|----------------------|-------------|
| k6_frac_N10_frac_chain_mem32K_40nm.xml | bgm.v              | 24575                 | 2258                   | 2809              | 84             | 297718                      | 20.4406             | 652.17                | 141.53    | 108.26     | 142.42                    | 15.63                | 1329712     |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | blob_merge.v       | 11407                 | 700                    | 900               | 64             | 75615                       | 15.3479             | 198.58                | 67.89     | 11.3       | 47.6                      | 3.48                 | 307756      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | boundtop.v         | 1141                  | 389                    | 169               | 34             | 3767                        | 3.96224             | 7.24                  | 2.55      | 0.82       | 2.1                       | 0.15                 | 87552       |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | ch_intrinsics.v    | 493                   | 247                    | 100               | 46             | 1438                        | 2.4542              | 2.59                  | 0.46      | 0.31       | 0.94                      | 0.09                 | 62684       |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | diffeq1.v          | 886                   | 313                    | 256               | 60             | 9624                        | 17.9648             | 15.59                 | 2.45      | 1.36       | 9.93                      | 0.93                 | 86524       |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | diffeq2.v          | 599                   | 201                    | 256               | 52             | 8928                        | 13.7083             | 13.14                 | 1.41      | 0.87       | 9.14                      | 0.94                 | 85760       |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | LU8PEEng.v         | 31396                 | 2286                   | 2916              | 100            | 348085                      | 79.4512             | 1514.51               | 175.67    | 153.01     | 1009.08                   | 45.47                | 1410872     |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | LU32PEEng.v        | 101542                | 7251                   | 9216              | 158            | 1554942                     | 80.062              | 28051.68              | 625.03    | 930.58     | 25050.73                  | 251.87               | 4647936     |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | mcml.v             | 165809                | 6767                   | 8649              | 128            | 1311825                     | 51.1905             | 9088.1                | 524.8     | 742.85     | 4001.03                   | 127.42               | 4999124     |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | mkDelayWorker32B.v | 4145                  | 1327                   | 2500              | 38             | 30086                       | 8.39902             | 65.54                 | 7.73      | 15.39      | 26.19                     | 3.23                 | 804720      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | mkPktMerge.v       | 1160                  | 516                    | 784               | 44             | 13370                       | 4.4408              | 21.75                 | 2.45      | 2.14       | 13.95                     | 1.96                 | 122872      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | mkSMAdapter4B.v    | 2852                  | 548                    | 400               | 48             | 19274                       | 5.26765             | 47.64                 | 16.22     | 4.16       | 19.95                     | 1.14                 | 116012      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | or1200.v           | 4530                  | 1321                   | 729               | 62             | 51633                       | 9.67406             | 105.62                | 33.37     | 12.93      | 44.95                     | 3.33                 | 219376      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | raygentop.v        | 2934                  | 710                    | 361               | 58             | 22045                       | 5.14713             | 39.72                 | 9.54      | 4.06       | 19.8                      | 2.34                 | 126056      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | sha.v              | 3024                  | 236                    | 289               | 62             | 16653                       | 10.0144             | 390.89                | 11.47     | 2.7        | 6.18                      | 0.75                 | 117612      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | stereovision0.v    | 21801                 | 1122                   | 1156              | 58             | 64935                       | 3.63177             | 82.74                 | 20.45     | 15.49      | 24.5                      | 2.6                  | 411884      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | stereovision1.v    | 19538                 | 1096                   | 1600              | 100            | 143517                      | 5.61925             | 272.41                | 26.99     | 18.15      | 149.46                    | 15.49                | 676844      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | stereovision2.v    | 42078                 | 2534                   | 7396              | 134            | 650583                      | 15.3151             | 3664.98               | 66.72     | 119.26     | 3388.7                    | 62.6                 | 3114880     |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | stereovision3.v    | 324                   | 55                     | 49                | 30             | 768                         | 2.66429             | 2.25                  | 0.75      | 0.2        | 0.57                      | 0.05                 | 61148       |

**Modified QoR Metrics:**

| arch                                   | circuit            | num_pre_packed_blocks | num_post_packed_blocks | device_grid_tiles | min_chan_width | crit_path_routed_wirelength | critical_path_delay | vtr_flow_elapsed_time | pack_time | place_time | min_chan_width_route_time | crit_path_route_time | max_vpr_mem |
|----------------------------------------|--------------------|-----------------------|------------------------|-------------------|----------------|-----------------------------|---------------------|-----------------------|-----------|------------|---------------------------|----------------------|-------------|
| k6_frac_N10_frac_chain_mem32K_40nm.xml | bgm.v              | 24575                 | 2193                   | 2809              | 82             | 303891                      | 20.414              | 642.01                | 70.09     | 113.58     | 198.09                    | 16.27                | 1222072     |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | blob_merge.v       | 11407                 | 684                    | 900               | 72             | 77261                       | 14.6676             | 178.16                | 34.31     | 13.38      | 57.89                     | 3.35                 | 281468      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | boundtop.v         | 1141                  | 369                    | 169               | 40             | 3465                        | 3.5255              | 4.48                  | 1.13      | 0.7        | 0.9                       | 0.17                 | 82912       |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | ch_intrinsics.v    | 493                   | 241                    | 100               | 54             | 1424                        | 2.50601             | 1.75                  | 0.19      | 0.27       | 0.43                      | 0.09                 | 60796       |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | diffeq1.v          | 886                   | 293                    | 256               | 50             | 9972                        | 17.3124             | 15.24                 | 0.69      | 0.97       | 11.27                     | 1.44                 | 72204       |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | diffeq2.v          | 599                   | 187                    | 256               | 50             | 7621                        | 13.1714             | 14.14                 | 0.63      | 1.04       | 10.93                     | 0.78                 | 68900       |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | LU8PEEng.v         | 31396                 | 2236                   | 2916              | 98             | 349074                      | 77.8611             | 1269.26               | 88.44     | 153.25     | 843.31                    | 49.13                | 1319276     |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | LU32PEEng.v        | 101542                | 6933                   | 9216              | 176            | 1700697                     | 80.1368             | 28290.01              | 306.21    | 897.95     | 25668.4                   | 278.74               | 4224048     |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | mcml.v             | 165809                | 6435                   | 8649              | 124            | 1240060                     | 45.6693             | 9384.4                | 296.99    | 686.27     | 4782.43                   | 99.4                 | 4370788     |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | mkDelayWorker32B.v | 4145                  | 1207                   | 2500              | 36             | 33354                       | 8.3986              | 53.94                 | 3.85      | 14.75      | 19.53                     | 2.95                 | 785316      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | mkPktMerge.v       | 1160                  | 494                    | 784               | 36             | 13881                       | 4.57189             | 20.75                 | 0.82      | 1.97       | 15.01                     | 1.88                 | 117636      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | mkSMAdapter4B.v    | 2852                  | 529                    | 400               | 56             | 19817                       | 5.21349             | 27.58                 | 5.05      | 2.66       | 14.65                     | 1.11                 | 103060      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | or1200.v           | 4530                  | 1008                   | 729               | 76             | 48034                       | 8.70797             | 202.25                | 10.1      | 8.31       | 171.96                    | 2.86                 | 178712      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | raygentop.v        | 2934                  | 634                    | 361               | 58             | 20799                       | 5.04571             | 22.58                 | 2.75      | 2.42       | 12.86                     | 1.64                 | 108116      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | sha.v              | 3024                  | 236                    | 289               | 62             | 16052                       | 10.5007             | 337.19                | 5.32      | 2.25       | 4.52                      | 0.69                 | 105948      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | stereovision0.v    | 21801                 | 1121                   | 1156              | 58             | 70046                       | 3.61684             | 86.5                  | 9.5       | 15.02      | 41.81                     | 2.59                 | 376100      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | stereovision1.v    | 19538                 | 1080                   | 1600              | 92             | 142805                      | 6.02319             | 343.83                | 10.68     | 16.21      | 247.99                    | 11.66                | 480352      |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | stereovision2.v    | 42078                 | 2416                   | 7396              | 124            | 646793                      | 14.6606             | 5614.79               | 34.81     | 107.66     | 5383.58                   | 62.27                | 2682976     |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | stereovision3.v    | 324                   | 54                     | 49                | 34             | 920                         | 2.5281              | 1.55                  | 0.31      | 0.14       | 0.43                      | 0.05                 | 63444       |

Based on these metrics we then calculate the following ratios and summary.

**QoR Metric Ratio** (Modified QoR / Baseline QoR):

| arch                                   | circuit            | num_pre_packed_blocks | num_post_packed_blocks | device_grid_tiles | min_chan_width | crit_path_routed_wirelength | critical_path_delay | vtr_flow_elapsed_time | pack_time | place_time | min_chan_width_route_time | crit_path_route_time | max_vpr_mem |
|----------------------------------------|--------------------|-----------------------|------------------------|-------------------|----------------|-----------------------------|---------------------|-----------------------|-----------|------------|---------------------------|----------------------|-------------|
| k6_frac_N10_frac_chain_mem32K_40nm.xml | bgm.v              | 1.00                  | 0.97                   | 1.00              | 0.98           | 1.02                        | 1.00                | 0.98                  | 0.50      | 1.05       | 1.39                      | 1.04                 | 0.92        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | blob_merge.v       | 1.00                  | 0.98                   | 1.00              | 1.13           | 1.02                        | 0.96                | 0.90                  | 0.51      | 1.18       | 1.22                      | 0.96                 | 0.91        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | boundtop.v         | 1.00                  | 0.95                   | 1.00              | 1.18           | 0.92                        | 0.89                | 0.62                  | 0.44      | 0.85       | 0.43                      | 1.13                 | 0.95        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | ch_intrinsics.v    | 1.00                  | 0.98                   | 1.00              | 1.17           | 0.99                        | 1.02                | 0.68                  | 0.41      | 0.87       | 0.46                      | 1.00                 | 0.97        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | diffeq1.v          | 1.00                  | 0.94                   | 1.00              | 0.83           | 1.04                        | 0.96                | 0.98                  | 0.28      | 0.71       | 1.13                      | 1.55                 | 0.83        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | diffeq2.v          | 1.00                  | 0.93                   | 1.00              | 0.96           | 0.85                        | 0.96                | 1.08                  | 0.45      | 1.20       | 1.20                      | 0.83                 | 0.80        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | LU8PEEng.v         | 1.00                  | 0.98                   | 1.00              | 0.98           | 1.00                        | 0.98                | 0.84                  | 0.50      | 1.00       | 0.84                      | 1.08                 | 0.94        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | LU32PEEng.v        | 1.00                  | 0.96                   | 1.00              | 1.11           | 1.09                        | 1.00                | 1.01                  | 0.49      | 0.96       | 1.02                      | 1.11                 | 0.91        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | mcml.v             | 1.00                  | 0.95                   | 1.00              | 0.97           | 0.95                        | 0.89                | 1.03                  | 0.57      | 0.92       | 1.20                      | 0.78                 | 0.87        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | mkDelayWorker32B.v | 1.00                  | 0.91                   | 1.00              | 0.95           | 1.11                        | 1.00                | 0.82                  | 0.50      | 0.96       | 0.75                      | 0.91                 | 0.98        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | mkPktMerge.v       | 1.00                  | 0.96                   | 1.00              | 0.82           | 1.04                        | 1.03                | 0.95                  | 0.33      | 0.92       | 1.08                      | 0.96                 | 0.96        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | mkSMAdapter4B.v    | 1.00                  | 0.97                   | 1.00              | 1.17           | 1.03                        | 0.99                | 0.58                  | 0.31      | 0.64       | 0.73                      | 0.97                 | 0.89        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | or1200.v           | 1.00                  | 0.76                   | 1.00              | 1.23           | 0.93                        | 0.90                | 1.91                  | 0.30      | 0.64       | 3.83                      | 0.86                 | 0.81        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | raygentop.v        | 1.00                  | 0.89                   | 1.00              | 1.00           | 0.94                        | 0.98                | 0.57                  | 0.29      | 0.60       | 0.65                      | 0.70                 | 0.86        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | sha.v              | 1.00                  | 1.00                   | 1.00              | 1.00           | 0.96                        | 1.05                | 0.86                  | 0.46      | 0.83       | 0.73                      | 0.92                 | 0.90        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | stereovision0.v    | 1.00                  | 1.00                   | 1.00              | 1.00           | 1.08                        | 1.00                | 1.05                  | 0.46      | 0.97       | 1.71                      | 1.00                 | 0.91        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | stereovision1.v    | 1.00                  | 0.99                   | 1.00              | 0.92           | 1.00                        | 1.07                | 1.26                  | 0.40      | 0.89       | 1.66                      | 0.75                 | 0.71        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | stereovision2.v    | 1.00                  | 0.95                   | 1.00              | 0.93           | 0.99                        | 0.96                | 1.53                  | 0.52      | 0.90       | 1.59                      | 0.99                 | 0.86        |
| k6_frac_N10_frac_chain_mem32K_40nm.xml | stereovision3.v    | 1.00                  | 0.98                   | 1.00              | 1.13           | 1.20                        | 0.95                | 0.69                  | 0.41      | 0.70       | 0.75                      | 1.00                 | 1.04        |
|                                        | GEOMEAN            | 1.00                  | 0.95                   | 1.00              | 1.02           | 1.01                        | 0.98                | 0.92                  | 0.42      | 0.87       | 1.03                      | 0.96                 | 0.89        |

**QoR Summary:**

|                             | baseline | modified |
|-----------------------------|----------|----------|
| num_pre_packed_blocks       | 1.00     | 1.00     |
| num_post_packed_blocks      | 1.00     | 0.95     |
| device_grid_tiles           | 1.00     | 1.00     |
| min_chan_width              | 1.00     | 1.02     |
| crit_path_routed_wirelength | 1.00     | 1.01     |
| critical_path_delay         | 1.00     | 0.98     |
| vtr_flow_elapsed_time       | 1.00     | 0.92     |
| pack_time                   | 1.00     | 0.42     |
| place_time                  | 1.00     | 0.87     |
| min_chan_width_route_time   | 1.00     | 1.03     |
| crit_path_route_time        | 1.00     | 0.96     |
| max_vpr_mem                 | 1.00     | 0.89     |

From the results we can see that our change, on average, achieved a small reduction in the number of logic blocks (0.95) in return for a 2% increase in minimum channel width and 1% increase in routed wirelength. From a run-time perspective the packer is substantially faster (0.42).

### Automated QoR Comparison Script
To automate some of the QoR comparison VTR includes a script to compare `parse_results.txt` files and generate a spreadsheet including the ratio and summary tables.

For example:
```shell
#From the VTR Root
$ ./vtr_flow/scripts/qor_compare.py parse_results1.txt parse_results2.txt parse_results3.txt -o comparison.xlsx
```
will produce ratio tables and a summary table for the files parse_results1.txt, parse_results2.txt and parse_results3.txt, where the first file (parse_results1.txt) is assumed to be the baseline used to produce normalized ratios.

### Generating New QoR Golden Result
There may be times when a regression test fails its QoR test because its golden_result needs to be changed due to known changes in code behaviour. In this case, a new golden result needs to be generated so that the test can be passed. To generate a new golden result, follow the steps outlined below.

1. Move to the `vtr_flow/tasks` directory from the VTR root, and run the failing test. For example, if a test called `vtr_ex_test` in `vtr_reg_nightly_test3` was failing:

	```shell
    #From the VTR root
    $ cd vtr_flow/tasks
    $ ../scripts/run_vtr_task.py regression_tests/vtr_reg_nightly_test3/vtr_ex_test
	```
2. Next, generate new golden reference results using `parse_vtr_task.py` and the `-create_golden` option.

    ```shell
    $ ../scripts/python_libs/vtr/parse_vtr_task.py regression_tests/vtr_reg_nightly_test3/vtr_ex_test -create_golden
    ```
3. Lastly, check that the results match with the `-check_golden` option

    ```shell
    $ ../scripts/python_libs/vtr/parse_vtr_task.py regression_tests/vtr_reg_nightly_test3/vtr_ex_test -check_golden
    ```
Once the `-check_golden` command passes, the changes to the golden result can be committed so that the reg test will pass in future runs of vtr_reg_nightly_test3.

**Attention** Even though the parsed files are located in different locations, the names of the parsed files 
should be different.

# Adding Tests

Any time you add a feature to VTR you **must** add a test which exercises the feature.
This ensures that regression tests will detect if the feature breaks in the future.

Consider which regression test suite your test should be added to (see [Running Tests](#running-tests) descriptions).

Typically, test which exercise new features should be added to `vtr_reg_strong`.
These tests should use small benchmarks to ensure they:
 * run quickly (so they get run often!), and
 * are easier to debug.
If your test will take more than ~1 minute it should probably go in a longer running regression test (but see first if you can create a smaller testcase first).

## Adding a test to vtr_reg_strong
This describes adding a test to `vtr_reg_strong`, but the process is similar for the other regression tests.

1. Create a configuration file

    First move to the vtr_reg_strong directory:
    ```shell
    #From the VTR root directory
    $ cd vtr_flow/tasks/regression_tests/vtr_reg_strong
    $ ls
    qor_geomean.txt             strong_flyover_wires        strong_pack_and_place
    strong_analysis_only        strong_fpu_hard_block_arch  strong_power
    strong_bounding_box         strong_fracturable_luts     strong_route_only
    strong_breadth_first        strong_func_formal_flow     strong_scale_delay_budgets
    strong_constant_outputs     strong_func_formal_vpr      strong_sweep_constant_outputs
    strong_custom_grid          strong_global_routing       strong_timing
    strong_custom_pin_locs      strong_manual_annealing     strong_titan
    strong_custom_switch_block  strong_mcnc                 strong_valgrind
    strong_echo_files           strong_minimax_budgets      strong_verify_rr_graph
    strong_fc_abs               strong_multiclock           task_list.txt
    strong_fix_pins_pad_file    strong_no_timing            task_summary
    strong_fix_pins_random      strong_pack
    ```
    Each folder (prefixed with `strong_` in this case) defines a task (sub-test).

    Let's make a new task named `strong_mytest`.
    An easy way is to copy an existing configuration file such as `strong_timing/config/config.txt`
    ```shell
    $ mkdir -p strong_mytest/config
    $ cp strong_timing/config/config.txt strong_mytest/config/.
    ```
    You can now edit `strong_mytest/config/config.txt` to customize your test.

2. Generate golden reference results

    Now we need to test our new test and generate 'golden' reference results.
    These will be used to compare future runs of our test to detect any changes in behaviour (e.g. bugs).

    From the VTR root, we move to the `vtr_flow/tasks` directory, and then run our new test:
    ```shell
    #From the VTR root
    $ cd vtr_flow/tasks
    $ ../scripts/run_vtr_task.py regression_tests/vtr_reg_strong/strong_mytest

    regression_tests/vtr_reg_strong/strong_mytest
    -----------------------------------------
    Current time: Jan-25 06:51 PM.  Expected runtime of next benchmark: Unknown
    k6_frac_N10_mem32K_40nm/ch_intrinsics...OK
    ```

    Next we can generate the golden reference results using `parse_vtr_task.py` with the `-create_golden` option:
    ```shell
    $ ../scripts/python_libs/vtr/parse_vtr_task.py regression_tests/vtr_reg_strong/strong_mytest -create_golden
    ```

    And check that everything matches with `-check_golden`:
    ```shell
    $ ../scripts/python_libs/vtr/parse_vtr_task.py regression_tests/vtr_reg_strong/strong_mytest -check_golden
    regression_tests/vtr_reg_strong/strong_mytest...[Pass]
    ```

3. Add it to the task list

    We now need to add our new `strong_mytest` task to the task list, so it is run whenever `vtr_reg_strong` is run.
    We do this by adding the line `regression_tests/vtr_reg_strong/strong_mytest` to the end of `vtr_reg_strong`'s `task_list.txt`:
    ```shell
    #From the VTR root directory
    $ vim vtr_flow/tasks/regression_tests/vtr_reg_strong/task_list.txt
    # Add a new line 'regression_tests/vtr_reg_strong/strong_mytest' to the end of the file
    ```

    Now, when we run `vtr_reg_strong`:
    ```shell
    #From the VTR root directory
    $ ./run_reg_test.py vtr_reg_strong
    #Output trimmed...
    regression_tests/vtr_reg_strong/strong_mytest
    -----------------------------------------
    #Output trimmed...
    ```
    we see our test is run.

4. Commit the new test

    Finally you need to commit your test:
    ```shell
    #Add the config.txt and golden_results.txt for the test
    $ git add vtr_flow/tasks/regression_tests/vtr_reg_strong/strong_mytest/
    #Add the change to the task_list.txt
    $ git add vtr_flow/tasks/regression_tests/vtr_reg_strong/task_list.txt
    #Commit the changes, when pushed the test will automatically be picked up by BuildBot
    $ git commit
    ```

## Creating Unit Tests

You can find the source code for the unit tests in their respective directories. New unit tests must also be created in
these directories.

| Test            | Directory                         |
|-----------------|-----------------------------------|
| `test_archfpga` | `$VTR_ROOT/libs/libarchfpga/test` |
| `test_vtrutil`  | `$VTR_ROOT/libs/libvtrutil/test`  |
| `test_fasm`     | `$VTR_ROOT/utils/fasm/test`       |
| `test_vpr`      | `$VTR_ROOT/vpr/test`              |

VTR uses [Catch2](https://github.com/catchorg/Catch2) for its unit testing framework. For a full tutorial of how to use
the framework, see `$VTR_ROOT/libs/EXTERNAL/libcatch2/docs/Readme.md`.

### Example: Creating and Running a VPR Test Case

Navigate to `$VTR_ROOT/vpr/test`.

```shell
$ cd $VTR_ROOT/vpr/test
```

From here, let's create and open a new file `test_new_vpr.cpp` (begin the file name with `test_`). Be sure to `#include "catch2/catch_test_macros.hpp"`.
Introduce a test case using the `TEST_CASE` macro, and include a name and a tag. For boolean assertions, use `REQUIRE`.

```shell
#include "catch2/catch_test_macros.hpp"

// To choose a tag (written with square brackets "[tag]"), see examples from when you run ./test_vpr
// --list-tests in the tester exectuable directory, as shown earlier. A good default tag name is the name
// of the tester: in this case, [vpr].
TEST_CASE("a_vpr_test_name", "[vpr]") {
  int x = 0;
  REQUIRE(x == 0);
}
```

To run our test case, we must navigate back to `$VTR_ROOT/build/vpr` (from the table
under [Running Individual Testers](#running-individual-testers)). Since we created a test, we need to rebuild the
tester. Then, we can run our test.

```shell
$ cd $VTR_ROOT/build/vpr
$ make                         // rebuild tester
$ ./test_vpr a_vpr_test_name   // run new unit test
```

Output:

```shell
Filters: "a_vpr_test_name"
Randomness seeded to: 2089861684
===============================================================================
All tests passed (1 assertion in 1 test case)
```


# Debugging Aids
VTR has support for several additional tools/features to aid debugging.

## Basic
To build vpr with make in debug mode, simply add `BUILD_TYPE=debug` at the end of your make command. 
```shell
$ make vpr BUILD_TYPE=debug
```

## Sanitizers
VTR can be compiled using *sanitizers* which will detect invalid memory accesses, memory leaks and undefined behaviour (supported by both GCC and LLVM):
```shell
#From the VTR root directory
$ cmake -D VTR_ENABLE_SANITIZE=ON build
$ make
```

You can suppress reporting of known memory leaks in libraries used by vpr by setting the environment variable below:
```shell
LSAN_OPTIONS=suppressions=$VTR_ROOT/vpr/lsan.supp
```
where $VTR_ROOT is the root directory of your vtr source code tree.

Note that some of the continuous integration (CI) regtests (run automatically on pull requests) turn on sanitizers (currently S: Basic and R: Odin-II Basic Tests)

## Valgrind
An alternative way to run vtr programs to check for invalid memory accesses and memory leaks is to use the valgrind tool. valgrind can be run on any build except the sanitized build, without recompilation. For example, to run on vpr use 
```shell
#From the VTR root directory
valgrind --leak-check=full --suppressions=./vpr/valgrind.supp ./vpr/vpr [... usual vpr options here ...]
```
The suppression file included in the command above will suppress reporting of known memory leaks in libraries included by vpr.

Note that valgrind is run on some flows by the continuous integration (CI) tests.

## Assertion Levels
VTR supports configurable assertion levels.

The default level (`2`) which turns on most assertions which don't cause significant run-time penalties.

This level can be increased:
```shell
#From the VTR root directory
$ cmake -D VTR_ASSERT_LEVEL=3 build
$ make
```
this turns on more extensive assertion checking and re-builds VTR.

## GDB Pretty Printers
To make it easier to debug some of VTR's data structures with [GDB](https://www.sourceware.org/gdb/).

### STL Pretty Printers

It is helpful to enable [STL pretty printers](https://sourceware.org/gdb/wiki/STLSupport), which make it much easier to debug data structures using STL.

For example printing a `std::vector<int>` by default prints:

    (gdb) p/r x_locs
    $2 = {<std::_Vector_base<int, std::allocator<int> >> = {
        _M_impl = {<std::allocator<int>> = {<__gnu_cxx::new_allocator<int>> = {<No data fields>}, <No data fields>}, _M_start = 0x555556f063b0, 
          _M_finish = 0x555556f063dc, _M_end_of_storage = 0x555556f064b0}}, <No data fields>}

which is not very helpful.

But with STL pretty printers it prints:

    (gdb) p x_locs
    $2 = std::vector of length 11, capacity 64 = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}

which is much more helpful for debugging!

If STL pretty printers aren't already enabled on your system, add the following to your [.gdbinit file](https://sourceware.org/gdb/current/onlinedocs/gdb/gdbinit-man.html):

    python
    import sys
    sys.path.insert(0, '$STL_PRINTER_ROOT')
    from libstdcxx.v6.printers import register_libstdcxx_printers
    register_libstdcxx_printers(None)

    end

where `$STL_PRINTER_ROOT` should be replaced with the appropriate path to the STL pretty printers.
For example recent versions of GCC include these under `/usr/share/gcc-*/python` (e.g. `/usr/share/gcc-9/python`)


### VTR Pretty Printers

VTR includes some pretty printers for some VPR/VTR specific types.

For example, without the pretty printers you would see the following when printing a VPR `AtomBlockId`:

    (gdb) p blk_id
    $1 = {
      id_ = 71
    }

But with the VTR pretty printers enabled you would see:

    (gdb) p blk_id
    $1 = AtomBlockId(71)

To enable the VTR pretty printers in GDB add the following to your [.gdbinit file](https://sourceware.org/gdb/current/onlinedocs/gdb/gdbinit-man.html):

    python
    import sys

    sys.path.insert(0, "$VTR_ROOT/dev")
    import vtr_gdb_pretty_printers
    gdb.pretty_printers.append(vtr_gdb_pretty_printers.vtr_type_lookup)

    end

where ``$VTR_ROOT`` should be replaced with the root of the VTR source tree on your system.

## RR (Record Replay) Debugger

[RR](https://rr-project.org/) extends GDB with the ability to to record a run of a tool and then re-run it to reproduce any observed issues.
RR also enables efficient reverse execution (!) which can be *extremely helpful* when tracking down the source of a bug.

# Speeding up the edit-compile-test cycle
Rapid iteration through the edit-compile-test/debug cycle is very helpful when making code changes to VTR.

The following is some guidance on techniques to reduce the time required.

# Speeding Compilation

1. Parallel compilation

    For instance when [building VTR](BUILDING.md) using make, you can specify the `-j N` option to compile the code base with N parallel jobs:
    ```
    $ make -j N
    ```

    A reasonable value for `N` is equal to the number of threads you system can run. For instance, if your system has 4 cores with HyperThreading (i.e. 2-way SMT) you could run:
    ```
    $ make -j8
    ```

2. Building only a subset of VTR

    If you know your changes only effect a specific tool in VTR, you can request that only that tool is rebuilt.
    For instance, if you only wanted to re-compile VPR you could run:
    ```
    $ make vpr
    ```
    which would avoid re-building other tools (e.g. ODIN, ABC).

3. Use ccache

    [ccache](https://ccache.dev/) is a program which caches previous compilation results.
    This can save significant time, for instance, when switching back and forth between release and debug builds.

    VTR's cmake configuration should automatically detect and make use of ccache once it is installed.

    For instance on Ubuntu/Debian systems you can install ccache with:
    ```
    $ sudo apt install ccache
    ```
    This only needs to be done once on your development system.

4. Disable Interprocedural Optimizatiaons (IPO)

    IPO re-optimizes an entire executable at link time, and is automatically enabled by VTR if a supporting compiler is found.
    This can notably improve performance (e.g. ~10-20% faster), but can significantly increase compilation time (e.g. >2x in some cases).
    When frequently re-compiling and debugging the extra execution speed may not be worth the longer compilation times.
    In such cases you can manually disable IPO by setting the cmake parameter `VTR_IPO_BUILD=off`.

    For instance using the wrapper Makefile:
    ```
    $ make CMAKE_PARAMS="-DVTR_IPO_BUILD=off"
    ```
    Note that this option is sticky, so subsequent calls to make don't need to keep specifying VTR_IPO_BUILD, until you want to re-enable it.

    This setting can also be changed with the ccmake tool (i.e. `ccmake build`).

All of these option can be used in combination.
For example, the following will re-build only VPR using 8 parallel jobs with IPO disabled:
```
make CMAKE_PARAMS="-DVTR_IPO_BUILD=off" -j8 vpr
```

# Profiling VTR

## Use GNU Profiler gprof

1. **Installation**: Install `gprof`, `gprof2dot`, and `xdot` (optional).
   1. `gprof` is part of [GNU Binutils](https://www.gnu.org/software/binutils/), which is a commonly-installed package alongside the standard GCC package on most systems. `gprof` should already exist. If not, use `sudo apt install binutils`.
   2. `gprof2dot` requires python3 or conda. You can install with `pip3 install gprof2dot` or `conda install -c conda-forge gprof2dot`.
   3. `xdot` is optional. To install it, use `sudo apt install`.
    ```
    sudo apt install binutils
    pip3 install gprof2dot
    sudo apt install xdot # optional
    ```

    Contact your administrator if you do not have the `sudo` rights.

2. **VPR build**: Use the CMake option below to enable VPR profiler build.
    ```
    make CMAKE_PARAMS="-DVTR_ENABLE_PROFILING=ON" vpr
    ```

3. **Profiling**:
   1. With the profiler build, each time you run the VTR flow script, it will produce an extra file `gmon.out` that contains the raw profile information. Run `gprof` to parse this file. You will need to specify the path to the VPR executable.
        ```
        gprof $VTR_ROOT/vpr/vpr gmon.out > gprof.txt
        ```

   2. Next, use `gprof2dot` to transform the parsed results to a `.dot` file (Graphviz graph description), which describes the graph of your final profile results. If you encounter long function names, specify the `-s` option for a cleaner graph. For other useful options, please refer to its [online documentation](https://github.com/jrfonseca/gprof2dot?tab=readme-ov-file#documentation).
        ```
        gprof2dot -s gprof.txt > vpr.dot
        ```

   - Note: You can chain the above commands to directly produce the `.dot` file:
        ```
        gprof $VTR_ROOT/vpr/vpr gmon.out | gprof2dot -s > vpr.dot
        ```

4. **Visualization**:
   - **Option 1** (Recommended): Use the [Edotor](https://edotor.net/) online Graphviz visualizer.
     1. Open a browser and go to [https://edotor.net/](https://edotor.net/) (on any device, not necessarily the one where VPR is running).
     2. Choose `dot` as the "Engine" at the top navigation bar.
     3. Next, copy and paste `vpr.dot` into the editor space on the left side of the web view.
     4. Then, you can interactively (i.e., pan and zoom) view the results and download an SVG or PNG image.
   - **Option 2**: Use the locally-installed `xdot` visualization tool.
     1. Use `xdot` to view your results:
        ```
        xdot vpr.dot
        ```
     2. To save your results as a PNG file:
        ```
        dot -Tpng -Gdpi=300 vpr.dot > vpr.png
        ```
        Note that you can use the `-Gdpi` option to make your picture clearer if you find the default dpi settings not clear enough.

## Use Linux Perf Tool

1. **Installation**: Install `perf` and `gprof2dot` (optional).
    ```
    sudo apt install linux-tools-common linux-tools-generic
    pip3 install gprof2dot # optional
    ```

2. **VPR build**: *No need* to enable any CMake options for using `perf`, unless you want to utilize specific features, such as `perf annotate`.
    ```
    make vpr
    ```

3. **Profiling**: `perf` needs to know the process ID (i.e., pid) of the running VPR you want to monitor and profile, which can be obtained using the Linux command `top -u <username>`.
   - **Option 1**: Real-time analysis
        ```
        sudo perf top -p <vpr pid>
        ```
   - **Option 2** (Recommended): Record and offline analysis

        Use `perf record` to record the profile data and the call graph. (Note: The argument `lbr` for `--call-graph` only works on Intel platforms. If you encounter issues with call graph recording, please refer to the [`perf record` manual](https://perf.wiki.kernel.org/index.php/Latest_Manual_Page_of_perf-record.1) for more information.)
        ```
        sudo perf record --call-graph lbr -p <vpr pid>
        ```
        After VPR completes its run, or if you stop `perf` with CTRL+C (if you are focusing on a specific portion of the VPR execution), the `perf` tool will produce an extra file `perf.data` containing the raw profile results in the directory where you ran `perf`. You can further analyze the results by parsing this file using `perf report`.
        ```
        sudo perf report -i perf.data
        ```
   - Note 1: The official `perf` [wiki](https://perf.wiki.kernel.org/index.php/Main_Page) and [tutorial](https://perf.wiki.kernel.org/index.php/Tutorial) are highly recommended for those who want to explore more uses of the tool.
   - Note 2: It is highly recommended to run `perf` with `sudo`, but you can find a workaround [here](https://superuser.com/questions/980632/run-perf-without-root-rights) to allow running `perf` without root rights.
   - Note 3: You may also find [Hotspot](https://github.com/KDAB/hotspot) useful if you want to run `perf` with GUI support.

4. **Visualization** (optional): If you want a better illustration of the profiling results, first run the following command to transform the `perf` report into a Graphviz dot graph. The remaining steps are exactly the same as those described under [Use GNU Profiler gprof
](#use-gnu-profiler-gprof).
     ```
     perf script -i perf.data | c++filt | gprof2dot.py -f perf -s > vpr.dot
     ```


# External Subtrees
VTR includes some code which is developed in external repositories, and is integrated into the VTR source tree using [git subtrees](https://www.atlassian.com/blog/git/alternatives-to-git-submodule-git-subtree).

To simplify the process of working with subtrees we use the [`dev/external_subtrees.py`](https://github.com/verilog-to-routing/vtr-verilog-to-routing/blob/master/dev/external_subtrees.py) script.

For instance, running `./dev/external_subtrees.py --list` from the VTR root it shows the subtrees:
```
Component: abc             Path: abc                            URL: https://github.com/berkeley-abc/abc.git       URL_Ref: master
Component: libargparse     Path: libs/EXTERNAL/libargparse      URL: https://github.com/kmurray/libargparse.git    URL_Ref: master
Component: libblifparse    Path: libs/EXTERNAL/libblifparse     URL: https://github.com/kmurray/libblifparse.git   URL_Ref: master
Component: libsdcparse     Path: libs/EXTERNAL/libsdcparse      URL: https://github.com/kmurray/libsdcparse.git    URL_Ref: master
Component: libtatum        Path: libs/EXTERNAL/libtatum         URL: https://github.com/kmurray/tatum.git          URL_Ref: master
```

Code included in VTR by subtrees should *not be modified within the VTR source tree*.
Instead changes should be made in the relevant up-stream repository, and then synced into the VTR tree.

## Updating an existing Subtree

The following are instructions on how to pull in external changes from an
existing subtree. Which instructions to follow depend on if you are changing
the external ref or not.

### External Ref Does Not Change

These instructions are for if the subtree is tracking a ref of a repo which has
changes we want to pull in. For example, if the subtree is tracking main/master.

1. From the VTR root run: `./dev/external_subtrees.py $SUBTREE_NAME`, where `$SUBTREE_NAME` is the name of an existing subtree.

    For example to update the `libtatum` subtree:
    ```shell
    ./dev/external_subtrees.py --update libtatum -m "commit message describing why component is being updated"
    ```

### External Ref Changes

These instructions are for if you want to change the ref that a subtree is
tracking. For example, if you want to change the version of a subtree (which
exists on a different branch).

1. Update `./dev/subtree_config.xml` with the new external ref.

2. Run `git log <internal_path>` and take note of any local changes to the
   subtree. It is bad practice to have local changes to subtrees you cannot
   modify; however, some changes must be made to allow the library to work in
   VTR. The next step will clear all these changes, and they may be important
   and need to be recreated.

3. Delete the subtree folder (the internal path) entirely and commit it to git.
   The issue is that changing the external ref basically creates a new subtree,
   so the regular way of updating the subtree does not work. You need to
   completely wipe all of the code from the old subtree. NOTE: This will remove
   all changes locally made to the subtree.

4. Run `./dev/external_subtrees.py --update $SUBTREE_NAME`. This will pull in
   the most recent version of the subtree, squash the changes, and raise a
   commit.

5. Recreate the local changes from step 2 above, such that the library builds
   without issue; preferrably in a concise way such that the library can be
   easily updated in the future.

## Adding a new Subtree

To add a new external subtree to VTR do the following:

1. Add the subtree specification to `dev/subtree_config.xml`.

    For example to add a subtree name `libfoo` from the `master` branch of `https://github.com/kmurray/libfoo.git` to `libs/EXTERNAL/libfoo` you would add:
    ```xml
    <subtree
        name="libfoo"
        internal_path="libs/EXTERNAL/libfoo"
        external_url="https://github.com/kmurray/libfoo.git"
        default_external_ref="master"/>
    ```
    within the existing `<subtrees>` tag.

    Note that the internal_path directory should not already exist.

    You can confirm it works by running: `dev/external_subtrees.py --list`:
    ```
    Component: abc             Path: abc                            URL: https://github.com/berkeley-abc/abc.git       URL_Ref: master
    Component: libargparse     Path: libs/EXTERNAL/libargparse      URL: https://github.com/kmurray/libargparse.git    URL_Ref: master
    Component: libblifparse    Path: libs/EXTERNAL/libblifparse     URL: https://github.com/kmurray/libblifparse.git   URL_Ref: master
    Component: libsdcparse     Path: libs/EXTERNAL/libsdcparse      URL: https://github.com/kmurray/libsdcparse.git    URL_Ref: master
    Component: libtatum        Path: libs/EXTERNAL/libtatum         URL: https://github.com/kmurray/tatum.git          URL_Ref: master
    Component: libfoo          Path: libs/EXTERNAL/libfoo           URL: https://github.com/kmurray/libfoo.git         URL_Ref: master
    ```
    which shows libfoo is now recognized.

2. Run `./dev/external_subtrees.py --update $SUBTREE_NAME` to add the subtree.

    For the `libfoo` example above this would be:
    ```shell
    ./dev/external_subtrees.py --update libfoo
    ```

    This will create two commits to the repository.
    The first will squash all the upstream changes, the second will merge those changes into the current branch.

## Pushing VTR Changes Back to Upstream Subtree

If there are changes in the VTR repo in a subtree that should be merged back
into the source repo of the subtree, the changes can be pushed back manually.

The instructions above used a Python script to simplify updating subtrees in
VTR. This is fine for pulling in changes from a remote repo; however, it is not
good for pushing changes back. This is because these changes need to be pushed
somewhere, and it is not a good idea to just push it back to the master branch
directly. Instead, it should be pushed to a temporary branch. Then a PR can be
made to bring the changes into the target repo.

To push changes VTR made to a subtree do the following:

1. Create a fork of the target repo. Optionally you can create a branch to be
   the target of the push, or you can just use master.

2. Run:
   ```shell
   cd $VTR_ROOT
   git subtree push --prefix=<subtree_path> <forked_repo_url> <branch_name>
   ```
   The prefix is the internal path to the subtree, as written in
   `dev/subtree_config.xml`.

3. Create a PR from your forked repo to the main repo, sharing the amazing
   changes with the world.

## Tutorial: Syncing Tatum with VTR

This tutorial will show you how to synchronize `libtatum` in VTR and
[Tatum](https://github.com/verilog-to-routing/tatum); however, similar steps
can be done to synchronize any subtree in VTR.

First, we will pull in (update) any changes in Tatum that are not in VTR yet.
On a clean branch (based off master), execute the following:
```shell
cd $VTR_ROOT
./dev/external_subtrees.py --update libtatum -m "Pulling in changes from Tatum."
```
If the output in the terminal says `Subtree is already at commit <commit_hash>`,
then there is nothing to pull in. If it says changes were pulled in, a commit
would have already been made for you. Push these changes to your branch and
raise a PR on VTR to merge these changes in.

After pulling in all the changes from Tatum, without changing branches, we will
push our VTR changes to Tatum. This is a bit more complicated since, as stated
in the section on pushing to subtrees, the changes cannot just be pushed to
master.

Create a fork of Tatum and make sure the master branch of that fork is
synchronized with Tatum's master branch. Then execute the following:
```shell
cd $VTR_ROOT
git subtree push --prefix=libs/EXTERNAL/libtatum <forked_repo_url> master
```
After that command finishes, raise a PR from your forked repo onto the Tatum
repo for the changes to be reviewed and merged in.

## Subtree Rationale

VTR uses subtrees to allow easy tracking of upstream dependencies.

Their main advantages included:
 * Works out-of-the-box: no actions needed post checkout to pull in dependencies (e.g. no `git submodule update --init --recursive`)
 * Simplified upstream version tracking
 * Potential for local changes (although in VTR we do not use this to make keeping in sync easier)

See [here](https://blogs.atlassian.com/2013/05/alternatives-to-git-submodule-git-subtree/) for a more detailed discussion.

# Finding Bugs with Coverity
[Coverity Scan](https://scan.coverity.com) is a static code analysis service which can be used to detect bugs.

## Browsing Defects
To view defects detected do the following:

1. Get a coverity scan account

    Contact a project maintainer for an invitation.

2. Browse the existing defects through the coverity web interface


## Submitting a build
To submit a build to coverity do the following:

1. [Download](https://scan.coverity.com/download) the coverity build tool

2. Configure VTR to perform a *debug* build. This ensures that all assertions are enabled, without assertions coverity may report bugs that are guarded against by assertions. We also set VTR asserts to the highest level.

    ```shell
    #From the VTR root
    mkdir -p build
    cd build
    CC=gcc CXX=g++ cmake -DCMAKE_BUILD_TYPE=debug -DVTR_ASSERT_LEVEL=3 ..
    ```

Note that we explicitly asked for gcc and g++, the coverity build tool defaults to these compilers, and may not like the default 'cc' or 'c++' (even if they are linked to gcc/g++).

3. Run the coverity build tool

    ```shell
    #From the build directory where we ran cmake
    cov-build --dir cov-int make -j8
    ```

4. Archive the output directory

    ```shell
    tar -czvf vtr_coverity.tar.gz cov-int
    ```

5. Submit the archive through the coverity web interface

Once the build has been analyzed you can browse the latest results through the coverity web interface

## No files emitted
If you get the following warning from cov-build:

    [WARNING] No files were emitted.

You may need to configure coverity to 'know' about your compiler. For example:

    ```shell
    cov-configure --compiler `which gcc-7`
    ```

On unix-like systems run `scan-build make` from the root VTR directory.
to output the html analysis to a specific folder, run `scan-build make -o /some/folder`

# Release Procedures

## General Principles

We periodically make 'official' VTR releases.
While we aim to keep the VTR master branch stable through-out development some users prefer to work of off an official release.
Historically this has coincided with the publishing of a paper detailing and carefully evaluating the changes from the previous VTR release.
This is particularly helpful for giving academics a named baseline version of VTR to which they can compare which has a known quality.

In preparation for a release it may make sense to produce 'release candidates' which when fully tested and evaluated (and after any bug fixes) become the official release.

## Checklist

The following outlines the procedure to following when making an official VTR release:

 * Check the code compiles on the list of supported compilers
 * Check that all regression tests pass functionality
 * Update regression test golden results to match the released version
 * Check that all regression tests pass QoR
 * Create a new entry in the CHANGELOG.md for the release, summarizing at a high-level user-facing changes
 * Increment the version number (set in root CMakeLists.txt)
 * Create a git annotated tag (e.g. `v8.0.0`) and push it to github
 * GitHub will automatically create a release based on the tag
 * Add the new change log entry to the [GitHub release description](https://github.com/verilog-to-routing/vtr-verilog-to-routing/releases)
 * Update the [ReadTheDocs configuration](https://readthedocs.org/projects/vtr/versions/) to build and serve documentation for the relevant tag (e.g. `v8.0.0`)
 * Send a release announcement email to the [vtr-announce](mailto:vtr-announce@googlegroups.com) mailing list (make sure to thank all contributors!)




/README.md
--------------------------------------
# Verilog to Routing (VTR)
[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/verilog-to-routing/vtr-verilog-to-routing.git)
[![Build Status](https://github.com/verilog-to-routing/vtr-verilog-to-routing/workflows/Test/badge.svg)](https://github.com/verilog-to-routing/vtr-verilog-to-routing/actions?query=workflow%3ATest) [![Documentation Status](https://readthedocs.org/projects/vtr/badge/?version=latest)](http://docs.verilogtorouting.org/en/latest/)

## Introduction
The Verilog-to-Routing (VTR) project is a world-wide collaborative effort to provide an open-source framework for conducting FPGA architecture and CAD research and development.
The VTR design flow takes as input a Verilog description of a digital circuit, and a description of the target FPGA architecture.
It then performs:
  * Elaboration, Synthesis & Partial Mapping (PARMYS)
  * Logic Optimization & Technology Mapping (ABC)
  * Packing, Placement, Routing & Timing Analysis (VPR)

to generate FPGA speed and area results.
VTR includes a set of benchmark designs known to work with the design flow.

VTR can also produce [FASM](https://fasm.readthedocs.io/en/latest/) to program some commercial FPGAs (via [Symbiflow](https://chipsalliance.org/announcement/2022/02/18/chips-alliance-forms-f4pga-workgroup-to-accelerate-adoption-of-open-source-fpga-tooling/))

| Placement (carry-chains highlighted) | Critical Path |
| ------------------------------------ | ------------- |
| <img src="https://verilogtorouting.org/img/des90_placement_macros.gif" width="350"/> | <img src="https://verilogtorouting.org/img/des90_cpd.gif" width="350"/> |

| Logical Connections | Routing Utilziation |
| ------------------- | ------------------- |
| <img src="https://verilogtorouting.org/img/des90_nets.gif" width="350"/> | <img src="https://verilogtorouting.org/img/des90_routing_util.gif" width="350"/> |


## Documentation
VTR's [full documentation](https://docs.verilogtorouting.org) includes tutorials, descriptions of the VTR design flow, and tool options.

Also check out our [additional support resources](SUPPORT.md).

## License
Generally most code is under MIT license, with the exception of ABC which is distributed under its own (permissive) terms.
See the [full license](LICENSE.md) for details.

## How to Cite
The following paper may be used as a general citation for VTR:

K. E. Murray, O. Petelin, S. Zhong, J. M. Wang, M. ElDafrawy, J.-P. Legault, E. Sha, A. G. Graham, J. Wu, M. J. P. Walker, H. Zeng, P. Patros, J. Luu, K. B. Kent and V. Betz "VTR 8: High Performance CAD and Customizable FPGA Architecture Modelling", ACM TRETS, 2020.

Bibtex:
```
@article{vtr8,
  title={VTR 8: High Performance CAD and Customizable FPGA Architecture Modelling},
  author={Murray, Kevin E. and Petelin, Oleg and Zhong, Sheng and Wang, Jai Min and ElDafrawy, Mohamed and Legault, Jean-Philippe and Sha, Eugene and Graham, Aaron G. and Wu, Jean and Walker, Matthew J. P. and Zeng, Hanqing and Patros, Panagiotis and Luu, Jason and Kent, Kenneth B. and Betz, Vaughn},
  journal={ACM Trans. Reconfigurable Technol. Syst.},
  year={2020}
}
```

## Download
For most users of VTR (rather than active developers) you should download the [latest official VTR release](https://verilogtorouting.org/download), which has been fully regression tested.

## Building
On unix-like systems run `make` from the root VTR directory.

For more details see the [building instructions](BUILDING.md).

#### Docker
We provide a Dockerfile that sets up all the necessary packages for VTR to run.
For more details see [here](dev/DOCKER_DEPLOY.md).

## Mailing Lists
If you have questions, or want to keep up-to-date with VTR, consider joining our mailing lists:

[VTR-Announce](https://groups.google.com/forum/#!forum/vtr-announce): VTR release announcements (low traffic)

[VTR-Users](https://groups.google.com/forum/#!forum/vtr-users): Discussions about using VTR

[VTR-Devel](https://groups.google.com/forum/#!forum/vtr-devel): Discussions about VTR development

[VTR-Commits](https://groups.google.com/forum/#!forum/vtr-commits): VTR revision control commits

## Development
This is the development trunk for the Verilog-to-Routing project.
Unlike the nicely packaged releases that we create, you are working with code in a constant state of flux.
You should expect that the tools are not always stable and that more work is needed to get the flow to run.

For new developers, please follow the [quickstart guide](https://docs.verilogtorouting.org/en/latest/quickstart/).

We follow a feature branch flow, where you create a new branch for new code, test it, measure its Quality of Results, and eventually produce a pull request for review by other developers. Pull requests that meet all the quality and review criteria are then merged into the master branch by a developer with the authority to do so.

In addition to measuring QoR and functionality automatically on pull requests, we do periodic automated testing of the master using BuildBot, and the results can be viewed below to track QoR and stability.
* [Trunk Status](http://builds.verilogtorouting.org:8080/waterfall)
* [QoR Tracking](http://builds.verilogtorouting.org:8080/)

*IMPORTANT*: A broken build must be fixed at top priority. You break the build if your commit breaks any of the automated regression tests.

For additional information see the [developer README](README.developers.md).

### Contributing to VTR
If you'd like to contribute to VTR see our [Contribution Guidelines](CONTRIBUTING.md).

## Contributors
*Please keep this up-to-date*

Professors: Kenneth Kent, Vaughn Betz, Jonathan Rose, Jason Anderson, Peter Jamieson

Research Assistants: Aaron Graham


Graduate Students: Kevin Murray, Jason Luu, Oleg Petelin, Xifian Tang, Mohamed Elgammal, Mohamed Eldafrawy, Jeffrey Goeders, Chi Wai Yu, Andrew Somerville, Ian Kuon, Alexander Marquardt, Andy Ye, Wei Mark Fang, Tim Liu, Charles Chiasson, Panagiotis (Panos) Patros, Jean-Philippe Legault, Aaron Graham, Nasrin Eshraghi Ivari, Maria Patrou, Scott Young, Sarah Khalid, Seyed Alireza Damghani, Harpreet Kaur, Daniel Khadivi, Alireza Azadi


Summer Students: Opal Densmore, Ted Campbell, Cong Wang, Peter Milankov, Scott Whitty, Michael Wainberg, Suya Liu, Miad Nasr, Nooruddin Ahmed, Thien Yu, Long Yu Wang, Matthew J.P. Walker, Amer Hesson, Sheng Zhong, Hanqing Zeng, Vidya Sankaranarayanan, Jia Min Wang, Eugene Sha, Jean-Philippe Legault, Richard Ren, Dingyu Yang, Alexandrea Demmings, Hillary Soontiens, Julie Brown, Bill Hu, David Baines, Mahshad Farahani, Helen Dai, Daniel Zhai

Companies: Intel, Huawei, Lattice, Altera Corporation, Texas Instruments, Google, Antmicro

Funding Agencies: NSERC, Semiconductor Research Corporation





/SUPPORT.md
--------------------------------------
VTR Support Resources
=====================
For support using VPR please use these resources:

1. Check the VTR Documentation: https://docs.verilogtorouting.org

    The VTR documentation includes:
      * Overviews of what VTR is, and how the flow fits together
      * Tutorials on using VTR
      * Detailed descriptions of tools and their command-line options
      * Descriptions of the file-formats used by VTR

2. Contact the VTR users mailing list: vtr-users@googlegroups.com

    The mailing list includes developers and users of VTR.
    If you have a specific usage case not covered by the documentation, someone on the mailing list may be able to help.

3. If you've found a bug or have an idea for an enhancement consider filing an issue. See [here](CONTRIBUTING.md) for more details.



.github/PULL_REQUEST_TEMPLATE.md
--------------------------------------
<!--- Provide a general summary of your changes in the Title above -->

#### Description
<!--- Describe your changes in detail -->

#### Related Issue
<!--- Pull requests should be related to open issues -->
<!--- If suggesting a new feature or change, please discuss it in an issue first -->
<!--- If fixing a bug, there should be an issue describing it with steps to reproduce -->
<!--- Please link to the issue here: -->

#### Motivation and Context
<!--- Why is this change required? What problem does it solve? -->

#### How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->

#### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (change which fixes an issue)
- [ ] New feature (change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)

#### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [ ] My change requires a change to the documentation
- [ ] I have updated the documentation accordingly
- [ ] I have added tests to cover my changes
- [ ] All new and existing tests passed



ISSUE_TEMPLATE/bug_report.md
--------------------------------------
---
name: Bug report
about: Create a report to help us improve

---

<!--- Provide a general summary of the issue in the Title above -->

#### Expected Behaviour
<!--- Tell us what should happen -->

#### Current Behaviour
<!--- Tell us what happens instead of the expected behaviour -->

#### Possible Solution
<!--- Not obligatory, but suggest a fix/reason for the bug -->

#### Steps to Reproduce
<!--- Provide an unambiguous set of steps to reproduce this bug. -->
<!--- Include the exact command line arguments and source files -->
<!-- (e.g. architecture file, netlist) to reproduce, if relevant. -->
1. 
2. 


#### Context
<!--- How has this issue affected you? What are you trying to accomplish? -->
<!--- Providing context helps us come up with a solution that is most useful in the real world -->

#### Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in. -->
* VTR revision used:
* Operating System and version:
* Compiler version:



ISSUE_TEMPLATE/feature_request.md
--------------------------------------
---
name: Feature request
about: Suggest an idea for this project

---

<!--- Provide a general summary of the issue in the Title above -->

#### Proposed Behaviour
<!--- Tell us how your change/improvement should work. -->

#### Current Behaviour
<!--- Explain the difference from current behaviour. -->

#### Possible Solution
<!--- Not obligatory, but suggest ideas for how to implement the feature, -->
<!--- or alternative solutions you have considered. -->

#### Context
<!--- How has this issue affected you? What are you trying to accomplish? -->
<!--- Providing context helps us come up with a solution that is most useful in the real world -->



PULL_REQUEST_TEMPLATE/bug_fix.md
--------------------------------------
---
name: Bug fix
about: Submit a fix for a bug

---
<!--- Provide a general summary of your changes in the Title above -->

#### Description
<!--- Describe your changes in detail -->

#### Related Issue
<!--- Pull requests should be related to open issues -->
<!--- There should be an issue describing it with steps to reproduce -->
<!--- Please link to the issue here: -->

#### Motivation and Context
<!--- Why is this change required? What problem does it solve? -->

#### How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->

#### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [x] Bug fix (change which fixes an issue)
- [ ] Breaking change (fix would cause existing functionality to change)

#### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [ ] I have added tests to cover my changes
- [ ] All new and existing tests passed
- [ ] My change requires a change to the documentation
- [ ] I have updated the documentation accordingly



PULL_REQUEST_TEMPLATE/feature_enhancement.md
--------------------------------------
---
name: Feature enhancement
about: Submit a new, or enhanced feature

---
<!--- Provide a general summary of your changes in the Title above -->

#### Description
<!--- Describe your changes in detail -->

#### Related Issue
<!--- Pull requests should be related to open issues -->
<!--- Please discuss new features/changes in an issue first -->
<!--- Please link to the issue here: -->

#### Motivation and Context
<!--- Why is this change required? What problem does it solve? -->

#### How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->

#### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [x] New feature (change which adds functionality)
- [ ] Breaking change (feature that would cause existing functionality to change)

#### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [ ] I have added tests to cover my feature
- [ ] All new and existing tests passed
- [ ] My change requires a change to the documentation
- [ ] I have updated the documentation accordingly



abc/README.md
--------------------------------------
[![.github/workflows/build-posix.yml](https://github.com/berkeley-abc/abc/actions/workflows/build-posix.yml/badge.svg)](https://github.com/berkeley-abc/abc/actions/workflows/build-posix.yml)
[![.github/workflows/build-windows.yml](https://github.com/berkeley-abc/abc/actions/workflows/build-windows.yml/badge.svg)](https://github.com/berkeley-abc/abc/actions/workflows/build-windows.yml)
[![.github/workflows/build-posix-cmake.yml](https://github.com/berkeley-abc/abc/actions/workflows/build-posix-cmake.yml/badge.svg)](https://github.com/berkeley-abc/abc/actions/workflows/build-posix-cmake.yml)

# ABC: System for Sequential Logic Synthesis and Formal Verification

ABC is always changing but the current snapshot is believed to be stable.

## ABC fork with new features

Here is a [fork](https://github.com/yongshiwo/abc.git) of ABC containing Agdmap, a novel technology mapper for LUT-based FPGAs.  Agdmap is based on a technology mapping algorithm with adaptive gate decomposition [1]. It is a cut enumeration based mapping algorithm with bin packing for simultaneous wide gate decomposition, which is a patent pending technology.

The mapper is developed and maintained by Longfei Fan and Prof. Chang Wu at Fudan University in Shanghai, China.  The experimental results presented in [1] indicate that Agdmap can substantially improve area (by 10% or more) when compared against the best LUT mapping solutions in ABC, such as command "if".

The source code is provided for research and evaluation only. For commercial usage, please contact Prof. Chang Wu at wuchang@fudan.edu.cn.

References:

[1] L. Fan and C. Wu, "FPGA technology mapping with adaptive gate decompostion", ACM/SIGDA FPGA International Symposium on FPGAs, 2023. 

## Compiling:

To compile ABC as a binary, download and unzip the code, then type `make`.
To compile ABC as a static library, type `make libabc.a`.

When ABC is used as a static library, two additional procedures, `Abc_Start()` 
and `Abc_Stop()`, are provided for starting and quitting the ABC framework in 
the calling application. A simple demo program (file src/demo.c) shows how to 
create a stand-alone program performing DAG-aware AIG rewriting, by calling 
APIs of ABC compiled as a static library.

To build the demo program

 * Copy demo.c and libabc.a to the working directory
 * Run `gcc -Wall -g -c demo.c -o demo.o`
 * Run `g++ -g -o demo demo.o libabc.a -lm -ldl -lreadline -lpthread`

To run the demo program, give it a file with the logic network in AIGER or BLIF. For example:

    [...] ~/abc> demo i10.aig
    i10          : i/o =  257/  224  lat =    0  and =   2396  lev = 37
    i10          : i/o =  257/  224  lat =    0  and =   1851  lev = 35
    Networks are equivalent.
    Reading =   0.00 sec   Rewriting =   0.18 sec   Verification =   0.41 sec

The same can be produced by running the binary in the command-line mode:

    [...] ~/abc> ./abc
    UC Berkeley, ABC 1.01 (compiled Oct  6 2012 19:05:18)
    abc 01> r i10.aig; b; ps; b; rw -l; rw -lz; b; rw -lz; b; ps; cec
    i10          : i/o =  257/  224  lat =    0  and =   2396  lev = 37
    i10          : i/o =  257/  224  lat =    0  and =   1851  lev = 35
    Networks are equivalent.

or in the batch mode:

    [...] ~/abc> ./abc -c "r i10.aig; b; ps; b; rw -l; rw -lz; b; rw -lz; b; ps; cec"
    ABC command line: "r i10.aig; b; ps; b; rw -l; rw -lz; b; rw -lz; b; ps; cec".
    i10          : i/o =  257/  224  lat =    0  and =   2396  lev = 37
    i10          : i/o =  257/  224  lat =    0  and =   1851  lev = 35
    Networks are equivalent.

## Compiling as C or C++

The current version of ABC can be compiled with C compiler or C++ compiler.

 * To compile as C code (default): make sure that `CC=gcc` and `ABC_NAMESPACE` is not defined.
 * To compile as C++ code without namespaces: make sure that `CC=g++` and `ABC_NAMESPACE` is not defined.
 * To compile as C++ code with namespaces: make sure that `CC=g++` and `ABC_NAMESPACE` is set to
   the name of the requested namespace. For example, add `-DABC_NAMESPACE=xxx` to OPTFLAGS.

## Building a shared library

 * Compile the code as position-independent by adding `ABC_USE_PIC=1`.
 * Build the `libabc.so` target: 
 
     make ABC_USE_PIC=1 libabc.so

## Bug reporting:

Please try to reproduce all the reported bugs and unexpected features using the latest 
version of ABC available from https://github.com/berkeley-abc/abc

If the bug still persists, please provide the following information:    

 1. ABC version (when it was downloaded from GitHub)
 1. Linux distribution and version (32-bit or 64-bit)
 1. The exact command-line and error message when trying to run the tool
 1. The output of the `ldd` command run on the exeutable (e.g. `ldd abc`).
 1. Versions of relevant tools or packages used.


## Troubleshooting:

 1. If compilation does not start because of the cyclic dependency check, 
try touching all files as follows: `find ./ -type f -exec touch "{}" \;`
 1. If compilation fails because readline is missing, install 'readline' library or
compile with `make ABC_USE_NO_READLINE=1`
 1. If compilation fails because pthreads are missing, install 'pthread' library or
compile with `make ABC_USE_NO_PTHREADS=1`
    * See http://sourceware.org/pthreads-win32/ for pthreads on Windows
    * Precompiled DLLs are available from ftp://sourceware.org/pub/pthreads-win32/dll-latest
 1. If compilation fails in file "src/base/main/libSupport.c", try the following:
    * Remove "src/base/main/libSupport.c" from "src/base/main/module.make"
    * Comment out calls to `Libs_Init()` and `Libs_End()` in "src/base/main/mainInit.c"
 1. On some systems, readline requires adding '-lcurses' to Makefile.

The following comment was added by Krish Sundaresan:

"I found that the code does compile correctly on Solaris if gcc is used (instead of 
g++ that I was using for some reason). Also readline which is not available by default 
on most Sol10 systems, needs to be installed. I downloaded the readline-5.2 package 
from sunfreeware.com and installed it locally. Also modified CFLAGS to add the local 
include files for readline and LIBS to add the local libreadline.a. Perhaps you can 
add these steps in the readme to help folks compiling this on Solaris."

The following tutorial is kindly offered by Ana Petkovska from EPFL:
https://www.dropbox.com/s/qrl9svlf0ylxy8p/ABC_GettingStarted.pdf

## Final remarks:

Unfortunately, there is no comprehensive regression test. Good luck!                                

This system is maintained by Alan Mishchenko <alanmi@berkeley.edu>. Consider also 
using ZZ framework developed by Niklas Een: https://bitbucket.org/niklaseen/abc-zz (or https://github.com/berkeley-abc/abc-zz)



blifexplorer/README.md
--------------------------------------
# BlifExplorer
Odin II visualization tool

The tool visualizes blif files.
It allows to explore the netlist as well as to visualize the step based simulation using random input vectors.

Installation: 

The application requires the Qt5 framework, i.e. in ubuntu: 

	apt-get install qt5-default
	
The application is build using the VTR_ROOT directory make file using the following command:

	make CMAKE_PARAMS="-DWITH_BLIFEXPLORER=on"

Once the project is compiled the application will reside in the blifexplorer directory. 




dev/DOCKER_DEPLOY.md
--------------------------------------
# Building VTR on Docker

## Overview
Docker creates an isolated container on your system so you know that VTR will run without further configurations nor affecting any other work.

Our Docker file sets up this enviroment by installing all necessary Linux packages and applications as well as Perl modules.

## Setup

1. Install docker (Community Edition is free and sufficient for VTR): https://docs.docker.com/engine/install/

2. Clone the VTR project:

    ```
    git clone https://github.com/verilog-to-routing/vtr-verilog-to-routing
    ```

3. CD to the VTR folder and build the docker image:

    ```
    docker build . -t vtrimg
    ```

4. Start docker with the new image:

    ```
    docker run -it -d --name vtr vtrimg
    ```


## Running

1. Attach to the Docker container. Attaching will open a shell on the `/workspace` directory within the container.
The project root directory from the docker build process is copied and placed in the `/workspace` directory.

    ```sh
    # from host computer
    docker exec -it vtr /bin/bash
    ```

1. Verfiy that VTR has been installed correctly:

    ```sh
    # in container
    ./vtr_flow/scripts/run_vtr_task.py regression_tests/vtr_reg_basic/basic_timing
    ```

    The expected output is:

    ```
    k6_N10_mem32K_40nm/single_ff            OK
    k6_N10_mem32K_40nm/single_ff            OK
    k6_N10_mem32K_40nm/single_wire          OK
    k6_N10_mem32K_40nm/single_wire          OK
    k6_N10_mem32K_40nm/diffeq1              OK
    k6_N10_mem32K_40nm/diffeq1              OK
    k6_N10_mem32K_40nm/ch_intrinsics                OK
    k6_N10_mem32K_40nm/ch_intrinsics                OK
    ```

2. Run and/or modify VTR in the usual way.




dev/README.md
--------------------------------------
This folder contains addtional development related tools and documentation



coverity_model/README.rst
--------------------------------------
This directory contains the model file used to suppress
false-positives in Coverity Scan.



src/BUILDING.md
--------------------------------------
```{include} ../../BUILDING.md
:relative-docs: doc/src
```


src/CHANGELOG.md
--------------------------------------
# VTR Change Log
<!-- 
This file documents user-facing changes between releases of the VTR
project. The goal is to concicely communicate to end users what is new
or has changed in a particular release. It should *NOT* just be a dump
of the commit log, as that is far too detailed. Most code re-factoring
does not warrant a change log entry unless it has a significant impact
on the end users (e.g. substantial performance improvements).

Each release's change log should include headings (where releveant) with
bullet points listing what was: 
  - added           (new feature)
  - changed         (change to existing feature behaviour)
  - fixed           (bug fix)
  - deprecated      (features planned for future removal)
  - removed         (previous features which have been removed)

Changes which have landed in the master/trunk but not been released
should be included in the 'Unreleased' section and moved to the releveant
releases' section when released.

In the case of release candidates (e.g. v8.0.0-rc1) the current
set of unreleased changes should be moved under that heading. Any
subsequent fixes to the release candidate would be placed under
'Unreleased', eventually moving into the next release candidate's
heading (e.g. v8.0.0-rc2) when created. Note this means the change log for
subsequent release candidates (e.g. rc2) would only contain new changes
not included in previous release candidates (e.g. rc1).  When the final
(non-release candidate) release is made (e.g. v8.0.0) the change log
should contain all the relevant changes compared to the last non-release
candidate release (e.g. v7.0.0). That is, it should be the concatenation
of the unreleased and any previous release candidates change logs.
-->

_Note that changes from release candidates (e.g. v8.0.0-rc1, v8.0.0-rc2) are included/repeated in the final release (e.g. v8.0.0) change log._

## Unreleased
_The following are changes which have been implemented in the VTR master branch but have not yet been included in an official release._

### Added

### Changed

### Fixed

### Deprecated

### Removed


## v9.0.0 - 2024-12-23

### Added
  * Support for Advanced Architectures:
    * 3D FPGA and RAD architectures.
    * Architectures with hard Networks-on-Chip (NoCs).
    * Distinct horizontal and vertical channel widths and types.
    * Diagonal routing wires and other complex wire shapes (L-shaped, T-shaped, ....).
  
  * New Benchmark Suites:
    * Koios: A deep-learning-focused benchmark suite with various design sizes.
    * Hermes: Benchmarks utilizing hard NoCs.
    * TitanNew: Large benchmarks targeting the Stratix 10 architecture.

  * Commercial FPGAs Architecture Captures:
    * Intel’s Stratix 10 FPGA architecture.
    * AMD’s 7-series FPGA architecture.

  * Parmys Logic Synthesis Flow:
    * Better Verilog language coverage
    * More efficient hard block mapping

  * VPR Graphics Visualizations:
    * New interface for improved usability and underlying graphics rewritten using EZGL/GTK to allow more UI widgets.
    * Algorithm breakpoint visualizations for placement and routing algorithm debugging.
    * User-guided (manual) placement optimization features.
    * Enabled a live connection for client graphical application to VTR engines through sockets (server mode).
    * Interactive timing path analysis (IPA) client using server mode.
   
  * Performance Enhancements:
    * Parallel router for faster inter-cluster routing or flat routing.

  * Re-clustering API to modify packing decisions during the flow.
  * Support for floorplanning and placement constraints.
  * Unified intra- and inter-cluster (flat) routing.
  * Comprehensive web-based VTR utilities and API documentation.
  
### Changed
  * The default values of many command line options (e.g. inner_num is 0.5 instead of 1.0)
  * Changes to placement engine
    * Smart centroid initial placement algorithm.
    * Multiple smart placement directed moves.
    * Reinforcement learning-based placement algorithm.
  * Changes to routing engine
    * Faster lookahead creation.
    * More accurate lookahead for large blocks.
    * More efficient heap and pruning strategies.
    * max `pres_fac` capped to avoid possible numeric issues.
    
    
### Fixed
  * Many algorithmic and coding bugs are fixed in this release
   
### Removed
  * Breadth-first (non-timing-driven) router.
  * Non-linear congestion placement cost.

## v8.0.0 - 2020-03-24

### Added
 * Support for arbitrary FPGA device grids/floorplans
 * Support for clustered blocks with width > 1
 * Customizable connection-block and switch-blocks patterns (controlled from FPGA architecture file)
 * Fan-out dependent routing mux delays
 * VPR can generate/load a routing architecture (routing resource graph) in XML format
 * VPR can load routing from a `.route` file
 * VPR can performing analysis (STA/Power/Area) independently from optimization (via `vpr --analysis`)
 * VPR supports netlist primitives with multiple clocks
 * VPR can perform hold-time (minimum delay) timing analysis
 * Minimum delays can be annotated in the FPGA architecture file
 * Flow supports formal verification of circuit implementation against input netlist
 * Support for generating FASM to drive bitstream generators
 * Routing predictor which predicts and aborts impossible routings early (saves significant run-time during minimum channel width search)
 * Support for minimum routable channel width 'hints' (reduces minimum channel width search run-time if accurate)
 * Improved VPR debugging/verbosity controls
 * VPR can perform basic netlist cleaning (e.g. sweeping dangling logic)
 * VPR graphics visualizations:
   * Critical path during placement/routing
   * Cluster pin utilization heatmap
   * Routing utilization heatmap
   * Routing resource cost heatmaps
   * Placement macros
 * VPR can route constant nets
 * VPR can route clock nets
 * VPR can load netlists in extended BLIF (eBLIF) format
 * Support for generating post-placement timing reports
 * Improved router 'map' lookahead which adapts to routing architecture structure
 * Script to upgrade legacy architecture files (`vtr_flow/scripts/upgrade_arch.py`)
 * Support for Fc overrides which depend on both pin and target wire segment type
 * Support for non-configurable switches (shorts, inline-buffers) used to model structures like clock-trees and non-linear wires (e.g. 'L' or 'T' shapes)
 * Various other features since VTR 7

### Changed
 * VPR will exit with code 1 on errors (something went wrong), and code 2 when unable to implement a circuit (e.g. unroutable)
 * VPR now gives more complete help about command-line options (`vpr -h`)
 * Improved a wide variety of error messages
 * Improved STA timing reports (more details, clearer format)
 * VPR now uses Tatum as its STA engine
 * VPR now detects missmatched architecture (.xml) and implementation (.net/.place/.route) files more robustly
 * Improved router run-time and quality through incremental re-routing and improved handling of high-fanout nets
 * The timing edges within each netlist primitive must now be specified in the <models> section of the architecture file
 * All interconnect tags must have unique names in the architecture file
 * Connection block input pin switch must now be specified in <switchlist> section of the architecture file
 * Renamed switch types buffered/pass_trans to more descriptive tristate/pass_gate in architecture file
 * Require longline segment types to have no switchblock/connectionblock specification
 * Improve naming (true/false -> none/full/instance) and give more control over block pin equivalnce specifications
 * VPR will produce a .route file even if the routing is illegal (aids debugging), however analysis results will not be produced unless `vpr --analsysis` is specified
 * VPR long arguments are now always prefixed by two dashes (e.g. `--route`) while short single-letter arguments are prefixed by a single dash (e.g. `-h`)
 * Improved logic optimization through using a recent 2018 version of ABC and new synthesis script
 * Significantly improved implementation quality (~14% smaller minimum routable channel widths, 32-42% reduced wirelength, 7-10% lower critical path delay)
 * Significantly reduced run-time (~5.5-6.3x faster) and memory usage (~3.3-5x lower)
 * Support for non-contiguous track numbers in externally loaded RR graphs
 * Improved placer quality (reduced cost round-off)
 * Various other changes since VTR 7

### Fixed
 * FPGA Architecture file tags can be in arbitary orders
 * SDC command arguments can be in arbitary orders
 * Numerous other fixes since VTR 7

### Removed
 * Classic VPR timing analyzer
 * IO channel distribution section of architecture file

### Deprecated
 * VPR's breadth-first router (use the timing-driven router, which provides supperiour QoR and Run-time)

### Docker Image
 * A docker image is available for VTR 8.0 release on mohamedelgammal/vtr8:latest. You can run it using the following commands:
```
$ sudo docker pull mohamedelgammal/vtr8:latest
$ sudo docker run -it mohamedelgammal/vtr8:latest
```
 
## v8.0.0-rc2 - 2019-08-01

### Changed
 * Support for non-contiguous track numbers in externally loaded RR graphs
 * Improved placer quality (reduced cost round-off)

## v8.0.0-rc1 - 2019-06-13

### Added
 * Support for arbitrary FPGA device grids/floorplans
 * Support for clustered blocks with width > 1
 * Customizable connection-block and switch-blocks patterns (controlled from FPGA architecture file)
 * Fan-out dependent routing mux delays
 * VPR can generate/load a routing architecture (routing resource graph) in XML format
 * VPR can load routing from a `.route` file
 * VPR can performing analysis (STA/Power/Area) independently from optimization (via `vpr --analysis`)
 * VPR supports netlist primitives with multiple clocks
 * VPR can perform hold-time (minimum delay) timing analysis
 * Minimum delays can be annotated in the FPGA architecture file
 * Flow supports formal verification of circuit implementation against input netlist
 * Support for generating FASM to drive bitstream generators
 * Routing predictor which predicts and aborts impossible routings early (saves significant run-time during minimum channel width search)
 * Support for minimum routable channel width 'hints' (reduces minimum channel width search run-time if accurate)
 * Improved VPR debugging/verbosity controls
 * VPR can perform basic netlist cleaning (e.g. sweeping dangling logic)
 * VPR graphics visualizations:
   * Critical path during placement/routing
   * Cluster pin utilization heatmap
   * Routing utilization heatmap
   * Routing resource cost heatmaps
   * Placement macros
 * VPR can route constant nets
 * VPR can route clock nets
 * VPR can load netlists in extended BLIF (eBLIF) format
 * Support for generating post-placement timing reports
 * Improved router 'map' lookahead which adapts to routing architecture structure
 * Script to upgrade legacy architecture files (`vtr_flow/scripts/upgrade_arch.py`)
 * Support for Fc overrides which depend on both pin and target wire segment type
 * Support for non-configurable switches (shorts, inline-buffers) used to model structures like clock-trees and non-linear wires (e.g. 'L' or 'T' shapes)
 * Various other features since VTR 7

### Changed
 * VPR will exit with code 1 on errors (something went wrong), and code 2 when unable to implement a circuit (e.g. unroutable)
 * VPR now gives more complete help about command-line options (`vpr -h`)
 * Improved a wide variety of error messages
 * Improved STA timing reports (more details, clearer format)
 * VPR now uses Tatum as its STA engine
 * VPR now detects missmatched architecture (.xml) and implementation (.net/.place/.route) files more robustly
 * Improved router run-time and quality through incremental re-routing and improved handling of high-fanout nets
 * The timing edges within each netlist primitive must now be specified in the <models> section of the architecture file
 * All interconnect tags must have unique names in the architecture file
 * Connection block input pin switch must now be specified in <switchlist> section of the architecture file
 * Renamed switch types buffered/pass_trans to more descriptive tristate/pass_gate in architecture file
 * Require longline segment types to have no switchblock/connectionblock specification
 * Improve naming (true/false -> none/full/instance) and give more control over block pin equivalnce specifications
 * VPR will produce a .route file even if the routing is illegal (aids debugging), however analysis results will not be produced unless `vpr --analsysis` is specified
 * VPR long arguments are now always prefixed by two dashes (e.g. `--route`) while short single-letter arguments are prefixed by a single dash (e.g. `-h`)
 * Improved logic optimization through using a recent 2018 version of ABC and new synthesis script
 * Significantly improved implementation quality (~14% smaller minimum routable channel widths, 32-42% reduced wirelength, 7-10% lower critical path delay)
 * Significantly reduced run-time (~5.5-6.3x faster) and memory usage (~3.3-5x lower)
 * Various other changes since VTR 7

### Fixed
 * FPGA Architecture file tags can be in arbitary orders
 * SDC command arguments can be in arbitary orders
 * Numerous other fixes since VTR 7

### Deprecated

### Removed
 * Classic VPR timing analyzer
 * IO channel distribution section of architecture file



src/CONTRIBUTING.md
--------------------------------------
```{include} ../../CONTRIBUTING.md
```


src/LICENSE.md
--------------------------------------
```{include} ../../LICENSE.md
```


src/README.developers.md
--------------------------------------
```{include} ../../README.developers.md
```


src/SUPPORT.md
--------------------------------------
```{include} ../../SUPPORT.md
```


src/contact.md
--------------------------------------

# Contact

## Mailing Lists
VTR maintains several mailing lists.
Most users will be interested in VTR Users and VTR Announce.

* [VTR Announce](https://groups.google.com/forum/#!forum/vtr-announce)

  VTR release announcements (low traffic)

* [VTR Users](https://groups.google.com/forum/#!forum/vtr-users): vtr-users@googlegroups.com

  Discussions about using the VTR project.

* [VTR Devel](https://groups.google.com/forum/#!forum/vtr-devel): vtr-devel@googlegroups.com

  Discussions about VTR development.

* [VTR Commits](https://groups.google.com/forum/#!forum/vtr-commits):

  Revision Control Commits to the VTR project.

## Issue Tracker
Please file bugs on our [issue tracker](https://github.com/verilog-to-routing/vtr-verilog-to-routing/issues).

Pull Requests are welcome!



src/glossary.rst
--------------------------------------
Glossary
========

.. glossary::

    $VTR_ROOT
        The directory containing the root of the VTR source tree.

        For instance, if you extracted/cloned the VTR source into ``/home/myusername/vtr``, your ``$VTR_ROOT`` would be ``/home/myusername/vtr``.

    MWTA
        Minimum Width Transitor Area (MWTA) is a simple process technology independent unit for measuring circuit area.
        It corresponds to the size the smallest (minimum width) transistor area.

        For example, a 1x (unit-sized) CMOS inverter consists of two minimum width transistors (a PMOS pull-up, and NMOS pull-down).

        For more details see :cite:`betz_arch_cad` (the original presentation of the MWTA model), and :cite:`chiasson_coffe` (an updated MWTA model).



src/index.rst
--------------------------------------
.. Verilog-to-Routing documentation master file, created by
   sphinx-quickstart on Fri Jan 22 10:08:21 2016.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Welcome to Verilog-to-Routing's documentation!
==============================================

.. image:: https://www.verilogtorouting.org/img/des90_placement_macros.gif
    :width: 30%

.. image:: https://www.verilogtorouting.org/img/des90_nets.gif
    :width: 30%

.. image:: https://www.verilogtorouting.org/img/des90_routing_util.gif
    :width: 30%

For more information on the Verilog-to-Routing (VTR) project see :ref:`vtr` and :ref:`vtr_cad_flow`.

For documentation and tutorials on the FPGA architecture description language see: :ref:`fpga_architecture_description`.

For more specific documentation about VPR see :ref:`vpr`.


.. toctree::
   :maxdepth: 2
   :caption: Quick Start

   quickstart/index

.. toctree::
   :maxdepth: 2
   :caption: Usage

   vtr/index
   arch/index
   vpr/index
   parmys/index
   odin/index
   abc/index
   tutorials/index
   utils/index

.. toctree::
   :maxdepth: 2
   :caption: Development

   dev/index
   CHANGELOG

.. toctree::
   :maxdepth: 2
   :caption: Appendix

   contact
   glossary
   zreferences

.. toctree::
   :maxdepth: 2
   :caption: API Reference

   api/vpr/index
   api/vtrutil/index
   api/vprinternals/index

Indices and tables
==================

* :ref:`genindex`
* :ref:`search`




src/zreferences.rst
--------------------------------------
Publications & References
=========================

.. bibliography:: z_references.bib
    :all:



abc/index.rst
--------------------------------------
.. _abc:

ABC
===

ABC is included with in VTR to perform technology independant logic optimization and technology mapping.

ABC is developed at UC Berkeley, see the `ABC homepage <http://www.eecs.berkeley.edu/~alanmi/abc/>`_ for details.



vpr/contexts.rst
--------------------------------------
========
Contexts
========

Classes
-------

.. doxygenclass:: VprContext
   :project: vpr
   :members:

Structures
----------

.. doxygenstruct:: AtomContext
   :project: vpr
   :members:

.. doxygenstruct:: ClusteringContext
   :project: vpr
   :members:

.. doxygenstruct:: Context
   :project: vpr
   :members:

.. doxygenstruct:: DeviceContext
   :project: vpr
   :members:

.. doxygenstruct:: PlacementContext
   :project: vpr
   :members:

.. doxygenstruct:: PowerContext
   :project: vpr
   :members:

.. doxygenstruct:: RoutingContext
   :project: vpr
   :members:

.. doxygenstruct:: TimingContext
   :project: vpr
   :members:

.. doxygenstruct:: ServerContext
   :project: vpr
   :members:


vpr/grid.rst
--------------------------------------
========
Grid
========

DeviceGrid
-------

.. doxygenclass:: DeviceGrid
   :project: vpr
   :members:


vpr/index.rst
--------------------------------------
.. _vpr_api:

VPR API
=======

.. toctree::
   :maxdepth: 1

   contexts
   grid
   mapping
   netlist
   route_tree
   rr_graph
   server



vpr/mapping.rst
--------------------------------------
===============
Netlist mapping
===============
As shown in the previous section, there are multiple levels of abstraction (multiple netlists) in VPR which are the ClusteredNetlist and the AtomNetlist. To fully use these netlists, we provide some functions to map between them. 

In this section, we will state how to map between the atom and clustered netlists.

Block Id
--------

Atom block Id to Cluster block Id
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
To get the block Id of a cluster in the ClusteredNetlist from the block Id of one of its atoms in the AtomNetlist:

* Using AtomLookUp class

.. code-block:: cpp

    ClusterBlockId clb_index = g_vpr_ctx.atom().lookup.atom_clb(atom_blk_id);


* Using re_cluster_util.h helper functions
    
.. code-block:: cpp

    ClusterBlockId clb_index = atom_to_cluster(atom_blk_id);


Cluster block Id to Atom block Id
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
To get the block Ids of all the atoms in the AtomNetlist that are packed in one cluster block in ClusteredNetlist:

* Using ClusterAtomLookup class

.. code-block:: cpp

    ClusterAtomsLookup cluster_lookup;
    std::vector<AtomBlockId> atom_ids = cluster_lookup.atoms_in_cluster(clb_index);

* Using re_cluster_util.h helper functions

.. code-block:: cpp

    std::vector<AtomBlockId> atom_ids = cluster_to_atoms(clb_index);


Net Id
------

Atom net Id to Cluster net Id
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
To get the net Id in the ClusteredNetlist from its Id in the AtomNetlist, use AtomLookup class as follows

.. code-block:: cpp

   ClusterNetId clb_net = g_vpr_ctx.atom().lookup.clb_net(atom_net);


Cluster net Id to Atom net Id
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
To get the net Id in the AtomNetlist from its Id in the ClusteredNetlist, use AtomLookup class as follows

.. code-block:: cpp

   ClusterNetId atom_net = g_vpr_ctx.atom().lookup.atom_net(clb_net);



vpr/netlist.rst
--------------------------------------
========
Netlists
========

Netlist
-------

.. doxygenfile:: netlist.h
   :project: vpr
   :sections: detaileddescription


.. doxygenclass:: Netlist
   :project: vpr
   :members:

Clustered  Netlist
------------------

.. doxygenfile:: clustered_netlist.h
   :project: vpr
   :sections: detaileddescription

.. doxygenclass:: ClusteredNetlist 
   :project: vpr
   :members:

Atom Netlist
------------

.. doxygenfile:: atom_netlist.h
   :project: vpr
   :sections: detaileddescription

.. doxygenclass:: AtomNetlist
    :project: vpr
    :members:



vpr/route_tree.rst
--------------------------------------
========
Route Tree
========

RouteTree
-------

.. doxygenfile:: route_tree.h
   :project: vpr
   :sections: detaileddescription


.. doxygenclass:: RouteTree
   :project: vpr
   :members:

RouteTreeNode
-------------

.. doxygenclass:: RouteTreeNode
   :project: vpr
   :members:

RTExploredNode
-------------

.. doxygenclass:: RTExploredNode
   :project: vpr
   :members:



vpr/rr_graph.rst
--------------------------------------
======================
Routing Resource Graph
======================

RRGraphView
-----------

.. doxygenfile:: rr_graph_view.h
   :project: librrgraph
   :sections: briefdescription detaileddescription func prototype user-defined public-func

.. doxygenclass:: RRGraphView
   :project: librrgraph
   :members:

RRGraphBuilder
--------------

.. doxygenfile:: rr_graph_builder.h
   :project: librrgraph
   :sections: briefdescription detaileddescription func prototype user-defined public-func

.. doxygenclass:: RRGraphBuilder
   :project: librrgraph
   :members:

RRSpatialLookup
---------------

.. doxygenfile:: rr_spatial_lookup.h
   :project: librrgraph
   :sections: briefdescription detaileddescription func prototype user-defined public-func

.. doxygenclass:: RRSpatialLookup
   :project: librrgraph
   :members:

rr_graph_utils
---------------

.. doxygenfile:: rr_graph_utils.h
   :project: librrgraph
   :sections: briefdescription detaileddescription func prototype user-defined public-func



vpr/server.rst
--------------------------------------
========
Server
========

server::update
--------------

.. doxygenfunction:: server::update
   :project: vpr

server::GateIO
--------------

.. doxygenclass:: server::GateIO
   :project: vpr
   :members:

server::Task
------------

.. doxygenfile:: commcmd.h
   :project: vpr

.. doxygenclass:: server::Task
   :project: vpr
   :members:

server::TaskResolver
--------------------

.. doxygenclass:: server::TaskResolver
   :project: vpr
   :members:

.. doxygenstruct:: server::CritPathsResult 
   :project: vpr
   :members:

.. doxygenfunction:: server::calc_critical_path
   :project: vpr

.. doxygenenum:: e_timing_report_detail
   :project: vpr

comm::Telegram
--------------

.. doxygenclass:: comm::TelegramHeader
   :project: vpr
   :members:

.. doxygenstruct:: comm::TelegramFrame
   :project: vpr
   :members:

.. doxygenclass:: comm::TelegramBuffer
   :project: vpr
   :members:

.. doxygenclass:: comm::ByteArray
   :project: vpr
   :members:

Parsers
-------

.. doxygenclass:: server::TelegramOptions
   :project: vpr
   :members:

.. doxygenclass:: comm::TelegramParser
   :project: vpr
   :members:


Compression utils
-----------------

.. doxygenfunction:: try_compress
   :project: vpr

.. doxygenfunction:: try_decompress
   :project: vpr



vprinternals/draw_files.rst
--------------------------------------
===============
VPR Draw Files
===============

breakpoint.h/cpp
--------------
.. doxygenfile:: breakpoint.h
   :project: vpr
   :sections: detaileddescription

draw_basic.h/cpp
--------------
.. doxygenfile:: draw_basic.h
   :project: vpr
   :sections: detaileddescription

draw_color.h
--------------
.. doxygenfile:: draw_color.h
   :project: vpr
   :sections: detaileddescription

draw_debug.h/cpp
--------------
.. doxygenfile:: draw_debug.h
   :project: vpr
   :sections: detaileddescription

draw_floorplanning.h/cpp
--------------
.. doxygenfile:: draw_floorplanning.h
   :project: vpr
   :sections: detaileddescription

draw_global.h/cpp
--------------
.. doxygenfile:: draw_global.h
   :project: vpr
   :sections: detaileddescription

draw_mux.h/cpp
--------------
.. doxygenfile:: draw_mux.h
   :project: vpr
   :sections: detaileddescription

draw_noc.h/cpp
--------------
.. doxygenfile:: draw_noc.h
   :project: vpr
   :sections: detaileddescription

draw_rr_edges.h/cpp
--------------
.. doxygenfile:: draw_rr_edges.h
   :project: vpr
   :sections: detaileddescription

draw_rr.h/cpp
--------------
.. doxygenfile:: draw_rr.h
   :project: vpr
   :sections: detaileddescription

draw_searchbar.h/cpp
--------------
.. doxygenfile:: draw_searchbar.h
   :project: vpr
   :sections: detaileddescription

draw_toggle_functions.h/cpp
--------------
.. doxygenfile:: draw_toggle_functions.h
   :project: vpr
   :sections: detaileddescription

draw_triangle.h/cpp
--------------
.. doxygenfile:: draw_triangle.h
   :project: vpr
   :sections: detaileddescription

draw_types.h/cpp
--------------
.. doxygenfile:: draw_types.h
   :project: vpr
   :sections: detaileddescription

draw.h/cpp
--------------
.. doxygenfile:: draw.h
   :project: vpr
   :sections: detaileddescription

hsl.h/cpp
--------------
.. doxygenfile:: hsl.h
   :project: vpr
   :sections: detaileddescription

intra_logic_block.h/cpp
--------------
.. doxygenfile:: intra_logic_block.h
   :project: vpr
   :sections: detaileddescription

manual_moves.h/cpp
--------------
.. doxygenfile:: manual_moves.h
   :project: vpr
   :sections: detaileddescription

save_graphics.h/cpp
--------------
.. doxygenfile:: save_graphics.h
   :project: vpr
   :sections: detaileddescription

search_bar.h/cpp
--------------
.. doxygenfile:: search_bar.h
   :project: vpr
   :sections: detaileddescription

ui_setup.h/cpp
--------------
.. doxygenfile:: ui_setup.h
   :project: vpr
   :sections: detaileddescription



vprinternals/draw_structs.rst
--------------------------------------
===============
VPR Draw Structures
===============

T_Draw_State
----------
.. doxygenstruct:: t_draw_state
   :project: vpr
   :members:

T_Draw_Coords
----------
.. doxygenstruct:: t_draw_coords
   :project: vpr
   :members:



vprinternals/index.rst
--------------------------------------
.. _vpr_internals:

VPR INTERNALS
=======

.. toctree::
   :maxdepth: 1

   draw_structs
   vpr_ui
   draw_files
   vpr_noc
   vpr_router



vprinternals/noc_data_types.rst
--------------------------------------
==============
NoC Data Types
==============

Data Types
----------
.. doxygenfile:: noc_data_types.h
   :project: vpr


vprinternals/noc_link.rst
--------------------------------------
========
NoC Link
========

NocLink
-------
.. doxygenfile:: noc_link.h
   :project: vpr


vprinternals/noc_router.rst
--------------------------------------
==========
NoC Router
==========

NocRouter
---------
.. doxygenfile:: noc_router.h
   :project: vpr


vprinternals/noc_routing.rst
--------------------------------------
===========
NoC Routing
===========

NocRouting
---------------
.. doxygenfile:: noc_routing.h
   :project: vpr

NocRoutingAlgorithmCreator
--------------------------
.. doxygenfile:: noc_routing_algorithm_creator.h
   :project: vpr

XYRouting
---------
.. doxygenfile:: xy_routing.h
   :project: vpr

BFSRouting
----------
.. doxygenfile:: bfs_routing.h
   :project: vpr


vprinternals/noc_storage.rst
--------------------------------------
===========
NoC Storage
===========

NocStorage
----------
.. doxygenfile:: noc_storage.h
   :project: vpr


vprinternals/noc_traffic_flows.rst
--------------------------------------
=================
NoC Traffic Flows
=================

NocTrafficFlows
---------------
.. doxygenfile:: noc_traffic_flows.h
   :project: vpr



vprinternals/router_heap.rst
--------------------------------------
==============
Router Heap
==============

HeapInterface
----------
.. doxygenclass:: HeapInterface
   :project: vpr
   :members:

DAryHeap
----------
.. doxygenclass:: DAryHeap
   :project: vpr



vprinternals/router_lookahead.rst
--------------------------------------
==============
Router Lookahead
==============

e_router_lookahead
----------
.. doxygenenum:: e_router_lookahead
   :project: vpr


RouterLookahead
----------
.. doxygenclass:: RouterLookahead
   :project: vpr
   :members:

MapLookahead
----------
.. doxygenclass:: MapLookahead
   :project: vpr

make_router_lookahead
----------
.. doxygenfunction:: make_router_lookahead
   :project: vpr

get_cached_router_lookahead
----------
.. doxygenfunction:: get_cached_router_lookahead
   :project: vpr

invalidate_router_lookahead_cache
----------
.. doxygenfunction:: invalidate_router_lookahead_cache
   :project: vpr



vprinternals/vpr_noc.rst
--------------------------------------
.. _noc:

=======
VPR NoC
=======

.. toctree::
   :maxdepth: 1

   noc_router
   noc_link
   noc_storage
   noc_traffic_flows
   noc_routing 
   noc_data_types  


vprinternals/vpr_router.rst
--------------------------------------
.. _router:

=======
VPR Router
=======

.. toctree::
   :maxdepth: 1

   router_heap
   router_lookahead



vprinternals/vpr_ui.rst
--------------------------------------
=============================
VPR UI
=============================

UI SETUP
-------
.. doxygenfile:: ui_setup.h
   :project: vpr
   :sections: func


vtrutil/container_utils.rst
--------------------------------------
===============
Container Utils
===============

vtr_hash
--------
.. doxygenfile:: vtr_hash.h
   :project: vtr

vtr_memory
----------
.. doxygenfile:: vtr_memory.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace innerclass func

vtr_pair_util
-------------
.. doxygenfile:: vtr_pair_util.h
   :project: vtr
   :sections: innernamespace innerclass briefdescription detaileddescription user-defined public-func typedef

vtr_map_util
------------
.. doxygenfile:: vtr_map_util.h
   :project: vtr




vtrutil/containers.rst
--------------------------------------
==========
Containers
==========

vtr_vector
----------
.. doxygenfile:: vtr_vector.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace innerclass public-func typedef func

vtr_small_vector
----------------
.. doxygenclass:: vtr::small_vector
   :project: vtr
   :members:

vtr_vector_map
--------------
.. doxygenfile:: vtr_vector_map.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace innerclass public-func typedef func

vtr_linear_map
--------------
.. doxygenfile:: vtr_linear_map.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace func innerclass user-defined public-func typedef

vtr_flat_map
------------
.. doxygenclass:: vtr::flat_map
   :project: vtr
   :members:

.. doxygenclass:: vtr::flat_map2
   :project: vtr
   :members:

.. doxygenfile:: vtr_flat_map.h
   :project: vtr
   :sections: innernamespace func

vtr_bimap
---------
.. doxygenfile:: vtr_bimap.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace innerclass user-defined public-func derivedcompoundref typedef

vtr_vec_id_set
--------------
.. doxygenfile:: vtr_vec_id_set.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace innerclass public-func

vtr_list
--------
.. doxygenfile:: vtr_list.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace struct innerclass user-defined public-func typedef

.. doxygenfunction:: insert_in_vptr_list
   :project: vtr


.. doxygenfunction:: delete_in_vptr_list
   :project: vtr

vtr_ragged_matrix
-----------------
.. doxygenclass:: vtr::FlatRaggedMatrix
   :project: vtr
   :members:

vtr_ndmatrix
------------
.. doxygenfile:: vtr_ndmatrix.h
   :project: vtr
   :sections: innernamespace innerclass briefdescription detaileddescription user-defined public-func typedef

vtr_ndoffsetmatrix
------------------
.. doxygenfile:: vtr_ndoffsetmatrix.h
   :project: vtr
   :sections: innernamespace innerclass briefdescription detaileddescription user-defined public-func typedef

vtr_array_view
--------------
.. doxygenclass:: vtr::array_view_id
   :project: vtr
   :members:

.. doxygenclass:: vtr::array_view
   :project: vtr
   :members:

vtr_string_view
---------------
.. doxygenclass:: vtr::string_view
   :project: vtr
   :members:

vtr_cache
---------
.. doxygenclass:: vtr::Cache
   :project: vtr
   :members:

vtr_dynamic_bitset
------------------
.. doxygenclass:: vtr::dynamic_bitset
   :project: vtr
   :members:



vtrutil/geometry.rst
--------------------------------------
========
Geometry
========

vtr_geometry
------------
.. doxygenfile:: vtr_geometry.h
   :project: vtr
   :sections: briefdescription

.. doxygenclass:: vtr::Point 
   :project: vtr
   :members:

.. doxygenclass:: vtr::Rect 
   :project: vtr
   :members:

.. doxygenclass:: vtr::Line 
   :project: vtr
   :members:

.. doxygenclass:: vtr::RectUnion 
   :project: vtr
   :members:



vtrutil/ids.rst
--------------------------------------
============
IDs - Ranges
============

vtr_range
---------
.. doxygenfile:: vtr_range.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace func innerclass public-func typedef

vtr_strong_id
-------------
.. doxygenfile:: vtr_strong_id.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace 

.. doxygenclass:: vtr::StrongId
   :project: vtr
   :members:

vtr_strong_id_range
-------------------
.. doxygenfile:: vtr_strong_id_range.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace func innerclass user-defined public-func




vtrutil/index.rst
--------------------------------------
.. _vtrutil_api:

VTRUTIL API
============

.. toctree::
   :maxdepth: 1

   ids
   containers
   container_utils
   logging
   geometry
   other



vtrutil/logging.rst
--------------------------------------
=============================
Logging - Errors - Assertions
=============================

vtr_log
-------
.. doxygenfile:: vtr_log.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace func

vtr_error
---------
.. doxygenfile:: vtr_error.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace innerclass user-defined public-func

vtr_assertion
-------------
.. doxygenfile:: vtr_assert.h
   :project: vtr
   :sections: briefdescription detaileddescription

vtr_time
--------
.. doxygenfile:: vtr_time.h
   :project: vtr
   :sections: briefdescription detaileddescription 

.. doxygenclass:: vtr::ScopedStartFinishTimer 
   :project: vtr

.. doxygenclass:: vtr::ScopedFinishTimer 
   :project: vtr

.. doxygenclass:: vtr::ScopedActionTimer
   :project: vtr

.. doxygenclass:: vtr::Timer 
   :project: vtr



vtrutil/other.rst
--------------------------------------
=====
Other
=====

vtr_expr_eval
-------------
.. doxygenfile:: vtr_expr_eval.h
   :project: vtr
   :sections: briefdescription detaileddescription func innernamespace enum

.. doxygenclass:: vtr::Formula_Object
   :project: vtr
   :members:

.. doxygenclass:: vtr::FormulaParser
   :project: vtr
   :members:

.. doxygenclass:: vtr::t_formula_data
   :project: vtr
   :members:

 
vtr_color_map
-------------
.. doxygenfile:: vtr_color_map.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace innerclass user-defined public-func

vtr_digest
----------
.. doxygenfunction:: secure_digest_file
   :project: vtr

.. doxygenfunction:: secure_digest_stream
   :project: vtr

vtr_logic
---------
.. doxygenfile:: vtr_logic.h
   :project: vtr
   :sections: innernamespace enum

vtr_math
--------
.. doxygenfile:: vtr_math.h
   :project: vtr
   :sections: briefdescription innernamespace func prototype

.. doxygenfile:: vtr_math.cpp
   :project: vtr
   :sections: briefdescription innernamespace func prototype

 
vtr_ostream_guard
-----------------
.. doxygenfile:: vtr_ostream_guard.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace func innerclass user-defined public-func

vtr_path
--------
.. doxygenfile:: vtr_path.h
   :project: vtr
   :sections: briefdescription

.. doxygenfunction:: split_ext
   :project: vtr

.. doxygenfunction:: basename
   :project: vtr

.. doxygenfunction:: dirname
   :project: vtr

.. doxygenfunction:: getcwd
   :project: vtr


vtr_random
----------
.. doxygenfile:: vtr_random.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace func prototype

.. doxygenfile:: vtr_random.cpp
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace func prototype

vtr_rusage
----------
.. doxygenfile:: vtr_rusage.cpp
   :project: vtr
   :sections: innernamespace func 

vtr_sentinels
-------------
.. doxygenfile:: vtr_sentinels.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace innerclass 

vtr_string_interning
--------------------
.. doxygenfile:: vtr_string_interning.h
   :project: vtr
   :sections: briefdescription detaileddescription

.. doxygenclass:: vtr::string_internment
   :project: vtr
   :members:

.. doxygenclass:: vtr::interned_string
   :project: vtr
   :members:

.. doxygenclass:: vtr::bound_interned_string
   :project: vtr
   :members:

.. doxygenclass:: vtr::interned_string_iterator
   :project: vtr
   :members:

vtr_token
---------
.. doxygenfile:: vtr_token.h
   :project: vtr

vtr_util
--------
.. doxygenfile:: vtr_util.h
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace func

.. doxygenfile:: vtr_util.cpp
   :project: vtr
   :sections: briefdescription detaileddescription innernamespace func




arch/example_arch.rst
--------------------------------------
Example Architecture Specification
==================================

The listing below is for an FPGA with I/O pads, soft logic blocks (called CLB), configurable memory hard blocks, and fracturable multiplier hard blocks.

Notice that for the CLB, all the inputs are logically equivalent (line 157), and all the outputs are logically equivalent (line 158).
This is usually true for cluster-based logic blocks, as the local routing within the block usually provides full (or near full) connectivity.

However, for other logic blocks, the inputs and all the outputs are not logically equivalent.
For example, consider the memory block (lines 311-316).
Swapping inputs going into the data input port changes the logic of the block because the data output order no longer matches the data input.

.. literalinclude:: example_arch.xml
    :language: xml
    :linenos:
    :emphasize-lines: 157-158, 217-218, 311-316




arch/index.rst
--------------------------------------
.. _fpga_architecture_description:

FPGA Architecture Description
=============================

VTR uses an XML-based architecture description language to describe the targeted FPGA architecture.
This flexible description language allows the user to describe a large number of hypothetical and commercial-like FPGA architectures.

See the :ref:`arch_tutorial` for an introduction to the architecture description langauge.
For a detailed reference on the supported options see the :ref:`arch_reference`.

.. toctree::
   :maxdepth: 2

   reference
   example_arch



arch/reference.rst
--------------------------------------
.. _arch_reference:

Architecture Reference
======================
This section provides a detailed reference for the FPGA Architecture description used by VTR.
The Architecture description uses XML as its representation format.

As a convention, curly brackets ``{`` ``}`` represents an option with each option separated by ``|``.  For example, ``a={1 | 2 | open}`` means field ``a`` can take a value of ``1``, ``2``, or ``open``.

.. _arch_top_level_tags:

Top Level Tags
--------------
The first tag in all architecture files is the ``<architecture>`` tag.
This tag contains all other tags in the architecture file.
The architecture tag contains the following tags:

* ``<models>``
* ``<tiles>``
* ``<layout>``
* ``<device>``
* ``<switchlist>``
* ``<segmentlist>``
* ``<directlist>``
* ``<complexblocklist>``
* ``<noc>``

.. _arch_models:

Recognized BLIF Models (<models>)
---------------------------------
The ``<models>`` tag contains ``<model name="string" never_prune="string">`` tags.
Each ``<model>`` tag describes the BLIF ``.subckt`` model names that are accepted by the FPGA architecture.
The name of the model must match the corresponding name of the BLIF model.

The never_prune flag is optional and can be either:

* false (default)
* true

Normally blocks with no output nets are pruned away by the netlist sweepers in vpr (removed from the netlist); this is the default behaviour. If never_prune = "true" is set on a model, then blocks that are instances of that model will not be swept away during netlist cleanup. This can be helpful for some special blocks that do have only input nets and are required to be placed on the device for some features to be active, so space on the chip is still reserved for them, despite them not driving any connection.
One example is the IDELAYCTRL of the Series7 devices, which takes as input a reference clock and internally controls and synchronizes all the IDELAYs in a specific clock region, with no output net necessary for it to function correctly.

.. note::
    Standard blif structures (``.names``, ``.latch``, ``.input``, ``.output``) are accepted by default, so these models should not be described in the <models> tag.

Each model tag must contain 2 tags: ``<input_ports>`` and ``<output_ports>``.
Each of these contains ``<port>`` tags:

.. arch:tag:: <port name="string" is_clock="{0 | 1} clock="string" combinational_sink_ports="string1 string2 ..."/>

    :req_param name: The port name.

    :opt_param is_clock: Identifies if the port as a clock port.

        .. seealso:: The :ref:`Primitive Timing Modelling Tutorial <arch_model_timing_tutorial>` for usage of ``is_clock`` to model clock control blocks  such as clock generators, clock buffers/gates and clock muxes.

        Default: ``0``

    :opt_param clock: Indicates the port is sequential and controlled by the specified clock (which must be another port on the model marked with ``is_clock=1``). Default: port is treated as combinational (if unspecified)

    :opt_param combinational_sink_ports: A space-separated list of output ports which are combinationally connected to the current input port. Default: No combinational connections (if unspecified)

    Defines the port for a model.

An example models section containing a combinational primitive ``adder`` and a sequential primitive ``single_port_ram`` follows:

.. code-block:: xml

    <models>
      <model name="single_port_ram">
        <input_ports>
          <port name="we" clock="clk" />
          <port name="addr" clock="clk" combinational_sink_ports="out"/>
          <port name="data" clock="clk" combinational_sink_ports="out"/>
          <port name="clk" is_clock="1"/>
        </input_ports>
        <output_ports>
          <port name="out" clock="clk"/>
        </output_ports>
      </model>

      <model name="adder">
        <input_ports>
          <port name="a" combinational_sink_ports="cout sumout"/>
          <port name="b" combinational_sink_ports="cout sumout"/>
          <port name="cin" combinational_sink_ports="cout sumout"/>
        </input_ports>
        <output_ports>
          <port name="cout"/>
          <port name="sumout"/>
        </output_ports>
      </model>
    </models>

Note that for ``single_port_ram`` above, the ports ``we``, ``addr``, ``data``, and ``out`` are sequential since they have a clock specified.
Additionally ``addr`` and ``data`` are shown to be combinationally connected to ``out``; this corresponds to an internal timing path between the ``addr`` and ``data`` input registers, and the ``out`` output registers.

For the ``adder`` the input ports ``a``, ``b`` and ``cin`` are each combinationally connected to the output ports ``cout`` and ``sumout`` (the adder is a purely combinational primitive).

.. seealso:: For more examples of primitive timing modeling specifications see the :ref:`arch_model_timing_tutorial`

.. _arch_global_info:

Global FPGA Information
-----------------------

.. arch:tag:: <tiles>content</tiles>

    Content inside this tag contains a group of ``<pb_type>`` tags that specify the types of functional blocks and their properties.

.. arch:tag:: <layout/>

    Content inside this tag specifies device grid layout.

    .. seealso:: :ref:`arch_grid_layout`

.. arch:tag:: <layer die='int'>content</layer>
    
    Content inside this tag specifies the layout of a single (2D) die; using multiple layer tags one can describe multi-die FPGAs (e.g. 3D stacked FPGAs).

.. arch:tag:: <device>content</device>

    Content inside this tag specifies device information.

    .. seealso:: :ref:`arch_device_info`

.. arch:tag:: <switchlist>content</switchlist>

    Content inside this tag contains a group of <switch> tags that specify the types of switches and their properties.

.. arch:tag:: <segmentlist>content</segmentlist>

    Content inside this tag contains a group of <segment> tags that specify the types of wire segments and their properties.

.. arch:tag:: <complexblocklist>content</complexblocklist>

    Content inside this tag contains a group of ``<pb_type>`` tags that specify the types of functional blocks and their properties.

.. arch:tag:: <noc link_bandwidth="float" link_latency="float" router_latency="float" noc_router_tile_name="string">content</noc>
    
    Content inside this tag specifies the Network-on-Chip (NoC) architecture on the FPGA device and its properties.

.. _arch_grid_layout:

FPGA Grid Layout
----------------
The valid tags within the ``<layout>`` tag are:

.. arch:tag:: <auto_layout aspect_ratio="float">

    :opt_param aspect_ratio:
        The device grid's target aspect ratio (:math:`width / height`)

        **Default**: ``1.0``

    Defines a scalable device grid layout which can be automatically scaled to a desired size.

    .. note:: At most one ``<auto_layout>`` can be specified.

.. _fixed_arch_grid_layout:

.. arch:tag:: <fixed_layout name="string" width="int" height="int">

    :req_param name:
        The unique name identifying this device grid layout.

    :req_param width:
        The device grid width

    :req_param height:
        The device grid height

    Defines a device grid layout with fixed dimensions.

    .. note:: Multiple ``<fixed_layout>`` tags can be specified.

Each ``<auto_layout>`` or ``<fixed_layout>`` tag should contain a set of grid location tags.

FPGA Layer Information
----------------
The layer tag is an optional tag to specify multi-die FPGAs. If not specified, a single-die FPGA with a single die (with index 0) is assumed.

.. arch:tag:: <layer die="int">
    
    :opt_param die:
        Specifies the index of the die; index 0 is assumed to be at the bottom of a stack. 

        **Default**: ``0``

    .. note:: If die number left unspecified, a single-die FPGA (die number = 0) is assumed.
    
    .. code-block:: xml

        <!-- Describe 3D FPGA using layer tag -->
        <fixed_layout name="3D-FPGA" width="device_width" height="device_height">
            <!-- First die (base die) -->
            <layer die="0"/>
                <!-- Specifiy base die Grid layout (e.g., fill with Network-on-Chips) -->
                <fill type="NoC">
            </layer>
            <!-- Second die (upper die) -->
            <layer die="1">
                <!-- Specifiy upper die Grid layout (e.g., fill with logic blocks) -->
                <fill type="LAB">
            </layer>
        </fixed_layout>

    .. note:: Note that all dice have the same width and height. Since we can always fill unused parts of a die with EMPTY blocks this does not restrict us to have the same usable area on each die.

Grid Location Priorities
~~~~~~~~~~~~~~~~~~~~~~~~
Each grid location specification has an associated numeric *priority*.
Larger priority location specifications override those with lower priority.

.. note:: If a grid block is partially overlapped by another block with higher priority the entire lower priority block is removed from the grid.

Empty Grid Locations
~~~~~~~~~~~~~~~~~~~~
Empty grid locations can be specified using the special block type ``EMPTY``.

.. note:: All grid locations default to ``EMPTY`` unless otherwise specified.

.. _grid_expressions:

Grid Location Expressions
~~~~~~~~~~~~~~~~~~~~~~~~~
Some grid location tags have attributes (e.g. ``startx``) which take an *expression* as their argument.
An *expression* can be an integer constant, or simple mathematical formula evaluated when constructing the device grid.

Supported operators include: ``+``, ``-``, ``*``, ``/``, along with ``(`` and ``)`` to override the default evaluation order.
Expressions may contain numeric constants (e.g. ``7``) and the following special variables:

* ``W``: The width of the device
* ``H``: The height of the device
* ``w``: The width of the current block type
* ``h``: The height of the current block type

.. warning:: All expressions are evaluated as integers, so operations such as division may have their result truncated.

As an example consider the expression ``W/2 - w/2``.
For a device width of 10 and a block type of width 3, this would be evaluated as :math:`\lfloor \frac{W}{2} \rfloor - \lfloor \frac{w}{2} \rfloor  = \lfloor \frac{10}{2} \rfloor - \lfloor \frac{3}{2} \rfloor = 5 - 1 = 4`.

Grid Location Tags
~~~~~~~~~~~~~~~~~~

.. arch:tag:: <fill type="string" priority="int"/>

    :req_param type:
        The name of the top-level complex block type (i.e. ``<pb_type>``) being specified.

    :req_param priority:
        The priority of this layout specification.
        Tags with higher priority override those with lower priority.

    Fills the device grid with the specified block type.

    Example:

    .. code-block:: xml

        <!-- Fill the device with CLB blocks -->
        <fill type="CLB" priority="1"/>

    .. figure:: fill_fpga_grid.*

        <fill> CLB example

.. arch:tag:: <perimeter type="string" priority="int"/>

    :req_param type:
        The name of the top-level complex block type (i.e. ``<pb_type>``) being specified.

    :req_param priority:
        The priority of this layout specification.
        Tags with higher priority override those with lower priority.

    Sets the perimeter of the device (i.e. edges) to the specified block type.

    .. note:: The perimeter includes the corners

    Example:

    .. code-block:: xml

        <!-- Create io blocks around the device perimeter -->
        <perimeter type="io" priority="10"/>

    .. figure:: perimeter_fpga_grid.*

        <perimeter> io example

.. arch:tag:: <corners type="string" priority="int"/>

    :req_param type:
        The name of the top-level complex block type (i.e. ``<pb_type>``) being specified.

    :req_param priority:
        The priority of this layout specification.
        Tags with higher priority override those with lower priority.

    Sets the corners of the device to the specified block type.

    Example:

    .. code-block:: xml

        <!-- Create PLL blocks at all corners -->
        <corners type="PLL" priority="20"/>

    .. figure:: corners_fpga_grid.*

        <corners> PLL example

.. arch:tag:: <single type="string" priority="int" x="expr" y="expr"/>

    :req_param type:
        The name of the top-level complex block type (i.e. ``<pb_type>``) being specified.

    :req_param priority:
        The priority of this layout specification.
        Tags with higher priority override those with lower priority.

    :req_param x:
        The horizontal position of the block type instance.

    :req_param y:
        The vertical position of the block type instance.

    Specifies a single instance of the block type at a single grid location.

    Example:

    .. code-block:: xml

        <!-- Create a single instance of a PCIE block (width 3, height 5)
             at location (1,1)-->
        <single type="PCIE" x="1" y="1" priority="20"/>

    .. figure:: single_fpga_grid.*

        <single> PCIE example

.. arch:tag:: <col type="string" priority="int" startx="expr" repeatx="expr" starty="expr" incry="expr"/>

    :req_param type:
        The name of the top-level complex block type (i.e. ``<pb_type>``) being specified.

    :req_param priority:
        The priority of this layout specification.
        Tags with higher priority override those with lower priority.

    :req_param startx:
        An expression specifying the horizontal starting position of the column.

    :opt_param repeatx:
        An expression specifying the horizontal repeat factor of the column.

    :opt_param starty:
        An expression specifying the vertical starting offset of the column.

        **Default:** ``0``

    :opt_param incry:
        An expression specifying the vertical increment between block instantiations within the region.

        **Default:** ``h``

    Creates a column of the specified block type at ``startx``.

    If ``repeatx`` is specified the column will be repeated wherever :math:`x = startx + k \cdot repeatx`, is satisfied for any positive integer :math:`k`.

    A non-zero ``starty`` is typically used if a ``<perimeter>`` tag is specified to adjust the starting position of blocks with height > 1.

    Example:

    .. code-block:: xml

        <!-- Create a column of RAMs starting at column 2, and
             repeating every 3 columns -->
        <col type="RAM" startx="2" repeatx="3" priority="3"/>

    .. figure:: col_fpga_grid.*

        <col> RAM example

    Example:

    .. code-block:: xml

        <!-- Create IO's around the device perimeter -->
        <perimeter type="io" priority=10"/>

        <!-- Create a column of RAMs starting at column 2, and
             repeating every 3 columns. Note that a vertical offset
             of 1 is needed to avoid overlapping the IOs-->
        <col type="RAM" startx="2" repeatx="3" starty="1" priority="3"/>

    .. figure:: col_perim_fpga_grid.*

        <col> RAM and <perimeter> io example

.. arch:tag:: <row type="string" priority="int" starty="expr" repeaty="expr" startx="expr"/>

    :req_param type:
        The name of the top-level complex block type (i.e. ``<pb_type>``) being specified.

    :req_param priority:
        The priority of this layout specification.
        Tags with higher priority override those with lower priority.

    :req_param starty:
        An expression specifying the vertical starting position of the row.

    :opt_param repeaty:
        An expression specifying the vertical repeat factor of the row.

    :opt_param startx:
        An expression specifying the horizontal starting offset of the row.

        **Default:** ``0``

    :opt_param incrx:
        An expression specifying the horizontal increment between block instantiations within the region.

        **Default:** ``w``

    Creates a row of the specified block type at ``starty``.

    If ``repeaty`` is specified the column will be repeated wherever :math:`y = starty + k \cdot repeaty`, is satisfied for any positive integer :math:`k`.

    A non-zero ``startx`` is typically used if a ``<perimeter>`` tag is specified to adjust the starting position of blocks with width > 1.

    Example:

    .. code-block:: xml

        <!-- Create a row of DSPs (width 1, height 3) at
             row 1 and repeating every 7th row -->
        <row type="DSP" starty="1" repeaty="7" priority="3"/>

    .. figure:: row_fpga_grid.*

        <row> DSP example

.. arch:tag:: <region type="string" priority="int" startx="expr" endx="expr repeatx="expr" incrx="expr" starty="expr" endy="expr" repeaty="expr" incry="expr"/>

    :req_param type:
        The name of the top-level complex block type (i.e. ``<pb_type>``) being specified.

    :req_param priority:
        The priority of this layout specification.
        Tags with higher priority override those with lower priority.

    :opt_param startx:
        An expression specifying the horizontal starting position of the region (inclusive).

        **Default:** ``0``

    :opt_param endx:
        An expression specifying the horizontal ending position of the region (inclusive).

        **Default:** ``W - 1``

    :opt_param repeatx:
        An expression specifying the horizontal repeat factor of the column.

    :opt_param incrx:
        An expression specifying the horizontal increment between block instantiations within the region.

        **Default:** ``w``

    :opt_param starty:
        An expression specifying the vertical starting position of the region (inclusive).

        **Default:** ``0``

    :opt_param endy:
        An expression specifying the vertical ending position of the region (inclusive).

        **Default:** ``H - 1``

    :opt_param repeaty:
        An expression specifying the vertical repeat factor of the column.

    :opt_param incry:
        An expression specifying the vertical increment between block instantiations within the region.

        **Default:** ``h``


    Fills the rectangular region defined by (``startx``, ``starty``) and (``endx``, ``endy``) with the specified block type.

    .. note:: ``endx`` and ``endy`` are included in the region

    If ``repeatx`` is specified the region will be repeated wherever :math:`x = startx + k_1*repeatx`, is satisified for any positive integer :math:`k_1`.

    If ``repeaty`` is specified the region will be repeated wherever :math:`y = starty + k_2*repeaty`, is satisified for any positive integer :math:`k_2`.


    Example:

    .. code-block:: xml

        <!-- Fill RAMs withing the rectangular region bounded by (1,1) and (5,4) -->
        <region type="RAM" startx="1" endx="5" starty="1" endy="4" priority="4"/>

    .. figure:: region_single_fpga_grid.*

        <region> RAM example

    Example:

    .. code-block:: xml

        <!-- Create RAMs every 2nd column withing the rectangular region bounded
             by (1,1) and (5,4) -->
        <region type="RAM" startx="1" endx="5" starty="1" endy="4" incrx="2" priority="4"/>

    .. figure:: region_incr_fpga_grid.*

        <region> RAM increment example

    Example:

    .. code-block:: xml

        <!-- Fill RAMs within a rectangular 2x4 region and repeat every 3 horizontal
             and 5 vertical units -->
        <region type="RAM" startx="1" endx="2" starty="1" endy="4" repeatx="3" repeaty="5" priority="4"/>

    .. figure:: region_repeat_fpga_grid.*

        <region> RAM repeat example

    Example:

    .. code-block:: xml

        <!-- Create a 3x3 mesh of NoC routers (width 2, height 2) whose relative positions
             will scale with the device dimensions -->
        <region type="NoC" startx="W/4 - w/2" starty="W/4 - w/2" incrx="W/4" incry="W/4" priority="3"/>

    .. figure:: region_incr_mesh_fpga_grid.*

        <region> NoC mesh example

Grid Layout Example
~~~~~~~~~~~~~~~~~~~

.. code-block:: xml

    <layout>
        <!-- Specifies an auto-scaling square FPGA floorplan -->
        <auto_layout aspect_ratio="1.0">
            <!-- Create I/Os around the device perimeter -->
            <perimeter type="io" priority=10"/>

            <!-- Nothing in the corners -->
            <corners type="EMPTY" priority="100"/>

            <!-- Create a column of RAMs starting at column 2, and
                 repeating every 3 columns. Note that a vertical offset (starty)
                 of 1 is needed to avoid overlapping the IOs-->
            <col type="RAM" startx="2" repeatx="3" starty="1" priority="3"/>

            <!-- Create a single PCIE block along the bottom, overriding
                 I/O and RAM slots -->
            <single type="PCIE" x="3" y="0" priority="20"/>

            <!-- Create an additional row of I/Os just above the PCIE,
                 which will not override RAMs -->
            <row type="io" starty="5" priority="2"/>

            <!-- Fill remaining with CLBs -->
            <fill type="CLB" priority="1"/>
        </auto_layout>
    </layout>

.. figure:: fpga_grid_example.*

    Example FPGA grid

.. _arch_device_info:

FPGA Device Information
-----------------------
The tags within the ``<device>`` tag are:

.. arch:tag:: <sizing R_minW_nmos="float" R_minW_pmos="float"/>

    :req_param R_minW_nmos:
        The resistance of minimum-width nmos transistor.
        This data is used only by the area model built into VPR.

    :req_param R_minW_pmos:
        The resistance of minimum-width pmos transistor.
        This data is used only by the area model built into VPR.

    :required: Yes

    Specifies parameters used by the area model built into VPR.


.. arch:tag:: <connection_block input_switch_name="string"/>

        .. figure:: ipin_diagram.*

            Input Pin Diagram.


    :req_param switch_name:
        Specifies the name of the ``<switch>`` in the ``<switchlist>`` used to connect routing tracks to block input pins (i.e. the input connection block switch).

    :required: Yes


.. arch:tag:: <area grid_logic_tile_area="float"/>

    :required: Yes

    Specifies the default area used by each 1x1 grid logic tile (in :term:`MWTAs<MWTA>`), *excluding routing*.

    Used for an area estimate of the amount of area taken by all the functional blocks.

    .. note:: This value can be overriden for specific ``<pb_type>``s with the ``area`` attribute.


.. arch:tag:: <switch_block type="{wilton | subset | universal | custom}" fs="int"/>

    :req_param type: The type of switch block to use.
    :req_param fs: The value of :math:`F_s`


    :required: Yes

    This parameter controls the pattern of switches used to connect the (inter-cluster) routing segments. Three fairly simple patterns can be specified with a single keyword each, or more complex custom patterns can be specified.

    **Non-Custom Switch Blocks:**

    When using bidirectional segments, all the switch blocks have :math:`F_s` = 3 :cite:`brown_fpgas`.
    That is, whenever horizontal and vertical channels intersect, each wire segment can connect to three other wire segments.
    The exact topology of which wire segment connects to which can be one of three choices.
    The subset switch box is the planar or domain-based switch box used in the Xilinx 4000 FPGAs -- a wire segment in track 0 can only connect to other wire segments in track 0 and so on.
    The wilton switch box is described in :cite:`wilton_phd`, while the universal switch box is described in :cite:`chang_universal_switch_modules`.
    To see the topology of a switch box, simply hit the "Toggle RR" button when a completed routing is on screen in VPR.
    In general the wilton switch box is the best of these three topologies and leads to the most routable FPGAs.

    When using unidirectional segments, one can specify an :math:`F_s` that is any multiple of 3.
    We use a modified wilton switch block pattern regardless of the specified switch_block_type.
    For all segments that start/end at that switch block, we follow the wilton switch block pattern.
    For segments that pass through the switch block that can also turn there, we cannot use the wilton pattern because a unidirectional segment cannot be driven at an intermediate point, so we assign connections to starting segments following a round robin scheme (to balance mux size).

    .. note:: The round robin scheme is not tileable.

    **Custom Switch Blocks:**

    Specifying ``custom`` allows custom switch blocks to be described under the ``<switchblocklist>`` XML node, the format for which is described in :ref:`custom_switch_blocks`.
    If the switch block is specified as ``custom``, the ``fs`` field does not have to be specified, and will be ignored if present.

.. arch:tag:: <chan_width_distr>content</chan_width_distr>

    Content inside this tag is only used when VPR is in global routing mode.
    The contents of this tag are described in :ref:`global_routing_info`.

.. arch:tag:: <default_fc in_type="{frac|abs}" in_val="{int|float}" out_type="{frac|abs}" out_val="{int|float}"/>

    This defines the default Fc specification, if it is not specified within a ``<fc>`` tag inside a top-level complex block.
    The attributes have the same meaning as the :ref:`\<fc\> tag attributes <arch_fc>`.

.. _arch_switches:

Switches
--------
The tags within the ``<switchlist>`` tag specifies the switches used to connect wires and pins together.

.. arch:tag::
    <switch type="{mux|tristate|pass_gate|short|buffer}" name="string" R="float" Cin="float" Cout="float" Cinternal="float" Tdel="float" buf_size="{auto|float}" mux_trans_size="float", power_buf_size="int"/>

    Describes a switch in the routing architecture.

    **Example:**

    .. code-block:: xml

        <switch type="mux" name="my_awesome_mux" R="551" Cin=".77e-15" Cout="4e-15" Cinternal="5e-15" Tdel="58e-12" mux_trans_size="2.630740" buf_size="27.645901"/>


    :req_param type:

        The type of switch:

        * ``mux``: An isolating, configurable multiplexer

        * ``tristate``: An isolating, configurable tristate-able buffer

        * ``pass_gate``: A *non-isolating*, configurable pass gate

        * ``short``: A *non-isolating*, *non-configurable* electrical short (e.g. between two segments).

        * ``buffer``: An isolating, *non-configurable* non-tristate-able buffer (e.g. in-line along a segment).

        **Isolation**

        Isolating switches include a buffer which partition their input and output into separate DC-connected sub-circuits.
        This helps reduce RC wire delays.

        *Non-isolating* switch do **not** isolate their input and output, which can increase RC wire delays.

        **Configurablity**

        Configurable switches can be turned on/off at configuration time.

        *Non-configurable* switches can **not** be controlled at configuration time.
        These are typically used to model non-optional connections such as electrical shorts and in-line buffers.

    :req_param name: A unique name identifying the switch
    :req_param R: Resistance of the switch.
    :req_param Cin:  Input capacitance of the switch.
    :req_param Cout:  Output capacitance of the switch.

    :opt_param Cinternal: 
        Since multiplexers and tristate buffers are modeled as a       
        parallel stream of pass transistors feeding into a buffer,     
        we would expect an additional "internal capacitance" to arise when the    
        pass transistor is enabled and the signal must propogate to    
        the buffer. See diagram of one stream below:: 
        
            Pass Transistor                                          
                      |                                                   
                    -----                                                 
                    -----      Buffer                                     
                   |     |       |\                                       
             ------       -------| \--------                              
               |             |   | /    |                                 
             =====         ===== |/   =====                               
             =====         =====      =====                               
               |             |          |                                 
             Input C    Internal C    Output C                             
    
        .. note:: Only specify a value for multiplexers and/or tristate switches.

    :opt_param Tdel:

        Intrinsic delay through the switch.
        If this switch was driven by a zero resistance source, and drove a zero capacitance load, its delay would be: :math:`T_{del} + R \cdot C_{out}`.

        The ‘switch’ includes both the mux and buffer ``mux`` type switches.

        .. note:: Required if no ``<Tdel>`` tags are specified

        .. note:: A ``<switch>``'s resistance (``R``) and output capacitance (``Cout``) have no effect on delay when used for the input connection block, since VPR does not model the resistance/capacitance of block internal wires.

    :opt_param buf_size:

        Specifies the buffer size in minimum-width transistor area (:term`MWTA`) units.

        If set to ``auto``, sized automatically from the R value.
        This allows you to use timing models without R’s and C’s and still be able to measure area.

        .. note:: Required for all **isolating** switch types.

        **Default:** ``auto``

    :opt_param mux_trans_size:
        Specifies the size (in minimum width transistors) of each transistor in the two-level mux used by ``mux`` type switches.

        .. note:: Valid only for ``mux`` type switches.

    :opt_param power_buf_size: *Used for power estimation.* The size is the drive strength of the buffer, relative to a minimum-sized inverter.

    .. arch:tag:: <Tdel num_inputs="int" delay="float"/>

        Instead of specifying a single Tdel value, a list of Tdel values may be specified for different values of switch fan-in.
        Delay is linearly extrapolated/interpolated for any unspecified fanins based on the two closest fanins.


        :req_param num_inputs: The number of switch inputs (fan-in)
        :req_param delay: The intrinsic switch delay when the switch topology has the specified number of switch inputs

        **Example:**

        .. code-block:: xml

            <switch type="mux" name="my_mux" R="522" Cin="3.1e-15" Cout="3e-15" Cinternal="5e-15" mux_trans_size="1.7" buf_size="23">
                <Tdel num_inputs="12" delay="8.00e-11"/>
                <Tdel num_inputs="15" delay="8.4e-11"/>
                <Tdel num_inputs="20" delay="9.4e-11"/>
            </switch>


.. _global_routing_info:

Global Routing Information
~~~~~~~~~~~~~~~~~~~~~~~~~~
If global routing is to be performed, channels in different directions and in different parts of the FPGA can be set to different relative widths.
This is specified in the content within the ``<chan_width_distr>`` tag.

.. note:: If detailed routing is to be performed, only uniform distributions may be used

.. arch:tag:: <x distr="{gaussian|uniform|pulse|delta}" peak="float" width=" float" xpeak=" float" dc=" float"/>

    :req_param distr: The channel width distribution function
    :req_param peak: The peak value of the distribution
    :opt_param width: The width of the distribution. Required for ``pulse`` and ``gaussian``.
    :opt_param xpeak: Peak location horizontally. Required for ``pulse``, ``gaussian`` and ``delta``.
    :opt_param dc: The DC level of the distribution. Required for ``pulse``, ``gaussian`` and ``delta``.

    Sets the distribution of tracks for the x-directed channels -- the channels that run horizontally.

    Most values are from 0 to 1.

    If uniform is specified, you simply specify one argument, peak.
    This value (by convention between 0 and 1) sets the width of the x-directed core channels relative to the y-directed channels and the channels between the pads and core.
    :numref:`fig_arch_channel_distribution` should clarify the specification of uniform (dashed line) and pulse (solid line) channel widths.
    The gaussian keyword takes the same four parameters as the pulse keyword, and they are all interpreted in exactly the same manner except that in the gaussian case width is the standard deviation of the function.

    .. _fig_arch_channel_distribution:

    .. figure:: channel_distribution.*

        Channel Distribution

    The delta function is used to specify a channel width distribution in which all the channels have the same width except one.
    The syntax is chan_width_x delta peak xpeak dc.
    Peak is the extra width of the single wide channel.
    Xpeak is between 0 and 1 and specifies the location within the FPGA of the extra-wide channel -- it is the fractional distance across the FPGA at which this extra-wide channel lies.
    Finally, dc specifies the width of all the other channels.
    For example, the statement chan_width_x delta 3 0.5 1 specifies that the horizontal channel in the middle of the FPGA is four times as wide as the other channels.

    Examples::

        <x distr="uniform" peak="1"/>
        <x distr="gaussian" width="0.5" peak="0.8" xpeak="0.6" dc="0.2"/>

.. arch:tag:: <y distr="{gaussian|uniform|pulse|delta}" peak=" float" width=" float" xpeak=" float" dc=" float"/>

    Sets the distribution of tracks for the y-directed channels.

    .. seealso:: <x distr>

.. _arch_tiles:

Physical Tiles
--------------

The content within the ``<tiles>`` describes the physical tiles available in the FPGA.
Each tile type is specified with the ``<tile>`` tag withing the ``<tiles>`` tag.

Tile
~~~~
.. arch:tag:: <tile name="string" capacity="int" width="int" height="int" area="float"/>

    A tile refers to a placeable element within an FPGA architecture and describes its physical compositions on the grid.
    The following attributes are applicable to each tile.
    The only required one is the name of the tile.

    **Attributes:**

    :req_param name: The name of this tile.

        The name must be unique with respect to any other sibling ``<tile>`` tag.

    :opt_param width: The width of the block type in grid tiles

        **Default:** ``1``

    :opt_param height: The height of the block type in grid tiles

        **Default:** ``1``

    :opt_param area: The logic area (in :term:`MWTA`) of the block type

        **Default:** from the ``<area>`` tag

The following tags are common to all ``<tile>`` tags:


.. arch:tag:: <sub_tile name"string" capacity="{int}">

    .. seealso:: For a tutorial on describing the usage of sub tiles for ``heterogeneous tiles`` (tiles which support multiple instances of the same or different :ref:`arch_complex_blocks`) definition see :ref:`heterogeneous_tiles_tutorial`.

    Describes one or many sub tiles corresponding to the physical tile.
    Each sub tile is identifies a set of one or more stack location on a specific x, y grid location.

    **Attributes:**

    :req_param name: The name of this tile.

        The name must be unique with respect to any other sibling ``<tile>`` tag.

    :opt_param capacity: The number of instances of this block type at each grid location.

        **Default:** ``1``

        For example:

        .. code-block:: xml

            <sub_tile name="IO" capacity="2"/>
                ...
            </sub_tile>

        specifies there are two instances of the block type ``IO`` at each of its grid locations.

    .. note:: It is mandatory to have at least one sub tile definition for each physical tile.

    .. arch:tag:: <input name="string" num_pins="int" equivalent="{none|full}" is_non_clock_global="{true|false}"/>

        Defines an input port.
        Multple input ports are described using multiple ``<input>`` tags.

        :req_param name: Name of the input port.
        :req_param num_pins: Number of pins the input port has.

        :opt_param equivalent:

            Describes if the pins of the port are logically equivalent.
            Input logical equivalence means that the pin order can be swapped without changing functionality.
            For example, an AND gate has logically equivalent inputs because you can swap the order of the inputs and it’s still correct; an adder, on the otherhand, is not logically equivalent because if you swap the MSB with the LSB, the results are completely wrong.
            LUTs are also considered logically equivalent since the logic function (LUT mask) can be rotated to account for pin swapping.

            * ``none``: No input pins are logically equivalent.

                Input pins can not be swapped by the router. (Generates a unique SINK rr-node for each block input port pin.)

            * ``full``: All input pins are considered logically equivalent (e.g. due to logical equivalance or a full-crossbar within the cluster).

                All input pins can be swapped without limitation by the router. (Generates a single SINK rr-node shared by each input port pin.)

            **default:** ``none``

        :opt_param is_non_clock_global:

            .. note:: Applies only to top-level pb_type.

            Describes if this input pin is a global signal that is not a clock.
            Very useful for signals such as FPGA-wide asynchronous resets.
            These signals have their own dedicated routing channels and so should not use the general interconnect fabric on the FPGA.


    .. arch:tag:: <output name="string" num_pins="int" equivalent="{none|full|instance}"/>

        Defines an output port.
        Multple output ports are described using multiple ``<output>`` tags

        :req_param name: Name of the output port.
        :req_param num_pins: Number of pins the output port has.

        :opt_param equivalent:

            Describes if the pins of the output port are logically equivalent:

            * ``none``: No output pins are logically equivalent.

                Output pins can not be swapped by the router. (Generates a unique SRC rr-node for each block output port pin.)

            * ``full``: All output pins are considered logically equivalent.

                All output pins can be swapped without limitation by the router. For example, this option would be appropriate to model an output port which has a full crossbar between it and the logic within the block that drives it. (Generates a single SRC rr-node shared by each output port pin.)

            * ``instance``: Models that sub-instances within a block (e.g. LUTs/BLEs) can be swapped to achieve a limited form of output pin logical equivalence.

                Like ``full``, this generates a single SRC rr-node shared by each output port pin. However, each net originating from this source can use only one output pin from the equivalence group. This can be useful in modeling more complex forms of equivalence in which you can swap which BLE implements which function to gain access to different inputs.

                .. warning:: When using ``instance`` equivalence you must be careful to ensure output swapping would not make the cluster internal routing (previously computed by the clusterer) illegal; the tool does not update the cluster internal routing due to output pin swapping.

            **Default:** ``none``


    .. arch:tag:: <clock name="string" num_pins="int" equivalent="{none|full}"/>

        Describes a clock port.
        Multple clock ports are described using multiple ``<clock>`` tags.
        *See above descriptions on inputs*

    .. arch:tag:: <equivalent_sites>

        .. seealso:: For a step-by-step walkthrough on describing equivalent sites see :ref:`equivalent_sites_tutorial`.

        Describes the Complex Blocks that can be placed within a tile.
        Each physical tile can comprehend a number from 1 to N of possible Complex Blocks, or ``sites``.
        A ``site`` corresponds to a top-level Complex Block that must be placeable in at least 1 physical tile locations.

        .. arch:tag:: <site pb_type="string" pin_mapping="string"/>

        :req_param pb_type: Name of the corresponding pb_type.

        :opt_param pin_mapping: Specifies whether the pin mapping between physical tile and logical pb_type:

                * ``direct``: the pin mapping does not need to be specified as the tile pin definition is equal to the corresponding pb_type one;
                * ``custom``: the pin mapping is user-defined.


                **Default:** ``direct``

            **Example: Equivalent Sites**

            .. code-block:: xml

                <equivalent_sites>
                    <site pb_type="MLAB_SITE" pin_mapping="direct"/>
                </equivalent_sites>

            .. arch:tag:: <direct from="string" to="string">

                Desctibes the mapping of a physical tile's port on the logical block's (pb_type) port.
                ``direct`` is an option sub-tag of ``site``.

                .. note:: This tag is needed only if the pin_mapping of the ``site`` is defined as ``custom``

                Attributes:
                    - ``from`` is relative to the physical tile pins
                    - ``to`` is relative to the logical block pins

                    .. code-block:: xml

                        <direct from="MLAB_TILE.CX" to="MLAB_SITE.BX"/>


    .. arch:tag:: <fc in_type="{frac|abs}" in_val="{int|float}" out_type="{frac|abs}" out_val="{int|float}">

        :req_param in_type:
            Indicates how the :math:`F_c` values for input pins should be interpreted.

            ``frac``: The fraction of tracks of each wire/segment type.

            ``abs``: The absolute number of tracks of each wire/segment type.

        :req_param in_val:
            Fraction or absolute number of tracks to which each input pin is connected.

        :req_param out_type:
            Indicates how the :math:`F_c` values for output pins should be interpreted.

            ``frac``: The fraction of tracks of each wire/segment type.

            ``abs``: The absolute number of tracks of each wire/segment type.

        :req_param out_val:
            Fraction or absolute number of wires/segments to which each output pin connects.


        Sets the number of tracks/wires to which each logic block pin connects in each channel bordering the pin.

        The :math:`F_c` value :cite:`brown_fpgas` is interpreted as applying to each wire/segment type *individually* (see example).

        When generating the FPGA routing architecture VPR will try to make 'good' choices about how pins and wires interconnect; for more details on the criteria and methods used see :cite:`betz_automatic_generation_of_fpga_routing`.


        .. note:: If ``<fc>`` is not specified for a complex block, the architecture's ``<default_fc>`` is used.

        .. note:: For unidirection routing architectures absolute :math:`F_c` values must be a multiple of 2.

        **Example:**

        Consider a routing architecture with 200 length 4 (L4) wires and 50 length 16 (L16) wires per channel, and the following Fc specification:

        .. code-block:: xml

            <fc in_type="frac" in_val="0.1" out_type="abs" out_val="25">

        The above specifies that each:

        * input pin connects to 20 L4 tracks (10% of the 200 L4s) and 5 L16 tracks (10% of the 50 L16s), and

        * output pin connects to 25 L4 tracks and 25 L16 tracks.



        **Overriding Values:**

        .. arch:tag:: <fc_override fc_type="{frac|abs}" fc_val="{int|float}", port_name="{string}" segment_name="{string}">

            Allows :math:`F_c` values to be overriden on a port or wire/segment type basis.

            :req_param fc_type:
                Indicates how the override :math:`F_c` value should be interpreted.

                ``frac``: The fraction of tracks of each wire/segment type.

                ``abs``: The absolute number of tracks of each wire/segment type.

            :req_param fc_val:
                Fraction or absolute number of tracks in a channel.

            :opt_param port_name:
                The name of the port to which this override applies.
                If left unspecified this override applies to all ports.

            :opt_param segment_name:
                The name of the segment (defined under ``<segmentlist>``) to which this override applies.
                If left unspecified this override applies to all segments.

            .. note:: At least one of ``port_name`` or ``segment_name`` must be specified.


            **Port Override Example: Carry Chains**

            If you have complex block pins that do not connect to general interconnect (eg. carry chains), you would use the ``<fc_override>`` tag, within the ``<fc>`` tag, to specify them:

            .. code-block:: xml

                <fc_override fc_type="frac" fc_val="0" port_name="cin"/>
                <fc_override fc_type="frac" fc_val="0" port_name="cout"/>

            Where the attribute ``port_name`` is the name of the pin (``cin`` and ``cout`` in this example).


            **Segment Override Example:**

            It is also possible to specify per ``<segment>`` (i.e. routing wire) overrides:

            .. code-block:: xml

                <fc_override fc_type="frac" fc_val="0.1" segment_name="L4"/>

            Where the above would cause all pins (both inputs and outputs) to use a fractional :math:`F_c` of ``0.1`` when connecting to segments of type ``L4``.

            **Combined Port and Segment Override Example:**

            The ``port_name`` and ``segment_name`` attributes can be used together.
            For example:

            .. code-block:: xml

                <fc_override fc_type="frac" fc_val="0.1" port_name="my_input" segment_name="L4"/>
                <fc_override fc_type="frac" fc_val="0.2" port_name="my_output" segment_name="L4"/>

            specifies that port ``my_input`` use a fractional :math:`F_c` of ``0.1`` when connecting to segments of type ``L4``, while the port ``my_output`` uses a fractional :math:`F_c` of ``0.2`` when connecting to segments of type ``L4``.
            All other port/segment combinations would use the default :math:`F_c` values.

    .. arch:tag:: <pinlocations pattern="{spread|perimeter|custom}">

        :req_param pattern:
            * ``spread`` denotes that the pins are to be spread evenly on all sides of the complex block.

                .. note:: *Includes* internal sides of blocks with width > 1 and/or height > 1.

            * ``perimeter`` denotes that the pins are to be spread evenly on perimeter sides of the complex block.

                .. note:: *Excludes* the internal sides of blocks with width > 1 and/or height > 1.

            * ``spread_inputs_perimeter_outputs`` denotes that inputs pins are to be spread on all sides of the complex block, but output pins are to be spread only on perimeter sides of the block.

                .. note:: This is useful for ensuring outputs do not connect to wires which fly-over a width > 1 and height > 1 block (e.g. if using ``short`` or ``buffer`` connections instead of a fully configurable switch block within the block).

            * ``custom`` allows the architect to specify specifically where the pins are to be placed using ``<loc>`` tags.

        Describes the locations where the input, output, and clock pins are distributed in a complex logic block.

        .. arch:tag:: <loc side="{left|right|bottom|top}" xoffset="int" yoffset="int">name_of_complex_logic_block.port_name[int:int] ... </loc>

            .. note:: ``...`` represents repeat as needed. Do not put ``...`` in the architecture file.

            :req_param side: Specifies which of the four sides of a grid location the pins in the contents are located.

            :opt_param xoffset:
                Specifies the horizontal offset (in grid units) from block origin (bottom left corner).
                The offset value must be less than the width of the block.

                **Default:** ``0``

            :opt_param yoffset:
                Specifies the vertical offset (in grid units) from block origin (bottom left corner).
                The offset value must be less than the height of the block.

                **Default:** ``0``

        Physical equivalence for a pin is specified by listing a pin more than once for different locations.
        For example, a LUT whose output can exit from the top and bottom of a block will have its output pin specified twice: once for the top and once for the bottom.

        .. note:: If the ``<pinlocations>`` tag is missing, a ``spread`` pattern is assumed.

.. arch:tag:: <switchblock_locations pattern="{external_full_internal_straight|all|external|internal|none|custom}" internal_switch="string">

    Describes where global routing switchblocks are created in relation to the complex block.

    .. note:: If the ``<switchblock_locations>`` tag is left unspecified the default pattern is assumed.

    :opt_param pattern:

        * ``external_full_internal_straight``: creates *full* switchblocks outside and *straight* switchblocks inside the complex block

        * ``all``: creates switchblocks wherever routing channels cross

        * ``external``: creates switchblocks wherever routing channels cross *outside* the complex block

        * ``internal``: creates switchblocks wherever routing channels cross *inside* the complex block

        * ``none``: denotes that no switchblocks are created for the complex block

        * ``custom``: allows the architect to specify custom switchblock locations and types using ``<sb_loc>`` tags

        **Default:** ``external_full_internal_straight``


    .. _fig_sb_locations:

    .. figure:: sb_locations.*

        Switchblock Location Patterns for a width = 2, height = 3 complex block

    :opt_param internal_switch:

        The name of a switch (from ``<switchlist>``) which should be used for internal switch blocks.

        **Default:** The default switch for the wire ``<segment>``

        .. note:: This is typically used to specify that internal wire segments are electrically shorted together using a ``short`` type ``<switch>``.


    **Example: Electrically Shorted Internal Straight Connections**

    In some architectures there are no switch blocks located 'within' a block, and the wires crossing over the block are instead electrcially shorted to their 'straight-through' connections.

    To model this we first define a special ``short`` type switch to electrically short such segments together:

    .. code-block:: xml

        <switchlist>
            <switch type="short" name="electrical_short" R="0" Cin="0" Tdel="0"/>
        </switchlist>

    Next, we use the pre-defined ``external_full_internal_straight`` pattern, and that such connections should use our ``electrical_short`` switch.

    .. code-block:: xml

        <switchblock_locations pattern="external_full_internal_straight" internal_switch="electrical_short"/>



    .. arch:tag:: <sb_loc type="{full|straight|turns|none}" xoffset="int" yoffset="int", switch_override="string">

        Specifies the type of switchblock to create at a particular location relative to a complex block for the ``custom`` switchblock location pattern.

        :req_param type:
            Specifies the type of switchblock to be created at this location:

            * ``full``: denotes that a full switchblock will be created (i.e. both ``staight`` and ``turns``)
            * ``straight``: denotes that a switchblock with only straight-through connections will be created (i.e. no ``turns``)
            * ``turns``: denotes that a switchblock with only turning connections will be created (i.e. no ``straight``)
            * ``none``: denotes that no switchblock will be created

            **Default:** ``full``

            .. figure:: sb_types.*

                Switchblock Types


        :opt_param xoffset:
            Specifies the horizontal offset (in grid units) from block origin (bottom left corner).
            The offset value must be less than the width of the block.

            **Default:** ``0``

        :opt_param yoffset:
            Specifies the vertical offset (in grid units) from block origin (bottom left corner).
            The offset value must be less than the height of the block.

            **Default:** ``0``

        :opt_param switch_override:
            The name of a switch (from ``<switchlist>``) which should be used to construct the switch block at this location.

            **Default:** The default switch for the wire ``<segment>``

        .. note:: The switchblock associated with a grid tile is located to the top-right of the grid tile


        **Example: Custom Description of Electrically Shorted Internal Straight Connections**

        If we assume a width=2, height=3 block (e.g. :numref:`fig_sb_locations`), we can use a custom pattern to specify an architecture equivalent to the 'Electrically Shorted Internal Straight Connections' example:

        .. code-block:: xml

            <switchblock_locations pattern="custom">
                <!-- Internal: using straight electrical shorts -->
                <sb_loc type="straight" xoffset="0" yoffset="0" switch_override="electrical_short">
                <sb_loc type="straight" xoffset="0" yoffset="1" switch_override="electrical_short">

                <!-- External: using default switches -->
                <sb_loc type="full" xoffset="0" yoffset="2"> <!-- Top edge -->
                <sb_loc type="full" xoffset="1" yoffset="0"> <!-- Right edge -->
                <sb_loc type="full" xoffset="1" yoffset="1"> <!-- Right edge -->
                <sb_loc type="full" xoffset="1" yoffset="2"> <!-- Top Right -->
            <switchblock_locations/>

.. _arch_complex_blocks:

Complex Blocks
--------------

.. seealso:: For a step-by-step walkthrough on building a complex block see :ref:`arch_tutorial`.

The content within the ``<complexblocklist>`` describes the complex blocks found within the FPGA.
Each type of complex block is specified with a top-level ``<pb_type>`` tag within the ``<complexblocklist>`` tag.

PB Type
~~~~~~~
.. arch:tag:: <pb_type name="string" num_pb="int" blif_model="string"/>

    Specifies a top-level complex block, or a complex block's internal components (sub-blocks).
    Which attributes are applicable depends on where the ``<pb_type>`` tag falls within the hierarchy:

    * Top Level: A child of the ``<complexblocklist>``
    * Intermediate: A child of another ``<pb_type>``
    * Primitive/Leaf: Contains no ``<pb_type>`` children

    For example:

    .. code-block:: xml

        <complexblocklist>
            <pb_type name="CLB"/> <!-- Top level -->
                ...
                <pb_type name="ble"/> <!-- Intermediate -->
                    ...
                    <pb_type name="lut"/> <!-- Primitive -->
                        ...
                    </pb_type>
                    <pb_type name="ff"/> <!-- Primitive -->
                        ...
                    </pb_type>
                    ...
                </pb_type>
                ...
            </pb_type>
            ...
        </complexblocklist>

    .. note: Intermediate pb_types can contain other intermediate or primitive pb_types so arbitrary hierarchies can be specified.

    **General:**

    :req_param name: The name of this pb_type.

        The name must be unique with respect to any parent, sibling, or child ``<pb_type>``.


    **Top-level, Intermediate or Primitive:**

    :opt_param num_pb: The number of instances of this pb_type at the current hierarchy level.

        **Default:** ``1``

        For example:

        .. code-block:: xml

            <pb_type name="CLB">
                ...
                <pb_type name="ble" num_pb="10"/>
                   ...
                </pb_type>
                ...
            </pb_type>

        would specify that the pb_type ``CLB`` contains 10 instances of the ``ble`` pb_type.

    **Primitive Only:**

    :req_param blif_model: Specifies the netlist primitive which can be implemented by this pb_type.

        Accepted values:

        * ``.input``: A BLIF netlist input

        * ``.output``: A BLIF netlist output

        * ``.names``: A BLIF .names (LUT) primitive

        * ``.latch``: A BLIF .latch (DFF) primitive

        * ``.subckt <custom_type>``: A user defined black-box primitive.

        For example:

        .. code-block:: xml

            <pb_type name="my_adder" blif_model=".subckt adder"/>
               ...
            </pb_type>

        would specify that the pb_type ``my_adder`` can implement a black-box BLIF primitive named ``adder``.

        .. note:: The input/output/clock ports for primitive pb_types must match the ports specified in the ``<models>`` section.

    :opt_param class: Specifies that this primitive is of a specialized type which should be treated specially.

        .. seealso:: :ref:`arch_classes` for more details.

The following tags are common to all <pb_type> tags:

.. arch:tag:: <input name="string" num_pins="int" equivalent="{none|full}" is_non_clock_global="{true|false}"/>

    Defines an input port.
    Multple input ports are described using multiple ``<input>`` tags.

    :req_param name: Name of the input port.
    :req_param num_pins: Number of pins the input port has.

    :opt_param equivalent:

        .. note:: Applies only to top-level pb_type.

        Describes if the pins of the port are logically equivalent.
        Input logical equivalence means that the pin order can be swapped without changing functionality.
        For example, an AND gate has logically equivalent inputs because you can swap the order of the inputs and it’s still correct; an adder, on the otherhand, is not logically equivalent because if you swap the MSB with the LSB, the results are completely wrong.
        LUTs are also considered logically equivalent since the logic function (LUT mask) can be rotated to account for pin swapping.

        * ``none``: No input pins are logically equivalent.

            Input pins can not be swapped by the router. (Generates a unique SINK rr-node for each block input port pin.)

        * ``full``: All input pins are considered logically equivalent (e.g. due to logical equivalance or a full-crossbar within the cluster).

            All input pins can be swapped without limitation by the router. (Generates a single SINK rr-node shared by each input port pin.)

        **default:** ``none``

    :opt_param is_non_clock_global:

        .. note:: Applies only to top-level pb_type.

        Describes if this input pin is a global signal that is not a clock.
        Very useful for signals such as FPGA-wide asynchronous resets.
        These signals have their own dedicated routing channels and so should not use the general interconnect fabric on the FPGA.


.. arch:tag:: <output name="string" num_pins="int" equivalent="{none|full|instance}"/>

    Defines an output port.
    Multple output ports are described using multiple ``<output>`` tags

    :req_param name: Name of the output port.
    :req_param num_pins: Number of pins the output port has.

    :opt_param equivalent:

        .. note:: Applies only to top-level pb_type.

        Describes if the pins of the output port are logically equivalent:

        * ``none``: No output pins are logically equivalent.

            Output pins can not be swapped by the router. (Generates a unique SRC rr-node for each block output port pin.)

        * ``full``: All output pins are considered logically equivalent.

            All output pins can be swapped without limitation by the router. For example, this option would be appropriate to model an output port which has a full crossbar between it and the logic within the block that drives it. (Generates a single SRC rr-node shared by each output port pin.)

        * ``instance``: Models that sub-instances within a block (e.g. LUTs/BLEs) can be swapped to achieve a limited form of output pin logical equivalence.

            Like ``full``, this generates a single SRC rr-node shared by each output port pin. However, each net originating from this source can use only one output pin from the equivalence group. This can be useful in modeling more complex forms of equivalence in which you can swap which BLE implements which function to gain access to different inputs.

            .. warning:: When using ``instance`` equivalence you must be careful to ensure output swapping would not make the cluster internal routing (previously computed by the clusterer) illegal; the tool does not update the cluster internal routing due to output pin swapping.

        **Default:** ``none``


.. arch:tag:: <clock name="string" num_pins="int" equivalent="{none|full}"/>

    Describes a clock port.
    Multple clock ports are described using multiple ``<clock>`` tags.
    *See above descriptions on inputs*

.. arch:tag:: <mode name="string" disable_packing="bool">

    :req_param name:
        Name for this mode.
        Must be unique compared to other modes.

    Specifies a mode of operation for the ``<pb_type>``.
    Each child mode tag denotes a different mode of operation for the ``<pb_type>``.
    Each mode tag may contains other ``<pb_type>`` and ``<interconnect>`` tags.

    .. note:: Modes within the same parent ``<pb_type>`` are mutually exclusive.

    .. note:: If a ``<pb_type>`` has only one mode of operation the mode tag can be omitted.

    :opt_param disable_packing:
        Specify if a mode is disabled or not for VPR packer.
        When a mode is defined to be disabled for packing (``disable_packing="true"``), packer will not map any logic to the mode.
        This optional syntax aims to help debugging of multi-mode ``<pb_type>`` so that users can spot bugs in their XML definition quickly. 
        By default, it is set to ``false``.

    .. note:: When a mode is specified to be disabled for packing, its child ``<pb_type>`` and the ``<mode>`` of child ``<pb_type>`` will be considered as disabled for packing automatically. There is no need to specify ``disable_packing`` for every ``<mode>`` in the tree of ``<pb_type>``.

    .. warning:: This is a power-user debugging option. See :ref:`multi_mode_logic_block_tutorial` for a detailed how-to-use.

    For example:

    .. code-block:: xml

        <!--A fracturable 6-input LUT-->
        <pb_type name="lut">
            ...
            <mode name="lut6">
                <!--Can be used as a single 6-LUT-->
                <pb_type name="lut6" num_pb="1">
                    ...
                </pb_type>
                ...
            </mode>
            ...
            <mode name="lut5x2">
                <!--Or as two 5-LUTs-->
                <pb_type name="lut5" num_pb="2">
                    ...
                </pb_type>
                ...
            </mode>
        </pb_type>

    specifies the ``lut`` pb_type can be used as either a single 6-input LUT, or as two 5-input LUTs (but not both).

Interconnect
~~~~~~~~~~~~

As mentioned earlier, the mode tag contains ``<pb_type>`` tags and an ``<interconnect>`` tag.
The following describes the tags that are accepted in the ``<interconnect>`` tag.

.. arch:tag:: <complete name="string" input="string" output="string"/>

    :req_param name: Identifier for the interconnect.
    :req_param input: Pins that are inputs to this interconnect.
    :req_param output: Pins that are outputs of this interconnect.

    Describes a fully connected crossbar.
    Any pin in the inputs can connect to any pin at the output.

    **Example:**

    .. code-block:: xml

        <complete input="Top.in" output="Child.in"/>

    .. figure:: complete_example.*

        Complete interconnect example.

.. arch:tag:: <direct name="string" input="string" output="string"/>

    :req_param name: Identifier for the interconnect.
    :req_param input: Pins that are inputs to this interconnect.
    :req_param output: Pins that are outputs of this interconnect.

    Describes a 1-to-1 mapping between input pins and output pins.

    **Example:**

    .. code-block:: xml

        <direct input="Top.in[2:1]" output="Child[1].in"/>

    .. figure:: direct_example.*

        Direct interconnect example.

.. arch:tag:: <mux name="string" input="string" output="string"/>

    :req_param name: Identifier for the interconnect.
    :req_param input: Pins that are inputs to this interconnect. Different data lines are separated by a space.
    :req_param output: Pins that are outputs of this interconnect.

    Describes a bus-based multiplexer.

    .. note:: Buses are not yet supported so all muxes must use one bit wide data only!

    **Example:**

    .. code-block:: xml

        <mux input="Top.A Top.B" output="Child.in"/>

    .. figure:: mux_example.*

        Mux interconnect example.



A ``<complete>``, ``<direct>``, or ``<mux>`` tag may take an additional, optional, tag called ``<pack_pattern>`` that is used to describe *molecules*.
A pack pattern is a power user feature directing that the CAD tool should group certain netlist atoms (eg. LUTs, FFs, carry chains) together during the CAD flow.
This allows the architect to help the CAD tool recognize structures that have limited flexibility so that netlist atoms that fit those structures be kept together as though they are one unit.
This tag impacts the CAD tool only, there is no architectural impact from defining molecules.

.. arch:tag:: <pack_pattern name="string" in_port="string" out_port="string"/>

    .. warning:: This is a power user option. Unless you know why you need it, you probably shouldn't specify it.

    :req_param name: The name of the pattern.
    :req_param in_port: The input pins of the edges for this pattern.
    :req_param out_port: Which output pins of the edges for this pattern.

    This tag gives a hint to the CAD tool that certain architectural structures should stay together during packing.
    The tag labels interconnect edges with a pack pattern name.
    All primitives connected by the same pack pattern name becomes a single pack pattern.
    Any group of atoms in the user netlist that matches a pack pattern are grouped together by VPR to form a molecule.
    Molecules are kept together as one unit in VPR.
    This is useful because it allows the architect to help the CAD tool assign atoms to complex logic blocks that have interconnect with very limited flexibility.
    Examples of architectural structures where pack patterns are appropriate include: optionally registered inputs/outputs, carry chains, multiply-add blocks, etc.

    There is a priority order when VPR groups molecules.
    Pack patterns with more primitives take priority over pack patterns with less primitives.
    In the event that the number of primitives is the same, the pack pattern with less inputs takes priority over pack patterns with more inputs.

    **Special Case:**

    To specify carry chains, we use a special case of a pack pattern.
    If a pack pattern has exactly one connection to a logic block input pin and exactly one connection to a logic block output pin, then that pack pattern takes on special properties.
    The prepacker will assume that this pack pattern represents a structure that spans multiple logic blocks using the logic block input/output pins as connection points.
    For example, lets assume that a logic block has two, 1-bit adders with a carry chain that links adjacent logic blocks.
    The architect would specify those two adders as a pack pattern with links to the logic block cin and cout pins.
    Lets assume the netlist has a group of 1-bit adder atoms chained together to form a 5-bit adder.
    VPR will break that 5-bit adder into 3 molecules: two 2-bit adders and one 1-bit adder connected in order by a the carry links.

    **Example:**

    Consider a classic basic logic element (BLE) that consists of a LUT with an optionally registered flip-flop.
    If a LUT is followed by a flip-flop in the netlist, the architect would want the flip-flop to be packed with the LUT in the same BLE in VPR.
    To give VPR a hint that these blocks should be connected together, the architect would label the interconnect connecting the LUT and flip-flop pair as a pack_pattern:

    .. code-block:: xml

        <pack_pattern name="ble" in_port="lut.out" out_port="ff.D"/>

    .. figure:: pack_pattern_example.*

        Pack Pattern Example.

.. _arch_classes:

Classes
~~~~~~~
Using these structures, we believe that one can describe any digital complex logic block.
However, we believe that certain kinds of logic structures are common enough in FPGAs that special shortcuts should be available to make their specification easier.
These logic structures are: flip-flops, LUTs, and memories.
These structures are described using a ``class=string`` attribute in the ``<pb_type>`` primitive.
The classes we offer are:

.. arch:tag:: class="lut"

    Describes a K-input lookup table.

    The unique characteristic of a lookup table is that all inputs to the lookup table are logically equivalent.
    When this class is used, the input port must have a ``port_class="lut_in"`` attribute and the output port must have a ``port_class="lut_out"`` attribute.

.. arch:tag:: class="flipflop"

    Describes a flipflop.

    Input port must have a ``port_class="D"`` attribute added.
    Output port must have a ``port_class="Q"`` attribute added.
    Clock port must have a ``port_class="clock"`` attribute added.

.. arch:tag:: class="memory"

    Describes a memory.

    Memories are unique in that a single memory physical primitive can hold multiple, smaller, logical memories as long as:

    #. The address, clock, and control inputs are identical and
    #. There exists sufficient physical data pins to satisfy the netlist memories when the different netlist memories are merged together into one physical memory.

    Different types of memories require different attributes.

    **Single Port Memories Require:**

    * An input port with ``port_class="address"`` attribute
    * An input port with ``port_class="data_in"`` attribute
    * An input port with ``port_class="write_en"`` attribute
    * An output port with ``port_class="data_out"`` attribute
    * A clock port with ``port_class="clock"`` attribute


    **Dual Port Memories Require:**

    * An input port with ``port_class="address1"`` attribute
    * An input port with ``port_class="data_in1"`` attribute
    * An input port with ``port_class="write_en1"`` attribute
    * An input port with ``port_class="address2"`` attribute
    * An input port with ``port_class="data_in2"`` attribute
    * An input port with ``port_class="write_en2"`` attribute
    * An output port with ``port_class="data_out1"`` attribute
    * An output port with ``port_class="data_out2"`` attribute
    * A clock port with ``port_class="clock"`` attribute


Timing
~~~~~~

.. seealso:: For examples of primitive timing modeling specifications see the :ref:`arch_model_timing_tutorial`

Timing is specified through tags contained with in ``pb_type``, ``complete``, ``direct``, or ``mux`` tags as follows:

.. arch:tag:: <delay_constant max="float" min="float" in_port="string" out_port="string"/>

    :opt_param max: The maximum delay value.
    :opt_param min: The minimum delay value.
    :req_param in_port: The input port name.
    :req_param out_port: The output port name.

    Specifies a maximum and/or minimum delay from in_port to out_port.

    * If ``in_port`` and ``out_port`` are non-sequential (i.e combinational) inputs specifies the combinational path delay between them.
    * If ``in_port`` and ``out_port`` are sequential (i.e. have ``T_setup`` and/or ``T_clock_to_Q`` tags) specifies the combinational delay between the primitive's input and/or output registers.

    .. note:: At least one of the ``max`` or ``min`` attributes must be specified

    .. note:: If only one of ``max`` or ``min`` are specified the unspecified value is implicitly set to the same value

.. arch:tag:: <delay_matrix type="{max | min}" in_port="string" out_port="string"> matrix </delay>

    :req_param type: Specifies the delay type.
    :req_param in_port: The input port name.
    :req_param out_port: The output port name.
    :req_param matrix: The delay matrix.

    Describe a timing matrix for all edges going from ``in_port`` to ``out_port``.
    Number of rows of matrix should equal the number of inputs, number of columns should equal the number of outputs.

    * If ``in_port`` and ``out_port`` are non-sequential (i.e combinational) inputs specifies the combinational path delay between them.
    * If ``in_port`` and ``out_port`` are sequential (i.e. have ``T_setup`` and/or ``T_clock_to_Q`` tags) specifies the combinational delay between the primitive's input and/or output registers.

    **Example:**
    The following defines a delay matrix for a 4-bit input port ``in``, and 3-bit output port ``out``:

    .. code-block:: xml

        <delay_matrix type="max" in_port="in" out_port="out">
            1.2e-10 1.4e-10 3.2e-10
            4.6e-10 1.9e-10 2.2e-10
            4.5e-10 6.7e-10 3.5e-10
            7.1e-10 2.9e-10 8.7e-10
        </delay>

    .. note:: To specify both ``max`` and ``min`` delays two ``<delay_matrix>`` should be used.

.. arch:tag:: <T_setup value="float" port="string" clock="string"/>

    :req_param value: The setup time value.
    :req_param port: The port name the setup constraint applies to.
    :req_param clock: The port name of the clock the setup constraint is specified relative to.

    Specifies a port's setup constraint.

    * If ``port`` is an input specifies the external setup time of the primitive's input register (i.e. for paths terminating at the input register).
    * If ``port`` is an output specifies the internal setup time of the primitive's output register (i.e. for paths terminating at the output register) .

    .. note:: Applies only to primitive ``<pb_type>`` tags

.. arch:tag:: <T_hold value="float" port="string" clock="string"/>

    :req_param value: The hold time value.
    :req_param port: The port name the setup constraint applies to.
    :req_param clock: The port name of the clock the setup constraint is specified relative to.

    Specifies a port's hold constraint.

    * If ``port`` is an input specifies the external hold time of the primitive's input register (i.e. for paths terminating at the input register).
    * If ``port`` is an output specifies the internal hold time of the primitive's output register (i.e. for paths terminating at the output register) .

    .. note:: Applies only to primitive ``<pb_type>`` tags

.. arch:tag:: <T_clock_to_Q max="float" min="float" port="string" clock="string"/>

    :opt_param max: The maximum clock-to-Q delay value.
    :opt_param min: The minimum clock-to-Q delay value.
    :req_param port: The port name the delay value applies to.
    :req_param clock: The port name of the clock the clock-to-Q delay is specified relative to.

    Specifies a port's clock-to-Q delay.

    * If ``port`` is an input specifies the internal clock-to-Q delay of the primitive's input register (i.e. for paths starting at the input register).
    * If ``port`` is an output specifies the external clock-to-Q delay of the primitive's output register (i.e. for paths starting at the output register) .

    .. note:: At least one of the ``max`` or ``min`` attributes must be specified

    .. note:: If only one of ``max`` or ``min`` are specified the unspecified value is implicitly set to the same value

    .. note:: Applies only to primitive ``<pb_type>`` tags


Modeling Sequential Primitive Internal Timing Paths
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.. seealso:: For examples of primitive timing modeling specifications see the :ref:`arch_model_timing_tutorial`

By default, if only ``<T_setup>`` and ``<T_clock_to_Q>`` are specified on a primitive ``pb_type`` no internal timing paths are modeled.
However, such paths can be modeled by using ``<delay_constant>`` and/or ``<delay_matrix>`` can be used in conjunction with ``<T_setup>`` and ``<T_clock_to_Q>``.
This is useful for modeling the speed-limiting path of an FPGA hard block like a RAM or DSP.

As an example, consider a sequential black-box primitive named ``seq_foo`` which has an input port ``in``, output port ``out``, and clock ``clk``:

.. code-block:: xml

    <pb_type name="seq_foo" blif_model=".subckt seq_foo" num_pb="1">
        <input name="in" num_pins="4"/>
        <output name="out" num_pins="1"/>
        <clock name="clk" num_pins="1"/>

        <!-- external -->
        <T_setup value="80e-12" port="seq_foo.in" clock="clk"/>
        <T_clock_to_Q max="20e-12" port="seq_foo.out" clock="clk"/>

        <!-- internal -->
        <T_clock_to_Q max="10e-12" port="seq_foo.in" clock="clk"/>
        <delay_constant max="0.9e-9" in_port="seq_foo.in" out_port="seq_foo.out"/>
        <T_setup value="90e-12" port="seq_foo.out" clock="clk"/>
    </pb_type>

To model an internal critical path delay, we specify the internal clock-to-Q delay of the input register (10ps), the internal combinational delay (0.9ns) and the output register's setup time (90ps). The sum of these delays corresponds to a 1ns critical path delay.

.. note:: Primitive timing paths with only one stage of registers can be modeled by specifying ``<T_setup>`` and ``<T_clock_to_Q>`` on only one of the ports.

Power
~~~~~

.. seealso:: :ref:`power_estimation`, for the complete list of options, their descriptions, and required sub-fields.

.. arch:tag:: <power method="string">contents</power>

    :opt_param method:

        Indicates the method of power estimation used for the given pb_type.

        Must be one of:

            * ``specify-size``
            * ``auto-size``
            * ``pin-toggle``
            * ``C-internal``
            * ``absolute``
            * ``ignore``
            * ``sum-of-children``

        **Default:** ``auto-size``.

        .. seealso:: :ref:`Power Architecture Modelling <power_arch_modeling>` for a detailed description of the various power estimation methods.

    The ``contents`` of the tag can consist of the following tags:

      * ``<dynamic_power>``
      * ``<static_power>``
      * ``<pin>``


.. arch:tag:: <dynamic_power power_per_instance="float" C_internal="float"/>

    :opt_param power_per_instance: Absolute power in Watts.
    :opt_param C_internal: Block capacitance in Farads.

.. arch:tag:: <static_power power_per_instance="float"/>

    :opt_param power_per_instance: Absolute power in Watts.

.. arch:tag:: <port name="string" energy_per_toggle="float" scaled_by_static_prob="string" scaled_by_static_prob_n="string"/>

    :req_param name: Name of the port.
    :req_param energy_per_toggle: Energy consumed by a toggle on the port specified in ``name``.
    :opt_param scaled_by_static_prob: Port name by which to scale ``energy_per_toggle`` based on its logic high probability.
    :opt_param scaled_by_static_prob_n: Port name by which to scale ``energy_per_toggle`` based on its logic low probability.

NoC Description
---------------

The ``<noc>`` tag is an optional tag and its contents allows designers to describe a NoC on an FPGA device.
The ``<noc>`` tag is the top level tag for the NoC description and its attributes define the overall properties
of the NoC; refer below for its contents.

.. arch:tag:: <noc link_bandwidth="float" link_latency="float" router_latency="float" noc_router_tile_name="string">

    :req_param link_bandwidth:
        Specifies the maximum bandwidth in bits-per-second (bps) that a link in the NoC can support

    :req_param link_latency:
        Specifies the delay in seconds seen by a flit as it travels from one physical NoC router to another using a NoC link.

    :req_param router_latency:
        Specifies the un-loaded delays in seconds as it travels through a physical router.
    
    :req_param noc_router_tile_name:
        Specifies a string which represents the name used to identify a NoC router tile (physical hard block) in the 
        corresponding FPGA architecture. This information is needed to create a model of the NoC.

The ``<noc>`` tag contains a single ``<topology>`` tag which describes the topology of the NoC.

NoC topology
~~~~~~~~~~~~

As mentioned above the ``<topology>`` tag can be used to specify the topology or how the routers in the NoC
are connected to each other. The ``<topology>`` tag consists of a group ``<router>``tags.

Below is an example of how the ``<topology>`` tag is used.

.. code-block:: xml

    <topology>
        <!--A number of <router> tags go here-->
    </topology>

The ``<router>`` tag and its contents are described below.

.. arch:tag:: <router id="int" positionx="float" positiony="float" connections="int int int int ...">

    This tag represents a single physical NoC router on the FPGA device and specifies how it is connected within the NoC.

    :req_param  id:
        Specifies a user identification (ID) number which is associate to the physical
        router that this tag is identifying. This ID is used to report errors and
        warnings to the user.
    
    :req_param positionx:
        Specifies the horizontal position of the physical router block that this
        tag is identifying. This position does not have to be exact, it can
        be an approximate value.

    :req_param positiony:
        Specifies the vertical position of the physical router block that this
        tag is identifying. This position does not have to be exact, it can
        be an approximate value.

    :req_param connections:
        Specifies a list of numbers seperated by spaces which are the user IDs supplied to other
        ``<router>`` tags. This describes how the current physical Noc router
        that this tag is identifying is connected to the other physical NoC routers on the device.
    
    Below is an example of the ``<router>`` tag which identifies a physical router located near (0,0) with ID 0. This router
    is also connected to two other routers identified by IDs 1 and 2.

    .. code-block:: xml

        <router id="0" positionx="0" positiony="0" connections="1 2"/>

NoC Description Example
~~~~~~~~~~~~~~~~~~~~~~~

Below is an example which describes a NoC architecture which has 4 physical routers that are connected to each other to form a 
2x2 mesh topology.

.. code-block:: xml
    
    <!-- Description of a 2x2 mesh NoC-->
    <noc link_bandwidth="1.2e9" router_latency="1e-9" link_latency="1e-9" noc_router_tile_name="noc_router_adapter">
        <topology>
                <router id="0" positionx="0" positiony="0" connections="1 2"/>
                <router id="1" positionx="5" positiony="0" connections="0 3"/>
                <router id="2" positionx="0" positiony="5" connections="0 3"/>
                <router id="3" positionx="5" positiony="5" connections="1 2"/>
        </topology>
    </noc>

Wire Segments
-------------

The content within the ``<segmentlist>`` tag consists of a group of ``<segment>`` tags.
The ``<segment>`` tag and its contents are described below.

.. arch:tag:: <segment axis="{x|y}" name="unique_name" length="int" type="{bidir|unidir}" res_type="{GCLK|GENERAL}" freq="float" Rmetal="float" Cmetal="float">content</segment>


    :opt_param axis:
        Specifies if the given segment applies to either x or y channels only. If this tag is not given, it is assumed that the given segment
        description applies to both x-directed and y-directed channels.

        .. note:: It is required that both x and y segment axis details are given or that at least one segment within ``segmentlist`` 
            is specified without the ``axis`` tag (i.e. at least one segment applies to both x-directed and y-directed 
            chanels). 

    :req_param name:
        A unique alphanumeric name to identify this segment type.

    :req_param length:
        Either the number of logic blocks spanned by each segment, or the keyword ``longline``.
        Longline means segments of this type span the entire FPGA array.

        .. note:: ``longline`` is only supported on with ``bidir`` routing

    :opt_param res_type:
        Specifies whether the segment belongs to the general or a clock routing network. If this tag is not specified, the resource type for
        the segment is considered to be GENERAL (i.e. regular routing).
  
    :req_param freq:
        The supply of routing tracks composed of this type of segment.
        VPR automatically determines the percentage of tracks for each segment type by taking the frequency for the type specified and dividing with the sum of all frequencies.
        It is recommended that the sum of all segment frequencies be in the range 1 to 100.

    :req_param Rmetal:
        Resistance per unit length (in terms of logic blocks) of this wiring track, in Ohms.
        For example, a segment of length 5 with Rmetal = 10 Ohms / logic block would have an end-to-end resistance of 50 Ohms.

    :req_param Cmetal:
        Capacitance per unit length (in terms of logic blocks) of this wiring track, in Farads.
        For example, a segment of length 5 with Cmetal = 2e-14 F / logic block would have a total metal capacitance of 10e-13F.

    :req_param directionality:
        This is either unidirectional or bidirectional and indicates whether a segment has multiple drive points (bidirectional), or a single driver at one end of the wire segment (unidirectional).
        All segments must have the same directionality value.
        See :cite:`lemieux_directional_and_singale_driver_wires` for a description of unidirectional single-driver wire segments.

    :req_param content:
        The switch names and the depopulation pattern as described below.

.. _fig_sb_pattern:

.. figure:: sb_pattern.*

    Switch block and connection block pattern example with four tracks per channel

.. arch:tag:: <sb type="pattern">int list</sb>

    This tag describes the switch block depopulation (as illustrated in :numref:`fig_sb_pattern`) for this particular wire segment.
    For example, the first length 6 wire in the figure below has an sb pattern of ``1 0 1 0 1 0 1``.
    The second wire has a pattern of ``0 1 0 1 0 1 0``.
    A ``1`` indicates the existence of a switch block and a ``0`` indicates that there is no switch box at that point.
    Note that there a 7 entries in the integer list for a length 6 wire.
    For a length L wire there must be L+1 entries separated by spaces.

    .. note:: Can not be specified for ``longline`` segments (which assume full switch block population)

.. arch:tag:: <cb type="pattern">int list</cb>

    This tag describes the connection block depopulation (as illustrated by the circles in :numref:`fig_sb_pattern`) for this particular wire segment.
    For example, the first length 6 wire in the figure below has an sb pattern of ``1 1 1 1 1 1``.
    The third wire has a pattern of ``1 0 0 1 1 0``.
    A ``1`` indicates the existence of a connection block and a ``0`` indicates that there is no connection box at that point.
    Note that there a 6 entries in the integer list for a length 6 wire.
    For a length L wire there must be L entries separated by spaces.

    .. note:: Can not be specified for ``longline`` segments (which assume full connection block population)

.. arch:tag:: <mux name="string"/>

    :req_param name: Name of the mux switch type used to drive this type of segment by default, from both block outputs and other wires. This information is used during rr-graph construction, and a custom switch block can override this switch type for specific connections if desired.
                     The switch type specified with the <mux> tag will be used for both the incrementing and decrementing wires within this segment. 
                     If more control is needed, the mux_inc and mux_dec tags can be used to assign different muxes to drive incremental and decremental wires within the segment.

    .. note:: For UNIDIRECTIONAL only.

    Tag must be included and ``name`` must be the same as the name you give in ``<switch type="mux" name="...``

.. arch:tag:: <mux_inc name="string"/>

    :req_param name: 
        Name of the mux switch type used to drive the incremental wires in this segment from both block outputs and other wires.
        Incremental wires are tracks within this segment that are heading in the "right" direction on the x-axis and the "top" direction on the y-axis.
        This information is used during rr-graph construction, and a custom switch block can override this switch type for specific connections if desired.

    .. note:: For UNIDIRECTIONAL only.

.. arch:tag:: <mux_dec name="string">
    
    :req_param name: 
        Name of the mux switch type used to drive the decremental wires in this segment from both block outputs and other wires.
        Incremental wires are tracks within this segment that are heading in the "left" direction on the x-axis and the "bottom" direction on the y-axis.
        This information is used during rr-graph construction, and a custom switch block can override this switch type for specific connections if desired.

    .. note:: For UNIDIRECTIONAL only.

    .. note:: For unidirectional segments, either <mux> tag or both <mux_inc> and <mux_dec> should be defined in the architecture file. If only the <mux> tag is defined, we assume that the same mux drives both incremental and decremental wires within this segment.     

.. arch:tag:: <mux_inter_die name="string"/>

    :req_param name: Name of the mux switch type used to drive this segment type when the driver (block outputs and other wires) is located on a different die than the segment. This information is utilized during rr-graph construction.

    Tag must be included and ``name`` must be the same as the name you give in ``<switch type="mux" name="...``


.. arch:tag:: <wire_switch name="string"/>

    :req_param name: Name of the switch type used by other wires to drive this type of segment by default. This information is used during rr-graph construction, and a custom switch block can override this switch type for specific connections if desired.

    .. note:: For BIDIRECTIONAL only.

    Tag must be included and the name must be the same as the name you give in ``<switch type="tristate|pass_gate" name="...`` for the switch which represents the wire switch in your architecture.

.. arch:tag:: <opin_switch name="string"/>

    .. note:: For BIDIRECTIONAL only.

    :req_param name: Name of the switch type used by block pins to drive this type of segment.

    Tag must be included and ``name`` must be the same as the name you give in ``<switch type="tristate|pass_gate" name="...`` for the switch which represents the output pin switch in your architecture.

    .. note::

        In unidirectional segment mode, there is only a single buffer on the segment.
        Its type is specified by assigning the same switch index to both wire_switch and opin_switch.
        VPR will error out if these two are not the same.

    .. note::

        The switch used in unidirectional segment mode must be buffered.

.. _clock_architecture:

Clocks
------

There are two options for describing clocks.
One method allows you to specify clocking purely for power estimation, see :ref:`clock_power_format`.
The other method allows you to specify a clock architecture that is used as part of the routing resources, see :ref:`clock_architecture_format`.
Both methods should not be used in tandem.

.. _clock_power_format:

Specifing Clocking Purely for Power Estimation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The clocking configuration is specified with ``<clock>`` tags within the ``<clocks>`` section.

.. note:: Currently the information in the ``<clocks>`` section is only used for power estimation.

.. seealso:: :ref:`power_estimation` for more details.

.. arch:tag:: <clock C_wire="float" C_wire_per_m="float" buffer_size={"float"|"auto"}/>

    :opt_param C_wire: The absolute capacitance, in Farads, of the wire between each clock buffer.
    :opt_param C_wire_per_m: The wire capacitance, in Farads per Meter.
    :opt_param buffer_size: The size of each clock buffer.

.. _clock_architecture_format:

Specifing a Clock Architecture
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The element ``<clocknetworks>`` contains three sub-elements that collectively describe the clock architecture: the wiring parameters ``<metal_layers>``, the clock distribution ``<clock_network>``, and the clock connectivity ``<clock_routing>``.

    .. note:: The clock network architecture defined in this structure is assigned the fixed default name ``"clock_network"``. When the user wants to specify a net to be routed through the defined clock architecture using a :ref:`global routing constraints file <global_routing_constraints>`, the network name attribute in the constraint tag must be set to ``"clock_network"``.

.. _clock_arch_example:

Clock Architecture Example
^^^^^^^^^^^^^^^^^^^^^^^^^^

The following example shows how a rib-spine (row/column) style clock architecture can be defined.

.. code-block:: xml

    <clocknetworks>
        <metal_layers >
            <metal_layer name="global_spine" Rmetal="50.42" Cmetal="20.7e-15"/>
            <metal_layer name="global_rib" Rmetal="50.42" Cmetal="20.7e-15"/>
        </metal_layers >

        <!-- Full Device: Center Spine -->
        <clock_network  name="spine1" num_inst="2">
            <spine metal_layer="global_spine" x="W/2" starty="0" endy="H">
                <switch_point type="drive" name="drive_point" yoffset="H/2" buffer="drive_buff"/>
                <switch_point type="tap" name="taps" yoffset="0" yincr="1"/>
            </spine>
        </clock_network>

        <!-- Full Device: Each Grid Row -->
        <clock_network  name="rib1" num_inst="2">
            <rib metal_layer="global_rib" y="0" startx="0" endx="W" repeatx="W" repeaty="1">
                <switch_point type="drive" name="drive_point" xoffset="W/2" buffer="drive_buff"/>
                <switch_point type="tap" name="taps" xoffset="0" xincr="1"/>
            </rib>
        </clock_network>

        <clock_routing>
            <!-- connections from inter-block routing to central spine -->
            <tap from="ROUTING" to="spine1.drive_point" locationx="W/2" locationy="H/2" switch="general_routing_switch" fc_val="1.0"/>

            <!-- connections from spine to rib -->
            <tap from="spine1.taps" to="rib1.drive_point" switch="general_routing_switch" fc_val="0.5"/>

            <!-- connections from rib to clock pins -->
            <tap from="rib1.taps" to="CLOCK" switch="ipin_cblock" fc_val="1.0"/>
        </clock_routing >
    </clocknetworks >

.. _spine_visual:

.. figure:: vertical_distribution.*

    ``<spine>`` "spine1" vertical clock wire example.
    The two spines (``num_inst="2"``) are located horizontally at ``W/2`` (in the middle of the device), and spans the entire height of the device (0..H).
    The drive points are located at ``H/2``, with tap points located at unit increments along their length.
    Buffers of  ``drive_buff`` type (would be defined in ``<switches>``) are used to drive the two halves of the spines.

.. _rib_visual:

.. figure:: horizontal_distribution.*

    ``<rib>`` "rib1" horizontal clock wire example.
    Each rib spans the full width of the device (0..W), with the drive points located at the mid-point (``W/2``), and tap points in unit increments along each rib.
    There are two ribs at each vertical location (``num_inst="2"``), and pairs of ribs are stamped out at each row of the device (``repeaty="1"``).

.. _clock_architecture_tags:

Clock Architecture Tags
^^^^^^^^^^^^^^^^^^^^^^^

The ``<metal_layers>`` element describes the per unit length electrical parameters, resistance (``Rmetal``) and capacitance (``Cmetel``), used to implement the clock distribution wires.
Wires are modeled soley based on ``Rmetal`` and ``Cmetal`` parameters which are derived from the physical implementation of the metal layer width and spacing.
There can be one or more wiring implementation options (metal layer, width and spacing) that are used by the later clock network specification and each is described in a separate ``<metal_layer>`` sub-element.
The syntax of the wiring electrical information is:

.. arch:tag:: <metal_layer name="string" Rmetal="float" Cmetal="float"/>

    :req_param name: A unique string for reference.
    :req_param Rmetal: The resistance in Ohms of the wire per unit block in the FPGA architecture; a unit block usually corresponds to a logic cluster.
    :req_pram Cmetal: The capacitance in Farads of the wire per unit block.

The ``<clock_network>`` element contains sub-elements that describe the clock distribution wires for the clock architecture.
There could be more than one ``<clock_network>`` element to describe separate types of distribution wires.
The high-level start tag for a clock network is as follows:

.. arch:tag:: <clock_network name="string" num_inst="integer">

    :req_param name: A unique string for reference.
    :req_param num_inst: which describes the number of parallel instances of the clock distribution types described in the ``<clock_network>`` sub-elements.

    .. note::
        Many paramters used in the following clock architecture tags take an espression (``expr``) as an argument simular to :ref:`grid_expressions`.
        However, only a subset of special variables are suported: ``W`` (device width) and ``H`` (device height).

    The supported clock distribution types are ``<spine>`` and ``<rib>``.
    *Spines* are used to describe vertical clock distribution wires.
    Whereas, *Ribs* is used to describe a horizontal clock distribution wire.
    See :ref:`clock_arch_example` and accompanying figures :numref:`spine_visual` and :numref:`rib_visual` for example use of ``<spine>`` and ``<rib>`` parameters.

    .. arch:tag:: <spine metal_layer="string" x="expr" starty="expr" endy="expr" repeatx="expr" repeaty="expr"/>

        :req_param metal_layer: A referenced metal layer that sets the unit resistance and capacitance of the distribution wire over the length of the wire.
        :req_param starty: 
            The start y grid location, of the wire which runs parallel to the y-axis from starty and ends at endy, inclusive.
            Value can be relative to the device size.
        :req_param endy: The end of y grid location of the wire. Value can be relative to the device size.
        :req_param x: The location of the spine with respect to the x-axis. Value can be relative to the device size.
        :opt_param repeatx: The horizontal repeat factor of the spine along the device. Value can be relative to the device size.
        :opt_param repeaty: The vertical repeat factor of the spine along the device. Value can be relative to the device size.

    The provided example clock network (:ref:`clock_arch_example`) defines two spines, and neither repeats as each spans the entire height of the device and is locally at the horizontal midpoint of the device.

    .. arch:tag:: <rib metal_layer="string" y="expr" startx="expr" endx="expr" repeatx="expr" repeaty="expr"/>

        :req_param metal_layer: A referenced metal layer that sets the unit resistance and capacitance of the distribution wire over the length of the wire.
        :req_param startx:
            The start x grid location, of the wire which runs parallel to the x-axis from startx and ends at endx, inclusive.
            Value can be relative to the device size.
        :req_param endx: The end of x grid location of the wire. Value can be relative to the device size.
        :req_param y: The location of the rib with respect to the y-axis. Value can be relative to the device size.
        :opt_param repeatx: The horizontal repeat factor of the rib along the device. Value can be relative to the device size.
        :opt_param repeaty: The vertical repeat factor of the rib along the device. Value can be relative to the device size.

    Along each spine and rib is a group of switch points.
    Switch points are used to describe drive or tap locations along the clock distribution wire, and are enclosed in the relevant ``<rib>`` or ``<spine>`` tags:

    .. arch:tag:: <switch_point type="{drive | tap}" name="string" yoffset="expr" xoffset="expr" xinc="expr" yinc="expr" buffer="string">

        :req_param type:
            * ``drive`` -- Drive points are where the clock distribution wire can be driven by a routing switch or buffer.
            * ``tap`` --  Tap points are where it can drive a routing switch or buffer to send a signal to a different ``clock_network`` or logicblock.
        :opt_param xoffset: (Only for ``rib`` network) Offset from the ``startx`` of a rib network.
        :opt_param yoffset: (Only for ``spine`` network) Offset from the ``starty`` of a spine network.
        :opt_param xinc: (Only for rib ``tap`` points) Descibes the repeat factor of a series of evenly spaced tap points.
        :opt_param yinc: (Only for spine ``tap`` points) Descibes the repeat factor of a series of evenly spaced tap points.
        :req_param buffer:
            (Required only for ``drive`` points) A reference to a pre-defined routing switch; specfied by ``<switch>`` tag, see Section :ref:`arch_switches`.
            This switch will be used at the drive point.
            The clock architecture generator uses two of these buffers to drive the two portions of this ``clock_network`` wire when it is split at the drive point, see Figures :numref:`rib_visual` and :numref:`spine_visual`.

        .. note::

            A single ``<switch_point>`` specification may define a *set* of tap points (``type="tap"``, with either ``xincr`` or ``yincr``), or a single drive point (``type="drive"``)

Lastly the ``<clock_routing>`` element consists of a group of ``tap`` statements which separately describe the connectivity between clock-related routing resources (pin or wire).
The tap element and its attribute sare as follows:

.. arch:tag:: <tap from="string" to="string" locationx="expr" locationy="expr" switch="string" fc_val="float">

    :req_param from:
        The set of routing resources to make connections *from*.
        This can be either:
            * ``clock_name.tap_points_name``: A set of clock network ``tap``-type switchpoints. The format is clock network name, followed by the tap points name and delineated by a period (e.g. ``spine1.taps``), or
            * ``ROUTING``: a special literal which references a connection from general inter-block routing (at a location specified by ``locationx`` and ``locationy`` parameters).
        Examples can be see in :ref:`clock_arch_example`.
    :req_param to:
        The set of routing resources to make connections *to*.
        Can be a unique name or special literal:
            * ``clock_name.drive_point_name``: A clock network ``drive``-type switchpoint. The format is clock network name, followed by the drive point name and delineated by a period (e.g. ``rib1.drive_point``).
            * ``CLOCK``: a special literal which describes connections from clock network tap points that supply the clock to clock pins on blocks at the tap locations; these are clock inputs are already specified on blocks (top-level ``<pb_type>``/``<tile>``) in the VTR architecture file.
        Examples can be see in :ref:`clock_arch_example`.
    :req_param switch: The routing switch (defined in ``<switches>``) used for this connection.
    :req_param fc_val:
        A decimal value between 0 and 1 representing the connection block flexibility between the connecting routing resources; a value of 0.5 for example means that only 50% of the switches necessary to connect all the matching tap and drive points would be implemented.
    :opt_param locationx:
        (Required when using the special literal ``"ROUTING"``)
        The x grid location of inter-block routing.
    :opt_param locationy:
        (Required when using the special literal ``"ROUTING"``)
        The y grid location of inter-block routing.

    .. note:: 
    
        A single ``<tap>`` statement may create multiple connections if either the of the ``from`` or ``to`` correspond to multiple routing resources.
        In such cases the ``fc_val`` can control how many connections are created.

    .. note::

        ``locationx`` and ``locationy`` describe an (x,y) grid loction where all the wires passing this location source the source the clock network connection depending on the ``fc_val``

For more information you may wish to consult :cite:`mustafa_masc` which introduces the clock modeling language.

Power
-----
Additional power options are specified within the ``<architecture>`` level ``<power>`` section.

.. seealso:: See :ref:`power_estimation` for full documentation on how to perform power estimation.

.. arch:tag:: <local_interconnect C_wire="float" factor="float"/>

    :req_param C_wire: The local interconnect capacitance in Farads/Meter.
    :opt_param factor: The local interconnect scaling factor. **Default:** ``0.5``.

.. arch:tag:: <buffers logical_effort_factor="float"/>

    :req_param logical_effort_factor: **Default:** ``4``.


Direct Inter-block Connections
------------------------------

The content within the ``<directlist>`` tag consists of a group of ``<direct>`` tags.
The ``<direct>`` tag and its contents are described below.

.. arch:tag:: <direct name="string" from_pin="string" to_pin="string" x_offset="int" y_offset="int" z_offset="int" switch_name="string"/>

    :req_param name: is a unique alphanumeric string to name the connection.
    :req_param from_pin: pin of complex block that drives the connection.
    :req_param to_pin: pin of complex block that receives the connection.
    :req_param x_offset:  The x location of the receiving CLB relative to the driving CLB.
    :req_param y_offset: The y location of the receiving CLB relative to the driving CLB.
    :req_param z_offset: The z location of the receiving CLB relative to the driving CLB.
    :opt_param switch_name: [Optional, defaults to delay-less switch if not specified] The name of the ``<switch>`` from ``<switchlist>`` to be used for this direct connection.
    :opt_param from_side: The associated from_pin's block side (must be one of ``left``, ``right``, ``top``, ``bottom`` or left unspecified)
    :opt_param to_side: The associated to_pin's block side (must be one of ``left``, ``right``, ``top``, ``bottom`` or left unspecified)

    Describes a dedicated connection between two complex block pins that skips general interconnect.
    This is useful for describing structures such as carry chains as well as adjacent neighbour connections.

    The ``from_side`` and ``to_side`` options can usually be left unspecified.
    However they can be used to explicitly control how directs to physically equivalent pins (which may appear on multiple sides) are handled.

    **Example:**
    Consider a carry chain where the ``cout`` of each CLB drives the ``cin`` of the CLB immediately below it, using the delay-less switch one would enter the following:

    .. code-block:: xml

        <direct name="adder_carry" from_pin="clb.cout" to_pin="clb.cin" x_offset="0" y_offset="-1" z_offset="0"/>

.. _custom_switch_blocks:

Custom Switch Blocks
--------------------

The content under the ``<switchblocklist>`` tag consists of one or more ``<switchblock>`` tags that are used to specify connections between different segment types. An example is shown below:

    .. code-block:: xml

        <switchblocklist>
          <switchblock name="my_switchblock" type="unidir">
            <switchblock_location type="EVERYWHERE"/>
            <switchfuncs>
              <func type="lr" formula="t"/>
              <func type="lt" formula="W-t"/>
              <func type="lb" formula="W+t-1"/>
              <func type="rt" formula="W+t-1"/>
              <func type="br" formula="W-t-2"/>
              <func type="bt" formula="t"/>
              <func type="rl" formula="t"/>
              <func type="tl" formula="W-t"/>
              <func type="bl" formula="W+t-1"/>
              <func type="tr" formula="W+t-1"/>
              <func type="rb" formula="W-t-2"/>
              <func type="tb" formula="t"/>
            </switchfuncs>
            <wireconn from_type="l4" to_type="l4" from_switchpoint="0,1,2,3" to_switchpoint="0"/>
            <wireconn from_type="l8_global" to_type="l8_global" from_switchpoint="0,4"
                  to_switchpoint="0"/>
            <wireconn from_type="l8_global" to_type="l4" from_switchpoint="0,4"
                  to_switchpoint="0"/>
          </switchblock>

          <switchblock name="another_switch_block" type="unidir">
            ... another switch block description ...
          </switchblock>
        </switchblocklist>

This switch block format allows a user to specify mathematical permutation functions that describe how different types of segments (defined in the architecture file under the ``<segmentlist>`` tag) will connect to each other at different switch points.
The concept of a switch point is illustrated below for a length-4 unidirectional wire heading in the "left" direction.
The switch point at the start of the wire is given an index of 0 and is incremented by 1 at each subsequent switch block until the last switch point.
The last switch point has an index of 0 because it is shared between the end of the current segment and the start of the next one (similarly to how switch point 3 is shared by the two wire subsegments on each side).

.. figure:: switch_point_diagram.*

    Switch point diagram.

A collection of wire types and switch points defines a set of wires which will be connected to another set of wires with the specified permutation functions (the ‘sets’ of wires are defined using the ``<wireconn>`` tags).
This format allows for an abstract but very flexible way of specifying different switch block patterns.
For additional discussion on interconnect modeling see :cite:`petelin_masc`.
The full format is documented below.

**Overall Notes:**

#. The ``<sb type="pattern">`` tag on a wire segment (described under ``<segmentlist>``) is applied as a mask on the patterns created by this switch block format; anywhere along a wire’s length where a switch block has not been requested (set to 0 in this tag), no switches will be added.
#. You can specify multiple switchblock tags, and the switches described by the union of all those switch blocks will be created.

.. arch:tag:: <switchblock name="string" type="string">

    :req_param name: A unique alphanumeric string
    :req_param type: ``unidir`` or ``bidir``.
        A bidirectional switch block will implicitly mirror the specified permutation functions – e.g. if a permutation function of type ``lr`` (function used to connect wires from the left to the right side of a switch block) has been specified, a reverse permutation function of type ``rl`` (right-to-left) is automatically assumed.
        A unidirectional switch block makes no such implicit assumptions.
        The type of switch block must match the directionality of the segments defined under the ``<segmentlist>`` node.

    ``<switchblock>`` is the top-level XML node used to describe connections between different segment types.

.. arch:tag:: <switchblock_location type="string"/>

    :req_param type:
        Can be one of the following strings:

        * ``EVERYWHERE`` – at each switch block of the FPGA
        * ``PERIMETER`` – at each perimeter switch block (x-directed and/or y-directed channel segments may terminate here)
        * ``CORNER`` – only at the corner switch blocks (both x and y-directed channels terminate here)
        * ``FRINGE`` – same as PERIMETER but excludes corners
        * ``CORE`` – everywhere but the perimeter

    Sets the location on the FPGA where the connections described by this switch block will be instantiated.

.. arch:tag:: <switchfuncs>

    The switchfuncs XML node contains one or more entries that specify the permutation functions with which different switch block sides should be connected, as described below.

.. arch:tag:: <func type="string" formula="string"/>


    :req_param type:
        Specifies which switch block sides this function should connect.
        With the switch block sides being left, top, right and bottom, the allowed entries are one of {``lt``, ``lr``, ``lb``, ``tr``, ``tb``, ``tl``, ``rb``, ``rl``, ``rt``, ``bl``, ``bt``, ``br``} where ``lt`` means that the specified permutation formula will be used to connect the left-top sides of the switch block.

        .. note:: In a bidirectional architecture the reverse connection is implicit.

    :req_param formula:
        Specifies the mathematical permutation function that determines the pattern with which the source/destination sets of wires (defined using the <wireconn> entries) at the two switch block sides will be connected.
        For example, ``W-t`` specifies a connection where the ``t``’th wire in the source set will connect to the ``W-t`` wire in the destination set where ``W`` is the number of wires in the destination set and the formula is implicitly treated as modulo ``W``.

        Special characters that can be used in a formula are:

        * ``t`` -- the index of a wire in the source set
        * ``W`` -- the number of wires in the destination set (which is not necessarily the total number of wires in the channel)

        The operators that can be used in the formula are:

        * Addition (``+``)
        * Subtraction (``-``)
        * Multiplication (``*``)
        * Division (``/``)
        * Brackets ``(`` and ``)`` are allowed and spaces are ignored.

    Defined under the ``<switchfuncs>`` XML node, one or more ``<func...>`` entries is used to specify permutation functions that connect different sides of a switch block.


.. arch:tag:: <wireconn num_conns="expr" from_type="string, string, string, ..." to_type="string, string, string, ..." from_switchpoint="int, int, int, ..." to_switchpoint="int, int, int, ..." from_order="{fixed | shuffled}" to_order="{fixed | shuffled}" switch_override="string"/>

    :req_param num_conns:
        Specifies how many connections should be created between the from_type/from_switchpoint set and the to_type/to_switchpoint set.
        The value of this parameter is an expression which is evaluated when the switch block is constructed.

        The expression can be a single number or formula using the variables:

        * ``from`` -- The number of switchblock edges equal to the 'from' set size.

            This ensures that each element in the 'from' set is connected to an element of the 'to' set.
            However it may leave some elements of the 'to' set either multiply-connected or disconnected.

            .. figure:: wireconn_num_conns_type_from.*
                :width: 100%

        * ``to`` -- The number of switchblock edges equal to the 'to' set size size.

            This ensures that each element of the 'to' set is connected to precisely one element of the 'from' set.
            However it may leave some elements of the 'from' set either multiply-connected or disconnected.

            .. figure:: wireconn_num_conns_type_to.*
                :width: 100%

        Examples:

        * ``min(from,to)`` --  Creates number of switchblock edges equal to the minimum of the 'from' and 'to' set sizes.

            This ensures *no* element of the 'from' or 'to' sets is connected to multiple elements in the opposing set.
            However it may leave some elements in the larger set disconnected.

            .. figure:: wireconn_num_conns_type_min.*
                :width: 100%

        * ``max(from,to)`` -- Creates number of switchblock edges equal to the maximum of the 'from' and 'to' set sizes.

            This ensures *all* elements of the 'from' or 'to' sets are connected to at least one element in the opposing set.
            However some elements in the smaller set may be multiply-connected.

            .. figure:: wireconn_num_conns_type_max.*
                :width: 100%

        * ``3*to`` -- Creates number of switchblock edges equal to three times the 'to' set sizes.

    :req_param from_type:
        A comma-separated list segment names that defines which segment types will be a source of a connection.
        The segment names specified must match the names of the segments defined under the ``<segmentlist>`` XML node.
        Required if no ``<from>`` or ``<to>`` nodes are specified within the ``<wireconn>``.

    :req_param to_type:
        A comma-separated list of segment names that defines which segment types will be the destination of the connections specified.
        Each segment name must match an entry in the ``<segmentlist>`` XML node.
        Required if no ``<from>`` or ``<to>`` nodes are specified within the ``<wireconn>``.

    :req_param from_switchpoint:
        A comma-separated list of integers that defines which switchpoints will be a source of a connection.
        Required if no ``<from>`` or ``<to>`` nodes are specified within the ``<wireconn>``.

    :req_param to_switchpoint:
        A comma-separated list of integers that defines which switchpoints will be the destination of the connections specified.
        Required if no ``<from>`` or ``<to>`` nodes are specified within the ``<wireconn>``.

        .. note:: In a unidirectional architecture wires can only be driven at their start point so ``to_switchpoint="0"`` is the only legal specification in this case.

    :opt_param from_order:
        Specifies the order in which ``from_switchpoint``s are selected when creating edges.

        * ``fixed`` -- Switchpoints are selected in the order specified

            This is useful to specify a preference for connecting to specific switchpoints.
            For example,

            .. code-block:: xml

                <wireconn num_conns="1*to" from_type="L16" from_switchpoint="0,12,8,4" from_order="fixed" to_type="L4" to_switchpoint="0"/>

            specifies L4 wires should be connected first to L16 at switchpoint 0, then at switchpoints 12, 8, and 4.
            This is primarily useful when we want to ensure that some switchpoints are 'used-up' first.


        * ``shuffled`` -- Switchpoints are selected in a (randomly) shuffled order

            This is useful to ensure a diverse set of switchpoints are used.
            For example,

            .. code-block:: xml

                <wireconn num_conns="1*to" from_type="L4" from_switchpoint="0,1,2,3" from_order="shuffled" to_type="L4" to_switchpoint="0"/>

            specifies L4 wires should be connected to other L4 wires at any of switchpoints 0, 1, 2, or 3.
            Shuffling the switchpoints is useful if one of the sets (e.g. from L4's) is much larger than the other (e.g. to L4's), and we wish to ensure a variety of switchpoints from the larger set are used.

        **Default:** ``shuffled``


    :opt_param to_order:
        Specifies the order in which ``to_switchpoint``s are selected when creating edges.

        .. note:: See ``from_switchpoint_order`` for value descritpions.

    :opt_param switch_override:

        Specifies the name of a switch to be used to override the wire_switch of the segments in the ``to`` set. 
        Can be used to create switch patterns where different switches are used for different types of connections. 
        By using a zero-delay and zero-resistance switch one can also create T and L shaped wire segments.
        
        **Default:** If no override is specified, the usual wire_switch that drives the ``to`` wire will be used. 

    .. arch:tag:: <from type="string" switchpoint="int, int, int, ..."/>

        :req_param type:

            The name of a segment specified in the ``<segmentlist>``.

        :req_param switchpoint:

            A comma-separated list of integers that defines switchpoints.

            .. note:: In a unidirectional architecture wires can only be driven at their start point so ``to_switchpoint="0"`` is the only legal specification in this case.

        Specifies a subset of *source* wire switchpoints.

        This tag can be specified multiple times.
        The surrounding ``<wireconn>``'s source set is the union of all contained ``<from>`` tags.

    .. arch:tag:: <to type="string" switchpoint="int, int, int, ..."/>

        Specifies a subset of *destination* wire switchpoints.

        This tag can be specified multiple times.
        The surrounding ``<wireconn>``'s destination set is the union of all contained ``<to>`` tags.

        .. seealso:: ``<from>`` for attribute descriptions.


    As an example, consider the following ``<wireconn>`` specification:

    .. code-block:: xml

        <wireconn num_conns_type="to"/>
            <from type="L4" switchpoint="0,1,2,3"/>
            <from type="L16" switchpoint="0,4,8,12"/>
            <to type="L4" switchpoint="0/>
        </wireconn>

    This specifies that the 'from' set is the union of L4 switchpoints 0, 1, 2 and 3; and L16 switchpoints 0, 4, 8 and 12.
    The 'to' set is all L4 switchpoint 0's.
    Note that since different switchpoints are selected from different segment types it is not possible to specify this without using ``<from>`` sub-tags.

.. _arch_metadata:

Architecture metadata
---------------------

Architecture metadata enables tagging of architecture or routing graph
information that exists outside of the normal VPR flow (e.g. pack, place,
route, etc).  For example this could be used to enable bitstream generation by
tagging routing edges and pb_type features.

The metadata will not be used by the vpr executable, but can be leveraged by
new tools using the libvpr library.  These new tools can access the metadata
on the various VPR internal data structures.

To enable tagging of architecture structures with metadata, the ``<metadata>``
tag can be inserted under the following XML tags:

* ``<pb_type>``
* Any tag under ``<interconnect>`` (``<direct>``, ``<mux>``, etc).
* ``<mode>``
* Any grid location type (``<perimeter>``, ``<fill>``, ``<corners>``, ``<single>``, ``<col>``, ``<row>``, ``<region>``)

.. arch:tag:: <metadata>

Specifies the root of a metadata block.  Can have 0 or more ``<meta>`` Children.

.. arch:tag:: <meta name="string" >

    :req_param name: Key name of this metadata value.

Specifies a value within a metadata block.  The name is a key
for looking up the value contained within the ``<meta>`` tag.  Keys can be
repeated, and will be stored in a vector in order of occurrence.

The value of the ``<meta>`` is the text in the block. Both the ``name`` and
``<meta>`` value will be stored as a string.  XML children are not supported
in the ``<meta>`` tag.

Example of a metadata block with 2 keys:

    .. code-block:: xml

      <metadata>
        <meta name="some_key">Some value</meta>
        <meta name="other key!">Other value!</meta>
      </metadata>




dev/c_api_doc.rst
--------------------------------------
===========================================
Sphinx API Documentation for C/C++ Projects
===========================================

The Sphinx API documentation for VTR C/C++ projects is created using
Doxygen and Breathe plugin. Doxygen is a standard tool for generating
documentation from annotated code. It is used to generate XML output that can
then be parsed by the Breathe plugin, which provides the RST directives used to
embed the code comments into the Sphinx documentation.

Every VPR C/C++ project requires a few steps that have to be completed,
to generate the Sphinx documentation:

  - Create doxyfile for the project
  - Update the Breathe config
  - Create RST files with the API description using Breathe directives
  - Generate the project documentation


Create Doxyfile
---------------

A doxyfile a Doxygen configuration file that provides all the necessary
information about the documented project. It is used to generate Doxygen
output in the chosen format.

The configuration includes the specification of input files, output directory,
generated documentation formats, and much more. The config for a particular
VPR project should be saved in the ``<vtr-verilog-to-routing>/doc/_doxygen``
directory. The doxyfile should be named as ``<key>.dox``, where ``<key>`` is
a ``breathe_projects`` dictionary key associated with the VPR project.

The minimal doxyfile should contain only the configuration
values that are not set by default. As mentioned before, the Breathe plugin
expects the XML input. Therefore the ``GENERATE_XML`` option should be turned on.
Below there is a content of ``vpr.dox`` file content, which contains
the VPR Doxygen configuration:

.. literalinclude:: ../../_doxygen/vpr.dox
   :name: vpr.dox

The general Doxyfile template can be generated using::

   doxygen -g template.dox


Breathe Configuration
---------------------

Breathe plugin is responsible for parsing the XML file generated
by the Doxygen. It provides the convenient RST directives that allow to
embed the read documentation into the Sphinx documentation.

To add the new project to the Sphinx API generation mechanism, you have to
update the ``breathe_projects`` dictionary in the Sphinx ``conf.py`` file.
The dictionary consist of key-value pairs which describe the project.
The key is related to the project name that will be used in the Breathe plugin
directives. The value associated with the key points to the directory where
the XML Doxygen output is generated.

Example of this configuration structure is presented below::

    breathe_projects = {
        "vpr"          : "../_build/doxygen/vpr/xml",
        "abc"          : "../_build/doxygen/abc/xml",
        "ace2"         : "../_build/doxygen/ace2/xml",
        "odin_ii"      : "../_build/doxygen/odin_ii/xml",
        "blifexplorer" : "../_build/doxygen/blifexplorer/xml",
    }

More information about the Breathe plugin can be found in the
`Breathe Documentation`_.


Create RST with API Documentation
---------------------------------

.. _Directives & Config Variables: https://breathe.readthedocs.io/en/latest/directives.html

To generate the Sphinx API documentation, you should use the directives
provided by the Breathe plugin. A complete list of Breathe directives
can be found in the `Directives & Config Variables`_ section of the
`Breathe Documentation`_.

Example of ``doxygenclass`` directive used for the VPR project is presented below::

   .. doxygenclass:: VprContext
      :project: vpr
      :members:


Generate the Documentation
--------------------------

Currently, the Doxygen is set up to run automatically whenever
the documentation is regenerated. The Doxygen XML generation is skipped
when the Doxygen is not installed on your machine or when the
``SKIP_DOXYGEN=True`` environment variable is set.

The Doxygen is being run for every project described in
the ``breathe_projects`` dictionary. Therefore it is essential to keep
the same name of the project name key and the doxyfile name.

.. _Breathe Documentation: https://breathe.readthedocs.io/en/latest/
.. _Directives & Config Variables: https://breathe.readthedocs.io/en/latest/directives.html



dev/code_documentation.rst
--------------------------------------
Documenting VTR Code with Doxygen
=================================

VTR uses Doxygen and Sphinx for generating code documentation. Doxygen is used
to parse a codebase, extract code comments, and save them into an XML file.
The XML is then read by the Sphinx Breathe plugin, which converts it
to an HTML available publicly in the project documentation. The documentation
generated with Sphinx can be found in the API Reference section.

This note presents how to document source code in the VTR project
and check whether Doxygen can parse the created description. Code
conventions presented below were chosen arbitrarily for the project,
from many more options available in Doxygen. To read more about the tool,
visit the official `Doxygen documentation`_.

Documenting Code
----------------

There are three basic types of Doxygen code comments used in the VTR documentation:

- block comments
- one-line comments before a code element
- one-line comments after an element member

In most cases, a piece of documentation should be placed before a code
element. Comments after an element should be used only for documenting
members of enumeration types, structures, and unions.

Block Comments
++++++++++++++

You should use Doxygen block comments with both brief and detailed
descriptions to document code by default. As the name suggests, a brief
description should be a one-liner with general information about
the code element. A detailed description provides more specific
information about the element, its usage, or implementation details.
In the case of functions and methods, information about parameters and
returned value comes after the detailed description. Note that brief
and detailed descriptions have to be separated with at least one empty line.

Here is an example of a Doxygen block comment:

.. code-block:: c

   /**
    * @brief This is a brief function description
    *
    * This is a detailed description. It should be separated from
    * the brief description with one blank line.
    *
    *   @param a    A description of a
    *   @param b    A description of b
    *
    *   @return     A return value description
    */
   int my_func(int a, int b) {
       return a + b;
   }

General guidelines for using Doxygen block comments:

#. A block-comment block **has to** start with the ``/**``, otherwise
   Doxygen will not recognize it. All the comment lines **have to** be
   preceded by an asterisk. All the asterisks **have to** create a straight
   vertical line.

#. Brief and detailed descriptions **have to** be separated with one
   empty line.

#. A detailed description and a parameter list **should be** separated with
   one empty line.

#. A parameter list **should be** indented one level. All the parameter
   descriptions should be aligned together.

#. A returned value description should be separated with one empty line
   from either a detailed or a parameter description.

One-line Comments Before an Element
++++++++++++++++++++++++++++++++++++

One-line comments can be used instead of the block comments described above,
only if a brief description is sufficient for documenting the particular code
element. Usually, this is the case with global variables and defines.

Here is an example of a one-line Doxygen comment (before a code element):

.. code-block:: c

  /// @brief This is one-line documentation comment
  int var = 0;

General guidelines for using Doxygen one-line comments (before a code element):

#. A one-line comment before an element **has to** start with ``///``,
   otherwise Doxygen will not recognize it.

#. Since this style of code comments **should be** used only for
   brief descriptions, it **should** contain a ``@brief`` tag.

#. One-line comments **should not** be overused. They are acceptable for
   single variables and defines, but more complicated elements like classes and
   structures should be documented more carefully with Doxygen block
   comments.

One-line Comments After an Element Member
++++++++++++++++++++++++++++++++++++++++++

There is another type of one-line code comments used to document members
of enumeration types, structures, and unions. In those situations, the whole
element should be documented in a standard way using a Doxygen block comment.
However, the particular element members should be described after
they appear in the code with the one-line comments.

Here is an example of a one-line Doxygen comment (after an element member):

.. code-block:: c

   /**
    * @brief This is a brief enum description
    *
    * This is a detailed description. It should be separated from
    * the brief description with one blank line
    */
   enum seasons {
       spring = 3, ///< Describes spring enum value
       summer,     ///< Describes summer enum value
       autumn = 7, ///< Describes autumn enum value
       winter      ///< Describes winter enum value
   };

General guidelines for using Doxygen one-line comments (after an element member):

#. One-line code comment after an element member **has to** start with
   ``///<``, otherwise Doxygen will not recognize it.

#. This comment style **should be** used together with a Doxygen block
   comment for describing the whole element, before the members' description.

Documenting Files
-----------------

All files that contain the source code should be documented with
a Doxygen-style header. The file description in Doxygen is similar to
code element description, and should be placed at the beginning of the file.
The comment should contain information about an author, date of the document
creation, and a description of functionalities introduced in the file.

Here is an example of file documentation:

.. code-block:: c

   /**
    * @file
    * @author  John Doe
    * @date    2020-09-03
    * @brief   This is a brief document description
    *
    * This is a detailed description. It should be separated from
    * the brief description with one blank line
    */

General suggestions about a Doxygen file comments:

#. A file comment **has to** start with the ``@file`` tag,
   otherwise it will not be recognized by Doxygen.

#. The ``@file``, ``@author``, ``@date``, and ``@brief`` tags **should** form
   a single group of elements. A detailed description (if available)
   **has to** be placed one empty line after the brief description.

#. A file comment **should** consist of at least the ``@file`` and ``@brief``
   tags.

Validation of Doxygen Comments (Updating API Reference)
-------------------------------------------------------

Validation of Doxygen code comments might be time-consuming since it
requires setting the whole Doxygen project using Doxygen configuration
files (doxyfiles). One solution to that problem is to use the configuration
created for generating the official VTR documentation. The following steps
will show you how to add new code comments to the Sphinx API Reference,
available in the VTR documentation:

#. Ensure that the documented project has a doxyfile, and it is added to
   breathe configuration. All the doxyfiles used by the Sphinx documentation
   are placed in ``<vtr_root>/doc/_doxygen`` (For details, check
   :doc:`Sphinx API Documentation for C/C++ Projects <c_api_doc>`)
   This will ensure that Doxygen XMLs will be created for that project
   during the Sphinx documentation building process.

#. Check that the ``<vtr_root>/doc/src/api/<project_name>`` directory with
   a ``index.rst`` file exists. If not, create both the directory and the
   index file. Here is an example of the ``index.rst`` file for the VPR project.

   .. code-block:: rst

      VPR API
      =======

      .. toctree::
         :maxdepth: 1

         contexts
         netlist

   .. note::

      Do not forget about adding the index file title. The ``====`` marks
      should be of the same length as the title.

#. Create a RST file, which will contain the references to the Doxygen
   code comments. Sphinx uses the Breathe plugin for extracting Doxygen
   comments from the generated XML files. The simplest check can be done by
   dumping all the Doxygen comments from the single file with
   a ``..doxygenfile ::`` directive.

   Assuming that your RST file name is ``myrst.rst``, and you created it to check
   the Doxygen comments in the ``mycode.cpp`` file within the ``vpr`` project,
   the contents of the file might be the following:

   .. code-block:: rst

      =====
      MyRST
      =====

      .. doxygenfile:: mycode.cpp
         :project: vpr

   .. note::

      A complete list of Breathe directives can be found in the
      `Breathe documentation`_

#. Add the newly created RST file to the ``index.rst``. In this example, that
   will lead to the following change in the ``index.rst``:

   .. code-block:: rst

      VPR API
      =======

      .. toctree::
         :maxdepth: 1

         contexts
         netlist
         myrst

#. Generate the Sphinx documentation by using ``make html`` command inside
   the ``<vtr_root>/doc/`` directory.

#. The new section should be available in the API Reference. To verify that
   open the ``<vtr_root>/doc/_build/html/index.html`` with your browser and
   check the API Reference section. If the introduced code comments are
   unavailable, you can analyze the Sphinx build log.

Additional Resources
--------------------

- `Doxygen documentation`_
- `Breathe documentation`_

.. _Breathe documentation: https://breathe.readthedocs.io/en/latest/
.. _Doxygen documentation: https://www.doxygen.nl/index.html



dev/index.rst
--------------------------------------
.. _developer_guide:

Developer Guide
---------------

.. toctree::
    :glob:

    ../CONTRIBUTING
    ../README.developers.md
    c_api_doc
    code_documentation
    tutorials/index
    ../SUPPORT
    ../LICENSE



tutorials/edit_vpr_ui.rst
--------------------------------------
.. _edit_vpr_ui:

VPR UI and Graphics
--------------

VPR's UI is created with GTK, and actively maintained/edited through the use of Glade and a main.ui file. We prefer to not use code initializations of Gtk Buttons/UI objects, and instead make them with Glade. 
This allows for better organized menus and visual editing of the UI. Please consult the attached guide for Glade: <Link Glade/Gtk quickstart> (WIP as of August 24th, 2022). 

When connecting a button to its function, place it in an appropriate function depending on the drop down menu it will go in. Button setups are done in ui_setup.cpp/h and callback functions are in draw_toggle_functions.cpp/h.

VPR Graphics are drawn using the EZGL graphics library, which is a wrapper around the GTK graphics library (which is used for the UI). EZGL Documentation is found here: http://ug251.eecg.utoronto.ca/ece297s/ezgl_doc/index.html and GTK documentation is found here: https://docs.gtk.org/gtk3/

Make sure to test the UI when you edit it. Ensure that the graphics window opens (using the --disp on command) and click around the UI to ensure the buttons still work. Test all phases (Placement -> Routing) as the UI changes between them. 



tutorials/index.rst
--------------------------------------
.. _developer_tutorials:

Developer Tutorials
-------------------

.. toctree::
    :glob:

    new_developer_tutorial
    timing_graph_debug/index
    edit_vpr_ui


tutorials/new_developer_tutorial.rst
--------------------------------------
.. _new_developer_tutorial:

New Developer Tutorial
----------------------

Overview
~~~~~~~~

Welcome to the Verilog-to-Routing (VTR) Project. This project is an open-source, international, collaboration towards a comprehensive FPGA architecture exploration system that includes CAD tools, benchmarks, transistor-optimized architecture files, and documentation, along with support to make this all fit together and work. The purpose of this tutorial is to equip you, the new developer, with the tools and understanding that you need to begin making a useful contribution to this project.

While you are going through this tutorial, please record down things that should be changed. Whether it is the tutorial itself, documentation, or other parts of the VTR project. Your thoughts are valuable and welcome because fresh eyes help evaluate whether or not our work is clearly presented.


Environment Setup
~~~~~~~~~~~~~~~~~

Log into your workstation/personal computer. Check your account for general features such as internet, printing, git, etc. If there are problems at this stage, talk to your advisor to get this setup.

If you are not familiar with development on Linux, this is the time to get up to speed. Look up online tutorials on general commands, basic development using Makefiles, etc.

Background Reading
~~~~~~~~~~~~~~~~~~

Read the first two chapters of "Architecture and CAD for deep-submicron FPGAs" by Vaughn Betz, et al. This is a great introduction to the topic of FPGA CAD and architecture. Note though that this book is old so it only covers a small core of what the VTR project is currently capable of.

Read chapters 1 to 5 of "FPGA Architecture: Survey and Challenges" by Ian Kuon et al.

Review material learned with fellow colleagues.

Setup VTR
~~~~~~~~~

Use git to clone a copy of VTR from the GitHub repository:

    https://github.com/verilog-to-routing/vtr-verilog-to-routing

Build the project by running the ``make`` command

Run ``./run_quick_test.pl`` to check that the build worked

Follow the Quick Start Guilde, and Basic Design Flow Tutorial found in the VTR Documentation (docs.verilogtorouting.org).
These tutorials will allow you to run a circuit through the entire flow and read the statistics gathered from that run.

Use VTR
~~~~~~~

Create your own custom Verilog file. Create your own custom architecture file using one of the existing architecture files as a template. Use VTR to map that circuit that you created to that architecture that you created. The VTR documentation, to be found at the https:///docs.verilogtorouting.org will prove useful. You may also wish to look at the following links for descriptions of the language used inside the architecture files:

  * Architecture Description and Packing: http://www.eecg.utoronto.ca/~jluu/publications/luu_vpr_fpga2011.pdf

  * Classical Soft Logic Block Example: http://www.eecg.utoronto.ca/vpr/utfal_ex1.html

Perform a simple architecture experiment. Run an experiment that varies Fc_in from 0.01 to 1.00 on the benchmarks ch_intrinsics, or1200, and sha.  Use `tasks/timing` as your template.  Graph the geometric average of minimum channel width and critical path delay for these three benchmarks across your different values of Fc_in. Review the results with your colleagues and/or advisor.

Open the Black Box
~~~~~~~~~~~~~~~~~~

At this stage, you have gotten a taste of how an FPGA architect would go about using VTR.  As a developer though, you need a much deeper understanding of how this tool works.  The purpose of this section is to have you to learn the details of the VTR CAD flow by having you manually do what the scripts do.

Using the custom Verilog circuit and architecture created in the previous step, directly run Odin II on it to generate a blif netlist.  You may need to skim the ``odin_ii/README.rst`` and the ``vtr_flow/scripts/run_vtr_flow.py``.

Using the output netlist of Odin II, run ABC to generate a technology-mapped blif file.  You may need to skim the ABC homepage (http://www.eecs.berkeley.edu/~alanmi/abc/).

.. code-block:: shell

    # Run the ABC program from regular terminal (bash shell)
    $VTR_ROOT/abc abc

    # Using the ABC shell to read and write blif file
    abc 01> read_blif Odin_II_output.blif
    abc 01> write_blif abc_output.blif

Using the output of ABC and your architecture file, run VPR to complete the mapping of a user circuit to a target architecture.  You may need to consult the VPR User Manual.


.. code-block:: shell

    # Run the VPR program 
    $VTR_ROOT/vpr vpr architecture.xml abc_output.blif

Read the VPR section of the online documentation.

Submitting Changes and Regression Testing
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Read ``README.developers.md`` in the base directory of VTR. Code changes rapidly so please help keep this up to date if you see something that is out of date.

Make your first change to git by modifying ``README.md`` and pushing it.  I recommend adding your name to the list of contributors.  If you have nothing to modify, just add/remove a line of whitespace at the bottom of the file.

Now that you have completed the tutorial, you should have a general sense of what the VTR project is about and how the different parts work together.  It's time to talk to your advisor to get your first assignment.









timing_graph_debug/index.rst
--------------------------------------
.. _timing_graph_debug_tutorial:

Timing Graph Debugging Tutorial
-------------------------------

When developing VPR or creating/calibrating the timing characteristics of a new architectural model it can be helpful to look 'inside' at VPR's timing graph and analysis results.

.. warning:: This is a very low-level tutorial suitable for power-users and VTR developers

Generating a GraphViz DOT file of the Entire Timing Graph
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

One approach is to have VPR generate a GraphViz DOT file, which visualizes the structure of the timing graph, and the analysis results.
This is enabled by running VPR with :option:`vpr --echo_file` set to ``on``.
This will generate a set of ``.dot`` files in the current directory representing the timing graph, delays, and results of Static Timing Analysis (STA).

.. code-block:: bash

    $ vpr $VTR_ROOT/vtr_flow/arch/timing/EArch.xml $VTR_ROOT/vtr_flow/benchmarks/blif/multiclock/multiclock.blif --echo_file on

    $ ls *.dot
    timing_graph.place_final.echo.dot  timing_graph.place_initial.echo.dot  timing_graph.pre_pack.echo.dot

The ``.dot`` files can then be visualized using a tool like ``xdot`` which draws an interactive version of the timing graph.

.. code-block:: bash

    $ xdot timing_graph.place_final.echo.dot

.. warning:: On all but the smallest designs the full timing graph .dot file is too large to visualize with xdot. See the next section for how to show only a subset of the timing graph.

Which will bring up an interactive visualization of the graph:

.. figure:: multiclock_timing_graph_dot.*

    Full timing graph visualized with xdot on a very small multi-clock circuit.

Where each node in the timing graph is labeled

.. code-block:: none

    Node(X) (TYPE)

Where ``Node(X)`` (e.g. ``Node(3)``) represents the ID of the timing graph node, and ``(TYPE)`` (e.g. ``OPIN``) is the type of node in the graph.

Each node is also annotated with timing information (produced by STA) like

.. code-block:: none

    DATA_ARRIVAL
    Domain(1) to * from Node(16)
    time: 5.2143e-10

    DATA_ARRIVAL
    Domain(2) to * from Node(20)
    time: 6.9184e-10

    DATA_REQUIRED
    Domain(1) to Domain(1) for Node(24)
    time: -2.357e-10

    SLACK
    Domain(1) to Domain(1) for Node(24)
    time: -5.45e-10

where the first line of each entry is the type of timing information (e.g. data arrival time, data required time, slack),
the second line indicates the related launching and capture clocks (with ``*`` acting as a wildcard) and the relevant timing graph node which originated the value,
and the third line is the actual time value (in seconds).

The edges in the timing graph are also annoated with their Edge IDs and delays.
Special edges related to setup/hold (``tsu``, ``thld``) and clock-to-q delays (``tcq``) of sequential elements (e.g. Flip-Flops) are also labeled and drawn with different colors.


Generating a GraphViz DOT file of a subset of the Timing Graph
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For most non-trivial designs the entire timing graph is too large and difficult to visualize.

To assist this you can generate a DOT file for a subset of the timing graph with :option:`vpr --echo_dot_timing_graph_node`

.. code-block:: bash

    $ vpr $VTR_ROOT/vtr_flow/arch/timing/EArch.xml $VTR_ROOT/vtr_flow/benchmarks/blif/multiclock/multiclock.blif --echo_file on --echo_dot_timing_graph_node 23

Running ``xdot timing_graph.place_final.echo.dot`` now shows the only the subset of the timing graph which fans-in or fans-out of the specified node (in this case node ``23``).

.. figure:: multiclock_timing_graph_dot_node_23.*

    Subset of the timing graph which fans in and out of node 23.

Cross-referencing Node IDs with VPR Timing Reports
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The DOT files only label timing graph nodes with their node IDs.
When debugging it is often helpful to correlate these with what are seen in timing reports.

To do this, we need to have VPR generate more detailed timing reports which have additional debug information.
This can be done with :option:`vpr --timing_report_detail` set to ``debug``:

.. code-block:: bash

    $ vpr $VTR_ROOT/vtr_flow/arch/timing/EArch.xml $VTR_ROOT/vtr_flow/benchmarks/blif/multiclock/multiclock.blif --timing_report_detail debug

    $ ls report_timing*
    report_timing.hold.rpt report_timing.setup.rpt

Viewing ``report_timing.setup.rpt``:

.. code-block:: none

    #Path 6
    Startpoint: FFB.Q[0] (.latch at (1,1) tnode(15) clocked by clk2)
    Endpoint  : FFD.D[0] (.latch at (1,1) tnode(25) clocked by clk2)
    Path Type : setup

    Point                                                             Incr      Path
    --------------------------------------------------------------------------------
    clock clk2 (rise edge)                                           0.000     0.000
    clock source latency                                             0.000     0.000
    clk2.inpad[0] (.input at (3,2) tnode(4))                         0.000     0.000
    | (intra 'io' routing)                                           0.042     0.042
    | (inter-block routing:global net)                               0.000     0.042
    | (intra 'clb' routing)                                          0.000     0.042
    FFB.clk[0] (.latch at (1,1) tnode(9))                            0.000     0.042
    | (primitive '.latch' Tcq_max)                                   0.124     0.166
    FFB.Q[0] (.latch at (1,1) tnode(15)) [clock-to-output]           0.000     0.166
    | (intra 'clb' routing)                                          0.120     0.286
    to_FFD.in[1] (.names at (1,1) tnode(21))                         0.000     0.286
    | (primitive '.names' combinational delay)                       0.235     0.521
    to_FFD.out[0] (.names at (1,1) tnode(23))                        0.000     0.521
    | (intra 'clb' routing)                                          0.000     0.521
    FFD.D[0] (.latch at (1,1) tnode(25))                             0.000     0.521
    data arrival time                                                          0.521

    clock clk2 (rise edge)                                           0.000     0.000
    clock source latency                                             0.000     0.000
    clk2.inpad[0] (.input at (3,2) tnode(4))                         0.000     0.000
    | (intra 'io' routing)                                           0.042     0.042
    | (inter-block routing:global net)                               0.000     0.042
    | (intra 'clb' routing)                                          0.000     0.042
    FFD.clk[0] (.latch at (1,1) tnode(8))                            0.000     0.042
    clock uncertainty                                                0.000     0.042
    cell setup time                                                 -0.066    -0.024
    data required time                                                        -0.024
    --------------------------------------------------------------------------------
    data required time                                                        -0.024
    data arrival time                                                         -0.521
    --------------------------------------------------------------------------------
    slack (VIOLATED)                                                          -0.545

We can see that the elements corresponding to specific timing graph nodes are labeled with ``tnode(X)``.
For instance:

.. code-block:: none

    to_FFD.out[0] (.names at (1,1) tnode(23))                        0.000     0.521

shows the netlist pin named ``to_FFD.out[0]`` is ``tnode(23)``, which corresponds to ``Node(23)`` in the DOT file.



odin/index.rst
--------------------------------------
.. _odin_II:

###########
Odin II
###########

Odin II is used for logic synthesis and elaboration, converting a subset of the Verilog Hardware Description Language (HDL) into a BLIF netlist.

.. note::
    Odin-II has been deprecated and will be removed in a future version. Now VTR uses Parmys as the default frontend which utilizes Yosys as elaborator with partial mapping features enabled.

    To build the VTR flow with the Odin-II front-end you may use the VTR Makefile wrapper, by calling the ``make CMAKE_PARAMS="-DWITH_ODIN=ON"`` command in the `$VTR_ROOT` directory.

.. toctree::
   :glob:
   :maxdepth: 2
   :Caption: Quickstart

   quickstart


.. toctree::
   :glob:
   :maxdepth: 2
   :Caption: User Guide

   user_guide

.. toctree::
   :glob:
   :maxdepth: 2
   :Caption: verilog Support

   verilog_support

.. toctree::
   :glob:
   :maxdepth: 2
   :Caption: Developer Guide

   dev_guide/contributing
   dev_guide/regression_test
   dev_guide/verify_script
   dev_guide/testing



odin/quickstart.md
--------------------------------------
# Quickstart

## Prerequisites

- ctags
- bison
- flex
- gcc 5.x
- cmake 3.9 (minimum version)
- time
- cairo

## Building

To build you may use the Makefile wrapper in the $VTR_ROOT/odin_ii ``make build`` To build with debug symbols you may use the Makefile wrapper in $VTR_ROOT/odin_ii ``make debug``

> *NOTE*
>
> ODIN uses CMake as it’s build system. CMake provides a portable cross-platform build systems with many useful features.
> For unix-like systems we provide a wrapper Makefile which supports the traditional make and make clean commands, but calls CMake behind the scenes.

> *WARNING*
>
> After you build Odin, please run from the $VTR_ROOT/odin_ii ``make test``.
> This will simulate and verify all of the included microbenchmark circuits to ensure that Odin is working correctly on your system.

## Basic Usage

./odin_ii [arguments]

*Requires one and only one of `-c`, `-v`, or `-b`

| arg  | following argument     | Description                                                                           |
|------|---|---|
| `-c` | XML Configuration File | an XML configuration file dictating the runtime parameters of odin                    |
| `-v` | Verilog HDL File       | You may specify multiple space-separated verilog HDL files                            |
| `-b` | BLIF File              | You may specify multiple space-separated blif files                                   |
| `-o` | BLIF output file       | full output path and file name for the blif output file                               |
| `-a` | architecture file      | You may not specify the architecture file, which results in pure soft logic synthesis |
| `-h` |                        | Print help                                                                            |

## Example Usage

The following are simple command-line arguments and a description of what they do. 
It is assumed that they are being performed in the odin_ii directory.

```bash
   ./odin_ii -v <path/to/verilog/File>
```

Passes a verilog HDL file to Odin II where it is synthesized. 
Warnings and errors may appear regarding the HDL code.

```bash
   ./odin_ii -b <path/to/blif/file>
```

Passes a blif file to Odin II where it is synthesized.

```bash
   ./odin_ii -v <path/to/verilog/File> -a <path/to/arch/file> -o myModel.blif
```

Passes a verilog HDL file and and architecture to Odin II where it is synthesized. 
Odin will use the architecture to do technology mapping.
Odin will output the blif in the current directory at `./myModel.blif`
Warnings and errors may appear regarding the HDL code.



odin/user_guide.md
--------------------------------------
# User guide

## Synthesis Arguments

|  arg  | following argument      | Description                                            |
|-------|:-----------------------:|------------------------------------------------------- |
|  `-c` |  XML Configuration File | XML runtime directives for the syntesizer such as the verilog file, and parametrized synthesis |
|  `-v` |  Verilog HDL File       | You may specify multiple space-separated verilog HDL files for synthesis                       |
|  `-b` |  BLIF File              | You may **not** specify multiple BLIF files as only single input BLIF file is accepted         |
|  `-o` |  BLIF output file       | full output path and file name for the BLIF output file                                        |
|  `-a` |  architecture file      | You may specify multiple space-separated Verilog HDL files for synthesis                       |
|  `-G` |                         | Output netlist graph in GraphViz viewable .dot format. (net.dot, opens with dotty)             |
|  `-A` |                         | Output AST graph in in GraphViz viewable .dot format.                                          |
|  `-W` |                         | Print all warnings. (Can be substantial.)                                                      |
|  `-h` |                         | Print help                                                                                     |

## Simulation Arguments

*To activate simulation you must pass one and only one of the following argument:*

- `-g <number of random vector>`
- `-t <input vector file>`
  
Simulation always produces the folowing files:

- input\_vectors
- output\_vectors
- test.do (ModelSim)

| arg    |  following argument         |  Description                                                                        |
|--------|:---------------------------:|------------------------------------------------------------------------------------|   
|  `-g`  |Number of random test vectors| will simulate the generated netlist with the entered number of clock cycles using pseudo-random test vectors. \\These vectors and the resulting output vectors are written to “input_vectors” and “output_vectors” respectively|
|  `-t`  | Input vector file                 | Supply a predefined input vector file           |
|  `-T` |  output vector file|The output vectors is verified against the supplied predefined output vector file              |
|  `-E` |                    |Output after both edges of the clock. (Default is to output only after the falling edge.)      |
|  `-R` |                    |Output after rising edge of the clock only. (Default is to output only after the falling edge.)|
|  `-p` |comma seperated list|Comma-separated list of additional pins/nodes to monitor during simulation. (view NOTES)       |
|  `-U0` |                                   |initial register value to 0      |
|  `-U1` |                                   |initial resigster value to 1 |
|  `-UX` |                                   |initial resigster value to X (unknown) (DEFAULT) |
|  `-L`  |  Comma seperated list|Comma-separated list of primary inputs to hold high at cycle 0, and low for all subsequent cycles.|
|  `-3`  |   |Generate three valued logic. (Default is binary.) |

## Examples

### Example for `-p`

|  argument                  |    explanation                                                    |
|-----------------------|:------------------------------------------------------:|
| `-p input~0,input~1`  | monitors pin 0 and 1 of input                          |
| `-p input`           | monitors all pins of input as a single port            |
| `-p input~`          | monitors all pins of input as separate ports. (split)  |

> **NOTE**
>
> Matching for `-p` is done via strstr so general strings will match all
> similar pins and nodes. (Eg: FF\_NODE will create a single port with
> all flipflops)

### Example of .xml configuration file for `-c`

```XML
<config>
    <verilog_files>
        <!-- Way of specifying multiple files in a project! -->
        <verilog_file>verilog_file.v</verilog_file>
    </verilog_files>
    <output>
        <!-- These are the output flags for the project -->
        <output_type>blif</output_type>
        <output_path_and_name>./output_file.blif</output_path_and_name>
        <target>
            <!-- This is the target device the output is being built for -->
            <arch_file>fpga_architecture_file.xml</arch_file>
        </target>
    </output>
    <optimizations>
        <!-- This is where the optimization flags go -->
    </optimizations>
    <debug_outputs>
        <!-- Various debug options -->
        <debug_output_path>.</debug_output_path>
        <output_ast_graphs>1</output_ast_graphs>
        <output_netlist_graphs>1</output_netlist_graphs>
    </debug_outputs>
</config>
```

> **NOTE**
>
> Hard blocks can be simulated; given a hardblock named `block` in the
> architecture file with an instance of it named `instance` in the
> verilog file, write a C method with signature defined in
> `SRC/sim_block.h` and compile it with an output filename of
> `block+instance.so` in the directory you plan to invoke Odin\_II from.
>
> When compiling the file, you'll need to specify the following
> arguments to the compiler (assuming that you're in the SANBOX
> directory):
>
> `cc -I../../libarchfpga_6/include/ -L../../libarchfpga_6 -lvpr_6 -lm --shared -o block+instance.so block.c.`
>
> If the netlist generated by Odin II contains the definition of a
> hardblock which doesn't have a shared object file defined for it in
> the working directory, Odin II will not work if you specify it to use
> the simulator with the `-g` or `-t` options.

> **WARNING**
>
> Use of static memory within the simulation code necessitates compiling
> a distinct shared object file for each instance of the block you wish
> to simulate. The method signature the simulator expects contains only
> int and int[] parameters, leaving the code provided to simulate the
> hard block agnostic of the internal Odin II data structures. However,
> a cycle parameter is included to provide researchers with the ability
> to delay results of operations performed by the simulation code.
>

### Examples vector file for `-t` or `-T`

``` bash
## Example vector input file
GLOBAL_SIM_BASE_CLK intput_1 input_2 input_3 clk_input
## Comment
0 0XA 1011 0XD 0
0 0XB 0011 0XF 1
0 0XC 1100 0X2 0
```

``` bash
## Example vector output file
output_1 output_2
## Comment
1011 0Xf
0110 0X4
1000 0X5
```

> **NOTE**
>
> Each line represents a vector. Each value must be specified in binary
> or hex. Comments may be included by placing an \# at the start of the
> line. Blank lines are ignored. Values may be separated by non-newline
> whitespace. (tabs and spaces) Hex values must be prefixed with 0X or 0x.
>
> Each line in the vector file represents one cycle, or one falling edge
> and one rising edge. Input vectors are read on a falling edge, while
> output vectors are written on a rising edge.
>
> The input vector file does not have a clock input, it is assumed it is
> controlled by a single global clock that is why it is necessary to add
> a GLOBAL_SIM_BASE_CLK to the input. To read more about this please
> visit [here](http://www.cs.columbia.edu/~cs6861/sis/blif/index.html).

### Examples using vector files `-t` and `-T`

A very useful function of Odin II is to compare the simulated output vector file with the expected output vector file based on an input vector file and a verilog file.
To do this the command line should be:

```shell
./odin_ii -v <Path/to/verilog/file> -t <Path/to/Input/Vector/File> -T <Path/to/Output/Vector/File>
```

An error will arrise if the output vector files do not match.

Without an expected vector output file the command line would be:

```shell
./odin_ii -v <Path/to/verilog/file> -t <Path/to/Input/Vector/File>
```

The generated output file can be found in the current directory under the name output_vectors.

### Example using vector files `-g`

This function generates N amounnt of random input vectors for Odin II to simulate with.

```shell
./odin_ii -v <Path/to/verilog/file> -g 10
```

This example will produce 10 autogenerated input vectors. These vectors can be found in the current directory under input_vectors  and the resulting output vectors can be found under output_vectors.

## Getting Help

If you have any questions or concerns there are multiple outlets to express them.
There is a [google group](https://groups.google.com/forum/#!forum/vtr-users) for users who have questions that is checked regularly by Odin II team members.
If you have found a bug please make an issue in the [vtr-verilog-to-routing GitHub repository](https://github.com/verilog-to-routing/vtr-verilog-to-routing/issues?q=is%3Aopen+is%3Aissue+label%3AOdin).

## Reporting Bugs and Feature Requests

### Creating an Issue on GitHub

Odin II is still in development and there may be bugs present.
If Odin II doesn't perform as expected or doesn't adhere to the Verilog Standard, it is important to create a [bug report](https://github.com/verilog-to-routing/vtr-verilog-to-routing/issues/new/choose) in the GitHub repository.
There is a template included, but make sure to include micro-benchmark(s) that reproduces the bug. This micro-benchmark should be as simple as possible.
It is important to link some documentation that provides insight on what Odin II is doing that differs from the Verilog Standard.
Linked below is a pdf of the IEEE Standard of Verilog (2005) that could help.

[IEEE Standard for Verilog Hardware Description Language](http://staff.ustc.edu.cn/~songch/download/IEEE.1364-2005.pdf)

If unsure, there are several outlets to ask questions in the [Help](./help.md) section.

### Feature Requests

If there are any features that the Odin II system overlooks or would be a great addition, please make a [feature request](https://github.com/verilog-to-routing/vtr-verilog-to-routing/issues/new/choose) in the GitHub repository. There is a template provided and be as in-depth as possible.



odin/verilog_support.md
--------------------------------------
# Verilog Support

## Lexicon

### Verilog Synthesizable Operators Support

| Supported Operators|NOT Sup. Operators |
|--------------------|-------------------|
| !=                 |                   |
| !==                |                   |
|  ==                |                   |
|  ===               |                   |  
|  =\>               |                   |  
|  \*\*              |                   |  
|  \^\~              |                   |  
|  \<\<\<            |                   |  
|  \>\>\>            |                   |  
| \>=                |                   |
| \|\|               |                   |
|  \~&               |                   |  
| &&                 |                   |
| \<\<               |                   |
|\<=                 |                   |
|\>\>                |                   |
| \~\^               |                   |
| \~|                |                   |
|-:                  |                   |
|+:                  |                   |

### Verilog NON-Synthesizable Operator Support

| Supported Operators|NOT Sup. Operators|
|--------------------|------------------|
|    &&&             |                  |

### Verilog Synthesizable Keyword Support

| Supported Keyword | NOT Sup. Keyword |
|-------------------|------------------|
| @()               |   repeat         |
|@\*                |   deassign       |
|always             |   edge           |  
|and                |   forever        |  
|assign             |   disable        |
|case               |                  |  
|defparam           |                  |  
|end                |                  |
|endfunction        |                  |
|endmodule          |                  |
|begin              |                  |
|default            |                  |
|else               |                  |
|endcase            |                  |
|endspecify         |                  |
|for                |                  |
|function           |                  |
|if                 |                  |
|inout              |                  |
|input              |                  |
|integer            |                  |
|localparam         |                  |
|module             |                  |
|nand               |                  |
|negedge            |                  |
|nor                |                  |
|not                |                  |
|or                 |                  |
|output             |                  |
|parameter          |                  |
|posedge            |                  |
|reg                |                  |
|specify            |                  |
|while              |                  |
|wire               |                  |
|xnor               |                  |
|xor                |                  |
|macromodule        |                  |
|generate           |                  |
|genvar             |                  |
|automatic          |                  |
|task               |                  |
|endtask            |                  |
|signed             |                  |

### Verilog NON-Synthesizable Keyword Support

|Supported Keyword| NOT Sup. Keyword|
|-----------------|-----------------|
|initial          |casex            |
|specparam        | casez           |
|                 | endprimitive    |
|                 | endtable        |
|                 | event           |
|                 | force           |
|                 | fork            |
|                 | join            |
|                 | primitive       |
|                 | release         |
|                 | table           |
|                 | time            |
|                 | wait            |
  
### Verilog Gate Level Modeling Support

|Supported Keyword   | NOT Sup. Keyword |
|--------------------|------------------|
|buf                 | bufif0           |
|                    | bufif1           |
|                    | cmos             |
|                    | highz0           |
|                    | highz0           |
|                    | highz1           |
|                    | highz1           |
|                    | large            |
|                    | medium           |
|                    | nmos             |
|                    | notif0           |
|                    | notif1           |
|                    | pmos             |
|                    | pull0            |
|                    | pull1            |
|                    | pulldown         |
|                    | pullup           |
|                    | rcmos            |
|                    | rnmos            |
|                    | rpmos            |
|                    | rtran            |
|                    | rtranif0         |
|                    | rtranif1         |
|                    | scalared         |
|                    | small            |
|                    | strong0          |
|                    | strong0          |
|                    | strong1          |
|                    |strong1           |
|                    |supply0           |
|                    | supply1          |
|                    |tran              |
|                    | tranif0          |
|                    | tranif1          |
|                    | tri              |
|                    | tri0             |
|                    | tri1             |
|                    | triand           |
|                    | trior            |
|                    | vectored         |
|                    | wand             |
|                    | weak0            |
|                    | weak0            |
|                    | weak1            |
|                    | weak1            |
|                    | wor              |

### C Functions support

| Supported Functions|
|--------------------|
|$clog2              |
|$unsigned           |
|$signed             |
|$finish             |
|$display            |

### Verilog Synthesizable preprocessor Keywords Support

| Supported Keywords |NOT Sup. Keywords      |
|--------------------|-----------------------|
| \`ifdef            | \`timescale           |
| \`elsif            | \`pragma              |
| \`ifndef           | \`line                |
| \`else             | \`celldefine          |
| \`define           | \`endcelldefine       |  
| \`undef            | \`endcelldefine       |  
| \`endif            | \`begin_keywords      |  
| \`include          | \`end_keywords        |  
| \`default_nettype  | \`nounconnected_drive |
| \`resetall         | \`unconnected_drive   |

## Syntax

inline port declaration in the module declaration i.e:

```verilog
module a(input clk)
...
endmodule
```



dev_guide/contributing.md
--------------------------------------
# Contributing

The Odin II team welcomes outside help from anyone interested.
To fix issues or add a new feature submit a PR or WIP PR following the provided guidelines.

## Creating a Pull Request (PR)

**Important** Before creating a Pull Request (PR), if it is a bug you have happened upon and intend to fix make sure you create an issue beforehand.

Pull requests are intended to correct bugs and improve Odin's performance.
To create a pull request, clone the [vtr-verilog-to-routing repository](https://github.com/verilog-to-routing/vtr-verilog-to-routing) and branch from the master.
Make changes to the branch that improve Odin II and correct the bug.
**Important** In addition to correcting the bug, it is required that test cases (benchmarks) are created that reproduce the issue and are included in the regression tests.
An example of a good test case could be the benchmark found in the "Issue" being addressed.
The results of these new tests need to be regenerate. See [regression tests](regression_test.md) for further instruction.
Push these changes to the cloned repository and create the pull request.
Add a description of the changes made and reference the "issue" that it corrects. There is a template provided on GitHub.

### Creating a "Work in progress" (WIP) PR

**Important** Before creating a WIP PR, if it is a bug you have happened upon and intend to fix make sure you create an issue beforehand.

A "work in progress" PR is a pull request that isn't complete or ready to be merged.
It is intended to demonstrate that an Issue is being addressed and indicates to other developers that they don't need to fix it.
Creating a WIP PR is similar to a regular PR with a few adjustments.
First, clone the [vtr-verilog-to-routing repository](https://github.com/verilog-to-routing/vtr-verilog-to-routing) and branch from the master.
Make changes to that branch.
Then, create a pull request with that branch and **include WIP in the title.**
This will automatically indicate that this PR is not ready to be merged.
Continue to work on the branch, pushing the commits regularly.
Like a PR, test cases must be included through the use of benchmarks.
See [regression tests](regression_test.md) for further instruction.

### Formating

Odin II shares the same contributing philosophy as [VPR](https://docs.verilogtorouting.org/en/latest/dev/contributing/contributing/).
Most importantly PRs will be rejected if they do not respect the coding standard: see [VPRs coding standard](https://docs.verilogtorouting.org/en/latest/dev/developing/#code-formatting)

To correct any code formatting issues flagged by the CI system, simply run ``make format`` to adapt the newly added code to VPR's coding standard.
If you have made alterations to python scripts, you would probably need to run ``make format-py`` and ``./dev/pylint_check.py`` from the VTR root directory to correct the python code formatting and check for lint errors. 


## Odin II's Flow

Odin II functions by systematically executing a set of steps determined by the files and arguments passed in.
The figure below illustrates the flow of Odin II if a Verilog File is passed, with an optional FPGA Architecture Specification File.
The simulator is only activated if an Input Vector file is passed in which creates the Output Vector File.

.. graphviz ::
digraph G {
    0 [label="Verilog HDL File",shape=plaintext];
    2 [label="Input Vector File",shape=plaintext];
    3 [label="Output Vector File",shape=diamond];
    4 [label="FPGA Architecture Specification File",shape=plaintext];
    5 [label="Build Abstract Syntax Tree",shape=box];
    6 [label="Elaborate AST",shape=box];
    7 [label="Build Netlist",shape=box];
    8 [label="Partial Mapping",shape=box];
    10 [label="Simulator",shape=box];
    11 [label="Output Blif",shape=diamond];

    0 -> 5 -> 6 -> 7 -> 8
    7->10 [color=purple]
    4->8  [style=dotted] [color=purple]
    8->11
    4->10 [style=dotted] [color=purple]
    2->10 [color=purple]
    10->3 [color=purple]
}

Currently, BLIF files being passed in are only used for simulation; no partial mapping takes place.
The flow is depicted in the figure below.

.. graphviz ::
digraph G {
    0 [label="Input Blif File",shape=plaintext];
    1 [label="Read Blif",shape=box];
    3 [label="Build Netlist",shape=box];
    4 [label="Output Blif",shape=diamond];
    5 [label="Simulator",shape=box];
    6 [label="FPGA Architecture Specification File",shape=box];
    7 [label="Input Vector File",shape=plaintext];
    8 [label="Output Vector File",shape=diamond];

    0->1->3
    3->5 [color=purple]
    3->4
    5->8 [color=purple]
    7->5 [color=purple]
    6->5 [style=dotted] [color=purple]
}

### Building the Abstract Syntax Tree (AST)

Odin II uses Bison and Flex to parse a passed Verilog file and produce an Abstract Syntax Tree for each module found in the Verilog File.
The AST is considered the "front-end" of Odin II.
Most of the code for this can be found in verilog_bison.y, verilog_flex.l and parse_making_ast.cpp located in the odin_ii/SRC directory.

### AST Elaboration

In this step, Odin II parses through the ASTs and elaborates specific parts like for loops, function instances, etc.
It also will simplify the tree and rid itself of useless parts, such as an unused if statement.
It then builds one large AST, incorporating each module.
The code for this can mostly be found in ast_elaborate.cpp located in the odin_ii/SRC directory.

> **NOTE**
>
> These ASTs can be viewed via graphviz using the command -A. The file(s) will appear in the main directory.

### Building the Netlist

Once again, Odin II parses through the AST assembling a Netlist.
During the Netlist creation, pins are assigned and connected.
The code for this can be found in netlist_create_from_ast.cpp located in the odin_ii/SRC directory.

> **NOTE**
>
> The Netlist can be viewed via graphviz using the command -G. The file will appear in the main directory under net.dot.

### Partial Mapping

During partial mapping, Odin II maps the logic using an architecture.
If no architecture is passed in, Odin II will create the soft logic and use LUTs for mapping.
However, if an architecture is passed, Odin II will map accordingly to the available hard blocks and LUTs.
It uses a combination of soft logic and hard logic.

### Simulator

The simulator of Odin II takes an Input Vector file and creates an Output Vector file determined by the behaviour described in the Verilog file or BLIF file.

## Useful tools of Odin II for Developers

When making improvements to Odin II, there are some features the developer should be aware of to make their job easier.
For instance, Odin II has a -A and -G command that prints the ASTs and Netlist viewable with GraphViz.
These files can be found in the odin_ii directory.
This is very helpful to visualize what is being created and how everything is related to each other in the Netlist and AST.

Another feature to be aware of is ``make test``.
This build runs through all the regression tests and will list all the benchmarks that fail.
It is important to run this after every major change implemented to ensure the change only affects benchmarks it was intended to effect (if any).
It sheds insight on what needs to be fixed and how close it is to being merged with the master.



dev_guide/regression_test.md
--------------------------------------
# Regression Tests

Regression tests are tests that are repeatedly executed to assess functionality.
Each regression test targets a specific function of Odin II.
There are two main components of a regression test; benchmarks and a configuration file.
The benchmarks are comprised of verilog files, input vector files and output vector files.
The configuration file calls upon each benchmark and synthesizes them with different architectures.
The current regression tests of Odin II can be found in regression_test/benchmark.

## Benchmarks

Benchmarks are used to test the functionality of Odin II and ensure that it runs properly.
Benchmarks of Odin II can be found in regression_test/benchmark/verilog/any_folder.
Each benchmark is comprised of a verilog file, an input vector file, and an output vector file.
They are called upon during regression tests and synthesized with different architectures to be compared against the expected results.
These tests are useful for developers to test the functionality of Odin II after implementing changes.
The command `make test` runs through all these tests, comparing the results to previously generated results, and should be run through when first installing.

### Unit Benchmarks

Unit benchmarks are the simplest of benchmarks. They are meant to isolate different functions of Odin II.
The goal is that if it does not function properly, the error can be traced back to the function being tested.
This cannot always be achieved as different functions depend on others to work properly.
It is ideal that these benchmarks test bit size capacity, erroneous cases, as well as standards set by the IEEE Standard for Verilog® Hardware Description Language - 2005.

### Micro Benchmarks

Micro benchmarks are precise, like unit benchmarks, however are more syntactic.
They are meant to isolate the behaviour of different functions.
They trace the behaviour of functions to ensure they adhere to the IEEE Standard for Verilog® Hardware Description Language - 2005.
Like unit benchmarks, they should check erroneous cases and behavioural standards set by the IEEE Standard for Verilog® Hardware Description Language - 2005.

### Macro Benchmarks

Macro benchmarks are more realistic tests that incorporate multiple functions of Odin II.
They are intended to simulate real-user behaviour to ensure that functions work together properly.
These tests are designed to test things like syntax and more complicated standards set by the IEEE Standard for Verilog® Hardware Description Language - 2005.

### External Benchmarks

External benchmarks are benchmarks created by outside users to the project.
It is possible to pull an outside directory and build them on the fly thus creating a benchmark for Odin II.

## Creating Regression Tests

### New Regression Test Checklist

* Create benchmarks [here](#creating-benchmarks)
* Create configuration file [here](#creating-a-configuration-file)
* Create a folder in the task directory for the configuration file [here](#creating-a-task)
* Generate the results [here](#regenerating-results)
* Add the task to a suite (large suite if generating the results takes longer than 3 minutes, otherwise put in light suite) [here](#creating-a-suite)
* Update the documentation by providing a summary in Regression Test Summary section and updating the Directory Tree [here](#regression-test-summaries)

### New Benchmarks added to Regression Test Checklist

* Create benchmarks and add them to the correct regression test folder found in the benchmark/verilog directory [here](#creating-benchmarks)  (There is a description of each regression test [here](#regression-test-summaries))
* Regenerate the results [here](#regenerating-results)

### Include

* verilog file
* input vector file
* expected output vector file
* configuration file (conditional)
* architecture file (optional)

### Creating Benchmarks

If only a few benchmarks are needed for a PR, simply add the benchmarks to the appropriate set of regression tests.
The [Regression Test Summary](#regression-test-summaries) summarizes the target of each regression test which may be helpful.

The standard of naming the benchmarks are as follows:

* verilog file: meaningful_title.v
* input vector file: meaningful_title_input
* output vector file: meaningful_title_output

If the tests needed do not fit in an already existing set of regression tests or need certain architecture(s), create a separate folder in the verilog directory and label appropriately.
Store the benchmarks in that folder.
Add the architecture (if it isn't one that already exists) to ../vtr_flow/arch.

> **NOTE**
>
> If a benchmark fails and should pass, include a $display statement in the verilog file in the following format:
>
> `$display("Expect::FUNCTION < message >);`
>  
> The function should be in all caps and state what is causing the issue. For instance, if else if was behaving incorrectly it should read ELSE_IF. The message
> should illustrate what should happen and perhaps a suggestion in where things are going wrong.

### Creating a Configuration File

A configuration file is only necessary if the benchmarks added are placed in a new folder.
The configuration file is where architectures and commands are specified for the synthesis of the benchmarks.
**The configuration file must be named task.conf.**
The following is an example of a standard task.conf (configuration) file:  

```bash
########################
# <title> benchmarks config
########################

# commands
regression_params=--include_default_arch
script_synthesis_params=--time_limit 3600s 
script_simulation_params=--time_limit 3600s
simulation_params= -L reset rst -H we

# setup the architecture (standard architectures already available)
archs_dir=../vtr_flow/arch/timing

arch_list_add=k6_N10_40nm.xml
arch_list_add=k6_N10_mem32K_40nm.xml
arch_list_add=k6_frac_N10_frac_chain_mem32K_40nm.xml

# setup the circuits
circuits_dir=regression_test/benchmark/verilog/

circuit_list_add=<verilog file group>/*.vh
circuit_list_add=<verilog file group>/*.v


synthesis_parse_file=regression_test/parse_result/conf/synth.toml
simulation_parse_file=regression_test/parse_result/conf/sim.toml
```

The following key = value are available for configuration files:

|  key                     |following argument                                  |
|--------------------------|----------------------------------------------------|
|circuits_dir              |< path/to/circuit/dir >                             |
|circuit_list_add          |< circuit file path relative to [circuits_dir] >    |
|archs_dir                 |< path/to/arch/dir >                                |
|arch_list_add             |< architecture file path relative to [archs_dir] >  |
|synthesis_parse_file      |< path/to/parse/file >                              |
|simulation_parse_file     |< path/to/parse/file >                              |
|script_synthesis_params   | [see exec_wrapper.sh options]                      |
|script_simulation_params  |[see exec_wrapper.sh options]                       |
|synthesis_params          |[see Odin options]                                  |
|simulation_params         |[see Odin options]                                  |
|regression_params         |[see Regression Parameters bellow]

Regression Parameters:

* `--verbose`                display error logs after batch of tests
* `--concat_circuit_list`    concatenate the circuit list and pass it straight through to odin
* `--generate_bench`         generate input and output vectors from scratch
* `--disable_simulation`     disable the simulation for this task
* `--disable_parallel_jobs`  disable running circuit/task pairs in parallel
* `--randomize`             perform a dry run randomly to check the validity of the task and flow                        |
* `--regenerate_expectation`regenerate expectation and override the expected value only if there's a mismatch          |
* `--generate_expectation`   generate the expectation and override the expectation file                                  |

### Creating a Task

The following diagram illustrates the structure of regression tests.
Each regression test needs a corresponding folder in the task directory containing the configuration file.
The \<task display name\> should have the same name as the verilog file group in the verilog directory.
This folder is where the synthesis results and simulation results will be stored.
The task diplay name and the verilog file group should share the same title.

```bash
└── odin_ii
      └── regression_test
              └── benchmark
                    ├── task
                    │     └── < task display name >
                    │             ├── [ simulation_result.json ]
                    │             ├── [ synthesis_result.json ]
                    │             └── task.conf
                    └── verilog
                          └── < verilog file group >
                                  ├── *.v
                                  └── *.vh
```

#### Creating a Complicated Task

There are times where multiple configuration files are needed in a regression test due to different commands wanted or architectures.
The task cmd_line_args is an example of this.
If that is the case, each configuration file will still need its own folder, however these folders should be placed in a parent folder.

```bash
└── odin_ii
      └── regression_test
              └── benchmark
                    ├── task
                    │     └──  < parent task display name >
                    |              ├── < task display name >
                    │              │             ├── [ simulation_result.json ]
                    │              │             ├── [ synthesis_result.json ]
                    |              │             └── task.conf
                    │              └── < task display name >
                    │              .             ├── [ simulation_result.json ]
                    │              .             ├── [ synthesis_result.json ]
                    |              .             └── task.conf
                    └── verilog
                          └── < verilog file group >
                                   ├── *.v
                                   └── *.vh
```

#### Creating a Suite

Suites are used to call multiple tasks at once. This is handy for regenerating results for multiple tasks.
In the diagram below you can see the structure of the suite.
The suite contains a configuration file that calls upon the different tasks named **task_list.conf**.

```bash
└── odin_ii
      └── regression_test
              └── benchmark
                    ├── suite
                    │     └──  < suite display name >
                    |              └── task_list.conf
                    ├── task
                    │     ├── < parent task display name >
                    |     │        ├── < task display name >
                    │     │        │             ├── [ simulation_result.json ]
                    │     │        │             ├── [ synthesis_result.json ]
                    |     │        │             └── task.conf
                    │     │        └── < task display name >
                    │     │                      ├── [ simulation_result.json ]
                    │     │                      ├── [ synthesis_result.json ]
                    |     │                      └── task.conf
                    │     └── < task display name >
                    │              ├── [ simulation_result.json ]
                    │              ├── [ synthesis_result.json ]
                    |              └── task.conf  
                    └── verilog
                          └── < verilog file group >
                                   ├── *.v
                                   └── *.vh
```

In the configuration file all that is required is to list the tasks to be included in the suite with the path.
For example, if the wanted suite was to call the binary task and the operators task, the configuration file would be as follows:

```bash
regression_test/benchmark/task/operators
regression_test/benchmark/task/binary
```

For more examples of task_list.conf configuration files look at the already existing configuration files in the suites.

### Regenerating Results

> **WARNING**
>
> **BEFORE** regenerating the result, run `make test` to ensure any changes in the code don't affect the results of benchmarks beside your own. If they do, the failing benchmarks will be listed.

Regenerating results is necessary if any regression test is changed (added benchmarks), if a regression test is added, or if a bug fix was implemented that changes the results of a regression test.
For all cases, it is necessary to regenerate the results of the task corresponding to said change.
The following commands illustrate how to do so:

```shell
make sanitize
```

then: where N is the number of processors in the computer, and the path following -t ends with the same name as the folder you placed

```shell
./verify_odin.sh -j N --regenerate_expectation -t regression_test/benchmark/task/<task_display_name>
```

> **NOTE**
>
> **DO NOT** run the `make sanitize` if regenerating the large test. It is probable that the computer will not have enough ram to do so and it will take a long time. Instead run `make build`

For more on regenerating results, refer to the [Verify Script](./verify_script.md) section.

## Regression Test Summaries

### c_functions

This regression test targets c functions supported by Verilog such as clog_2.

### cmd_line_args

This is a more complicated regression test that incorporates multiple child tasks.
It targets different commands available in Odin II.
Although it doesn't have a dedicated set of benchmarks in the verilog folder, the configuration files call on different preexisting benchmarks.

### FIR

FIR is an acronym for "Finite Impulse Response".
These benchmarks were sourced from [Layout Aware Optimization of High Speed Fixed Coefficient FIR Filters for FPGAs](http://kastner.ucsd.edu/fir-benchmarks/?fbclid=IwAR0sLk_qaBXfeCeDuzD2EWBrCJ_qGQd7rNISYPemU6u98F6CeFjWOMAM2NM).
They test a method of implementing high speed FIR filters on FPGAs discussed in the paper.

### full

The full regression test is designed to test real user behaviour.  
It does this by simulating flip flop, muxes and other common uses of Verilog.  

### large

This regression test targets cases that require a lot of ram and time.  

### micro

The micro regression test targets hards blocks and pieces that can be easily instantiated in architectures.

### mixing_optimization

The mixing optimization regression test targets mixing implementations for operations implementable in hard blocks and their soft logic counterparts that can be can be easily instantiated in architectures. The tests support extensive command line coverage, as well as provide infrastructure to enable the optimization from an .xml configuration file, require for using the optimization as a part of VTR synthesis flow.

### operators

This regression test targets the functionality of different operators. It checks bit size capacity and behaviour.

### syntax

The syntax regression test targets syntactic behaviour. It checks that functions work cohesively together and adhere to the verilog standard.

### keywords

This regression test targets the function of keywords. It has a folder or child for each keyword containing their respective benchmarks. Some folders have benchmarks for two keywords like task_endtask because they both are required together to function properly.

### preprocessor

This set of regression test includes benchmarks targetting compiler directives available in Verilog.

### Regression Tests Directory Tree

```bash
benchmark
    ├── suite
    │     ├── complex_synthesis_suite
    │     │   └── task_list.conf
    │     ├── full_suite
    │     │   └── task_list.conf
    │     ├── heavy_suite
    │     │   └── task_list.conf
    │     └── light_suite
    │         └── task_list.conf
    ├── task
    │     ├── arch_sweep
    │     │   ├── synthesis_result.json
    │     │   └── task.conf
    │     ├── c_functions
    │     │   └── clog2
    │     │       ├── simulation_result.json
    │     │       ├── synthesis_result.json
    │     │       └── task.conf
    │     ├── cmd_line_args
    │     │   ├── batch_simulation
    │     │   │   ├── simulation_result.json
    │     │   │   ├── synthesis_result.json
    │     │   │   └── task.conf
    │     │   ├── best_coverage
    │     │   │   ├── simulation_result.json
    │     │   │   ├── synthesis_result.json
    │     │   │   └── task.conf
    │     │   ├── coverage
    │     │   │   ├── simulation_result.json
    │     │   │   ├── synthesis_result.json
    │     │   │   └── task.conf
    │     │   ├── graphviz_ast
    │     │   │   ├── synthesis_result.json
    │     │   │   └── task.conf
    │     │   ├── graphviz_netlist
    │     │   │   ├── synthesis_result.json
    │     │   │   └── task.conf
    │     │   └── parallel_simulation
    │     │       ├── simulation_result.json
    │     │       ├── synthesis_result.json
    │     │       └── task.conf
    │     ├── FIR
    │     │   ├── simulation_result.json
    │     │   ├── synthesis_result.json
    │     │   └── task.conf
    │     ├── fpu
    │     │   └── hardlogic
    │     │       ├── simulation_result.json
    │     │       ├── synthesis_result.json
    │     │       └── task.conf
    │     ├── full
    │     │   ├── simulation_result.json
    │     │   ├── synthesis_result.json
    │     │   └── task.conf
    │     ├── keywords
    │     │   ├── always
    │     │   ├── and
    │     │   ├── assign
    │     │   ├── at_parenthathese
    │     │   ├── automatic
    │     │   ├── begin_end
    │     │   ├── buf
    │     │   ├── case_endcase
    │     │   ├── default
    │     │   ├── defparam
    │     │   ├── else
    │     │   ├── for
    │     │   ├── function_endfunction
    │     │   ├── generate
    │     │   ├── genvar
    │     │   ├── if
    │     │   ├── initial
    │     │   ├── inout
    │     │   ├── input_output
    │     │   ├── integer
    │     │   ├── localparam
    │     │   ├── macromodule
    │     │   ├── nand
    │     │   ├── negedge
    │     │   ├── nor
    │     │   ├── not
    │     │   ├── or
    │     │   ├── parameter
    │     │   ├── posedge
    │     │   ├── reg
    │     │   ├── signed_unsigned
    │     │   ├── specify_endspecify
    │     │   ├── specparam
    │     │   ├── star
    │     │   ├── task_endtask
    │     │   ├── while
    │     │   ├── wire
    │     │   ├── xnor
    │     │   └── xor
    │     ├── koios
    │     │   ├── synthesis_result.json
    │     │   └── task.conf
    │     ├── large
    │     │   ├── synthesis_result.json
    │     │   └── task.conf
    │     ├── micro
    │     │   ├── simulation_result.json
    │     │   ├── synthesis_result.json
    │     │   └── task.conf
    │     ├── mixing_optimization
    │     │   ├── mults_auto_full
    │     │   │   ├── simulation_result.json
    │     │   │   │── synthesis_result.json
    │     │   │   └── task.conf
    │     │   ├── mults_auto_half
    │     │   │   ├── simulation_result.json
    │     │   │   │── synthesis_result.json
    │     │   │   └── task.conf
    │     │   ├── mults_auto_none
    │     │   │   ├── simulation_result.json
    │     │   │   │── synthesis_result.json
    │     │   │   └── task.conf
    │     │   ├── config_file_half
    │     │   │   ├── config_file_half.xml
    │     │   │   ├── simulation_result.json
    │     │   │   │── synthesis_result.json
    │     │   │   └── task.conf
    │     ├── operators
    │     │   ├── simulation_result.json
    │     │   ├── synthesis_result.json
    │     │   └── task.conf
    │     ├── preprocessor
    │     │   ├── simulation_result.json
    │     │   ├── synthesis_result.json
    │     │   └── task.conf
    │     ├── syntax
    │     │   ├── simulation_result.json
    │     │   ├── synthesis_result.json
    │     │   └── task.conf
    │     └── vtr
    │         ├── synthesis_result.json
    │         └── task.conf
    │             
    ├── third_party
    │     └── SymbiFlow
    │         ├── build.sh
    │         └── task.mk
    └── verilog
            ├── FIR
            ├── c_functions
            ├── common
            ├── full
            ├── keywords
            ├── large
            ├── micro
            ├── operators
            ├── preprocessor
            └── syntax
```



dev_guide/testing.md
--------------------------------------
# TESTING ODIN II

The ``verify_odin.sh`` script will simulate the microbenchmarks and a larger set of benchmark circuits.
These scripts use simulation results which have been verified against ModelSim.

After you build Odin-II, run ``make test`` to ensure that everything is working correctly on your system.
The ``verify_odin.sh`` also simulates the blif output, as well as simulating the verilog with and without the
architecture file.


Before checking in any changes to Odin II, please run both of these scripts to ensure that both of these scripts execute correctly.
If there is a failure, use ModelSim to verify that the failure is within Odin II and not a faulty regression test.
If it is a faulty regression test, make an [issue on GitHub](https://github.com/verilog-to-routing/vtr-verilog-to-routing/issues/new/choose).
The Odin II simulator will produce a ``test.do`` file containing clock and input vector information for ModelSim.

When additional circuits are found to agree with ModelSim, they should be added to the regression tests.
When new features are added to Odin II, new microbenchmarks should be developed which test those features for regression.
This process is illustrated in the Developer Guide, in the Regression Tests section.

## USING MODELSIM TO TEST ODIN II

ModelSim may be installed as part of the Quartus II Web Edition IDE.
Load the Verilog circuit into a new project in ModelSim.
Compile the circuit, and load the resulting library for simulation.

You may use random vectors via the ``-g`` option, or specify your own input vectors using the ``-t`` option.
When simulation is complete, load the resulting ``test.do`` file into your ModelSim project and execute it.
You may now directly compare the vectors in the ``output_vectors`` file with those produced by ModelSim.

> NOTE
>
> For simulation purposes, you may need to handle the ``GLOBAL_SIM_BASE_CLK`` signal in the ``input_vector`` by either adding this signal as an input signal to the top module or removing it from the ``input_vector`` file.

To add the verified vectors and circuit to an existing test set, move the Verilog file (eg: ``test_circuit.v``) to the test set folder.
Next, move the ``input_vectors`` file to the test set folder, and rename it ``test_circuit_odin_input``. Finally, move the ``output_vectors`` file to the test set folder and rename it ``test_circuit_odin_output``.



dev_guide/verify_script.md
--------------------------------------
# Verify Script

The verify_odin.sh script is designed for generating regression test results. 

./verify_odin.sh [args]

## Arguments

*The tool requires a task to run hence `-t <task directory>` must be passed in

| arg   |  equivalent form          | Following argument                  |      Description                                                                               |
|-------|---------------------------|-------------------------------------|------------------------------------------------------------------------------------------------|
| `-t`  |  path to directory containing a test |The test types are a task.conf,task_list.conf or a predefined test                             |
|  `-j` | `-- nb_of_process`        | # of processors requested           |  This allows the user to choose how many processor are used in the computer, the default is 1  |
|  `-d` | `--output_dir`            | path to directory                   |  This allows the user to change the directory of the run output                                |
|  `-C` | `--config`                | path to configuration file          |  This directs the compiler to a configuration file to append the configuration file of the test|
|  `-t` | `--test`                  | path to directory containing a test |  The test types are a task.conf,task_list.conf or a predefined test                            |
|  `-h` | `--help`                  | N/A                                 |  Prints all possible commands and directives                                                   |
|  `-g` | `--generate_bench`        | N/A                                 |  Generates input and output vectors for test                                                    |
|  `-o` | `--generate_output`       | N/A                                 |  Generates output vector for test given its input vector                                       |
|  `-b` | `--build_config`          | path to directory                   |  Generates a configuration file for a given directory                                          |
|  `-c` | `--clean`                 | N/A                                 |  Clean temporary directory                                                                     |
|  `-f` | `--force_simulate`        | N/A                                 |  Force the simulation to be executed regardless of the configuration file                      |
|  N/A  | `--override`              | N/A                                 |  if a configuration file is passed in, override arguments rather than append                   |
|  N/A  | `--dry_run`               | N/A                                 |  performs a dry run to check the validity of the task and flow                                 |
|  N/A  | `--randomize`             | N/A                                 |  performs a dry run randomly to check the validity of the task and flow                        |
|  N/A  | `--regenerate_expectation`| N/A                                 |  regenerates expectation and overrides the expected value only if there's a mismatch           |
|  N/A  | `--generate_expectation`  | N/A                                 |  generate the expectation and overrides the expectation file                                   |

## Examples

The following examples are being performed in the odin_ii directory:

### Generating Results for a New Task

To generate new results, `synthesis_parse_file` and `simulation_parse_file` must be specified
in task.conf file.

The following commands will generate the results of a new regression test using N processors:

```bash
make sanitize
```

```bash
./verify_odin.sh --generate_expectation -j N -t <regression_test/benchmark/task/<task_name>
```

A synthesis_result.json and a simulation_result.json will be generated in the task's folder.
The simulation results for each benchmark are only generated if they synthesize correctly (no exit error), thus if none of the benchmarks synthesize there will be no simulation_result.json generated.

### Regenerating Results for a Changed Test

The following commands will only generate the results of the changes.
If there are new benchmarks it will add to the results.
If there are deleted benchmarks or modified benchmarks the results will be updated accordingly.

```bash
make sanitize
```

```bash
./verify_odin.sh --regenerate_expectation -t <regression_test/benchmark/task/<task_name>
```

### Generating Results for a Suite

The following commands generate the results for all the tasks called upon in a suite.

```bash
make sanitize
```

> **NOTE**
>
> If the suite calls upon the large test **DO NOT** run `make sanitize`.
> Instead run `make build`.

```bash
./verify_odin.sh --generate_expectation -t <regression_test/benchmark/suite/<suite_name>
```

### Checking the configuration file

The following commands will check if a configuration file is being read properly.

```bash
make build
```

```bash
./verify_odin.sh --dry_run -t <regression_test/benchmark/<path/to/config_file/difrectory>
```

### Running a subset of tests in a suite

The following commands will run only the tests matching `<test regex>`:

```bash
./verify_odin.sh -t <regression_test/benchmark/suite/<suite_name> <test regex>
```

You may specify as many test regular expressions as desired and the script will run any test that matches at least one regex

> **NOTE**
>
> This uses grep's extended regular expression syntax for matching test names.  
> Test names matched are of the form <suite_name>/<test_name>/<architecture>


parmys/index.rst
--------------------------------------
.. _parmys:

#####
Parmys
#####

Parmys frontend utilizes Yosys which is a framework for Verilog RTL synthesis and Parmys-plugin as partial mapper.

.. toctree::
   :glob:
   :maxdepth: 2
   :Caption: Quickstart

   quickstart

.. toctree::
   :glob:
   :maxdepth: 2
   :Caption: Yosys

   yosys

.. toctree::
   :glob:
   :maxdepth: 2
   :Caption: Parmys Plugin

   parmys_plugin

.. toctree::
   :glob:
   :maxdepth: 2
   :Caption: Structure

   structure



parmys/parmys_plugin.rst
--------------------------------------
.. _parmys_plugin:

Parmys Plugin
===============

Parmys (Partial Mapper for Yosys) is a Yosys plugin that performs intelligent partial mapping (inference, binding, and hard/soft logic trade-offs) from Odin-II.
Please see `Parmys-Plugin GitHub <https://github.com/CAS-Atlantic/parmys-plugin.git>`_ repository for more information.

Available parameters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. option:: -a ARCHITECTURE_FILE

    VTR FPGA architecture description file (XML)

.. option:: -c XML_CONFIGURATION_FILE

    Configuration file

.. option:: -top top_module

    set the specified module as design top module

.. option:: -nopass

    No additional passes will be executed.

.. option:: -exact_mults int_value

    To enable mixing hard block and soft logic implementation of adders

.. option:: -mults_ratio float_value

    To enable mixing hard block and soft logic implementation of adders

.. option:: -vtr_prim

    No additional passes will be executed.

.. option:: -vtr_prim

    loads vtr primitives as modules, if the design uses vtr primitives then this flag is mandatory for first run




parmys/quickstart.rst
--------------------------------------
.. _quickstart:

Quickstart
==========

Prerequisites
-------------

* ctags
* bison
* flex
* g++ 9.x
* cmake 3.16 (minimum version)
* time
* cairo
* build-essential
* libreadline-dev
* gawk tcl-dev
* libffi-dev 
* git
* graphviz
* xdot
* pkg-config
* python3-dev
* libboost-system-dev
* libboost-python-dev
* libboost-filesystem-dev 
* zlib1g-dev

Building
--------

To build the VTR flow with the Parmys front-end you may use the VTR Makefile wrapper, by calling the ``make CMAKE_PARAMS="-DWITH_PARMYS=ON"`` command in the `$VTR_ROOT` directory.

.. note::
    Our CI testing is on Ubuntu 22.04, so that is the best tested platform and recommended for development.

.. note::

    Compiling the VTR flow with the ``-DYOSYS_F4PGA_PLUGINS=ON`` flag is required to build and install Yosys SystemVerilog and UHDM plugins.
    Using this compile flag, the `Yosys-F4PGA-Plugins <https://github.com/chipsalliance/yosys-f4pga-plugins>`_ and `Surelog <https://github.com/chipsalliance/Surelog>`_ repositories are cloned in the ``$VTR_ROOT/libs/EXTERNAL`` directory and then will be compiled and added as external plugins to the Parmys front-end.

Basic Usage
-----------

To run the VTR flow with the Parmys front-end, you would need to run the `run_vtr_flow.py <https://github.com/verilog-to-routing/vtr-verilog-to-routing/blob/master/vtr_flow/scripts/run_vtr_flow.py>`_ script with the start stage specified as `parmys`.

.. code-block:: bash

    ./run_vtr_flow `PATH_TO_VERILOG_FILE.v` `PATH_TO_ARCH_FILE.xml` -start parmys

.. note::

    Please see `Run VTR Flow <https://docs.verilogtorouting.org/en/latest/vtr/run_vtr_flow/#advanced-usage>`_ for advanced usage of the Parmys front-end with external plugins.

.. note::

    Parmys is the default frontend in VTR flow which means it is no more necessary to pass build flags to cmake or explicitly define the start stage of vtr flow as `parmys`.



parmys/structure.rst
--------------------------------------
.. _structure:

Structure
=========

Structure of Parmys Frontend (Yosys + Parmys Plugin)
-------------------------------------------------------------------------

.. code-block:: bash

    └── $VTR_ROOT
        ├── vtr_flow
        │	└── misc
        │		└── parmys
        │			└── synthesis.tcl
        ├── parmys
        │   ├── parmys-plugin
        │   │   ├── core
        │   │   ├── mapping
        │   │   ├── netlist
        │   │   ├── techlibs
        │   │   ├── tests
        │   │   └──utils
        │   ├── test-utils
        │   └── third_party
        └── yosys
            ├── backends
            ├── examples
            ├── frontends
            ├── guidelines
            ├── kernel
            ├── libs
            ├── manual
            ├── misc
            ├── passes
            ├── techlibs
            └── tests


parmys/yosys.rst
--------------------------------------
.. _yosys:

Yosys
===============

Yosys is a Verilog RTL synthesis framework to perform logic synthesis, elaboration, and converting a subset of the Verilog Hardware Description Language (HDL) into a BLIF netlist.
Please see `Yosys GitHub <https://github.com/YosysHQ/yosys.git>`_ repository for more information.



quickstart/index.rst
--------------------------------------
###############
VTR Quick Start
###############

This is a quick introduction to VTR which covers how to run VTR and some of its associated tools (:ref:`VPR`, :ref:`Parmys`, :ref:`ABC`).

Setting Up VTR
==============

Download VTR
------------

The first step is to `download VTR <https://verilogtorouting.org/download/>`_ and extract it on your local machine.

.. note:: Developers planning to modify VTR should clone the `VTR git repository <https://github.com/verilog-to-routing/vtr-verilog-to-routing/>`_.


Environment Setup
-----------------
If you cloned the repository, you will need to set up the git submodules (if you downloaded and extracted a release, you can skip this step):

.. code-block:: bash

    > git submodule init
    > git submodule update
    
VTR requires several system packages and Python packages to build and run the flow.  You can install the required system packages using the following command (this works on Ubuntu 18.04, 20.04 and 22.04, but you may require different packages on other Linux distributions). Our CI testing is on Ubuntu 22.04, so that is the best tested platform and recommended for development.

.. code-block:: bash

    > ./install_apt_packages.sh

Then, to install the required Python packages (optionally within a new Python virtual environment):

.. code-block:: bash

    > make env                          # optional: install python virtual environment
    > source .venv/bin/activate         # optional: activate python virtual environment
    > pip install -r requirements.txt   # install python packages (in virtual environment if prior commands run, system wide otherwise)


Build VTR
---------

On most unix-like systems you can run:

.. code-block:: bash

    > make

.. note:: 

    In the VTR documentation lines starting with ``>`` (like ``> make`` above), indicate a command (i.e. ``make``) to run from your terminal.
    When the ``\`` symbol appears at the end of a line, it indicates line continuation.

.. note::

    :term:`$VTR_ROOT` refers to the root directory of the VTR project source tree.
    To run the examples in this guide on your machine, either:

    * define VTR_ROOT as a variable in your shell (e.g. if ``~/trees/vtr`` is the path to the VTR source tree on your machine, run the equivalent of ``VTR_ROOT=~/trees/vtr`` in BASH) which will allow you to run the commands as written in this guide, or
    * manually replace `$VTR_ROOT` in the example commands below with your path to the VTR source tree.



For more details on building VTR on various operating systems/platforms see :doc:`Building VTR</BUILDING>`.

Running the VTR Flow
----------------------------------
Running each stage of the flow manually is time consuming (and potentially error prone).
For convenience, VTR provides a script (:ref:`run_vtr_flow`) which automates this process.

First, make sure you have activated the Python virtual environment created at the beginning of this tutorial:

.. code-block:: bash

    > source $VTR_ROOT/.venv/bin/activate

Define the working directory where the flow will be executed. For convenience, use an environment variable:

.. code-block:: bash

    export VTR_FLOW_DIR=~/vtr_work/quickstart/blink_run_flow

Alternatively, you can manually replace ``$VTR_FLOW_DIR`` with your preferred directory path in the commands below.

Create the working directory and navigate into it:

.. code-block:: bash

    > mkdir -p $VTR_FLOW_DIR
    > cd $VTR_FLOW_DIR

Now lets run the script (``$VTR_ROOT/vtr_flow/scripts/run_vtr_flow.py``) passing in:

* The circuit verilog file (``$VTR_ROOT/doc/src/quickstart/blink.v``)
* The FPGA architecture file (``$VTR_ROOT/vtr_flow/arch/timing/EArch.xml``)

and also specifying the options:

* ``--route_chan_width 100`` a fixed FPGA routing architecture channel width.


The resulting command is:

.. code-block:: bash

    > $VTR_ROOT/vtr_flow/scripts/run_vtr_flow.py \
        $VTR_ROOT/doc/src/quickstart/blink.v \
        $VTR_ROOT/vtr_flow/arch/timing/EArch.xml \
        --route_chan_width 100

.. note:: Options unrecognized by run_vtr_flow (like ``--route_chan_width``) are passed on to VPR.

which should produce output similar to::

    EArch/blink             OK     (took 0.26 seconds, overall memory peak 63.71 MiB consumed by vpr run)

There are also multiple log files (including for ABC, Parmys and VPR), which by convention the script names with the ``.out`` suffix:

.. code-block:: bash

    > ls $VTR_FLOW_DIR/temp/*.out

    0_blackboxing_latch.out  parmys.out        report_clocks.abc.out  vanilla_restore_clocks.out
    abc0.out                 report_clk.out  restore_latch0.out     vpr.out

With the main log files of interest including the Parmys log file (``parmys.out``), log files produced by ABC (e.g. ``abc0.out``), and the VPR log file (``vpr.out``).

.. note::

    ABC may be invoked multiple times if a circuit has multiple clock domains, producing multiple log files (``abc0.out``, ``abc1.out``, ...)
    

You will also see there are several BLIF files produced:

.. code-block:: bash

    > ls $VTR_FLOW_DIR/temp/*.blif

    0_blink.abc.blif   0_blink.raw.abc.blif  blink.parmys.blif
    0_blink.parmys.blif  blink.abc.blif        blink.pre-vpr.blif

With the main files of interest being ``blink.parmys.blif`` (netlist produced by Parmys), ``blink.abc.blif`` (final netlist produced by ABC after clock restoration), ``blink.pre-vpr.blif`` netlist used by VPR (usually identical to ``blink.abc.blif``).

Like before, we can also see the implementation files generated by VPR:

.. code-block:: bash

    > ls $VTR_FLOW_DIR/temp/*.net $VTR_FLOW_DIR/temp/*.place $VTR_FLOW_DIR/temp/*.route

    blink.net  blink.place  blink.route

which we can visualize with:

.. code-block:: bash

    > $VTR_ROOT/vpr/vpr \
        $VTR_ROOT/vtr_flow/arch/timing/EArch.xml \
        blink --circuit_file $VTR_FLOW_DIR/temp/blink.pre-vpr.blif \
        --route_chan_width 100 \
        --analysis --disp on


Running VPR Manually
===========
Sometimes you may wish to run only the vpr (placement, routing and timing analysis) parts of the flow rather than the full VTR flow (which includes synthesis). To show how to do this, let's now try taking a simple pre-synthesized circuit (consisting of LUTs and Flip-Flops) and use the VPR tool to implement it on a specific FPGA architecture.

Running VPR on a Pre-Synthesized Circuit
----------------------------------------

First, let's make a directory in our home directory where we can work:

.. code-block:: bash

    #Move to our home directory
    > cd ~

    #Make a working directory
    > mkdir -p vtr_work/quickstart/vpr_tseng

    #Move into the working directory
    > cd ~/vtr_work/quickstart/vpr_tseng

Now, lets invoke the VPR tool to implement:

* the ``tseng`` circuit (``$VTR_ROOT/vtr_flow/benchmarks/blif/tseng.blif``), on 
* the ``EArch`` FPGA architecture (``$VTR_ROOT/vtr_flow/arch/timing/EArch.xml``).

We do this by passing these files to the VPR tool, and also specifying that we want to route the circuit on a version of ``EArch`` with a routing architecture :option:`channel width <vpr --route_chan_width>` of ``100`` (``--route_chan_wdith 100``):

.. code-block:: bash

    > $VTR_ROOT/vpr/vpr \
        $VTR_ROOT/vtr_flow/arch/timing/EArch.xml \
        $VTR_ROOT/vtr_flow/benchmarks/blif/tseng.blif \
        --route_chan_width 100

This will produce a large amount of output as VPR implements the circuit, but you should see something similar to::

    VPR FPGA Placement and Routing.
    Version: 8.1.0-dev+2b5807ecf
    Revision: v8.0.0-1821-g2b5807ecf
    Compiled: 2020-05-21T16:39:33
    Compiler: GNU 7.3.0 on Linux-4.15.0-20-generic x86_64
    Build Info: release VTR_ASSERT_LEVEL=2

    University of Toronto
    verilogtorouting.org
    vtr-users@googlegroups.com
    This is free open source code under MIT license.

    #
    #Lots of output trimmed for brevity....
    #

    Geometric mean non-virtual intra-domain period: 6.22409 ns (160.666 MHz)
    Fanout-weighted geomean non-virtual intra-domain period: 6.22409 ns (160.666 MHz)

    VPR suceeded
    The entire flow of VPR took 3.37 seconds (max_rss 40.7 MiB)
    
which shows that VPR as successful (``VPR suceeded``), along with how long VPR took to run (~3 seconds in this case).

You will also see various result files generated by VPR which define the circuit implementation:

.. code-block:: bash

    > ls *.net *.place *.route

    tseng.net  tseng.place  tseng.route

along with a VPR log file which contains what VPR printed when last invoked:

.. code-block:: bash

    > ls *.log

    vpr_stdout.log

and various report files describing the characteristics of the implementation:

.. code-block:: bash

    > ls *.rpt

    packing_pin_util.rpt              report_timing.hold.rpt   report_unconstrained_timing.hold.rpt
    pre_pack.report_timing.setup.rpt  report_timing.setup.rpt  report_unconstrained_timing.setup.rpt


Visualizing Circuit Implementation
-----------------------------------

.. note:: This section requires that VPR was compiled with graphic support. See :ref:`VPR Graphics <vpr_graphics>` for details.

The ``.net``, ``.place`` and ``.route`` files (along with the input ``.blif`` and architecture ``.xml`` files) fully defined the circuit implementation.
We can visualize the circuit implementation by:

* Re-running VPR's analysis stage (:option:`--analysis <vpr --analysis>`), and
* enabling VPR's graphical user interface (:option:`--disp <vpr --disp>` ``on``).
   
This is done by running the following:

.. code-block:: bash

    > $VTR_ROOT/vpr/vpr \
        $VTR_ROOT/vtr_flow/arch/timing/EArch.xml \
        $VTR_ROOT/vtr_flow/benchmarks/blif/tseng.blif \
        --route_chan_width 100 \
        --analysis --disp on

which should open the VPR graphics and allow you to explore the circuit implementation.

As an exercise try the following:

* View the connectivity of a block (connections which drive it, and those which it drives)
* View the internals of a logic block (e.g. try to find the LUTs/``.names`` and Flip-Flops/``.latch``)
* Visualize all the routed circuit connections

.. seealso:: For more details on the various graphics options, see :ref:`VPR Graphics <vpr_graphics>` 

.. figure:: tseng_nets.png

    Routed net connections of ``tseng`` on ``EArch``.

.. figure:: tseng_blk1.png

    Input (blue)/output (red) nets of block ``n_n3226`` (highlighted green).

.. note:: 
    If you do not provide :option:`--analysis <vpr --analysis>`, VPR will re-implement the circuit from scratch.
    If you also specify :option:`--disp <vpr --disp>` ``on``, you can see how VPR modifies the implementation as it runs.
    By default ``--disp on`` stops at key stages to allow you to view and explore the implementation.
    You will need to press the ``Proceed`` button in the GUI to allow VPR to continue to the next stage.

Manually Running the VTR Flow
-----------------------------
In the previous section we have implemented a pre-synthesized circuit onto a pre-existing FPGA architecture using VPR, and visualized the result.
We now turn to how we can implement *our own circuit* on a pre-existing FPGA architecture.

To do this, we begin by describing a circuit behaviourally using the Verilog Hardware Description Language (HDL).
This allows us to quickly and consisely define the circuit's behaviour.
We will then use the VTR Flow to synthesize the behavioural Verilog description it into a circuit netlist, and implement it onto an FPGA.

Example Circuit
---------------
We will use the following simple example circuit, which causes its output to toggle on and off:

.. literalinclude:: blink.v
    :language: verilog
    :linenos:
    :emphasize-lines: 10,15,26
    :caption: blink.v (``$VTR_ROOT/doc/src/quickstart/blink.v``)

This Verilog creates a sequential 5-bit register (``r_counter``) which increments every clock cycle.
If the count is below ``16`` it drives the output (``o_led``) high, otherwise it drives it low.


With the circuit defined, we are now ready to move forward with implementing it on an FPGA. To begin the process, we’ll set up a fresh working directory where we can manage our files and run the necessary VTR flow steps. Let’s get started by creating that directory and organizing our workspace:

- ``$WRK_DIR`` will serve as our working directory for this example.

To set this variable, define it in your shell:

.. code-block:: bash

    export WRK_DIR=~/vtr_work/quickstart/blink_manual

Alternatively, manually replace ``$WRK_DIR`` in the example commands with your path.

Now, create and navigate to the working directory:

.. code-block:: bash

    > mkdir -p $WRK_DIR
    > cd $WRK_DIR

Next, we need to run the three main sets of tools, depending on the method of synthesis you choose:

1. **Synthesis Options**:
   
   - **Parmys**: Parmys is the default synthesis tool in the VTR flow.

   - **Odin II**: Odin II is the alternative synthesis tool supported by VTR.

2. **Finally, regardless of the synthesis method**, run :ref:`ABC` to perform logic optimization and technology mapping, and then run :ref:`VPR` to handle packing, placement, and routing, which completes the implementation on the FPGA architecture.

.. _synthesizing_with_parmys:
Synthesizing with Parmys
~~~~~~~~~~~~~~~~~~~~~~~~~

To synthesize our Verilog file into a circuit netlist, we will utilize the `run_vtr_flow.py` script, which streamlines the process. This command synthesizes the provided Verilog file (`blink.v`) while targeting the specified FPGA architecture (`EArch.xml`).

The command is as follows:

.. code-block:: bash

    > $VTR_ROOT/vtr_flow/scripts/run_vtr_flow.py \
        $VTR_ROOT/doc/src/quickstart/blink.v \
        $VTR_ROOT/vtr_flow/arch/timing/EArch.xml \
        -start parmys -end parmys

When executed, the output should indicate successful synthesis, similar to:

.. code-block:: bash

    EArch/blink        OK (took 0.16 seconds, overall memory peak 20.00 MiB consumed by parmys run)

This output confirms that the synthesis was successful and provides information on the duration and memory usage during the process.

We can now take a look at the circuit which Parmys produced (``blink.parmys.blif``).

.. seealso:: For more information on the BLIF file format see :ref:`blif_format`.

Optimizing and Technology Mapping with ABC
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Next, we'll optimize and technology map our circuit using ABC, providing the option:

 * ``-c <script>``, where ``<script>`` is a set of commands telling ABC how to synthesize our circuit.

We'll use the following, simple ABC commands::

    read blink.parmys.blif;                               #Read the circuit synthesized by parmys
    if -K 6;                                            #Technology map to 6 input LUTs (6-LUTs)
    write_hie blink.parmys.blif blink.abc_no_clock.blif   #Write new circuit to blink.abc_no_clock.blif

.. note:: Usually you should use a more complicated script (such as that used by :ref:`run_vtr_flow`) to ensure ABC optitmizes your circuit well.

The corresponding command to run is:

.. code-block:: bash

    > $VTR_ROOT/abc/abc \
        -c "read $WRK_DIR/temp/blink.parmys.blif; if -K 6; write_hie $WRK_DIR/temp/blink.parmys.blif $WRK_DIR/temp/blink.abc_no_clock.blif"

When run, ABC's output should look similar to::

    ABC command line: "read blink.parmys.blif; if -K 6; write_hie blink.parmys.blif blink.abc_no_clock.blif".

    Hierarchy reader converted 6 instances of blackboxes.
    The network was strashed and balanced before FPGA mapping.
    Hierarchy writer reintroduced 6 instances of blackboxes.

If we now inspect the produced BLIF file (``blink.abc_no_clock.blif``) we see that ABC was able to significantly simplify and optimize the circuit's logic (compared to ``blink.parmys.blif``):

.. literalinclude:: blink.abc_no_clock.blif
    :linenos:
    :emphasize-lines: 6-10,13-18,21-38
    :caption: blink.abc_no_clock.blif

ABC has kept the ``.latch`` and ``.subckt adder`` primitives, but has significantly simplified the other logic (``.names``).

However, there is an issue with the above BLIF produced by ABC: the latches (rising edge Flip-Flops) do not have any clocks or edge sensitivity specified, which is information required by VPR.

Re-inserting clocks
^^^^^^^^^^^^^^^^^^^
We will restore the clock information by running a script which will transfer that information from the original parmys BLIF file (writing it to the new file ``blink.pre-vpr.blif``):

.. code-block:: bash

    > $VTR_ROOT/vtr_flow/scripts/restore_multiclock_latch.pl \
        $WRK_DIR/temp/blink.parmys.blif \
        $WRK_DIR/temp/blink.abc_no_clock.blif \
        $WRK_DIR/temp/blink.pre-vpr.blif

If we inspect ``blink.pre-vpr.blif`` we now see that the clock (``blink^clk``) has been restored to the Flip-Flops:

.. code-block:: bash

    > grep 'latch' blink.pre-vpr.blif

    .latch n19 blink^r_counter~0_FF re blink^clk 3
    .latch n24 blink^r_counter~4_FF re blink^clk 3
    .latch n29 blink^r_counter~3_FF re blink^clk 3
    .latch n34 blink^r_counter~2_FF re blink^clk 3
    .latch n39 blink^r_counter~1_FF re blink^clk 3

Implementing the circuit with VPR
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Now that we have the optimized and technology mapped netlist (``blink.pre-vpr.blif``), we can invoke VPR to implement it onto the ``EArch`` FPGA architecture (in the same way we did with the ``tseng`` design earlier).
However, since our BLIF file doesn't match the design name we explicitly specify:

 * ``blink`` as the circuit name, and
 * the input circuit file with :option:`--circuit_file <vpr --circuit_file>`.

to ensure the resulting ``.net``, ``.place`` and ``.route`` files will have the correct names.

The resulting command is:

.. code-block:: bash

    > $VTR_ROOT/vpr/vpr \
        $VTR_ROOT/vtr_flow/arch/timing/EArch.xml \
        blink --circuit_file $WRK_DIR/temp/blink.pre-vpr.blif \
        --route_chan_width 100

and after VPR finishes we should see the resulting implementation files:

.. code-block:: bash

    > ls $WRK_DIR/*.net $WRK_DIR/*.place $WRK_DIR/*.route

    blink.net  blink.place  blink.route

We can then view the implementation as usual by appending ``--analysis --disp on`` to the command:

.. code-block:: bash

    > $VTR_ROOT/vpr/vpr \
        $VTR_ROOT/vtr_flow/arch/timing/EArch.xml \
        blink --circuit_file $WRK_DIR/temp/blink.pre-vpr.blif \
        --route_chan_width 100 \
        --analysis --disp on

.. figure:: blink_implementation.png

    ``blink.v`` circuit implementation on the ``EArch`` FPGA architecture as viewed in the VPR GUI

Manually Running VTR with ODIN II
----------------------------------
VTR includes a second synthesis tool, ODIN II. Below we explain how to run this alternative synthesis flow.

To synthesize your Verilog design with ODIN II, you need to build VTR with the following command:

.. code-block:: bash

    > cd ~/$VTR_ROOT
    > make CMAKE_PARAMS="-DWITH_ODIN=on"

This step enables ODIN II support in the VTR build process, which is essential for the subsequent synthesis operations.

Lets make a new directory for us to work in:

.. code-block:: bash

    > mkdir -p ~/vtr_work/quickstart/blink_manual
    > cd ~/vtr_work/quickstart/blink_manual

Next, run ODIN II on your Verilog file to synthesize it into a circuit netlist. Use the command below, specifying the required options:

 * ``-a $VTR_ROOT/vtr_flow/arch/timing/EArch.xml`` which specifies what FPGA architecture we are targeting,
 * ``-V $VTR_ROOT/doc/src/quickstart/blink.v`` which specifies the verilog file we want to synthesize, and
 * ``-o blink.odin.blif`` which specifies the name of the generated ``.blif`` circuit netlist.

The resulting command is:

.. code-block:: bash

    > $VTR_ROOT/odin_ii/odin_ii \
        -a $VTR_ROOT/vtr_flow/arch/timing/EArch.xml \
        -V $VTR_ROOT/doc/src/quickstart/blink.v \
        -o blink.odin.blif

After running the command, you should see an output similar to the following::

    Total time: 14.7ms
    Odin ran with exit status: 0
    Odin II took 0.01 seconds (max_rss 5.1 MiB)

where ``Odin ran with exit status: 0`` indicates Odin successfully synthesized our verilog.

We can now take a look at the circuit which ODIN produced (``blink.odin.blif``).
The file is long and likely harder to follow than our code in ``blink.v``; however it implements the same functionality.
Some interesting highlights are shown below:

.. literalinclude:: blink.odin.blif
    :lines: 14,40
    :caption: Instantiations of rising-edge triggered Latches (i.e. Flip-Flops) in ``blink.odin.blif`` (implements part of ``r_counter`` in blink.v)

.. literalinclude:: blink.odin.blif
    :lines: 17-19,21-22
    :caption: Adder primitive instantiations in ``blink.odin.blif``, used to perform addition (implements part of the ``+`` operator in blink.v)

.. literalinclude:: blink.odin.blif
    :lines: 45-50
    :caption: Logic equation (.names truth-table) in ``blink.odin.blif``, implementing logical OR (implements part of the ``<`` operator in blink.v)

.. seealso:: For more information on the BLIF file format see :ref:`blif_format`.


After generating ``blink.odin.blif``, we can proceed to optimize the netlist and implement it on the FPGA. Follow the steps outlined in the section titled :ref:`Optimizing and Technology Mapping with ABC`, using the generated ``blink.odin.blif`` as input instead of ``blink.parmys.blif``. 

Once the netlist is optimized with ABC, continue with the :ref:`Implementing the circuit with VPR` section to pack, place, and route the circuit onto the FPGA architecture.

Next Steps
==========
Now that you've finished the VTR quickstart, you're ready to start experimenting and using VTR.

Here are some possible next steps for users wishing to use VTR:

 * Try modifying the Verilog file (e.g. ``blink.v``) or make your own circuit and try running it through the flow.

 * Learn about FPGA architecture modelling (:ref:`Tutorials <arch_tutorial>`, :ref:`Reference <fpga_architecture_description>`), and try modifying a copy of ``EArch`` to see how it changes the implementation of ``blink.v``.

 * Read more about the :ref:`VTR CAD Flow <vtr_cad_flow>`, and :ref:`Task <vtr_tasks>` automation framework.

 * Find out more about using other benchmark sets, like how to run the :ref:`Titan Benchmark Suite <titan_benchmarks_tutorial>`.

 * Discover how to :ref:`generate FASM <genfasm>` for bitstream creation.

 * :doc:`Suggest or make enhancements to VTR's documentation </CONTRIBUTING>`.

Here are some possible next steps for developers wishing to modify and improve VTR:

 * Try the next steps listed for users above to learn how VTR is used.

 * Work through the :ref:`new developer tutorial <new_developer_tutorial>`.

 * Read through the :ref:`developer guide <developer_guide>`.

 * Look for :doc:`open issues to which you can contribute </CONTRIBUTING>`.

 * Begin exploring the source code for the main tools in VTR (e.g. VPR in ``$VTR_ROOT/vpr/src``).



tutorials/index.rst
--------------------------------------
.. _tutorials:

Tutorials
=========

.. toctree::
    :maxdepth: 2

    flow/index
    arch/index
    titan_benchmarks/index
    timing_simulation/index



arch/classic_soft_logic.rst
--------------------------------------
Classic Soft Logic Block Tutorial
---------------------------------

The following is an example on how to use the VPR architecture description langauge to describe a classical academic soft logic block.
First we provide a step-by-step explanation on how to construct the logic block.
Afterwards, we present the complete code for the logic block.

.. _soft_logic_cluster_fig:

.. figure:: soft_logic_cluster.*

    Model of a classic FPGA soft logic cluster

:numref:`soft_logic_cluster_fig` shows an example of a classical soft logic block found in academic FPGA literature.
This block consists of N Basic Logic Elements (BLEs).
The BLE inputs can come from either the inputs to the logic block or from other BLEs within the logic block via a full crossbar.
The logic block in this figure has I general inputs, one clock input, and N outputs (where each output corresponds to a BLE).
A BLE can implement three configurations: a K-input look-up table (K-LUT), a flip-flop, or a K-LUT followed by a flip-flop.
The structure of a classical soft logic block results in a property known as logical equivalence for certain groupings of input/output pins.
Logically equivalent pins means that connections to those pins can be swapped without changing functionality.
For example, the input to AND gates are logically equivalent while the inputs to a 4-bit adders are not logically equivalent.
In the case of a classical soft logic block, all input pins are logically equivalent (due to the fully populated crossbar) and all output pins are logically equivalent (because one can swap any two BLEs without changing functionality).
Logical equivalence is important because it enables the CAD tools to make optimizations especially during routing.
We describe a classical soft logic block with N = 10, I = 22, and K = 4 below.

First, a complex block pb_type called CLB is declared with appropriate input, output and clock ports.
Logical equivalence is labelled at ports where it applies:

.. code-block:: xml

    <pb_type name="clb">
      <input name="I" num_pins="22" equivalent="full"/>
      <output name="O" num_pins="10" equivalent="instance"/>
      <clock name="clk" equivalent="false"/>

A CLB contains 10 BLEs.
Each BLE has 4 inputs, one output, and one clock.
A BLE block and its inputs and outputs are specified as follows:

.. code-block:: xml

      <pb_type name="ble" num_pb="10">
        <input name="in" num_pins="4"/>
        <output name="out" num_pins="1"/>
        <clock name="clk"/>

A BLE consists of one LUT and one flip-flop (FF).
Both of these are primitives.
Recall that primitive physical blocks must have a blif_model attribute that matches with the model name in the BLIF input netlist.
For the LUT, the model is ``.names`` in BLIF.
For the FF, the model is ``.latch`` in BLIF.
The class construct denotes that these are special (common) primitives.
The primitives contained in the BLE are specified as:

.. code-block:: xml

        <pb_type name="lut_4" blif_model=".names" num_pb="1" class="lut">
          <input name="in" num_pins="4" port_class="lut_in"/>
          <output name="out" num_pins="1" port_class="lut_out"/>
        </pb_type>
        <pb_type name="ff" blif_model=".latch" num_pb="1" class="flipflop">
          <input name="D" num_pins="1" port_class="D"/>
          <output name="Q" num_pins="1" port_class="Q"/>
          <clock name="clk" port_class="clock"/>
        </pb_type>

:numref:`classic_ble_fig` shows the ports of the BLE with the input and output pin sets.
The inputs to the LUT and flip-flop are direct connections.
The multiplexer allows the BLE output to be either the LUT output or the flip-flop output.
The code to specify the interconnect is:

.. _classic_ble_fig:

.. figure:: classic_ble.*

    Internal BLE names

.. code-block:: xml

        <interconnect>
          <direct input="lut_4.out" output="ff.D"/>
          <direct input="ble.in" output="lut_4.in"/>
          <mux input="ff.Q lut_4.out" output="ble.out"/>
          <direct input="ble.clk" output="ff.clk"/>
        </interconnect>
      </pb_type>

The CLB interconnect is then modeled (see :numref:`soft_logic_cluster_fig`).
The inputs to the 10 BLEs (ble[9:0].in) can be connected to any of the CLB inputs (clb.I) or any of the BLE outputs (ble[9:0].out) by using a full crossbar.
The clock of the CLB is wired to multiple BLE clocks, and is modeled as a full crossbar.
The outputs of the BLEs have direct wired connections to the outputs of the CLB and this is specified using one direct tag.
The CLB interconnect specification is:

.. code-block:: xml

      <interconnect>
        <complete input="{clb.I ble[9:0].out}" output="ble[9:0].in"/>
        <complete input="clb.clk" output="ble[9:0].clk"/>
        <direct input="ble[9:0].out" output="clb.O"/>
      </interconnect>

Finally, we model the connectivity between the CLB and the general FPGA fabric (recall that a CLB communicates with other CLBs and I/Os using general-purpose interconnect).
The ratio of tracks that a particular input/output pin of the CLB connects to is defined by fc_in/fc_out.
In this example, a fc_in of 0.15 means that each input pin connects to 15% of the available routing tracks in the external-to-CLB routing channel adjacent to that pin.
The pinlocations tag is used to associate pins on the CLB with which side of the logic block pins reside on where the pattern spread corresponds to evenly spreading out the pins on all sides of the CLB in a round-robin fashion.
In this example, the CLB has a total of 33 pins (22 input pins, 10 output pins, 1 clock pin) so 8 pins are assigned to all sides of the CLB except one side which gets assigned 9 pins.

.. code-block:: xml

      <!-- Describe complex block relation with FPGA -->

      <fc_in type="frac">0.150000</fc_in>
      <fc_out type="frac">0.125000</fc_out>

      <pinlocations pattern="spread"/>

    </pb_type>


Classic Soft Logic Block Complete Example
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: xml

    <!--
    Example of a classical FPGA soft logic block with
    N = 10, K = 4, I = 22, O = 10
    BLEs consisting of a single LUT followed by a flip-flop that can be bypassed
    -->

    <pb_type name="clb">
      <input name="I" num_pins="22" equivalent="full"/>
      <output name="O" num_pins="10" equivalent="instance"/>
      <clock name="clk" equivalent="false"/>

      <pb_type name="ble" num_pb="10">
        <input name="in" num_pins="4"/>
        <output name="out" num_pins="1"/>
        <clock name="clk"/>

        <pb_type name="lut_4" blif_model=".names" num_pb="1" class="lut">
          <input name="in" num_pins="4" port_class="lut_in"/>
          <output name="out" num_pins="1" port_class="lut_out"/>
        </pb_type>
        <pb_type name="ff" blif_model=".latch" num_pb="1" class="flipflop">
          <input name="D" num_pins="1" port_class="D"/>
          <output name="Q" num_pins="1" port_class="Q"/>
          <clock name="clk" port_class="clock"/>
        </pb_type>

        <interconnect>
          <direct input="lut_4.out" output="ff.D"/>
          <direct input="ble.in" output="lut_4.in"/>
          <mux input="ff.Q lut_4.out" output="ble.out"/>
          <direct input="ble.clk" output="ff.clk"/>
        </interconnect>
      </pb_type>

      <interconnect>
        <complete input="{clb.I ble[9:0].out}" output="ble[9:0].in"/>
        <complete input="clb.clk" output="ble[9:0].clk"/>
        <direct input="ble[9:0].out" output="clb.O"/>
      </interconnect>

      <!-- Describe complex block relation with FPGA -->

      <fc_in type="frac">0.150000</fc_in>
      <fc_out type="frac">0.125000</fc_out>

      <pinlocations pattern="spread"/>

    </pb_type>




arch/configurable_memory.rst
--------------------------------------
.. _configurable_memory_block_example:

Configurable Memory Block Example
---------------------------------

A memory block with a reconfigurable aspect ratio.

.. code-block:: xml

    <pb_type name="memory" height="1">
        <input name="addr1" num_pins="14"/>
        <input name="addr2" num_pins="14"/>
        <input name="data" num_pins="16"/>
        <input name="we1" num_pins="1"/>
        <input name="we2" num_pins="1"/>
        <output name="out" num_pins="16"/>
        <clock name="clk" num_pins="1"/>

        <mode name="mem_1024x16_sp">
          <pb_type name="mem_1024x16_sp" blif_model=".subckt single_port_ram" class="memory" num_pb="1" area="1000">
            <input name="addr" num_pins="10" port_class="address"/>
            <input name="data" num_pins="16" port_class="data_in"/>
            <input name="we" num_pins="1" port_class="write_en"/>
            <output name="out" num_pins="16" port_class="data_out"/>
            <clock name="clk" num_pins="1" port_class="clock"/>
          </pb_type>
          <interconnect>
            <direct name="address1" input="memory.addr1[9:0]" output="mem_1024x16_sp.addr">
            </direct>
            <direct name="data1" input="memory.data[15:0]" output="mem_1024x16_sp.data">
            </direct>
            <direct name="writeen1" input="memory.we1" output="mem_1024x16_sp.we">
            </direct>
            <direct name="dataout1" input="mem_1024x16_sp.out" output="memory.out[15:0]">
            </direct>
            <direct name="clk" input="memory.clk" output="mem_1024x16_sp.clk">
            </direct>
          </interconnect>
        </mode>
        <mode name="mem_2048x8_dp">
          <pb_type name="mem_2048x8_dp" blif_model=".subckt dual_port_ram" class="memory" num_pb="1" area="1000">
            <input name="addr1" num_pins="11" port_class="address1"/>
            <input name="addr2" num_pins="11" port_class="address2"/>
            <input name="data1" num_pins="8" port_class="data_in1"/>
            <input name="data2" num_pins="8" port_class="data_in2"/>
            <input name="we1" num_pins="1" port_class="write_en1"/>
            <input name="we2" num_pins="1" port_class="write_en2"/>
            <output name="out1" num_pins="8" port_class="data_out1"/>
            <output name="out2" num_pins="8" port_class="data_out2"/>
            <clock name="clk" num_pins="1" port_class="clock"/>
          </pb_type>
          <interconnect>
            <direct name="address1" input="memory.addr1[10:0]" output="mem_2048x8_dp.addr1">
            </direct>
            <direct name="address2" input="memory.addr2[10:0]" output="mem_2048x8_dp.addr2">
            </direct>
            <direct name="data1" input="memory.data[7:0]" output="mem_2048x8_dp.data1">
            </direct>
            <direct name="data2" input="memory.data[15:8]" output="mem_2048x8_dp.data2">
            </direct>
            <direct name="writeen1" input="memory.we1" output="mem_2048x8_dp.we1">
            </direct>
            <direct name="writeen2" input="memory.we2" output="mem_2048x8_dp.we2">
            </direct>
            <direct name="dataout1" input="mem_2048x8_dp.out1" output="memory.out[7:0]">
            </direct>
            <direct name="dataout2" input="mem_2048x8_dp.out2" output="memory.out[15:8]">
            </direct>
            <direct name="clk" input="memory.clk" output="mem_2048x8_dp.clk">
            </direct>
          </interconnect>
        </mode>

        <mode name="mem_2048x8_sp">
          <pb_type name="mem_2048x8_sp" blif_model=".subckt single_port_ram" class="memory" num_pb="1" area="1000">
            <input name="addr" num_pins="11" port_class="address"/>
            <input name="data" num_pins="8" port_class="data_in"/>
            <input name="we" num_pins="1" port_class="write_en"/>
            <output name="out" num_pins="8" port_class="data_out"/>
            <clock name="clk" num_pins="1" port_class="clock"/>
          </pb_type>
          <interconnect>
            <direct name="address1" input="memory.addr1[10:0]" output="mem_2048x8_sp.addr">
            </direct>
            <direct name="data1" input="memory.data[7:0]" output="mem_2048x8_sp.data">
            </direct>
            <direct name="writeen1" input="memory.we1" output="mem_2048x8_sp.we">
            </direct>
            <direct name="dataout1" input="mem_2048x8_sp.out" output="memory.out[7:0]">
            </direct>
            <direct name="clk" input="memory.clk" output="mem_2048x8_sp.clk">
            </direct>
          </interconnect>
        </mode>
        <mode name="mem_4096x4_dp">
          <pb_type name="mem_4096x4_dp" blif_model=".subckt dual_port_ram" class="memory" num_pb="1" area="1000">
            <input name="addr1" num_pins="12" port_class="address1"/>
            <input name="addr2" num_pins="12" port_class="address2"/>
            <input name="data1" num_pins="4" port_class="data_in1"/>
            <input name="data2" num_pins="4" port_class="data_in2"/>
            <input name="we1" num_pins="1" port_class="write_en1"/>
            <input name="we2" num_pins="1" port_class="write_en2"/>
            <output name="out1" num_pins="4" port_class="data_out1"/>
            <output name="out2" num_pins="4" port_class="data_out2"/>
            <clock name="clk" num_pins="1" port_class="clock"/>
          </pb_type>
          <interconnect>
            <direct name="address1" input="memory.addr1[11:0]" output="mem_4096x4_dp.addr1">
            </direct>
            <direct name="address2" input="memory.addr2[11:0]" output="mem_4096x4_dp.addr2">
            </direct>
            <direct name="data1" input="memory.data[3:0]" output="mem_4096x4_dp.data1">
            </direct>
            <direct name="data2" input="memory.data[7:4]" output="mem_4096x4_dp.data2">
            </direct>
            <direct name="writeen1" input="memory.we1" output="mem_4096x4_dp.we1">
            </direct>
            <direct name="writeen2" input="memory.we2" output="mem_4096x4_dp.we2">
            </direct>
            <direct name="dataout1" input="mem_4096x4_dp.out1" output="memory.out[3:0]">
            </direct>
            <direct name="dataout2" input="mem_4096x4_dp.out2" output="memory.out[7:4]">
            </direct>
            <direct name="clk" input="memory.clk" output="mem_4096x4_dp.clk">
            </direct>
          </interconnect>
        </mode>

        <mode name="mem_4096x4_sp">
          <pb_type name="mem_4096x4_sp" blif_model=".subckt single_port_ram" class="memory" num_pb="1" area="1000">
            <input name="addr" num_pins="12" port_class="address"/>
            <input name="data" num_pins="4" port_class="data_in"/>
            <input name="we" num_pins="1" port_class="write_en"/>
            <output name="out" num_pins="4" port_class="data_out"/>
            <clock name="clk" num_pins="1" port_class="clock"/>
          </pb_type>
          <interconnect>
            <direct name="address1" input="memory.addr1[11:0]" output="mem_4096x4_sp.addr">
            </direct>
            <direct name="data1" input="memory.data[3:0]" output="mem_4096x4_sp.data">
            </direct>
            <direct name="writeen1" input="memory.we1" output="mem_4096x4_sp.we">
            </direct>
            <direct name="dataout1" input="mem_4096x4_sp.out" output="memory.out[3:0]">
            </direct>
            <direct name="clk" input="memory.clk" output="mem_4096x4_sp.clk">
            </direct>
          </interconnect>
        </mode>
        <mode name="mem_8192x2_dp">
          <pb_type name="mem_8192x2_dp" blif_model=".subckt dual_port_ram" class="memory" num_pb="1" area="1000">
            <input name="addr1" num_pins="13" port_class="address1"/>
            <input name="addr2" num_pins="13" port_class="address2"/>
            <input name="data1" num_pins="2" port_class="data_in1"/>
            <input name="data2" num_pins="2" port_class="data_in2"/>
            <input name="we1" num_pins="1" port_class="write_en1"/>
            <input name="we2" num_pins="1" port_class="write_en2"/>
            <output name="out1" num_pins="2" port_class="data_out1"/>
            <output name="out2" num_pins="2" port_class="data_out2"/>
            <clock name="clk" num_pins="1" port_class="clock"/>
          </pb_type>
          <interconnect>
            <direct name="address1" input="memory.addr1[12:0]" output="mem_8192x2_dp.addr1">
            </direct>
            <direct name="address2" input="memory.addr2[12:0]" output="mem_8192x2_dp.addr2">
            </direct>
            <direct name="data1" input="memory.data[1:0]" output="mem_8192x2_dp.data1">
            </direct>
            <direct name="data2" input="memory.data[3:2]" output="mem_8192x2_dp.data2">
            </direct>
            <direct name="writeen1" input="memory.we1" output="mem_8192x2_dp.we1">
            </direct>
            <direct name="writeen2" input="memory.we2" output="mem_8192x2_dp.we2">
            </direct>
            <direct name="dataout1" input="mem_8192x2_dp.out1" output="memory.out[1:0]">
            </direct>
            <direct name="dataout2" input="mem_8192x2_dp.out2" output="memory.out[3:2]">
            </direct>
            <direct name="clk" input="memory.clk" output="mem_8192x2_dp.clk">
            </direct>
          </interconnect>
        </mode>

        <mode name="mem_8192x2_sp">
          <pb_type name="mem_8192x2_sp" blif_model=".subckt single_port_ram" class="memory" num_pb="1" area="1000">
            <input name="addr" num_pins="13" port_class="address"/>
            <input name="data" num_pins="2" port_class="data_in"/>
            <input name="we" num_pins="1" port_class="write_en"/>
            <output name="out" num_pins="2" port_class="data_out"/>
            <clock name="clk" num_pins="1" port_class="clock"/>
          </pb_type>
          <interconnect>
            <direct name="address1" input="memory.addr1[12:0]" output="mem_8192x2_sp.addr">
            </direct>
            <direct name="data1" input="memory.data[1:0]" output="mem_8192x2_sp.data">
            </direct>
            <direct name="writeen1" input="memory.we1" output="mem_8192x2_sp.we">
            </direct>
            <direct name="dataout1" input="mem_8192x2_sp.out" output="memory.out[1:0]">
            </direct>
            <direct name="clk" input="memory.clk" output="mem_8192x2_sp.clk">
            </direct>
          </interconnect>
        </mode>
        <mode name="mem_16384x1_dp">
          <pb_type name="mem_16384x1_dp" blif_model=".subckt dual_port_ram" class="memory" num_pb="1" area="1000">
            <input name="addr1" num_pins="14" port_class="address1"/>
            <input name="addr2" num_pins="14" port_class="address2"/>
            <input name="data1" num_pins="1" port_class="data_in1"/>
            <input name="data2" num_pins="1" port_class="data_in2"/>
            <input name="we1" num_pins="1" port_class="write_en1"/>
            <input name="we2" num_pins="1" port_class="write_en2"/>
            <output name="out1" num_pins="1" port_class="data_out1"/>
            <output name="out2" num_pins="1" port_class="data_out2"/>
            <clock name="clk" num_pins="1" port_class="clock"/>
          </pb_type>
          <interconnect>
            <direct name="address1" input="memory.addr1[13:0]" output="mem_16384x1_dp.addr1">
            </direct>
            <direct name="address2" input="memory.addr2[13:0]" output="mem_16384x1_dp.addr2">
            </direct>
            <direct name="data1" input="memory.data[0:0]" output="mem_16384x1_dp.data1">
            </direct>
            <direct name="data2" input="memory.data[1:1]" output="mem_16384x1_dp.data2">
            </direct>
            <direct name="writeen1" input="memory.we1" output="mem_16384x1_dp.we1">
            </direct>
            <direct name="writeen2" input="memory.we2" output="mem_16384x1_dp.we2">
            </direct>
            <direct name="dataout1" input="mem_16384x1_dp.out1" output="memory.out[0:0]">
            </direct>
            <direct name="dataout2" input="mem_16384x1_dp.out2" output="memory.out[1:1]">
            </direct>
            <direct name="clk" input="memory.clk" output="mem_16384x1_dp.clk">
            </direct>
          </interconnect>
        </mode>

        <mode name="mem_16384x1_sp">
          <pb_type name="mem_16384x1_sp" blif_model=".subckt single_port_ram" class="memory" num_pb="1" area="1000">
            <input name="addr" num_pins="14" port_class="address"/>
            <input name="data" num_pins="1" port_class="data_in"/>
            <input name="we" num_pins="1" port_class="write_en"/>
            <output name="out" num_pins="1" port_class="data_out"/>
            <clock name="clk" num_pins="1" port_class="clock"/>
          </pb_type>
          <interconnect>
            <direct name="address1" input="memory.addr1[13:0]" output="mem_16384x1_sp.addr">
            </direct>
            <direct name="data1" input="memory.data[0:0]" output="mem_16384x1_sp.data">
            </direct>
            <direct name="writeen1" input="memory.we1" output="mem_16384x1_sp.we">
            </direct>
            <direct name="dataout1" input="mem_16384x1_sp.out" output="memory.out[0:0]">
            </direct>
            <direct name="clk" input="memory.clk" output="mem_16384x1_sp.clk">
            </direct>
          </interconnect>
        </mode>

      <fc_in type="frac"> 0.15</fc_in>
      <fc_out type="frac"> 0.125</fc_out>

      <pinlocations pattern="spread"/>

    </pb_type>





arch/configurable_memory_bus.rst
--------------------------------------
.. _configurable_memory_block_bus_tutorial:

Configurable Memory Bus-Based Tutorial
--------------------------------------

.. warning:: The description in this tutorial is not yet supported by CAD tools due to bus-based routing.

.. seealso:: :ref:`configurable_memory_block_example` for a supported version.

Configurable memories are found in today's commercial FPGAs for two primary reasons:

#. Memories are found in a variety of different applications including image processing, soft processors, etc and
#. Implementing memories in soft logic (LUTs and flip-flops) is very costly in terms of area.

Thus it is important for modern FPGA architects be able to describe the specific properties of the configurable memory that they want to investigate.
The following is an example on how to use the langauge to describe a configurable memory block.
First we provide a step-by-step explanation on how to construct the memory block.
Afterwards, we present the complete code for the memory block.

.. _configurable_memory_fig:

.. figure:: configurable_memory.*

    Model of a configurable memory block

:numref:`configurable_memory_fig` shows an example of a single-ported memory.
This memory block can support multiple different width and depth combinations (called aspect ratios).
The inputs can be either registered or combinational.
Similarly, the outputs can be either registered or combinational.
Also, each memory configuration has groups of pins called ports that share common properties.
Examples of these ports include address ports, data ports, write enable, and clock.
In this example, the block memory has the following three configurations: 2048x1, 1024x2, and 512x4, which will be modeled using modes.
We begin by declaring the reconfigurable block RAM along with its I/O as follows:

.. code-block:: xml

    <pb_type name="block_RAM">
      <input name="addr" num_pins="11" equivalent="false"/>
      <input name="din" num_pins="4" equivalent="false"/>
      <input name="wen" num_pins="1" equivalent="false"/>
      <output name="dout" num_pins="4" equivalent="false"/>
      <clock name="clk" equivalent="false"/>

The input and output registers are defined as 2 sets of bypassable flip-flops at the I/Os of the block RAM.
There are a total of 16 inputs that can be registered as a bus so 16 flip-flops (for the 11 address lines, 4 data lines, and 1 write enable), named ``ff_reg_in``, must be declared.
There are 4 output bits that can also be registered, so 4 flip-flops (named ``ff_reg_out``) are declared:

.. code-block:: xml

      <pb_type name="ff_reg_in" blif_model=".latch" num_pb="16" class="flipflop">
        <input name="D" num_pins="1" port_class="D"/>
        <output name="Q" num_pins="1" port_class="Q"/>
        <clock name="clk" port_class="clock"/>
      </pb_type>
      <pb_type name="ff_reg_out" blif_model=".latch" num_pb="4" class="flipflop">
        <input name="D" num_pins="1" port_class="D"/>
        <output name="Q" num_pins="1" port_class="Q"/>
        <clock name="clk" port_class="clock"/>
      </pb_type>

Each aspect ratio of the memory is declared as a mode within the memory physical block type as shown below.
Also, observe that since memories are one of the special (common) primitives, they each have a ``class`` attribute:

.. _configurable_memory_modes_fig:

.. figure:: configurable_memory_modes.*

    Different modes of operation for the memory block.

.. code-block:: xml

      <pb_type name="mem_reconfig" num_pb="1">
        <input name="addr" num_pins="11"/>
        <input name="din" num_pins="4"/>
        <input name="wen" num_pins="1"/>
        <output name="dout" num_pins="4"/>

        <!-- Declare a 512x4 memory type -->
        <mode name="mem_512x4_mode">
          <!-- Follows the same pattern as the 1024x2 memory type declared below -->
        </mode>

        <!-- Declare a 1024x2 memory type -->
        <mode name="mem_1024x2_mode">
          <pb_type name="mem_1024x2" blif_model=".subckt sp_mem" class="memory">
            <input name="addr" num_pins="10" port_class="address"/>
            <input name="din" num_pins="2" port_class="data_in"/>
            <input name="wen" num_pins="1" port_class="write_en"/>
            <output name="dout" num_pins="2" port_class="data_out"/>
          </pb_type>
          <interconnect>
            <direct input="mem_reconfig.addr[9:0]" output="mem_1024x2.addr"/>
            <direct input="mem_reconfig.din[1:0]" output="mem_1024x2.din"/>
            <direct input="mem_reconfig.wen" output="mem_1024x2.wen"/>
            <direct input="mem_1024x2.dout" output="mem_reconfig.dout[1:0]"/>
          </interconnect>
        </mode>

        <!-- Declare a 2048x1 memory type -->
        <mode name="mem_2048x1_mode">
          <!-- Follows the same pattern as the 1024x2 memory type declared above -->
        </mode>

      </pb_type>

The top-level interconnect structure of the memory SPCB is shown in :numref:`configurable_block_ram_routing`.
The inputs of the SPCB can connect to input registers or bypass the registers and connect to the combinational memory directly.
Similarly, the outputs of the combinational memory can either be registered or connect directly to the outputs.
The description of the interconnect is as follows:

.. _configurable_block_ram_routing:

.. figure:: configurable_block_ram_routing.*

    Interconnect within the configurable memory block.

.. code-block:: xml
    :linenos:

     <interconnect>
       <direct input="{block_RAM.wen block_RAM.din block_RAM.addr}" output="ff_reg_in[15:0].D"/>
       <direct input="mem_reconfig.dout" output="ff_reg_out[3:0].D"/>
       <mux input="mem_reconfig.dout ff_reg_out[3:0].Q" output="block_RAM.dout"/>
       <mux input="{block_RAM.wen block_RAM.din[3:0] block_RAM.addr[10:0]} ff_reg_in[15:0].Q"
            output="{mem_reconfig.wen mem_reconfig.din mem_reconfig.addr}"/>
       <complete input="block_RAM.clk" output="ff_reg_in[15:0].clk"/>
       <complete input="block_RAM.clk" output="ff_reg_out[3:0].clk"/>
     </interconnect>
    </pb_type>


The interconnect for the bypassable registers is complex and so we provide a more detailed explanation.
First, consider the input registers.
Line 2 shows that the SPCB inputs drive the input flip-flops using direct wired connections.
Then, in line 5, the combinational configurable memory inputs ``{mem_reconfig.wen mem_reconfig.din mem_reconfig.addr}`` either come from the flip-flops ``ff_reg_in[15:0].Q`` or from the SPCB inputs ``{block_RAM.wen block_RAM.din[3:0] block_RAM.addr[10:0]}`` through a 16-bit 2-to-1 bus-based mux.
Thus completing the bypassable input register interconnect.
A similar scheme is used at the outputs to ensure that either all outputs are registered or none at all.
Finally, we model the relationship of the memory block with the general FPGA fabric.
The ratio of tracks that a particular input/output pin of the CLB connects to is defined by fc_in/fc_out.
The pinlocations describes which side of the logic block pins reside on where the pattern spread describes evenly spreading out the pins on all sides of the CLB in a round-robin fashion.

.. code-block:: xml

      <!-- Describe complex block relation with FPGA -->

      <fc_in type="frac">0.150000</fc_in>
      <fc_out type="frac">0.125000</fc_out>

      <pinlocations pattern="spread"/>



Configurable Memory Bus-Based Complete Example
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: xml

    <pb_type name="block_RAM">
      <input name="addr" num_pins="11" equivalent="false"/>
      <input name="din" num_pins="4" equivalent="false"/>
      <input name="wen" num_pins="1" equivalent="false"/>
      <output name="dout" num_pins="4" equivalent="false"/>
      <clock name="clk" equivalent="false"/>
      <pb_type name="ff_reg_in" blif_model=".latch" num_pb="16" class="flipflop">
        <input name="D" num_pins="1" port_class="D"/>
        <output name="Q" num_pins="1" port_class="Q"/>
        <clock name="clk" port_class="clock"/>
      </pb_type>
      <pb_type name="ff_reg_out" blif_model=".latch" num_pb="4" class="flipflop">
        <input name="D" num_pins="1" port_class="D"/>
        <output name="Q" num_pins="1" port_class="Q"/>
        <clock name="clk" port_class="clock"/>
      </pb_type>

      <pb_type name="mem_reconfig" num_pb="1">
        <input name="addr" num_pins="11"/>
        <input name="din" num_pins="4"/>
        <input name="wen" num_pins="1"/>
        <output name="dout" num_pins="4"/>

        <!-- Declare a 2048x1 memory type -->
        <mode name="mem_2048x1_mode">
          <pb_type name="mem_2048x1" blif_model=".subckt sp_mem" class="memory">
            <input name="addr" num_pins="11" port_class="address"/>
            <input name="din" num_pins="1" port_class="data_in"/>
            <input name="wen" num_pins="1" port_class="write_en"/>
            <output name="dout" num_pins="1" port_class="data_out"/>
          </pb_type>
          <interconnect>
            <direct input="mem_reconfig.addr[10:0]" output="mem_2048x1.addr"/>
            <direct input="mem_reconfig.din[0]" output="mem_2048x1.din"/>
            <direct input="mem_reconfig.wen" output="mem_2048x1.wen"/>
            <direct input="mem_2048x1.dout" output="mem_reconfig.dout[0]"/>
          </interconnect>
        </mode>

        <!-- Declare a 1024x2 memory type -->
        <mode name="mem_1024x2_mode">
          <pb_type name="mem_1024x2" blif_model=".subckt sp_mem" class="memory">
            <input name="addr" num_pins="10" port_class="address"/>
            <input name="din" num_pins="2" port_class="data_in"/>
            <input name="wen" num_pins="1" port_class="write_en"/>
            <output name="dout" num_pins="2" port_class="data_out"/>
          </pb_type>
          <interconnect>
            <direct input="mem_reconfig.addr[9:0]" output="mem_1024x2.addr"/>
            <direct input="mem_reconfig.din[1:0]" output="mem_1024x2.din"/>
            <direct input="mem_reconfig.wen" output="mem_1024x2.wen"/>
            <direct input="mem_1024x2.dout" output="mem_reconfig.dout[1:0]"/>
          </interconnect>
        </mode>

        <!-- Declare a 512x4 memory type -->
        <mode name="mem_512x4_mode">
          <pb_type name="mem_512x4" blif_model=".subckt sp_mem" class="memory">
            <input name="addr" num_pins="9" port_class="address"/>
            <input name="din" num_pins="4" port_class="data_in"/>
            <input name="wen" num_pins="1" port_class="write_en"/>
            <output name="dout" num_pins="4" port_class="data_out"/>
          </pb_type>
          <interconnect>
            <direct input="mem_reconfig.addr[8:0]" output="mem_512x4.addr"/>
            <direct input="mem_reconfig.din[3:0]" output="mem_512x4.din"/>
            <direct input="mem_reconfig.wen" output="mem_512x4.wen"/>
            <direct input="mem_512x4.dout" output="mem_reconfig.dout[3:0]"/>
          </interconnect>
        </mode>
      </pb_type>

      <interconnect>
        <direct input="{block_RAM.wen block_RAM.din block_RAM.addr}" output="ff_reg_in[15:0].D"/>
        <direct input="mem_reconfig.dout" output="ff_reg_out[3:0].D"/>
        <mux input="mem_reconfig.dout ff_reg_out[3:0].Q" output="block_RAM.dout"/>
        <mux input="{block_RAM.wen block_RAM.din[3:0] block_RAM.addr[10:0]} ff_reg_in[15:0].Q"
             output="{mem_reconfig.wen mem_reconfig.din mem_reconfig.addr}"/>
        <complete input="block_RAM.clk" output="ff_reg_in[15:0].clk"/>
        <complete input="block_RAM.clk" output="ff_reg_out[3:0].clk"/>
      </interconnect>
    </pb_type>

      <!-- Describe complex block relation with FPGA -->

      <fc_in type="frac">0.150000</fc_in>
      <fc_out type="frac">0.125000</fc_out>

      <pinlocations pattern="spread"/>



arch/equivalent_sites.rst
--------------------------------------
.. _equivalent_sites_tutorial:

Equivalent Sites tutorial
=========================

This tutorial aims at providing information to the user on how to model the equivalent sites to enable ``equivalent placement`` in VPR.

Equivalent site placement allows the user to define complex logical blocks (top-level pb_types) that can be used in multiple physical location types of the FPGA device grid.
In the same way, the user can define many physical tiles that have different physical attributes that can implement the same logical block.

The first case (multiple physical grid location types for one complex logical block) is explained below.
The device has at disposal two different Configurable Logic Blocks (CLB), SLICEL and SLICEM.
In this case, the SLICEM CLB is a superset that implements additional features w.r.t. the SLICEL CLB.
Therefore, the user can decide to model the architecture to be able to place the SLICEL Complex Block in a SLICEM physical tile, being it a valid grid location.
This behavior can lead to the generation of more accurate and better placement results, given that a Complex Logic Block is not bound to only one physical location type.

Below the user can find the implementation of this situation starting from an example that does not make use of the equivalent site placement:

.. code-block:: xml

    <tiles>
        <tile name="SLICEL_TILE">
            <input name="IN_A" num_pins="6"/>
            <input name="AX" num_pins="1"/>
            <input name="SR" num_pins="1"/>
            <input name="CE" num_pins="1"/>
            <input name="CIN" num_pins="1"/>
            <clock name="CLK" num_pins="1"/>
            <output name="A" num_pins="1"/>
            <output name="AMUX" num_pins="1"/>
            <output name="AQ" num_pins="1"/>

            <equivalent_sites>
                <site pb_type="SLICEL_SITE" pin_mapping="direct"/>
            </equivalent_sites>

            <fc />
            <pinlocations />
        </tile>
        <tile name="SLICEM_TILE">
            <input name="IN_A" num_pins="6"/>
            <input name="AX" num_pins="1"/>
            <input name="AI" num_pins="1"/>
            <input name="SR" num_pins="1"/>
            <input name="WE" num_pins="1"/>
            <input name="CE" num_pins="1"/>
            <input name="CIN" num_pins="1"/>
            <clock name="CLK" num_pins="1"/>
            <output name="A" num_pins="1"/>
            <output name="AMUX" num_pins="1"/>
            <output name="AQ" num_pins="1"/>

            <equivalent_sites>
                <site pb_type="SLICEM_SITE" pin_mapping="direct"/>
            </equivalent_sites>

            <fc />
            <pinlocations />
        </tile>
    </tiles>

    <complexblocklist>
        <pb_type name="SLICEL_SITE"/>
            <input name="IN_A" num_pins="6"/>
            <input name="AX" num_pins="1"/>
            <input name="AI" num_pins="1"/>
            <input name="SR" num_pins="1"/>
            <input name="CE" num_pins="1"/>
            <input name="CIN" num_pins="1"/>
            <clock name="CLK" num_pins="1"/>
            <output name="A" num_pins="1"/>
            <output name="AMUX" num_pins="1"/>
            <output name="AQ" num_pins="1"/>
            <mode />
            /
        </pb_type>
        <pb_type name="SLICEM_SITE"/>
            <input name="IN_A" num_pins="6"/>
            <input name="AX" num_pins="1"/>
            <input name="SR" num_pins="1"/>
            <input name="WE" num_pins="1"/>
            <input name="CE" num_pins="1"/>
            <input name="CIN" num_pins="1"/>
            <clock name="CLK" num_pins="1"/>
            <output name="A" num_pins="1"/>
            <output name="AMUX" num_pins="1"/>
            <output name="AQ" num_pins="1"/>
            <mode />
            /
        </pb_type>
    </complexblocklist>

As the user can see, ``SLICEL`` and ``SLICEM`` are treated as two different entities, even though they seem to be similar one to another.
To have the possibility to make VPR choose a ``SLICEM`` location when placing a ``SLICEL_SITE`` pb_type, the user needs to change the ``SLICEM`` tile accordingly, as shown below:

.. code-block:: xml

    <tile name="SLICEM_TILE">
        <input name="IN_A" num_pins="6"/>
        <input name="AX" num_pins="1"/>
        <input name="AI" num_pins="1"/>
        <input name="SR" num_pins="1"/>
        <input name="WE" num_pins="1"/>
        <input name="CE" num_pins="1"/>
        <input name="CIN" num_pins="1"/>
        <clock name="CLK" num_pins="1"/>
        <output name="A" num_pins="1"/>
        <output name="AMUX" num_pins="1"/>
        <output name="AQ" num_pins="1"/>

        <equivalent_sites>
            <site pb_type="SLICEM_SITE" pin_mapping="direct"/>
            <site pb_type="SLICEL_SITE" pin_mapping="custom">
                <direct from="SLICEM_TILE.IN_A" to="SLICEL_SITE.IN_A"/>
                <direct from="SLICEM_TILE.AX" to="SLICEL_SITE.AX"/>
                <direct from="SLICEM_TILE.SR" to="SLICEL_SITE.SR"/>
                <direct from="SLICEM_TILE.CE" to="SLICEL_SITE.CE"/>
                <direct from="SLICEM_TILE.CIN" to="SLICEL_SITE.CIN"/>
                <direct from="SLICEM_TILE.CLK" to="SLICEL_SITE.CLK"/>
                <direct from="SLICEM_TILE.A" to="SLICEL_SITE.A"/>
                <direct from="SLICEM_TILE.AMUX" to="SLICEL_SITE.AMUX"/>
                <direct from="SLICEM_TILE.AQ" to="SLICEL_SITE.AQ"/>
            </site>
        </equivalent_sites>

        <fc />
        <pinlocations />
    </tile>

With the above description of the ``SLICEM`` tile, the user can now have the ``SLICEL`` sites to be placed in ``SLICEM`` physical locations.
One thing to notice is that not all the pins have been mapped for the ``SLICEL_SITE``. For instance, the ``WE`` and ``AI`` port are absent from the ``SLICEL_SITE`` definition, hence they cannot appear in the pin mapping between physical tile and logical block.

The second case described in this tutorial refers to the situation for which there are multiple different physical location types in the device grid that are used by one complex logical blocks.
Imagine the situation for which the device has left and right I/O tile types which have different pinlocations, hence they need to be defined in two different ways.
With equivalent site placement, the user doesn't need to define multiple different pb_types that implement the same functionality.

Below the user can find the implementation of this situation starting from an example that does not make use of the equivalent site placement:

.. code-block:: xml

    <tiles>
        <tile name="LEFT_IOPAD_TILE">
            <input name="INPUT" num_pins="1"/>
            <output name="OUTPUT" num_pins="1"/>

            <equivalent_sites>
                <site pb_type="LEFT_IOPAD_SITE" pin_mapping="direct"/>
            </equivalent_sites>

            <fc />
            <pinlocations pattern="custom">
                <loc side="left">LEFT_IOPAD_TILE.INPUT</loc>
                <loc side="right">LEFT_IOPAD_TILE.OUTPUT</loc>
            </pinlocations>
        </tile>
        <tile name="RIGHT_IOPAD_TILE">
            <input name="INPUT" num_pins="1"/>
            <output name="OUTPUT" num_pins="1"/>

            <equivalent_sites>
                <site pb_type="RIGHT_IOPAD_SITE" pin_mapping="direct"/>
            </equivalent_sites>

            <fc />
            <pinlocations pattern="custom">
                <loc side="right">RIGHT_IOPAD_TILE.INPUT</loc>
                <loc side="left">RIGHT_IOPAD_TILE.OUTPUT</loc>
            </pinlocations>
        </tile>
    </tiles>

    <complexblocklist>
        <pb_type name="LEFT_IOPAD_SITE">
            <input name="INPUT" num_pins="1"/>
            <output name="OUTPUT" num_pins="1"/>
            <mode />
            /
        </pb_type>
        <pb_type name="RIGHT_IOPAD_SITE">
            <input name="INPUT" num_pins="1"/>
            <output name="OUTPUT" num_pins="1"/>
            <mode />
            /
        </pb_type>
    </complexblocklist>

To avoid duplicating the complex logic blocks in ``LEFT`` and ``RIGHT IOPADS``, the user can describe the pb_type only once and add it to the equivalent sites tag of the two different tiles, as follows:

.. code-block:: xml

    <tiles>
        <tile name="LEFT_IOPAD_TILE">
            <input name="INPUT" num_pins="1"/>
            <output name="OUTPUT" num_pins="1"/>

            <equivalent_sites>
                <site pb_type="IOPAD_SITE" pin_mapping="direct"/>
            </equivalent_sites>

            <fc />
            <pinlocations pattern="custom">
                <loc side="left">LEFT_IOPAD_TILE.INPUT</loc>
                <loc side="right">LEFT_IOPAD_TILE.OUTPUT</loc>
            </pinlocations>
        </tile>
        <tile name="RIGHT_IOPAD_TILE">
            <input name="INPUT" num_pins="1"/>
            <output name="OUTPUT" num_pins="1"/>

            <equivalent_sites>
                <site pb_type="IOPAD_SITE" pin_mapping="direct"/>
            </equivalent_sites>

            <fc />
            <pinlocations pattern="custom">
                <loc side="right">RIGHT_IOPAD_TILE.INPUT</loc>
                <loc side="left">RIGHT_IOPAD_TILE.OUTPUT</loc>
            </pinlocations>
        </tile>
    </tiles>

    <complexblocklist>
        <pb_type name="IOPAD_SITE">
            <input name="INPUT" num_pins="1"/>
            <output name="OUTPUT" num_pins="1"/>
            <mode>
                ...
            </mode>
        </pb_type>
    </complexblocklist>

With this implementation, the ``IOPAD_SITE`` can be placed both in the ``LEFT`` and ``RIGHT`` physical location types.
Note that the pin_mapping is set as ``direct``, given that the physical tile and the logical block share the same IO pins.

The two different cases can be mixed to have a N to M mapping of physical tiles/logical blocks.



arch/fracturable_multiplier.rst
--------------------------------------
.. _fracturable_multiplier_example:

Fracturable Multiplier Example
------------------------------

A 36x36 multiplier fracturable into 18x18s and 9x9s

.. code-block:: xml

    <pb_type name="mult_36" height="3">
        <input name="a" num_pins="36"/>
        <input name="b" num_pins="36"/>
        <output name="out" num_pins="72"/>

        <mode name="two_divisible_mult_18x18">
          <pb_type name="divisible_mult_18x18" num_pb="2">
            <input name="a" num_pins="18"/>
            <input name="b" num_pins="18"/>
            <output name="out" num_pins="36"/>

            <mode name="two_mult_9x9">
              <pb_type name="mult_9x9_slice" num_pb="2">
                <input name="A_cfg" num_pins="9"/>
                <input name="B_cfg" num_pins="9"/>
                <output name="OUT_cfg" num_pins="18"/>

                <pb_type name="mult_9x9" blif_model=".subckt multiply" num_pb="1" area="300">
                  <input name="a" num_pins="9"/>
                  <input name="b" num_pins="9"/>
                  <output name="out" num_pins="18"/>
                  <delay_constant max="2.03e-13" min="1.89e-13" in_port="{a b}" out_port="out"/>
                </pb_type>

                <interconnect>
                  <direct name="a2a" input="mult_9x9_slice.A_cfg" output="mult_9x9.a">
                    <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_9x9_slice.A_cfg" out_port="mult_9x9.a"/>
                    <C_constant C="1.89e-13" in_port="mult_9x9_slice.A_cfg" out_port="mult_9x9.a"/>
                  </direct>
                  <direct name="b2b" input="mult_9x9_slice.B_cfg" output="mult_9x9.b">
                    <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_9x9_slice.B_cfg" out_port="mult_9x9.b"/>
                    <C_constant C="1.89e-13" in_port="mult_9x9_slice.B_cfg" out_port="mult_9x9.b"/>
                  </direct>
                  <direct name="out2out" input="mult_9x9.out" output="mult_9x9_slice.OUT_cfg">
                    <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_9x9.out" out_port="mult_9x9_slice.OUT_cfg"/>
                    <C_constant C="1.89e-13" in_port="mult_9x9.out" out_port="mult_9x9_slice.OUT_cfg"/>
                  </direct>
                </interconnect>
              </pb_type>
              <interconnect>
                <direct name="a2a" input="divisible_mult_18x18.a" output="mult_9x9_slice[1:0].A_cfg">
                  <delay_constant max="2.03e-13" min="1.89e-13" in_port="divisible_mult_18x18.a" out_port="mult_9x9_slice[1:0].A_cfg"/>
                  <C_constant C="1.89e-13" in_port="divisible_mult_18x18.a" out_port="mult_9x9_slice[1:0].A_cfg"/>
                </direct>
                <direct name="b2b" input="divisible_mult_18x18.b" output="mult_9x9_slice[1:0].B_cfg">
                  <delay_constant max="2.03e-13" min="1.89e-13" in_port="divisible_mult_18x18.b" out_port="mult_9x9_slice[1:0].B_cfg"/>
                  <C_constant C="1.89e-13" in_port="divisible_mult_18x18.b" out_port="mult_9x9_slice[1:0].B_cfg"/>
                </direct>
                <direct name="out2out" input="mult_9x9_slice[1:0].OUT_cfg" output="divisible_mult_18x18.out">
                  <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_9x9_slice[1:0].OUT_cfg" out_port ="divisible_mult_18x18.out"/>
                  <C_constant C="1.89e-13" in_port="mult_9x9_slice[1:0].OUT_cfg" out_port="divisible_mult_18x18.out"/>
                </direct>
              </interconnect>
            </mode>

            <mode name="mult_18x18">
              <pb_type name="mult_18x18_slice" num_pb="1">
                <input name="A_cfg" num_pins="18"/>
                <input name="B_cfg" num_pins="18"/>
                <output name="OUT_cfg" num_pins="36"/>

                <pb_type name="mult_18x18" blif_model=".subckt multiply" num_pb="1"  area="1000">
                  <input name="a" num_pins="18"/>
                  <input name="b" num_pins="18"/>
                  <output name="out" num_pins="36"/>
                  <delay_constant max="2.03e-13" min="1.89e-13" in_port="{a b}" out_port="out"/>
                </pb_type>

                <interconnect>
                  <direct name="a2a" input="mult_18x18_slice.A_cfg" output="mult_18x18.a">
                    <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_18x18_slice.A_cfg" out_port="mult_18x18.a"/>
                    <C_constant C="1.89e-13" in_port="mult_18x18_slice.A_cfg" out_port="mult_18x18.a"/>
                  </direct>
                  <direct name="b2b" input="mult_18x18_slice.B_cfg" output="mult_18x18.b">
                    <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_18x18_slice.B_cfg" out_port="mult_18x18.b"/>
                    <C_constant C="1.89e-13" in_port="mult_18x18_slice.B_cfg" out_port="mult_18x18.b"/>
                  </direct>
                  <direct name="out2out" input="mult_18x18.out" output="mult_18x18_slice.OUT_cfg">
                    <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_18x18.out" out_port="mult_18x18_slice.OUT_cfg"/>
                    <C_constant C="1.89e-13" in_port="mult_18x18.out" out_port="mult_18x18_slice.OUT_cfg"/>
                  </direct>
                </interconnect>
              </pb_type>
              <interconnect>
                <direct name="a2a" input="divisible_mult_18x18.a" output="mult_18x18_slice.A_cfg">
                  <delay_constant max="2.03e-13" min="1.89e-13" in_port="divisible_mult_18x18.a" out_port="mult_18x18_slice.A_cfg"/>
                  <C_constant C="1.89e-13" in_port="divisible_mult_18x18.a" out_port="mult_18x18_slice.A_cfg"/>
                </direct>
                <direct name="b2b" input="divisible_mult_18x18.b" output="mult_18x18_slice.B_cfg">
                  <delay_constant max="2.03e-13" min="1.89e-13" in_port="divisible_mult_18x18.b" out_port="mult_18x18_slice.B_cfg"/>
                  <C_constant C="1.89e-13" in_port="divisible_mult_18x18.b" out_port="mult_18x18_slice.B_cfg"/>
                </direct>
                <direct name="out2out" input="mult_18x18_slice.OUT_cfg" output="divisible_mult_18x18.out">
                  <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_18x18_slice.OUT_cfg" out_port="divisible_mult_18x18.out"/>
                  <C_constant C="1.89e-13" in_port="mult_18x18_slice.OUT_cfg" out_port="divisible_mult_18x18.out"/>
                </direct>
              </interconnect>
            </mode>
          </pb_type>
          <interconnect>
            <direct name="a2a" input="mult_36.a" output="divisible_mult_18x18[1:0].a">
              <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_36.a" out_port="divisible_mult_18x18[1:0].a"/>
              <C_constant C="1.89e-13" in_port="mult_36.a" out_port="divisible_mult_18x18[1:0].a"/>
            </direct>
            <direct name="b2b" input="mult_36.b" output="divisible_mult_18x18[1:0].a">
              <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_36.b" out_port="divisible_mult_18x18[1:0].a"/>
              <C_constant C="1.89e-13" in_port="mult_36.b" out_port="divisible_mult_18x18[1:0].a"/>
            </direct>
            <direct name="out2out" input="divisible_mult_18x18[1:0].out" output="mult_36.out">
              <delay_constant max="2.03e-13" min="1.89e-13" in_port="divisible_mult_18x18[1:0].out" out_port ="mult_36.out"/>
              <C_constant C="1.89e-13" in_port="divisible_mult_18x18[1:0].out" out_port="mult_36.out"/>
            </direct>
          </interconnect>
        </mode>

        <mode name="mult_36x36">
          <pb_type name="mult_36x36_slice" num_pb="1">
            <input name="A_cfg" num_pins="36"/>
            <input name="B_cfg" num_pins="36"/>
            <output name="OUT_cfg" num_pins="72"/>

            <pb_type name="mult_36x36" blif_model=".subckt multiply" num_pb="1" area="4000">
              <input name="a" num_pins="36"/>
              <input name="b" num_pins="36"/>
              <output name="out" num_pins="72"/>
              <delay_constant max="2.03e-13" min="1.89e-13" in_port="{a b}" out_port="out"/>
            </pb_type>

            <interconnect>
              <direct name="a2a" input="mult_36x36_slice.A_cfg" output="mult_36x36.a">
                <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_36x36_slice.A_cfg" out_port="mult_36x36.a"/>
                <C_constant C="1.89e-13" in_port="mult_36x36_slice.A_cfg" out_port="mult_36x36.a"/>
              </direct>
              <direct name="b2b" input="mult_36x36_slice.B_cfg" output="mult_36x36.b">
                <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_36x36_slice.B_cfg" out_port="mult_36x36.b"/>
                <C_constant C="1.89e-13" in_port="mult_36x36_slice.B_cfg" out_port="mult_36x36.b"/>
              </direct>
              <direct name="out2out" input="mult_36x36.out" output="mult_36x36_slice.OUT_cfg">
                <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_36x36.out" out_port="mult_36x36_slice.OUT_cfg"/>
                <C_constant C="1.89e-13" in_port="mult_36x36.out" out_port="mult_36x36_slice.OUT_cfg"/>
              </direct>
            </interconnect>
          </pb_type>
          <interconnect>
            <direct name="a2a" input="mult_36.a" output="mult_36x36_slice.A_cfg">
              <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_36.a" out_port="mult_36x36_slice.A_cfg"/>
              <C_constant C="1.89e-13" in_port="mult_36.a" out_port="mult_36x36_slice.A_cfg"/>
            </direct>
            <direct name="b2b" input="mult_36.b" output="mult_36x36_slice.B_cfg">
              <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_36.b" out_port="mult_36x36_slice.B_cfg"/>
              <C_constant C="1.89e-13" in_port="mult_36.b" out_port="mult_36x36_slice.B_cfg"/>
            </direct>
            <direct name="out2out" input="mult_36x36_slice.OUT_cfg" output="mult_36.out">
              <delay_constant max="2.03e-13" min="1.89e-13" in_port="mult_36x36_slice.OUT_cfg" out_port="mult_36.out"/>
              <C_constant C="1.89e-13" in_port="mult_36x36_slice.OUT_cfg" out_port="mult_36.out"/>
            </direct>
          </interconnect>
        </mode>

      <fc_in type="frac"> 0.15</fc_in>
      <fc_out type="frac"> 0.125</fc_out>

      <pinlocations pattern="spread"/>

    </pb_type>





arch/fracturable_multiplier_bus.rst
--------------------------------------
.. _fracturable_multiplier_bus_based_tutorial:

Fracturable Multiplier Bus-Based Tutorial
-----------------------------------------

.. warning:: The description in this tutorial is not yet supported by CAD tools due to bus-based routing.

.. seealso:: :ref:`fracturable_multiplier_example` for a supported version.

Configurable multipliers are found in today's commercial FPGAs for two primary reasons:

#. Multipliers are found in a variety of different applications including DSP, soft processors, scientific computing, etc and
#. Implementing multipliers in soft logic is very area expensive.

Thus it is important for modern FPGA architects be able to describe the specific properties of the configurable multiplier that they want to investigate.
The following is an example on how to use the VPR architecture description langauge to describe a common type of configurable multiplier called a fracturable multiplier shown in :numref:`fig_fracturable_multiplier`.
We first give a step-by-step description on how to construct the multiplier block followed by a complete example.

.. _fig_fracturable_multiplier:

.. figure:: fracturable_multiplier.*

    Model of a fracturable multiplier block

The large ``block_mult`` can implement one 36x36 multiplier cluster called a ``mult_36x36_slice`` or it can implement two divisble 18x18 multipliers.
A divisible 18x18 multiplier can implement a 18x18 multiplier cluster called a ``mult_18x18_slice`` or it can be fractured into two 9x9 mulitplier clusters called ``mult_9x9_slice``.
:numref:`fig_fracturable_multiplier_slice` shows a multiplier slice.
Pins belonging to the same input or output port of a multiplier slice must be either all registered or none registered.
Pins belonging to different ports or different slices may have different register configurations.
A multiplier primitive itself has two input ports (``A`` and ``B``) and one output port (``OUT``).

.. _fig_fracturable_multiplier_slice:

.. figure:: fracturable_multiplier_slice.*

    Multiplier slice

First, we describe the ``block_mult`` complex block as follows:

.. code-block:: xml

    <pb_type name="block_mult">
      <input name="A" num_pins="36"/>
      <input name="B" num_pins="36"/>
      <output name="OUT" num_pins="72"/>
      <clock name="clk"/>

The ``block_mult`` complex block has two modes: a mode containing a 36x36 multiplier slice and a mode containing two fracturable 18x18 multipliers.
The mode containing the 36x36 multiplier slice is described first.
The mode and slice is declared here:

.. code-block:: xml

      <mode name="mult_36x36">
        <pb_type name="mult_36x36_slice" num_pb="1">
          <input name="A_cfg" num_pins="36"/>
          <input name="B_cfg" num_pins="36"/>
          <input name="OUT_cfg" num_pins="72"/>
          <clock name="clk"/>

This is followed by a description of the primitives within the slice.
There are two sets of 36 flip-flops for the input ports and one set of 72 flip-flops for the output port.
There is one 36x36 multiplier primitive.
These primitives are described by four *pb_types* as follows:

.. code-block:: xml

      <pb_type name="reg_36x36_A" blif_model=".latch" num_pb="36" class="flipflop">
        <input name="D" num_pins="1" port_class="D"/>
        <output name="Q" num_pins="1" port_class="Q"/>
        <clock name="clk" port_class="clock"/>
      </pb_type>
      <pb_type name="reg_36x36_B" blif_model=".latch" num_pb="36" class="flipflop">
        <input name="D" num_pins="1" port_class="D"/>
        <output name="Q" num_pins="1" port_class="Q"/>
        <clock name="clk" port_class="clock"/>
      </pb_type>
      <pb_type name="reg_36x36_out" blif_model=".latch" num_pb="72" class="flipflop">
        <input name="D" num_pins="1" port_class="D"/>
        <output name="Q" num_pins="1" port_class="Q"/>
        <clock name="clk" port_class="clock"/>
      </pb_type>

      <pb_type name="mult_36x36" blif_model=".subckt mult" num_pb="1">
        <input name="A" num_pins="36"/>
        <input name="B" num_pins="36"/>
        <output name="OUT" num_pins="72"/>
      </pb_type>

The slice description finishes with a specification of the interconnection.
Using the same technique as in the memory example, bus-based multiplexers are used to register the ports.
Clocks are connected using the complete tag because there is a one-to-many relationship.
Direct tags are used to make simple, one-to-one connections.

.. code-block:: xml

          <interconnect>
            <direct input="mult_36x36_slice.A_cfg" output="reg_36x36_A[35:0].D"/>
            <direct input="mult_36x36_slice.B_cfg" output="reg_36x36_B[35:0].D"/>
            <mux input="mult_36x36_slice.A_cfg reg_36x36_A[35:0].Q" output="mult_36x36.A"/>
            <mux input="mult_36x36_slice.B_cfg reg_36x36_B[35:0].Q" output="mult_36x36.B"/>

            <direct input="mult_36x36.OUT" output="reg_36x36_out[71:0].D"/>
            <mux input="mult_36x36.OUT reg_36x36_out[71:0].Q" output="mult_36x36_slice.OUT_cfg"/>

            <complete input="mult_36x36_slice.clk" output="reg_36x36_A[35:0].clk"/>
            <complete input="mult_36x36_slice.clk" output="reg_36x36_B[35:0].clk"/>
            <complete input="mult_36x36_slice.clk" output="reg_36x36_out[71:0].clk"/>
          </interconnect>
        </pb_type>

The mode finishes with a specification of the interconnect between the slice and its parent.

.. code-block:: xml

        <interconnect>
          <direct input="block_mult.A" output="mult_36x36_slice.A_cfg"/>
          <direct input="block_mult.B" output="mult_36x36_slice.A_cfg"/>
          <direct input="mult_36x36_slice.OUT_cfg" output="block_mult.OUT"/>
          <direct input="block_mult.clk" output="mult_36x36_slice.clk"/>
        </interconnect>
      </mode>

After the mode containing the 36x36 multiplier slice is described, the mode containing two fracturable 18x18 multipliers is described:

.. code-block:: xml

      <mode name="two_divisible_mult_18x18">
        <pb_type name="divisible_mult_18x18" num_pb="2">
          <input name="A" num_pins="18"/>
          <input name="B" num_pins="18"/>
          <input name="OUT" num_pins="36"/>
          <clock name="clk"/>

This mode has two additional modes which are the actual 18x18 multiply block or two 9x9 mulitplier blocks.
Both follow a similar description as the ``mult_36x36_slice`` with just the number of pins halved so the details are not repeated.

.. code-block:: xml

          <mode  name="two_divisible_mult_18x18">
            <pb_type name="mult_18x18_slice" num_pb="1">
              <!-- follows previous pattern for slice definition -->
            </pb_type>
            <interconnect>
              <!-- follows previous pattern for slice definition -->
            </interconnect>
          </mode>

          <mode name="two_mult_9x9">
            <pb_type name="mult_9x9_slice" num_pb="2">
              <!-- follows previous pattern for slice definition -->
            </pb_type>
            <interconnect>
              <!-- follows previous pattern for slice definition -->
            </interconnect>
          </mode>

        </pb_type>

The interconnect for the divisible 18x18 mode is shown in :numref:`fig_fracturable_multiplier_cluster`.
The unique characteristic of this interconnect is that the input and output ports of the parent is split in half, one half for each child.
A convenient way to specify this is to use the syntax divisible_mult_18x18[1:0] which will append the pins of the ports of the children together.
The interconnect for the fracturable 18x18 mode is described here:

.. _fig_fracturable_multiplier_cluster:

.. figure:: fracturable_multiplier_cluster.*

    Multiplier Cluster

.. code-block:: xml

        <interconnect>
          <direct input="block_mult.A" output="divisible_mult_18x18[1:0].A"/>
          <direct input="block_mult.B" output="divisible_mult_18x18[1:0].B"/>
          <direct input="divisible_mult_18x18[1:0].OUT" output="block_mult.OUT"/>
          <complete input="block_mult.clk" output="divisible_mult_18x18[1:0].clk"/>
        </interconnect>
      </mode>
    </pb_type>


Fracturable Multiplier Bus-Based Complete Example
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: xml

    <!-- Example of a fracturable mutliplier whose inputs and outputs may be optionally registered
         The multiplier hard logic block can implement one 36x36, two 18x18, or four 9x9 multiplies
     -->
    <pb_type name="block_mult">
      <input name="A" num_pins="36"/>
      <input name="B" num_pins="36"/>
      <output name="OUT" num_pins="72"/>
      <clock name="clk"/>

      <mode name="mult_36x36">
        <pb_type name="mult_36x36_slice" num_pb="1">
          <input name="A_cfg" num_pins="36" equivalence="false"/>
          <input name="B_cfg" num_pins="36" equivalence="false"/>
          <input name="OUT_cfg" num_pins="72" equivalence="false"/>
          <clock name="clk"/>

          <pb_type name="reg_36x36_A" blif_model=".latch" num_pb="36" class="flipflop">
            <input name="D" num_pins="1" port_class="D"/>
            <output name="Q" num_pins="1" port_class="Q"/>
            <clock name="clk" port_class="clock"/>
          </pb_type>
          <pb_type name="reg_36x36_B" blif_model=".latch" num_pb="36" class="flipflop">
            <input name="D" num_pins="1" port_class="D"/>
            <output name="Q" num_pins="1" port_class="Q"/>
            <clock name="clk" port_class="clock"/>
          </pb_type>
          <pb_type name="reg_36x36_out" blif_model=".latch" num_pb="72" class="flipflop">
            <input name="D" num_pins="1" port_class="D"/>
            <output name="Q" num_pins="1" port_class="Q"/>
            <clock name="clk" port_class="clock"/>
          </pb_type>

          <pb_type name="mult_36x36" blif_model=".subckt mult" num_pb="1">
            <input name="A" num_pins="36"/>
            <input name="B" num_pins="36"/>
            <output name="OUT" num_pins="72"/>
          </pb_type>

          <interconnect>
            <direct input="mult_36x36_slice.A_cfg" output="reg_36x36_A[35:0].D"/>
            <direct input="mult_36x36_slice.B_cfg" output="reg_36x36_B[35:0].D"/>
            <mux input="mult_36x36_slice.A_cfg reg_36x36_A[35:0].Q" output="mult_36x36.A"/>
            <mux input="mult_36x36_slice.B_cfg reg_36x36_B[35:0].Q" output="mult_36x36.B"/>

            <direct input="mult_36x36.OUT" output="reg_36x36_out[71:0].D"/>
            <mux input="mult_36x36.OUT reg_36x36_out[71:0].Q" output="mult_36x36_slice.OUT_cfg"/>

            <complete input="mult_36x36_slice.clk" output="reg_36x36_A[35:0].clk"/>
            <complete input="mult_36x36_slice.clk" output="reg_36x36_B[35:0].clk"/>
            <complete input="mult_36x36_slice.clk" output="reg_36x36_out[71:0].clk"/>
          </interconnect>
        </pb_type>
        <interconnect>
          <direct input="block_mult.A" output="mult_36x36_slice.A_cfg"/>
          <direct input="block_mult.B" output="mult_36x36_slice.A_cfg"/>
          <direct input="mult_36x36_slice.OUT_cfg" output="block_mult.OUT"/>
          <direct input="block_mult.clk" output="mult_36x36_slice.clk"/>
        </interconnect>
      </mode>

      <mode  name="two_divisible_mult_18x18">
        <pb_type name="divisible_mult_18x18" num_pb="2">
          <input name="A" num_pins="18"/>
          <input name="B" num_pins="18"/>
          <input name="OUT" num_pins="36"/>
          <clock name="clk"/>

          <mode name="mult_18x18">
            <pb_type name="mult_18x18_slice" num_pb="1">
              <input name="A_cfg" num_pins="18"/>
              <input name="B_cfg" num_pins="18"/>
              <input name="OUT_cfg" num_pins="36"/>
              <clock name="clk"/>

              <pb_type name="reg_18x18_A" blif_model=".latch" num_pb="18" class="flipflop">
                <input name="D" num_pins="1" port_class="D"/>
                <output name="Q" num_pins="1" port_class="Q"/>
                <clock name="clk" port_class="clock"/>
              </pb_type>
              <pb_type name="reg_18x18_B" blif_model=".latch" num_pb="18" class="flipflop">
                <input name="D" num_pins="1" port_class="D"/>
                <output name="Q" num_pins="1" port_class="Q"/>
                <clock name="clk" port_class="clock"/>
              </pb_type>
              <pb_type name="reg_18x18_out" blif_model=".latch" num_pb="36" class="flipflop">
                <input name="D" num_pins="1" port_class="D"/>
                <output name="Q" num_pins="1" port_class="Q"/>
                <clock name="clk" port_class="clock"/>
              </pb_type>

              <pb_type name="mult_18x18" blif_model=".subckt mult" num_pb="1">
                <input name="A" num_pins="18"/>
                <input name="B" num_pins="18"/>
                <output name="OUT" num_pins="36"/>
              </pb_type>

              <interconnect>
                <direct input="mult_18x18_slice.A_cfg" output="reg_18x18_A[17:0].D"/>
                <direct input="mult_18x18_slice.B_cfg" output="reg_18x18_B[17:0].D"/>
                <mux input="mult_18x18_slice.A_cfg reg_18x18_A[17:0].Q" output="mult_18x18.A"/>
                <mux input="mult_18x18_slice.B_cfg reg_18x18_B[17:0].Q" output="mult_18x18.B"/>

                <direct input="mult_18x18.OUT" output="reg_18x18_out[35:0].D"/>
                <mux input="mult_18x18.OUT reg_18x18_out[35:0].Q" output="mult_18x18_slice.OUT_cfg"/>

                <complete input="mult_18x18_slice.clk" output="reg_18x18_A[17:0].clk"/>
                <complete input="mult_18x18_slice.clk" output="reg_18x18_B[17:0].clk"/>
                <complete input="mult_18x18_slice.clk" output="reg_18x18_out[35:0].clk"/>
              </interconnect>
            </pb_type>
            <interconnect>
              <direct input="divisible_mult_18x18.A" output="mult_18x18_slice.A_cfg"/>
              <direct input="divisible_mult_18x18.B" output="mult_18x18_slice.A_cfg"/>
              <direct input="mult_18x18_slice.OUT_cfg" output="divisible_mult_18x18.OUT"/>
              <complete input="divisible_mult_18x18.clk" output="mult_18x18_slice.clk"/>
            </interconnect>
          </mode>

          <mode name="two_mult_9x9">
            <pb_type name="mult_9x9_slice" num_pb="2">
              <input name="A_cfg" num_pins="9"/>
              <input name="B_cfg" num_pins="9"/>
              <input name="OUT_cfg" num_pins="18"/>
              <clock name="clk"/>

              <pb_type name="reg_9x9_A" blif_model=".latch" num_pb="9" class="flipflop">
                <input name="D" num_pins="1" port_class="D"/>
                <output name="Q" num_pins="1" port_class="Q"/>
                <clock name="clk" port_class="clock"/>
              </pb_type>
              <pb_type name="reg_9x9_B" blif_model=".latch" num_pb="9" class="flipflop">
                <input name="D" num_pins="1" port_class="D"/>
                <output name="Q" num_pins="1" port_class="Q"/>
                <clock name="clk" port_class="clock"/>
              </pb_type>
              <pb_type name="reg_9x9_out" blif_model=".latch" num_pb="18" class="flipflop">
                <input name="D" num_pins="1" port_class="D"/>
                <output name="Q" num_pins="1" port_class="Q"/>
                <clock name="clk" port_class="clock"/>
              </pb_type>

              <pb_type name="mult_9x9" blif_model=".subckt mult" num_pb="1">
                <input name="A" num_pins="9"/>
                <input name="B" num_pins="9"/>
                <output name="OUT" num_pins="18"/>
              </pb_type>

              <interconnect>
                <direct input="mult_9x9_slice.A_cfg" output="reg_9x9_A[8:0].D"/>
                <direct input="mult_9x9_slice.B_cfg" output="reg_9x9_B[8:0].D"/>
                <mux input="mult_9x9_slice.A_cfg reg_9x9_A[8:0].Q" output="mult_9x9.A"/>
                <mux input="mult_9x9_slice.B_cfg reg_9x9_B[8:0].Q" output="mult_9x9.B"/>

                <direct input="mult_9x9.OUT" output="reg_9x9_out[17:0].D"/>
                <mux input="mult_9x9.OUT reg_9x9_out[17:0].Q" output="mult_9x9_slice.OUT_cfg"/>

                <complete input="mult_9x9_slice.clk" output="reg_9x9_A[8:0].clk"/>
                <complete input="mult_9x9_slice.clk" output="reg_9x9_B[8:0].clk"/>
                <complete input="mult_9x9_slice.clk" output="reg_9x9_out[17:0].clk"/>
              </interconnect>
            </pb_type>
            <interconnect>
              <direct input="divisible_mult_18x18.A" output="mult_9x9_slice[1:0].A_cfg"/>
              <direct input="divisible_mult_18x18.B" output="mult_9x9_slice[1:0].A_cfg"/>
              <direct input="mult_9x9_slice[1:0].OUT_cfg" output="divisible_mult_18x18.OUT"/>
              <complete input="divisible_mult_18x18.clk" output="mult_9x9_slice[1:0].clk"/>
            </interconnect>
          </mode>
        </pb_type>
        <interconnect>
          <direct input="block_mult.A" output="divisible_mult_18x18[1:0].A"/>
          <direct input="block_mult.B" output="divisible_mult_18x18[1:0].B"/>
          <direct input="divisible_mult_18x18[1:0].OUT" output="block_mult.OUT"/>
          <complete input="block_mult.clk" output="divisible_mult_18x18[1:0].clk"/>
        </interconnect>
      </mode>

      <fc_in type="frac">0.15</fc_in>
      <fc_out type="frac">0.125</fc_out>

      <pinlocations pattern="custom">
        <loc side="left">a[35:0]</loc>
        <loc side="left" offset="1">b[35:0]</loc>
        <loc side="right">out[19:0]</loc>
        <loc side="right" offset="1">out[39:20]</loc>
        <loc side="right" offset="2">out[63:40]</loc>
      </pinlocations>

    </pb_type>




arch/heterogeneous_tiles.rst
--------------------------------------
.. _heterogeneous_tiles_tutorial:

Heterogeneous tiles tutorial
============================

This tutorial aims at providing information to the user on how to model sub tiles to enable *heterogeneous tiles* in VPR.

An *heterogeneous tile* is a tile that includes two or more site types that may differ in the following aspects:

- *Block types* (pb_type)
- *Fc* definition
- *Pin locations* definition
- *IO ports* definition

As a result, an *heterogeneous tile* has the possibility of having multiple block types at the same (*x*, *y*) location in the grid.
This comes with the introduction of a third spatial coordinate (sub-block) that identifies the placement of the block type within the x and y grid coordinate.

Moreover, the placer can choose and assign different locations for each block type within the same coordinates as well.

.. figure:: sub_tiles_grid.png

    Device grid, with (x, y, sub-block) coordinates. Each block can be moved by the placer in all the three spatial dimensions.

To correctly model an architecture, each :ref:`arch_tiles` requires at least one sub tile definition. This represents a default
homogeneous architecture, composed of one or many instances of the sub tile within the physical tile (the number of such sub-tiles is referred to as the *capacity*).

To enhance the expressivity of VPR architecture, additional sub tiles can be inserted alongside with the default sub tile.
This enables the definition of the *heterogeneous tiles*.

With this new capability, the device grid of a given architecture does include a new sub-block coordinate that identifies the type of sub tile used and its actual location, in case the capacity is greater than 1.

Heterogeneous tiles examples
----------------------------

Following, there are two examples to illustrate some potential use cases of the *heterogeneous tiles*, that might be of interest to the reader.

.. note:: The examples below are a simplified versions of the real architectural specification.

Sub-tiles with different pin locations
######################################

The Xilinx Series 7 Clock tile is composed of 16 BUFGCTRL sites (pg. 36 of the `7 Series FPGAs
Clocking Resources <https://www.xilinx.com/support/documentation/user_guides/ug472_7Series_Clocking.pdf>`_ guide). Even though they are equivalent regarding the ports and Fc definition, some of the sites differ in terms of pin locations, as depicted by the simplified representation of the Clock tile in :numref:`clock_tile`.

.. _clock_tile:
.. figure:: clock_tile_figure.png

    Simplified view of the Clock tile of the Xilinx Series 7 fabric.

Heterogeneous tiles come in hand to model this kind of tiles and an example is the following:

.. code-block:: XML

    <tiles>
        <tile name="BUFG_TILE">
            <sub_tile name="BUFG_SUB_TILE_0" capacity="1">
                <clock name="I0" num_pins="1"/>
                <clock name="I1" num_pins="1"/>
                <input name="CE0" num_pins="1"/>
                <input name="CE1" num_pins="1"/>
                <input name="IGNORE0" num_pins="1"/>
                <input name="IGNORE1" num_pins="1"/>
                <input name="S0" num_pins="1"/>
                <input name="S1" num_pins="1"/>
                <output name="O" num_pins="1"/>
                <fc in_type="abs" in_val="2" out_type="abs" out_val="2"/>
                <pinlocations pattern="custom">
                    <loc side="top">BUFG_SUB_TILE_0.I1 BUFG_SUB_TILE_0.I0 BUFG_SUB_TILE_0.CE0 BUFG_SUB_TILE_0.S0 BUFG_SUB_TILE_0.IGNORE1 BUFG_SUB_TILE_0.CE1 BUFG_SUB_TILE_0.IGNORE0 BUFG_SUB_TILE_0.S1</loc>
                    <loc side="right">BUFG_SUB_TILE_0.I1 BUFG_SUB_TILE_0.I0 BUFG_SUB_TILE_0.O</loc>
                </pinlocations>
                <equivalent_sites>
                  <site pb_type="BUFGCTRL" pin_mapping="direct"/>
                </equivalent_sites>
            </sub_tile>
            <sub_tile name="BUFG_SUB_TILE_1" capacity="14">
                <clock name="I0" num_pins="1"/>
                <clock name="I1" num_pins="1"/>
                <input name="CE0" num_pins="1"/>
                <input name="CE1" num_pins="1"/>
                <input name="IGNORE0" num_pins="1"/>
                <input name="IGNORE1" num_pins="1"/>
                <input name="S0" num_pins="1"/>
                <input name="S1" num_pins="1"/>
                <output name="O" num_pins="1"/>
                <fc in_type="abs" in_val="2" out_type="abs" out_val="2"/>
                <pinlocations pattern="custom">
                    <loc side="top">BUFG_SUB_TILE_1.S1 BUFG_SUB_TILE_1.I0 BUFG_SUB_TILE_1.CE1 BUFG_SUB_TILE_1.I1 BUFG_SUB_TILE_1.IGNORE1 BUFG_SUB_TILE_1.IGNORE0 BUFG_SUB_TILE_1.CE0 BUFG_SUB_TILE_1.S0</loc>
                    <loc side="right">BUFG_SUB_TILE_1.I0 BUFG_SUB_TILE_1.I1 BUFG_SUB_TILE_1.O</loc>
                </pinlocations>
                <equivalent_sites>
                  <site pb_type="BUFGCTRL" pin_mapping="direct"/>
                </equivalent_sites>
            </sub_tile>
            <sub_tile name="BUFG_SUB_TILE_2" capacity="1">
                <clock name="I0" num_pins="1"/>
                <clock name="I1" num_pins="1"/>
                <input name="CE0" num_pins="1"/>
                <input name="CE1" num_pins="1"/>
                <input name="IGNORE0" num_pins="1"/>
                <input name="IGNORE1" num_pins="1"/>
                <input name="S0" num_pins="1"/>
                <input name="S1" num_pins="1"/>
                <output name="O" num_pins="1"/>
                <fc in_type="abs" in_val="2" out_type="abs" out_val="2"/>
                <pinlocations pattern="custom">
                    <loc side="right">BUFG_SUB_TILE_2.S1 BUFG_SUB_TILE_2.I0 BUFG_SUB_TILE_2.CE1 BUFG_SUB_TILE_2.I1 BUFG_SUB_TILE_2.IGNORE1 BUFG_SUB_TILE_2.IGNORE0 BUFG_SUB_TILE_2.CE0 BUFG_SUB_TILE_2.S0</loc>
                    <loc side="left">BUFG_SUB_TILE_2.I0 BUFG_SUB_TILE_2.I1 BUFG_SUB_TILE_2.O</loc>
                </pinlocations>
                <equivalent_sites>
                  <site pb_type="BUFGCTRL" pin_mapping="direct"/>
                </equivalent_sites>
            </sub_tile>
        </tile>
    </tiles>

    <complexblocklist>
        <pb_type name="BUFGCTRL"/>
            <clock name="I0" num_pins="1"/>
            <clock name="I1" num_pins="1"/>
            <input name="CE0" num_pins="1"/>
            <input name="CE1" num_pins="1"/>
            <input name="IGNORE0" num_pins="1"/>
            <input name="IGNORE1" num_pins="1"/>
            <input name="S0" num_pins="1"/>
            <input name="S1" num_pins="1"/>
            <output name="O" num_pins="1"/>
        </pb_type>
    </complexblocklist>

The above ``BUFG_TILE`` contains three types of sub-tiles (``BUFG_SUB_TILE_0``, ``BUFG_SUB_TILE_1`` and ``BUFG_SUB_TILE_2``).

While each sub-tile type contains the same pb_type (equivalent_sites of ``BUFGCTRL``), they differ in two ways:

1. Each sub-tile has different pin locations. For example ``BUFG_SUB_TILE_0`` has the ``I1`` pins on the top side of the tile, while ``BUFG_SUB_TILE_1`` and ``BUFG_SUB_TILE_2`` have them on the right and left sides respectively.
2. Each sub-tile has a different 'capacity' (i.e. a different number of sites). ``BUFG_SUB_TILE_1`` and ``BUFG_SUB_TILE_2`` have capacity 1, while ``BUFG_SUB_TILE_1`` has capacity 14. As a result the ``BUFG_TILE`` can implement a total of 16 ``BUFGCTRL`` blocks.

Sub-tiles containing different block types
##########################################

As another example taken from the Xilinx Series 7 fabric, the HCLK_IOI tile is composed of three different block types, namely BUFIO, BUFR and IDELAYCTRL.

.. figure:: hclk_ioi.png

    Simplified view of the HCLK_IOI tile in the Xilinx Series 7 fabric.

The reader might think that it is possible to model this situation using the :ref:`arch_complex_blocks` to model this situation, with a ``<pb_type>`` containing the various blocks.

Indeed, this could be done, but, for some architectures, the placement location of a sub block is particularly relevant, hence the need of leaving this choice to the placement algorithm instead of the packer one.

Each one of these site types has different IO pins as well as pin locations.

.. code-block:: XML

    <tile name="HCLK_IOI">
        <sub_tile name="BUFIO" capacity="4">
            <clock name="I" num_pins="1"/>
            <output name="O" num_pins = "1"/>
            <equivalent_sites>
                <site pb_type="BUFIO_SITE" pin_mapping="direct"/>
            </equivalent_sites>
            <fc />
            <pinlocations />
        </sub_tile>
        <sub_tile name="BUFR" capacity="4">
            <clock name="I" num_pins="1"/>
            <input name="CE" num_pins="1"/>
            <output name="O" num_pins = "1"/>
            <equivalent_sites>
                <site pb_type="BUFR_SITE" pin_mapping="direct"/>
            </equivalent_sites>
            <fc />
            <pinlocations />
        </sub_tile>
        <sub_tile name="IDELAYCTRL" capacity="1">
            <clock name="REFCLK" num_pins="1"/>
            <output name="RDY" num_pins="1"/>
            <equivalent_sites>
                <site pb_type="IDELAYCTRL_SITE" pin_mapping="direct"/>
            </equivalent_sites>
            <fc />
            <pinlocations />
        </sub_tile>
    </tile>

Each ``HCLK_IOI`` tile contains three sub-tiles, each containing a different type of pb_type:

- the ``BUFIO`` sub-tile supports 4 instances (capacity = 4) of pb_type ``BUFIO_SITE``
- the ``BUFR`` sub-tile supports 4 instances of ``BUFR_SITE`` pb_types
- the ``IDELAYCTRL`` sub-tile supports 1 instances of the ``IDELAYCTRL_SITE``



arch/index.rst
--------------------------------------
.. _arch_tutorial:

Architecture Modeling
---------------------

This page provides information on the FPGA architecture description language used by VPR.
This page is geared towards both new and experienced users of vpr.

New users may wish to consult the conference paper that introduces the language :cite:`luu_architecture_description_lanage`.
This paper describes the motivation behind this new language as well as a short tutorial on how to use the language to describe different complex blocks of an FPGA.

New and experienced users alike should consult the detailed :ref:`arch_reference` which serves to documents every property of the language.

Multiple examples of how this language can be used to describe different types of complex blocks are provided as follows:

**Complete Architecture Description Walkthrough Examples:**

.. toctree::
   :maxdepth: 1

   classic_soft_logic
   multi_mode_ble
   configurable_memory_bus
   fracturable_multiplier_bus

**Architecture Description Examples:**

.. toctree::
   :maxdepth: 1

   fracturable_multiplier
   configurable_memory
   xilinx_virtex_6_like
   equivalent_sites
   heterogeneous_tiles

**Modeling Guides:**

.. toctree::
    :maxdepth: 1

    timing_modeling/index



arch/multi_mode_ble.rst
--------------------------------------
.. _multi_mode_logic_block_tutorial:

Multi-mode Logic Block Tutorial
===============================

This tutorial aims to introduce how to build a representative multi-mode logic block by exploiting VPR architecture description language, as well as debugging tips to guarantee each mode of a logic block is functional.

Definition
----------

Modern FPGA logic blocks are designed to operate in various modes, so as to provide best performance for different applications.
VPR offers enriched syntax to support highly flexible multi-mode logic block architecture.

:numref:`fig_frac_lut_le` shows the physical implemenation of a Fracturable Logic Element (FLE), which consists of a fracturable 6-input Look-Up Table (LUT), two Flip-flops (FFs) and routing multiplexers to select between combinational and sequential outputs.

.. _fig_frac_lut_le:

.. figure:: frac_lut_le.png
   :scale: 125%
   
   Schematic of a fracturable logic element

The FLE in :numref:`fig_frac_lut_le` can operate in two different modes: (a) dual 5-input LUT mode (see :numref:`fig_frac_lut_le_dual_lut5_mode`); and (b) single 6-input LUT mode (see :numref:`fig_frac_lut_le_lut6_mode`). Note that each operating mode does not change the physical implementation of FLE but uses part of the programmable resources.

.. _fig_frac_lut_le_dual_lut5_mode:

.. figure:: frac_lut_le_dual_lut5_mode.png
   :scale: 125%
   
   Simplified organization when the FLE in :numref:`fig_frac_lut_le` operates in dual 5-input LUT mode

.. _fig_frac_lut_le_lut6_mode:

.. figure:: frac_lut_le_lut6_mode.png
   :scale: 125%
   
   Simplified organization when the FLE in :numref:`fig_frac_lut_le` operates in 6-input LUT mode

Architecture Description
------------------------

To accurately model the operating modes of the FLE, we will use the syntax ``<pb_type>`` and ``<mode>`` in architecture description language.

.. code-block:: xml

   <!-- Multi-mode Fracturable Logic Element definition begin -->
   <pb_type name="fle" num_pb="10">
     <input name="in" num_pins="6"/>
     <output name="out" num_pins="2"/>
     <clock name="clk" num_pins="1"/>

     <!-- Dual 5-input LUT mode definition begin -->
     <mode name="n2_lut5">
       <!-- Detailed definition of the dual 5-input LUT mode -->
     </mode>     
     <!-- Dual 5-input LUT mode definition end -->

     <!-- 6-input LUT mode definition begin -->
     <mode name="n1_lut6">
       <!-- Detailed definition of the 6-input LUT mode -->
     </mode>     
     <!-- 6-input LUT mode definition end -->
   </pb_type>

In the above XML codes, we define a ``<pb_type>`` for the FLE by following the port organization in :numref:`fig_frac_lut_le`.
Under the ``<pb_type>``, we create two modes, ``n2_lut5`` and ``n1_lut6``, corresponding to the two operating modes as shown in :numref:`fig_frac_lut_le_dual_lut5_mode` and :numref:`fig_frac_lut_le_lut6_mode`.
Note that we focus on operating modes here, which are sufficient to perform architecture evaluation.

Under the dual 5-input LUT mode, we can define ``<pb_type>`` and ``<interconnect>`` to model the schematic in :numref:`fig_frac_lut_le_dual_lut5_mode`.

.. code-block:: xml

   <!-- Dual 5-input LUT mode definition begin -->
   <mode name="n2_lut5">
     <pb_type name="lut5inter" num_pb="1">
       <input name="in" num_pins="5"/>
       <output name="out" num_pins="2"/>
       <clock name="clk" num_pins="1"/>
       <!-- Define two LUT5 + FF pairs (num_pb=2) corresponding to :numref:`fig_frac_lut_le_dual_lut5_mode` -->
       <pb_type name="ble5" num_pb="2">
         <input name="in" num_pins="5"/>
         <output name="out" num_pins="1"/>
         <clock name="clk" num_pins="1"/>
         <!-- Define the LUT -->
         <pb_type name="lut5" blif_model=".names" num_pb="1" class="lut">
           <input name="in" num_pins="5" port_class="lut_in"/>
           <output name="out" num_pins="1" port_class="lut_out"/>
           <!-- LUT timing using delay matrix -->
           <!-- These are the physical delay inputs on a Stratix IV LUT but because VPR cannot do LUT rebalancing,
                      we instead take the average of these numbers to get more stable results
                 82e-12
                 173e-12
                 261e-12
                 263e-12
                 398e-12
                 -->
           <delay_matrix type="max" in_port="lut5.in" out_port="lut5.out">
             235e-12
             235e-12
             235e-12
             235e-12
             235e-12
           </delay_matrix>
         </pb_type>
         <!-- Define the flip-flop -->
         <pb_type name="ff" blif_model=".latch" num_pb="1" class="flipflop">
           <input name="D" num_pins="1" port_class="D"/>
           <output name="Q" num_pins="1" port_class="Q"/>
           <clock name="clk" num_pins="1" port_class="clock"/>
           <T_setup value="66e-12" port="ff.D" clock="clk"/>
           <T_clock_to_Q max="124e-12" port="ff.Q" clock="clk"/>
         </pb_type>
         <interconnect>
           <direct name="direct1" input="ble5.in[4:0]" output="lut5[0:0].in[4:0]"/>
           <direct name="direct2" input="lut5[0:0].out" output="ff[0:0].D">
             <!-- Advanced user option that tells CAD tool to find LUT+FF pairs in netlist -->
             <pack_pattern name="ble5" in_port="lut5[0:0].out" out_port="ff[0:0].D"/>
           </direct>
           <direct name="direct3" input="ble5.clk" output="ff[0:0].clk"/>
           <mux name="mux1" input="ff[0:0].Q lut5.out[0:0]" output="ble5.out[0:0]">
             <!-- LUT to output is faster than FF to output on a Stratix IV -->
             <delay_constant max="25e-12" in_port="lut5.out[0:0]" out_port="ble5.out[0:0]"/>
             <delay_constant max="45e-12" in_port="ff[0:0].Q" out_port="ble5.out[0:0]"/>
           </mux>
         </interconnect>
       </pb_type>
       <interconnect>
         <direct name="direct1" input="lut5inter.in" output="ble5[0:0].in"/>
         <direct name="direct2" input="lut5inter.in" output="ble5[1:1].in"/>
         <direct name="direct3" input="ble5[1:0].out" output="lut5inter.out"/>
         <complete name="complete1" input="lut5inter.clk" output="ble5[1:0].clk"/>
       </interconnect>
     </pb_type>
     <interconnect>
       <direct name="direct1" input="fle.in[4:0]" output="lut5inter.in"/>
       <direct name="direct2" input="lut5inter.out" output="fle.out"/>
       <direct name="direct3" input="fle.clk" output="lut5inter.clk"/>
     </interconnect>
   </mode>
   <!-- Dual 5-input LUT mode definition end -->

Under the 6-input LUT mode, we can define ``<pb_type>`` and ``<interconnect>`` to model the schematic in :numref:`fig_frac_lut_le_lut6_mode`.

.. code-block:: xml

   <!-- 6-LUT mode definition begin -->
   <mode name="n1_lut6" disable_packing="false">
     <!-- Define 6-LUT mode, consisting of a LUT6 + FF pair (num_pb=1) corresponding to :numref:`fig_frac_lut_le_lut6_mode`-->
     <pb_type name="ble6" num_pb="1">
       <input name="in" num_pins="6"/>
       <output name="out" num_pins="1"/>
       <clock name="clk" num_pins="1"/>
       <!-- Define LUT -->
       <pb_type name="lut6" blif_model=".names" num_pb="1" class="lut">
         <input name="in" num_pins="6" port_class="lut_in"/>
         <output name="out" num_pins="1" port_class="lut_out"/>
         <!-- LUT timing using delay matrix -->
         <!-- These are the physical delay inputs on a Stratix IV LUT but because VPR cannot do LUT rebalancing,
                  we instead take the average of these numbers to get more stable results
             82e-12
             173e-12
             261e-12
             263e-12
             398e-12
             397e-12
             -->
         <delay_matrix type="max" in_port="lut6.in" out_port="lut6.out">
           261e-12
           261e-12
           261e-12
           261e-12
           261e-12
           261e-12
         </delay_matrix>
       </pb_type>
       <!-- Define flip-flop -->
       <pb_type name="ff" blif_model=".latch" num_pb="1" class="flipflop">
         <input name="D" num_pins="1" port_class="D"/>
         <output name="Q" num_pins="1" port_class="Q"/>
         <clock name="clk" num_pins="1" port_class="clock"/>
         <T_setup value="66e-12" port="ff.D" clock="clk"/>
         <T_clock_to_Q max="124e-12" port="ff.Q" clock="clk"/>
       </pb_type>
       <interconnect>
         <direct name="direct1" input="ble6.in" output="lut6[0:0].in"/>
         <direct name="direct2" input="lut6.out" output="ff.D">
           <!-- Advanced user option that tells CAD tool to find LUT+FF pairs in netlist -->
           <pack_pattern name="ble6" in_port="lut6.out" out_port="ff.D"/>
         </direct>
         <direct name="direct3" input="ble6.clk" output="ff.clk"/>
         <mux name="mux1" input="ff.Q lut6.out" output="ble6.out">
           <!-- LUT to output is faster than FF to output on a Stratix IV -->
           <delay_constant max="25e-12" in_port="lut6.out" out_port="ble6.out"/>
           <delay_constant max="45e-12" in_port="ff.Q" out_port="ble6.out"/>
         </mux>
       </interconnect>
     </pb_type>
     <interconnect>
       <!--direct name="direct1" input="fle.in" output="ble6.in"/-->
       <direct name="direct2" input="ble6.out" output="fle.out[0:0]"/>
       <direct name="direct3" input="fle.clk" output="ble6.clk"/>
     </interconnect>
   </mode>
   <!-- 6-LUT mode definition end -->

Full example can be found at `link
<https://github.com/verilog-to-routing/vtr-verilog-to-routing/blob/master/vtr_flow/arch/timing/k6_frac_N10_40nm.xml>`_.

Validation in packer
--------------------
After finishing the architecture description, the next step is to validate that VPR can map logic to each operating mode.
Since VPR packer will exhaustively try each operating mode and finally map logic to one of it.
As long as there is an operating mode that is feasible for mapping, VPR will complete packing without errors.
However, this may shadow the problems for other operating modes.
It is entirely possible that an operating mode is not defined correctly and is always dropped by VPR during packing.
Therefore, it is necessary to validate the correctness of each operating mode.
To efficiently reach the goal, we will temporarily apply the syntax ``disable_packing`` to specific modes, so as to narrow down the search space. 

First, we can disable the dual 5-input LUT mode for packer, by changing 

.. code-block:: xml

   <mode name="n2_lut5">

to 

.. code-block:: xml

   <mode name="n2_lut5" disable_packing="true">

As a result, VPR packer will only consider the 6-input LUT mode during packing.
We can try a benchmark `mult_2x2.blif
<https://github.com/verilog-to-routing/vtr-verilog-to-routing/blob/master/vtr_flow/benchmarks/microbenchmarks/mult_2x2.blif>`_
by following the design flow tutorial :ref:`basic_design_flow_tutorial`.
If the flow-run succeed, it means that the 6-input LUT mode is being successfully used by the packer.

Then, we can enable the dual 5-input LUT mode for packer, and disable the 6-input LUT mode, by changing

.. code-block:: xml

   <mode name="n2_lut5" disable_packing="true">

   <mode name="n1_lut6">

to 

.. code-block:: xml

   <mode name="n2_lut5">

   <mode name="n1_lut6" disable_packing="true">

In this case, VPR packer will consider the dual 5-input LUT mode during packing.
We can again try the same benchmark `mult_2x2.blif
<https://github.com/verilog-to-routing/vtr-verilog-to-routing/blob/master/vtr_flow/benchmarks/microbenchmarks/mult_2x2.blif>`_
by following the design flow tutorial :ref:`basic_design_flow_tutorial`.
If the flow-run succeed, it means that the dual 5-input LUT mode is being successfully used by the packer.

Finally, after having validated that both operating modes are being successfully used by the packer, we can re-enable both operating modes by changing to 

.. code-block:: xml

   <mode name="n2_lut5">
   <mode name="n1_lut6">

Now, VPR packer will try to choose the best operating mode to use.

Tips for Debugging
------------------
When packing fails on a multi-mode logic block, the following procedures are recommended to quickly spot the bugs.

- Apply ``disable_packing`` to all the modes, except the one you suspect to be problematic.
  In the example of this tutorial, you may disable the packing for mode ``n2_lut5`` and focus on debugging mode ``n1_lut6``.

  .. code-block:: xml

   <mode name="n2_lut5" disable_packing="true">
   <mode name="n1_lut6" disable_packing="false">


- Turn on verbose output of packer ``--pack_verbosity`` (see details in :ref:`packing_options`. Recommend to use a higher verbosity number than the default value, e.g., 5.
  Consider the example blif and architecture in this tutorial, you may execute ``vpr`` with 
   
  .. code-block:: shell

     vpr k6_frac_N10_40nm.xml mult_2x2.blif --pack_verbosity 5  
  
- Packer will show detailed information about why it fails.
  For example:
  
  .. code-block:: shell

      FAILED Detailed Routing Legality
      Placed atom 'p3' (.names) at clb[0][default]/fle[4][n1_lut6]/ble6[0][default]/lut6[0][lut6]/lut[0]
      (921:cluster-external source (LB_SOURCE)-->1:'clb[0].I[1]') (1:'clb[0].I[1]'-->62:'fle[0].in[1]') (62:'fle[0].in[1]'-->123:'ble6[0].in[1]') (123:'ble6[0].in[1]'-->131:'lut6[0].in[1]') (131:'lut6[0].in[1]'-->138:'lut[0].in[1]') (138:'lut[0].in[1]'-->930:cluster-internal sink (LB_SINK accessible via architecture pins: clb[0]/fle[0]/ble6[0]/lut6[0]/lut[0].in[0], clb[0]/fle[0]/ble6[0]/lut6[0]/lut[0].in[1], clb[0]/fle[0]/ble6[0]/lut6[0]/lut[0].in[2], clb[0]/fle[0]/ble6[0]/lut6[0]/lut[0].in[3], clb[0]/fle[0]/ble6[0]/lut6[0]/lut[0].in[4], clb[0]/fle[0]/ble6[0]/lut6[0]/lut[0].in[5]))

  Which indicates that input ports of ``<pb_type name=lut6>`` in the mode ``n1_lut6`` may be dangling, and thus leads to failures in routing stage of packing. 

- You may modify the architecture description and re-run vpr until packing succeeds.

- Move on to the next mode you will debug and repeat from the first step.

The debugging tips are not only applicable to the example showed in this tutorial but rather general to any multi-mode logic block architecture. 



arch/xilinx_virtex_6_like.rst
--------------------------------------
.. _virtex_6_like_logic_slice_example:

Virtex 6 like Logic Slice Example
---------------------------------

In order to demonstrate the expressiveness of the architecture description language, we use it to describe a section of a commercial logic block.
In this example, we describe the Xilinx Virtex-6 FPGA logic slice :cite:`xilinx_virtex_6_clb`, shown in :numref:`fig_v6_slice`, as follows:

.. _fig_v6_slice:

.. figure:: v6_logic_slice.*

    Commercial FPGA logic block slice (Xilinx Virtex-6)

.. code-block:: xml

    <pb_type name="v6_lslice">

      <input name="AX" num_pins="1" equivalent="false"/>
      <input name="A" num_pins="5" equivalent="false"/>
      <input name="AI" num_pins="1" equivalent="false"/>
      <input name="BX" num_pins="1" equivalent="false"/>
      <input name="B" num_pins="5" equivalent="false"/>
      <input name="BI" num_pins="1" equivalent="false"/>
      <input name="CX" num_pins="1" equivalent="false"/>
      <input name="C" num_pins="5" equivalent="false"/>
      <input name="CI" num_pins="1" equivalent="false"/>
      <input name="DX" num_pins="1" equivalent="false"/>
      <input name="D" num_pins="5" equivalent="false"/>
      <input name="DI" num_pins="1" equivalent="false"/>
      <input name="SR" num_pins="1" equivalent="false"/>
      <input name="CIN" num_pins="1" equivalent="false"/>
      <input name="CE" num_pins="1" equivalent="false"/>

      <output name="AMUX" num_pins="1" equivalent="false"/>
      <output name="Aout" num_pins="1" equivalent="false"/>
      <output name="AQ" num_pins="1" equivalent="false"/>
      <output name="BMUX" num_pins="1" equivalent="false"/>
      <output name="Bout" num_pins="1" equivalent="false"/>
      <output name="BQ" num_pins="1" equivalent="false"/>
      <output name="CMUX" num_pins="1" equivalent="false"/>
      <output name="Cout" num_pins="1" equivalent="false"/>
      <output name="CQ" num_pins="1" equivalent="false"/>
      <output name="DMUX" num_pins="1" equivalent="false"/>
      <output name="Dout" num_pins="1" equivalent="false"/>
      <output name="DQ" num_pins="1" equivalent="false"/>
      <output name="COUT" num_pins="1" equivalent="false"/>

      <clock name="CLK"/>

      <!--
        For the purposes of this example, the Virtex-6 fracturable LUT will be specified as a primitive.
        If the architect wishes to explore the Xilinx Virtex-6 further, add more detail into this pb_type.
        Similar convention for flip-flops
      -->
      <pb_type name="fraclut" num_pb="4" blif_model=".subckt vfraclut">
        <input name="A" num_pins="5"/>
        <input name="W" num_pins="5"/>
        <input name="DI1" num_pins="1"/>
        <input name="DI2" num_pins="1"/>
        <output name="MC31" num_pins="1"/>
        <output name="O6" num_pins="1"/>
        <output name="O5" num_pins="1"/>
      </pb_type>
      <pb_type name="carry" num_pb="4" blif_model=".subckt carry">
        <!-- This is actually the carry-chain but we don't have a special way to specify chain logic yet in UTFAL
             so it needs to be specified as regular gate logic, the xor gate and the two muxes to the left of it that are shaded grey
             comprise the logic gates representing the carry logic -->
        <input name="xor" num_pins="1"/>
        <input name="cmuxxor" num_pins="1"/>
        <input name="cmux" num_pins="1"/>
        <input name="cmux_select" num_pins="1"/>
        <input name="mmux" num_pins="2"/>
        <input name="mmux_select" num_pins="1"/>
        <output name="xor_out" num_pins="1"/>
        <output name="cmux_out" num_pins="1"/>
        <output name="mmux_out" num_pins="1"/>
      </pb_type>
      <pb_type name="ff_small" num_pb="4" blif_model=".subckt vffs">
        <input name="D" num_pins="1"/>
        <input name="CE" num_pins="1"/>
        <input name="SR" num_pins="1"/>
        <output name="Q" num_pins="1"/>
        <clock name="CK" num_pins="1"/>
      </pb_type>
      <pb_type name="ff_big" num_pb="4" blif_model=".subckt vffb">
        <input name="D" num_pins="1"/>
        <input name="CE" num_pins="1"/>
        <input name="SR" num_pins="1"/>
        <output name="Q" num_pins="1"/>
        <clock name="CK" num_pins="1"/>
      </pb_type>
      <!-- TODO: Add in ability to specify constants such as gnd/vcc -->

      <interconnect>
        <direct name="fraclutA" input="{v6_lslice.A v6_lslice.B v6_lslice.C v6_lslice.D}" output="fraclut.A"/>
        <direct name="fraclutW" input="{v6_lslice.A v6_lslice.B v6_lslice.C v6_lslice.D}" output="fraclut.W"/>
        <direct name="fraclutDI2" input="{v6_lslice.AX v6_lslice.BX v6_lslice.CX v6_lslice.DX}" output="fraclut.DI2"/>
        <direct name="DfraclutDI1" input="v6_lslice.DI" output="fraclut[3].DI1"/>

        <direct name="carryO6" input="fraclut.O6" output="carry.xor"/>
        <direct name="carrymuxxor" input="carry[2:0].cmux_out" output="carry[3:1].cmuxxor"/>
        <direct name="carrymmux" input="{fraclut[3].O6 fraclut[2].O6 fraclut[2].O6 fraclut[1].O6 fraclut[1].O6 fraclut[0].O6}" output="carry[2:0].mmux"/>
        <direct name="carrymmux_select" input="{v6_lslice.AX v6_lslice.BX v6_lslice.CX}" output="carry[2:0].mmux_select"/>

        <direct name="cout" input="carry[3].mmux_out" output="v6_lslice.COUT"/>
        <direct name="ABCD" input="fraclut[3:0].O6" output="{v6_lslice.Dout v6_lslice.Cout v6_lslice.Bout v6_lslice.Aout}"/>
        <direct name="Q" input="ff_big.Q" output="{DQ CQ BQ AQ}"/>

        <mux name="ff_smallA" input="v6_lslice.AX fraclut[0].O5" output="ff_small[0].D"/>
        <mux name="ff_smallB" input="v6_lslice.BX fraclut[1].O5" output="ff_small[1].D"/>
        <mux name="ff_smallC" input="v6_lslice.CX fraclut[2].O5" output="ff_small[2].D"/>
        <mux name="ff_smallD" input="v6_lslice.DX fraclut[3].O5" output="ff_small[3].D"/>

        <mux name="ff_bigA" input="fraclut[0].O5 fraclut[0].O6 carry[0].cmux_out carry[0].mmux_out carry[0].xor_out" output="ff_big[0].D"/>
        <mux name="ff_bigB" input="fraclut[1].O5 fraclut[1].O6 carry[1].cmux_out carry[1].mmux_out carry[1].xor_out" output="ff_big[1].D"/>
        <mux name="ff_bigC" input="fraclut[2].O5 fraclut[2].O6 carry[2].cmux_out carry[2].mmux_out carry[2].xor_out" output="ff_big[2].D"/>
        <mux name="ff_bigD" input="fraclut[3].O5 fraclut[3].O6 carry[3].cmux_out carry[3].mmux_out carry[3].xor_out" output="ff_big[3].D"/>

        <mux name="AMUX" input="fraclut[0].O5 fraclut[0].O6 carry[0].cmux_out carry[0].mmux_out carry[0].xor_out ff_small[0].Q" output="AMUX"/>
        <mux name="BMUX" input="fraclut[1].O5 fraclut[1].O6 carry[1].cmux_out carry[1].mmux_out carry[1].xor_out ff_small[1].Q" output="BMUX"/>
        <mux name="CMUX" input="fraclut[2].O5 fraclut[2].O6 carry[2].cmux_out carry[2].mmux_out carry[2].xor_out ff_small[2].Q" output="CMUX"/>
        <mux name="DMUX" input="fraclut[3].O5 fraclut[3].O6 carry[3].cmux_out carry[3].mmux_out carry[3].xor_out ff_small[3].Q" output="DMUX"/>

        <mux name="CfraclutDI1" input="v6_lslice.CI v6_lslice.DI fraclut[3].MC31" output="fraclut[2].DI1"/>
        <mux name="BfraclutDI1" input="v6_lslice.BI v6_lslice.DI fraclut[2].MC31" output="fraclut[1].DI1"/>
        <mux name="AfraclutDI1" input="v6_lslice.AI v6_lslice.BI v6_lslice.DI fraclut[2].MC31 fraclut[1].MC31" output="fraclut[0].DI1"/>

        <mux name="carrymuxxorA" input="v6_lslice.AX v6_lslice.CIN" output="carry[0].muxxor"/>
        <mux name="carrymuxA" input="v6_lslice.AX fraclut[0].O5" output="carry[0].cmux"/>
        <mux name="carrymuxB" input="v6_lslice.BX fraclut[1].O5" output="carry[1].cmux"/>
        <mux name="carrymuxC" input="v6_lslice.CX fraclut[2].O5" output="carry[2].cmux"/>
        <mux name="carrymuxD" input="v6_lslice.DX fraclut[3].O5" output="carry[3].cmux"/>


        <complete name="clock" input="v6_lslice.CLK" output="{ff_small.CK ff_big.CK}"/>
        <complete name="ce" input="v6_lslice.CE" output="{ff_small.CE ff_big.CE}"/>
        <complete name="SR" input="v6_lslice.SR" output="{ff_small.SR ff_big.SR}"/>
      </interconnect>
    </pb_type>



timing_modeling/index.rst
--------------------------------------
.. _arch_model_timing_tutorial:

Primitive Block Timing Modeling Tutorial
----------------------------------------
To accurately model an FPGA, the architect needs to specify the timing characteristics of the FPGA's primitives blocks.
This involves two key steps:

 #. Specifying the logical timing characteristics of a primitive including:

     * whether primitive pins are sequential or combinational, and
     * what the timing dependencies are between the pins.

 #. Specifying the physical delay values

These two steps separate the logical timing characteristics of a primitive, from the physically dependant delays.
This enables a single logical netlist primitive type (e.g. Flip-Flop) to be mapped into different physical locations with different timing characteristics.

The :ref:`FPGA architecture description <fpga_architecture_description>` describes the logical timing characteristics in the :ref:`models section <arch_models>`, while the physical timing information is specified on ``pb_types`` within :ref:`complex block <arch_complex_blocks>`.

The following sections illustrate some common block timing modeling approaches.


Combinational block
~~~~~~~~~~~~~~~~~~~
A typical combinational block is a full adder,

.. figure:: fa.*
    :width: 50%

    Full Adder

where ``a``, ``b`` and ``cin`` are combinational inputs, and ``sum`` and ``cout`` are combinational outputs.

We can model these timing dependencies on the model with the ``combinational_sink_ports``, which specifies the output ports which are dependant on an input port:

.. code-block:: xml

      <model name="adder">
        <input_ports>
          <port name="a" combinational_sink_ports="sum cout"/>
          <port name="b" combinational_sink_ports="sum cout"/>
          <port name="cin" combinational_sink_ports="sum cout"/>
        </input_ports>
        <output_ports>
          <port name="sum"/>
          <port name="cout"/>
        </output_ports>
      </model>

The physical timing delays are specified on any ``pb_type`` instances of the adder model.
For example:

.. code-block:: xml

    <pb_type name="adder" blif_model=".subckt adder" num_pb="1">
      <input name="a" num_pins="1"/>
      <input name="b" num_pins="1"/>
      <input name="cin" num_pins="1"/>
      <output name="cout" num_pins="1"/>
      <output name="sum" num_pins="1"/>

      <delay_constant max="300e-12" in_port="adder.a" out_port="adder.sum"/>
      <delay_constant max="300e-12" in_port="adder.b" out_port="adder.sum"/>
      <delay_constant max="300e-12" in_port="adder.cin" out_port="adder.sum"/>
      <delay_constant max="300e-12" in_port="adder.a" out_port="adder.cout"/>
      <delay_constant max="300e-12" in_port="adder.b" out_port="adder.cout"/>
      <delay_constant max="10e-12" in_port="adder.cin" out_port="adder.cout"/>
    </pb_type>

specifies that all the edges of 300ps delays, except to ``cin`` to ``cout`` edge which has a delay of 10ps.

.. _dff_timing_modeling:

Sequential block (no internal paths)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


A typical sequential block is a D-Flip-Flop (DFF).
DFFs have no internal timing paths between their input and output ports.

.. note:: If you are using BLIF's ``.latch`` directive to represent DFFs there is no need to explicitly provide a ``<model>`` definition, as it is supported by default.

.. figure:: dff.*
    :width: 50%

    DFF

Sequential model ports are specified by providing the ``clock="<name>"`` attribute, where ``<name>`` is the name of the associated clock ports.
The assoicated clock port must have ``is_clock="1"`` specified to indicate it is a clock.

.. code-block:: xml

      <model name="dff">
        <input_ports>
          <port name="d" clock="clk"/>
          <port name="clk" is_clock="1"/>
        </input_ports>
        <output_ports>
          <port name="q" clock="clk"/>
        </output_ports>
      </model>

The physical timing delays are specified on any ``pb_type`` instances of the model.
In the example below the setup-time of the input is specified as 66ps, while the clock-to-q delay of the output is set to 124ps.

.. code-block:: xml

    <pb_type name="ff" blif_model=".subckt dff" num_pb="1">
      <input name="D" num_pins="1"/>
      <output name="Q" num_pins="1"/>
      <clock name="clk" num_pins="1"/>

      <T_setup value="66e-12" port="ff.D" clock="clk"/>
      <T_clock_to_Q max="124e-12" port="ff.Q" clock="clk"/>
    </pb_type>


.. _mixed_sp_ram_timing_modeling:

Mixed Sequential/Combinational Block
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
It is possible to define a block with some sequential ports and some combinational ports.

In the example below, the ``single_port_ram_mixed`` has sequential input ports: ``we``, ``addr`` and ``data`` (which are controlled by ``clk``).

.. figure:: mixed_sp_ram.*
    :width: 75%

    Mixed sequential/combinational single port ram

However the output port (``out``) is a combinational output, connected internally to the ``we``, ``addr`` and ``data`` input registers.

.. code-block:: xml

      <model name="single_port_ram_mixed">
        <input_ports>
          <port name="we" clock="clk" combinational_sink_ports="out"/>
          <port name="addr" clock="clk" combinational_sink_ports="out"/>
          <port name="data" clock="clk" combinational_sink_ports="out"/>
          <port name="clk" is_clock="1"/>
        </input_ports>
        <output_ports>
          <port name="out"/>
        </output_ports>
      </model>


In the ``pb_type`` we define the external setup time of the input registers (50ps) as we did for :ref:`dff_timing_modeling`.
However, we also specify the following additional timing information:

 * The internal clock-to-q delay of the input registers (200ps)
 * The combinational delay from the input registers to the ``out`` port (800ps)

.. code-block:: xml

    <pb_type name="mem_sp" blif_model=".subckt single_port_ram_mixed" num_pb="1">
      <input name="addr" num_pins="9"/>
      <input name="data" num_pins="64"/>
      <input name="we" num_pins="1"/>
      <output name="out" num_pins="64"/>
      <clock name="clk" num_pins="1"/>

      <!-- External input register timing -->
      <T_setup value="50e-12" port="mem_sp.addr" clock="clk"/>
      <T_setup value="50e-12" port="mem_sp.data" clock="clk"/>
      <T_setup value="50e-12" port="mem_sp.we" clock="clk"/>

      <!-- Internal input register timing -->
      <T_clock_to_Q max="200e-12" port="mem_sp.addr" clock="clk"/>
      <T_clock_to_Q max="200e-12" port="mem_sp.data" clock="clk"/>
      <T_clock_to_Q max="200e-12" port="mem_sp.we" clock="clk"/>

      <!-- Internal combinational delay -->
      <delay_constant max="800e-12" in_port="mem_sp.addr" out_port="mem_sp.out"/>
      <delay_constant max="800e-12" in_port="mem_sp.data" out_port="mem_sp.out"/>
      <delay_constant max="800e-12" in_port="mem_sp.we" out_port="mem_sp.out"/>
    </pb_type>

.. _seq_sp_ram_timing_modeling:

Sequential block (with internal paths)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Some primitives represent more complex architecture primitives, which have timing paths contained completely within the block.

The model below specifies a sequential single-port RAM.
The ports ``we``, ``addr``, and ``data`` are sequential inputs, while the port ``out`` is a sequential output.
``clk`` is the common clock.

.. figure:: seq_sp_ram.*
    :width: 75%

    Sequential single port ram

.. code-block:: xml

      <model name="single_port_ram_seq">
        <input_ports>
          <port name="we" clock="clk" combinational_sink_ports="out"/>
          <port name="addr" clock="clk" combinational_sink_ports="out"/>
          <port name="data" clock="clk" combinational_sink_ports="out"/>
          <port name="clk" is_clock="1"/>
        </input_ports>
        <output_ports>
          <port name="out" clock="clk"/>
        </output_ports>
      </model>


Similarly to :ref:`mixed_sp_ram_timing_modeling` the ``pb_type`` defines the input register timing:

 * external input register setup time (50ps)
 * internal input register clock-to-q time (200ps)

Since the output port ``out`` is sequential we also define the:

 * internal *output* register setup time (60ps)
 * external *output* register clock-to-q time (300ps)

The combinational delay between the input and output registers is set to 740ps.

Note the internal path from the input to output registers can limit the maximum operating frequency.
In this case the internal path delay is 1ns (200ps + 740ps + 60ps) limiting the maximum frequency to 1 GHz.

.. code-block:: xml

    <pb_type name="mem_sp" blif_model=".subckt single_port_ram_seq" num_pb="1">
      <input name="addr" num_pins="9"/>
      <input name="data" num_pins="64"/>
      <input name="we" num_pins="1"/>
      <output name="out" num_pins="64"/>
      <clock name="clk" num_pins="1"/>

      <!-- External input register timing -->
      <T_setup value="50e-12" port="mem_sp.addr" clock="clk"/>
      <T_setup value="50e-12" port="mem_sp.data" clock="clk"/>
      <T_setup value="50e-12" port="mem_sp.we" clock="clk"/>

      <!-- Internal input register timing -->
      <T_clock_to_Q max="200e-12" port="mem_sp.addr" clock="clk"/>
      <T_clock_to_Q max="200e-12" port="mem_sp.data" clock="clk"/>
      <T_clock_to_Q max="200e-12" port="mem_sp.we" clock="clk"/>

      <!-- Internal combinational delay -->
      <delay_constant max="740e-12" in_port="mem_sp.addr" out_port="mem_sp.out"/>
      <delay_constant max="740e-12" in_port="mem_sp.data" out_port="mem_sp.out"/>
      <delay_constant max="740e-12" in_port="mem_sp.we" out_port="mem_sp.out"/>

      <!-- Internal output register timing -->
      <T_setup value="60e-12" port="mem_sp.out" clock="clk"/>

      <!-- External output register timing -->
      <T_clock_to_Q max="300e-12" port="mem_sp.out" clock="clk"/>
    </pb_type>

.. _seq_sp_ram_comb_inputs_timing_modeling:

Sequential block (with internal paths and combinational input)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
A primitive may have a mix of sequential and combinational inputs.

The model below specifies a mostly sequential single-port RAM.
The ports ``addr``, and ``data`` are sequential inputs, while the port ``we`` is a combinational input.
The port ``out`` is a sequential output.
``clk`` is the common clock.

.. figure:: seq_comb_sp_ram.*
    :width: 75%

    Sequential single port ram with a combinational input

.. code-block:: xml
    :emphasize-lines: 3

      <model name="single_port_ram_seq_comb">
        <input_ports>
          <port name="we" combinational_sink_ports="out"/>
          <port name="addr" clock="clk" combinational_sink_ports="out"/>
          <port name="data" clock="clk" combinational_sink_ports="out"/>
          <port name="clk" is_clock="1"/>
        </input_ports>
        <output_ports>
          <port name="out" clock="clk"/>
        </output_ports>
      </model>


We use register delays similar to :ref:`seq_sp_ram_timing_modeling`.
However we also specify the purely combinational delay between the combinational ``we`` input and sequential output ``out`` (800ps).
Note that the setup time of the output register still effects the ``we`` to ``out`` path for an effective delay of 860ps.

.. code-block:: xml
    :emphasize-lines: 17

    <pb_type name="mem_sp" blif_model=".subckt single_port_ram_seq_comb" num_pb="1">
      <input name="addr" num_pins="9"/>
      <input name="data" num_pins="64"/>
      <input name="we" num_pins="1"/>
      <output name="out" num_pins="64"/>
      <clock name="clk" num_pins="1"/>

      <!-- External input register timing -->
      <T_setup value="50e-12" port="mem_sp.addr" clock="clk"/>
      <T_setup value="50e-12" port="mem_sp.data" clock="clk"/>

      <!-- Internal input register timing -->
      <T_clock_to_Q max="200e-12" port="mem_sp.addr" clock="clk"/>
      <T_clock_to_Q max="200e-12" port="mem_sp.data" clock="clk"/>

      <!-- External combinational delay -->
      <delay_constant max="800e-12" in_port="mem_sp.we" out_port="mem_sp.out"/>

      <!-- Internal combinational delay -->
      <delay_constant max="740e-12" in_port="mem_sp.addr" out_port="mem_sp.out"/>
      <delay_constant max="740e-12" in_port="mem_sp.data" out_port="mem_sp.out"/>

      <!-- Internal output register timing -->
      <T_setup value="60e-12" port="mem_sp.out" clock="clk"/>

      <!-- External output register timing -->
      <T_clock_to_Q max="300e-12" port="mem_sp.out" clock="clk"/>
    </pb_type>


.. _multiclock_dp_ram_timing_modeling:

Multi-clock Sequential block (with internal paths)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
It is also possible for a sequential primitive to have multiple clocks.

The following model represents a multi-clock simple dual-port sequential RAM with:

 * one write port (``addr1`` and ``data1``, ``we1``) controlled by ``clk1``, and
 * one read port (``addr2`` and ``data2``) controlled by ``clk2``.

.. figure:: multiclock_dp_ram.*
    :width: 75%

    Multi-clock sequential simple dual port ram

.. code-block:: xml

      <model name="multiclock_dual_port_ram">
        <input_ports>
          <!-- Write Port -->
          <port name="we1" clock="clk1" combinational_sink_ports="data2"/>
          <port name="addr1" clock="clk1" combinational_sink_ports="data2"/>
          <port name="data1" clock="clk1" combinational_sink_ports="data2"/>
          <port name="clk1" is_clock="1"/>

          <!-- Read Port -->
          <port name="addr2" clock="clk2" combinational_sink_ports="data2"/>
          <port name="clk2" is_clock="1"/>
        </input_ports>
        <output_ports>
          <!-- Read Port -->
          <port name="data2" clock="clk2" combinational_sink_ports="data2"/>
        </output_ports>
      </model>

On the ``pb_type`` the input and output register timing is defined similarly to :ref:`seq_sp_ram_timing_modeling`, except multiple clocks are used.

.. code-block:: xml

    <pb_type name="mem_dp" blif_model=".subckt multiclock_dual_port_ram" num_pb="1">
      <input name="addr1" num_pins="9"/>
      <input name="data1" num_pins="64"/>
      <input name="we1" num_pins="1"/>
      <input name="addr2" num_pins="9"/>
      <output name="data2" num_pins="64"/>
      <clock name="clk1" num_pins="1"/>
      <clock name="clk2" num_pins="1"/>

      <!-- External input register timing -->
      <T_setup value="50e-12" port="mem_dp.addr1" clock="clk1"/>
      <T_setup value="50e-12" port="mem_dp.data1" clock="clk1"/>
      <T_setup value="50e-12" port="mem_dp.we1" clock="clk1"/>
      <T_setup value="50e-12" port="mem_dp.addr2" clock="clk2"/>

      <!-- Internal input register timing -->
      <T_clock_to_Q max="200e-12" port="mem_dp.addr1" clock="clk1"/>
      <T_clock_to_Q max="200e-12" port="mem_dp.data1" clock="clk1"/>
      <T_clock_to_Q max="200e-12" port="mem_dp.we1" clock="clk1"/>
      <T_clock_to_Q max="200e-12" port="mem_dp.addr2" clock="clk2"/>

      <!-- Internal combinational delay -->
      <delay_constant max="740e-12" in_port="mem_dp.addr1" out_port="mem_dp.data2"/>
      <delay_constant max="740e-12" in_port="mem_dp.data1" out_port="mem_dp.data2"/>
      <delay_constant max="740e-12" in_port="mem_dp.we1" out_port="mem_dp.data2"/>
      <delay_constant max="740e-12" in_port="mem_dp.addr2" out_port="mem_dp.data2"/>

      <!-- Internal output register timing -->
      <T_setup value="60e-12" port="mem_dp.data2" clock="clk2"/>

      <!-- External output register timing -->
      <T_clock_to_Q max="300e-12" port="mem_dp.data2" clock="clk2"/>
    </pb_type>

.. _clock_generator_timing_modeling:

Clock Generators
~~~~~~~~~~~~~~~~
Some blocks (such as PLLs) generate clocks on-chip.
To ensure that these generated clocks are identified as clock sources, the associated model output port should be marked with ``is_clock="1"``.


As an example consider the following simple PLL model:

.. code-block:: xml

    <model name="simple_pll">
      <input_ports>
        <port name="in_clock" is_clock="1"/>
      </input_ports>
      <output_ports>
        <port name="out_clock" is_clock="1"/>
      </output_ports>
    </model>

The port named ``in_clock`` is specified as a clock sink, since it is an input port with ``is_clock="1"``  set.

The port named ``out_clock`` is specified as a clock generator, since it is an *output* port with ``is_clock="1"`` set.

.. note:: Clock generators should not be the combinational sinks of primitive input ports.

Consider the following example netlist:

.. code-block:: none

    .subckt simple_pll \
        in_clock=clk \
        out_clock=clk_pll

Since we have specified that ``simple_pll.out_clock`` is a clock generator (see above), the user must specify what the clock relationship is between the input and output clocks.
This information must be either specified in the SDC file (if no SDC file is specified :ref:`VPR's default timing constraints <default_timing_constraints>` will be used instead).

.. note:: VPR has no way of determining what the relationship is between the clocks of a black-box primitive.

Consider the case where the ``simple_pll`` above creates an output clock which is 2 times the frequency of the input clock.
If the input clock period was 10ns then the SDC file would look like:

.. code-block:: tcl

    create_clock clk -period 10
    create_clock clk_pll -period 5                      #Twice the frequency of clk

It is also possible to specify in SDC that there is a phase shift between the two clocks:

.. code-block:: tcl

    create_clock clk -waveform {0 5} -period 10         #Equivalent to 'create_clock clk -period 10'
    create_clock clk_pll -waveform {0.2 2.7} -period 5  #Twice the frequency of clk with a 0.2ns phase shift


.. _clock_buffers_timing_modeling:

Clock Buffers & Muxes
~~~~~~~~~~~~~~~~~~~~~
Some architectures contain special primitives for buffering or controling clocks.
VTR supports modelling these using the ``is_clock`` attritube on the model to differentiate between 'data' and 'clock' signals, allowing users to control how clocks are traced through these primitives.

When VPR traces through the netlist it will propagate clocks from clock inputs to the downstream combinationally connected pins.

Clock Buffers/Gates
^^^^^^^^^^^^^^^^^^^
Consider the following black-box clock buffer with an enable:

.. code-block:: none

    .subckt clkbufce \
        in=clk3 \
        enable=clk3_enable \
        out=clk3_buf

We wish to have VPR understand that the ``in`` port of the ``clkbufce`` connects to the ``out`` port, and that as a result the nets ``clk3`` and ``clk3_buf`` are equivalent.

This is accomplished by tagging the ``in`` port as a clock (``is_clock="1"``), and combinationally connecting it to the ``out`` port (``combinational_sink_ports="out"``):

.. code-block:: xml

    <model name="clkbufce">
      <input_ports>
        <port name="in" combinational_sink_ports="out" is_clock="1"/>
        <port name="enable" combinational_sink_ports="out"/>
      </input_ports>
      <output_ports>
        <port name="out"/>
      </output_ports>
    </model>

With the corresponding pb_type:

.. code-block:: xml

    <pb_type name="clkbufce" blif_model="clkbufce" num_pb="1">
      <clock name="in" num_pins="1"/>
      <input name="enable" num_pins="1"/>
      <output name="out" num_pins="1"/>
      <delay_constant max="10e-12" in_port="clkbufce.in" out_port="clkbufce.out"/>
      <delay_constant max="5e-12" in_port="clkbufce.enable" out_port="clkbufce.out"/>
    </pb_type>

Notably, although the ``enable`` port is combinationally connected to the ``out`` port it will not be considered as a potential clock since it is not marked with ``is_clock="1"``.

Clock Muxes
^^^^^^^^^^^
Another common clock control block is a clock mux, which selects from one of several potential clocks.

For instance, consider:

.. code-block:: none

    .subckt clkmux \
        clk1=clka \
        clk2=clkb \
        sel=select \
        clk_out=clk_downstream

which selects one of two input clocks (``clk1`` and ``clk2``) to be passed through to (``clk_out``), controlled on the value of ``sel``.

This could be modelled as:

.. code-block:: xml

    <model name="clkmux">
      <input_ports>
        <port name="clk1" combinational_sink_ports="clk_out" is_clock="1"/>
        <port name="clk2" combinational_sink_ports="clk_out" is_clock="1"/>
        <port name="sel" combinational_sink_ports="clk_out"/>
      </input_ports>
      <output_ports>
        <port name="clk_out"/>
      </output_ports>
    </model>

    <pb_type name="clkmux" blif_model="clkmux" num_pb="1">
      <clock name="clk1" num_pins="1"/>
      <clock name="clk2" num_pins="1"/>
      <input name="sel" num_pins="1"/>
      <output name="clk_out" num_pins="1"/>
      <delay_constant max="10e-12" in_port="clkmux.clk1" out_port="clkmux.clk_out"/>
      <delay_constant max="10e-12" in_port="clkmux.clk2" out_port="clkmux.clk_out"/>
      <delay_constant max="20e-12" in_port="clkmux.sel" out_port="clkmux.clk_out"/>
    </pb_type>
  
where both input clock ports ``clk1`` and ``clk2`` are tagged with ``is_clock="1"`` and combinationally connected to the ``clk_out`` port.
As a result both nets ``clka`` and ``clkb`` in the netlist would be identified as independent clocks feeding ``clk_downstream``.

.. note::

    Clock propagation is driven by netlist connectivity so if one of the input clock ports (e.g. ``clk1``) was disconnected in the netlist no associated clock would be created/considered.

Clock Mux Timing Constraints
""""""""""""""""""""""""""""

For the clock mux example above, if the user specified the following :ref:`SDC timing constraints <sdc_commands>`:

.. code-block:: tcl
    
    create_clock -period 3 clka
    create_clock -period 2 clkb

VPR would propagate both ``clka`` and ``clkb`` through the clock mux.
Therefore the logic connected to ``clk_downstream`` would be analyzed for both the ``clka`` and ``clkb`` constraints.

Most likely (unless ``clka`` and ``clkb`` are used elswhere) the user should additionally specify:

.. code-block:: tcl
   
    set_clock_groups -exclusive -group clka -group clkb

Which avoids analyzing paths between the two clocks (i.e. ``clka`` -> ``clkb`` and ``clkb`` -> ``clka``) which are not physically realizable.
The muxing logic means only one clock can drive ``clk_downstream`` at any point in time (i.e. the mux enforces that ``clka`` and ``clkb`` are mutually exclusive).
This is the behaviour of :ref:`VPR's default timing constraints <default_timing_constraints>`.



flow/basic_flow.rst
--------------------------------------
.. _basic_design_flow_tutorial:

Basic Design Flow Tutorial
==========================

The following steps show you to run the VTR design flow to map a sample circuit to an FPGA architecture containing embedded memories and multipliers:

#.  From the :term:`$VTR_ROOT`  , move to the ``vtr_flow/tasks/regression_tests/vtr_reg_basic`` directory, and run:

    .. code-block:: shell
    
        ../../../scripts/run_vtr_task.py basic_no_timing
        
    or:
 
     .. code-block:: shell

        $VTR_ROOT/vtr_flow/scripts/run_vtr_task.py basic_no_timing 
        
    The subdirectory ``regression_tests/vtr_reg_basic`` contains tests that are to be run before each commit. They check for basic functionality to make sure nothing was extremely out of order. This command runs the VTR flow on a set of circuits and a single architecture. 
    The files generated from the run are stored in ``basic_no_timing/run[#]`` where ``[#]`` is the number of runs you have done.
    If this is your first time running the flow, the results will be stored in basic_no_timing/run001. The command parses out the information of the VTR run and outputs the results in a text file called ``run[#]/parse_results.txt``.

    More info on how to run the flow on multiple circuits and architectures along with different options later.
    Before that, we need to ensure that the run that you have done works.

#.  The basic_no_timing comes with golden results that you can use to check for correctness.
    To do this check, enter the following command:

    .. code-block:: shell

        ../../../scripts/python_libs/vtr/parse_vtr_task.py -check_golden basic_no_timing

    It should return: ``basic_no_timing...[Pass]``

    .. note::

        Due to the nature of the algorithms employed, the measurements that you get may not match exactly with the golden measurements.
        We included margins in our scripts to account for that noise during the check.
        We also included runtime estimates based on our machine.
        The actual runtimes that you get may differ dramatically from these values.

#.  To see precisely which see circuits, architecture, and CAD flow was employed by the run, look at ``vtr_flow/tasks/regression_tests/vtr_reg_basic/config.txt``.
    Inside this directory, the ``config.txt`` file contains the circuits and architecture file employed in the run.

    Some also contain a ``golden_results.txt`` file that is used by the scripts to check for correctness.

    The ``$VTR_ROOT/vtr_flow/scripts/run_vtr_flow.py`` script describes the CAD flow employed in the test.
    You can modify the flow by editing this script.

    At this point, feel free to run any of the tasks with the prefix ``vtr_reg`` 
    These are regression tests included with the flow that test various combinations of flows, architectures, and benchmarks.         Refer to the ``README`` for a description what each task aims to test. 

#.  For more information on how the vtr_flow infrastructure works (and how to add the tests that you want to do to this infrastructure) see :ref:`vtr_tasks`.



flow/index.rst
--------------------------------------
.. _flow_tutorials:

Design Flow Tutorials
=====================

These tutorials describe how to run the VTR design flow.

.. toctree::
    :maxdepth: 2

    basic_flow



timing_simulation/index.rst
--------------------------------------
.. _timing_simulation_tutorial:

Post-Implementation Timing Simulation
-------------------------------------

.. _fig_timing_simulation:

.. figure:: timing_simulation.*

    Timing simulation waveform for ``stereovision3``

This tutorial describes how to simulate a circuit which has been implemented by :ref:`VPR` with back-annotated timing delays.

Back-annotated timing simulation is useful for a variety of reasons:
 * Checking that the circuit logic is correctly implemented
 * Checking that the circuit behaves correctly at speed with realistic delays
 * Generating VCD (Value Change Dump) files with realistic delays (e.g. for power estimation)


Generating the Post-Implementation Netlist
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
For the purposes of this tutorial we will be using the ``stereovision3`` :ref:`benchmark <benchmarks>`, and will target the ``k6_N10_40nm`` architecture.

First lets create a directory to work in:

.. code-block:: console

    $ mkdir timing_sim_tut
    $ cd timing_sim_tut

Next we'll copy over the ``stereovision3`` benchmark netlist in BLIF format and the FPGA architecture description:

.. code-block:: console

    $ cp $VTR_ROOT/vtr_flow/benchmarks/vtr_benchmarks_blif/stereovision3.blif .
    $ cp $VTR_ROOT/vtr_flow/arch/timing/k6_N10_40nm.xml .

.. note:: Replace :term:`$VTR_ROOT` with the root directory of the VTR source tree

Now we can run VPR to implement the circuit onto the ``k6_N10_40nm`` architecture.
We also need to provide the :option:`vpr --gen_post_synthesis_netlist` option to generate the post-implementation netlist and dump the timing information in Standard Delay Format (SDF)::

    $ vpr k6_N10_40nm.xml stereovision3.blif --gen_post_synthesis_netlist on

Once VPR has completed we should see the generated verilog netlist and SDF:

.. code-block:: console

    $ ls *.v *.sdf
    sv_chip3_hierarchy_no_mem_post_synthesis.sdf  sv_chip3_hierarchy_no_mem_post_synthesis.v


Inspecting the Post-Implementation Netlist
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Lets take a quick look at the generated files.

First is a snippet of the verilog netlist:

.. code-block:: verilog
    :caption: Verilog netlist snippet
    :name: post_imp_verilog
    :emphasize-lines: 1,8,22

    fpga_interconnect \routing_segment_lut_n616_output_0_0_to_lut_n497_input_0_4  (
        .datain(\lut_n616_output_0_0 ),
        .dataout(\lut_n497_input_0_4 )
    );


    //Cell instances
    LUT_K #(
        .K(6),
        .LUT_MASK(64'b0000000000000000000000000000000000100001001000100000000100000010)
    ) \lut_n452  (
        .in({
            1'b0,
            \lut_n452_input_0_4 ,
            \lut_n452_input_0_3 ,
            \lut_n452_input_0_2 ,
            1'b0,
            \lut_n452_input_0_0 }),
        .out(\lut_n452_output_0_0 )
    );

    DFF #(
        .INITIAL_VALUE(1'b0)
    ) \latch_top^FF_NODE~387  (
        .D(\latch_top^FF_NODE~387_input_0_0 ),
        .Q(\latch_top^FF_NODE~387_output_0_0 ),
        .clock(\latch_top^FF_NODE~387_clock_0_0 )
    );

Here we see three primitives instantiated:

* ``fpga_interconnect`` represent connections between netlist primitives
* ``LUT_K`` represent look-up tables (LUTs) (corresponding to ``.names`` in the BLIF netlist). Two parameters define the LUTs functionality:

     * ``K`` the number of inputs, and
     * ``LUT_MASK`` which defines the logic function.

* ``DFF`` represents a D-Flip-Flop (corresponding to ``.latch`` in the BLIF netlist).

    * The ``INITIAL_VALUE`` parameter defines the Flip-Flop's initial state.

Different circuits may produce other types of netlist primitives corresponding to hardened primitive blocks in the FPGA such as adders, multipliers and single or dual port RAM blocks.

.. note:: The different primitives produced by VPR are defined in ``$VTR_ROOT/vtr_flow/primitives.v``


Lets now take a look at the Standard Delay Fromat (SDF) file:

.. code-block:: none
    :linenos:
    :caption: SDF snippet
    :name: post_imp_sdf_
    :emphasize-lines: 2-3,12-13,25-26

    (CELL
        (CELLTYPE "fpga_interconnect")
        (INSTANCE routing_segment_lut_n616_output_0_0_to_lut_n497_input_0_4)
        (DELAY
            (ABSOLUTE
                (IOPATH datain dataout (312.648:312.648:312.648) (312.648:312.648:312.648))
            )
        )
    )

    (CELL
        (CELLTYPE "LUT_K")
        (INSTANCE lut_n452)
        (DELAY
            (ABSOLUTE
                (IOPATH in[0] out (261:261:261) (261:261:261))
                (IOPATH in[2] out (261:261:261) (261:261:261))
                (IOPATH in[3] out (261:261:261) (261:261:261))
                (IOPATH in[4] out (261:261:261) (261:261:261))
            )
        )
    )

    (CELL
        (CELLTYPE "DFF")
        (INSTANCE latch_top\^FF_NODE\~387)
        (DELAY
            (ABSOLUTE
                (IOPATH (posedge clock) Q (124:124:124) (124:124:124))
            )
        )
        (TIMINGCHECK
            (SETUP D (posedge clock) (66:66:66))
        )
    )

The SDF defines all the delays in the circuit using the delays calculated by VPR's STA engine from the architecture file we provided.

Here we see the timing description of the cells in :numref:`post_imp_verilog`.

In this case the routing segment ``routing_segment_lut_n616_output_0_0_to_lut_n497_input_0_4`` has a delay of 312.648 ps, while the LUT ``lut_n452`` has a delay of 261 ps from each input to the output.
The DFF ``latch_top\^FF_NODE\~387`` has a clock-to-q delay of 124 ps and a setup time of 66ps.

Creating a Test Bench
~~~~~~~~~~~~~~~~~~~~~
In order to simulate a benchmark we need a test bench which will stimulate our circuit (the Device-Under-Test or DUT).

An example test bench which will randomly perturb the inputs is shown below:

.. code-block:: systemverilog
    :linenos:
    :caption: The test bench ``tb.sv``.
    :emphasize-lines: 69,72,75-76

    `timescale 1ps/1ps
    module tb();

    localparam CLOCK_PERIOD = 8000;
    localparam CLOCK_DELAY = CLOCK_PERIOD / 2;

    //Simulation clock
    logic sim_clk;

    //DUT inputs
    logic \top^tm3_clk_v0 ;
    logic \top^tm3_clk_v2 ;
    logic \top^tm3_vidin_llc ;
    logic \top^tm3_vidin_vs ;
    logic \top^tm3_vidin_href ;
    logic \top^tm3_vidin_cref ;
    logic \top^tm3_vidin_rts0 ;
    logic \top^tm3_vidin_vpo~0 ;
    logic \top^tm3_vidin_vpo~1 ;
    logic \top^tm3_vidin_vpo~2 ;
    logic \top^tm3_vidin_vpo~3 ;
    logic \top^tm3_vidin_vpo~4 ;
    logic \top^tm3_vidin_vpo~5 ;
    logic \top^tm3_vidin_vpo~6 ;
    logic \top^tm3_vidin_vpo~7 ;
    logic \top^tm3_vidin_vpo~8 ;
    logic \top^tm3_vidin_vpo~9 ;
    logic \top^tm3_vidin_vpo~10 ;
    logic \top^tm3_vidin_vpo~11 ;
    logic \top^tm3_vidin_vpo~12 ;
    logic \top^tm3_vidin_vpo~13 ;
    logic \top^tm3_vidin_vpo~14 ;
    logic \top^tm3_vidin_vpo~15 ;

    //DUT outputs
    logic \top^tm3_vidin_sda ;
    logic \top^tm3_vidin_scl ;
    logic \top^vidin_new_data ;
    logic \top^vidin_rgb_reg~0 ;
    logic \top^vidin_rgb_reg~1 ;
    logic \top^vidin_rgb_reg~2 ;
    logic \top^vidin_rgb_reg~3 ;
    logic \top^vidin_rgb_reg~4 ;
    logic \top^vidin_rgb_reg~5 ;
    logic \top^vidin_rgb_reg~6 ;
    logic \top^vidin_rgb_reg~7 ;
    logic \top^vidin_addr_reg~0 ;
    logic \top^vidin_addr_reg~1 ;
    logic \top^vidin_addr_reg~2 ;
    logic \top^vidin_addr_reg~3 ;
    logic \top^vidin_addr_reg~4 ;
    logic \top^vidin_addr_reg~5 ;
    logic \top^vidin_addr_reg~6 ;
    logic \top^vidin_addr_reg~7 ;
    logic \top^vidin_addr_reg~8 ;
    logic \top^vidin_addr_reg~9 ;
    logic \top^vidin_addr_reg~10 ;
    logic \top^vidin_addr_reg~11 ;
    logic \top^vidin_addr_reg~12 ;
    logic \top^vidin_addr_reg~13 ;
    logic \top^vidin_addr_reg~14 ;
    logic \top^vidin_addr_reg~15 ;
    logic \top^vidin_addr_reg~16 ;
    logic \top^vidin_addr_reg~17 ;
    logic \top^vidin_addr_reg~18 ;


    //Instantiate the dut
    sv_chip3_hierarchy_no_mem dut ( .* );

    //Load the SDF
    initial $sdf_annotate("sv_chip3_hierarchy_no_mem_post_synthesis.sdf", dut);

    //The simulation clock
    initial sim_clk = '1;
    always #CLOCK_DELAY sim_clk = ~sim_clk;

    //The circuit clocks
    assign \top^tm3_clk_v0 = sim_clk;
    assign \top^tm3_clk_v2 = sim_clk;

    //Randomized input
    always@(posedge sim_clk) begin
        \top^tm3_vidin_llc <= $urandom_range(1,0);
        \top^tm3_vidin_vs <= $urandom_range(1,0);
        \top^tm3_vidin_href <= $urandom_range(1,0);
        \top^tm3_vidin_cref <= $urandom_range(1,0);
        \top^tm3_vidin_rts0 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~0 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~1 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~2 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~3 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~4 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~5 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~6 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~7 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~8 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~9 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~10 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~11 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~12 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~13 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~14 <= $urandom_range(1,0);
        \top^tm3_vidin_vpo~15 <= $urandom_range(1,0);
    end

    endmodule

The testbench instantiates our circuit as ``dut`` at line 69.
To load the SDF we use the ``$sdf_annotate()`` system task (line 72) passing the SDF filename and target instance.
The clock is defined on lines 75-76 and the random circuit inputs are generated at the rising edge of the clock on lines 84-104.

Performing Timing Simulation in Modelsim
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
To perform the timing simulation we will use *Modelsim*, an HDL simulator from Mentor Graphics.

.. note:: Other simulators may use different commands, but the general approach will be similar.

It is easiest to write a ``tb.do`` file to setup and configure the simulation:

.. code-block:: tcl
    :linenos:
    :caption: Modelsim do file ``tb.do``. Note that :term:`$VTR_ROOT` should be replaced with the relevant path.
    :emphasize-lines: 12-14,17,31

    #Enable command logging
    transcript on

    #Setup working directories
    if {[file exists gate_work]} {
        vdel -lib gate_work -all
    }
    vlib gate_work
    vmap work gate_work

    #Load the verilog files
    vlog -sv -work work {sv_chip3_hierarchy_no_mem_post_synthesis.v}
    vlog -sv -work work {tb.sv}
    vlog -sv -work work {$VTR_ROOT/vtr_flow/primitives.v}

    #Setup the simulation
    vsim -t 1ps -L gate_work -L work -voptargs="+acc" +sdf_verbose +bitblast tb

    #Log signal changes to a VCD file
    vcd file sim.vcd
    vcd add /tb/dut/*
    vcd add /tb/dut/*

    #Setup the waveform viewer
    log -r /tb/*
    add wave /tb/*
    view structure
    view signals

    #Run the simulation for 1 microsecond
    run 1us -all

We link together the post-implementation netlist, test bench and VTR primitives on lines 12-14.
The simulation is then configured on line 17, some of the options are worth discussing in more detail:

* ``+bitblast``: Ensures Modelsim interprets the primitives in ``primitives.v`` correctly for SDF back-annotation.

.. warning:: Failing to provide ``+bitblast`` can cause errors during SDF back-annotation

* ``+sdf_verbose``: Produces more information about SDF back-annotation, useful for verifying that back-annotation succeeded.

Lastly, we tell the simulation to run on line 31.


Now that we have a ``.do`` file, lets launch the modelsim GUI:

.. code-block:: console

    $ vsim

and then run our ``.do`` file from the internal console:

.. code-block:: tcl

    ModelSim> do tb.do

Once the simulation completes we can view the results in the waveform view as shown in :ref:`at the top of the page <fig_timing_simulation>`, or process the generated VCD file ``sim.vcd``.



titan_benchmarks/index.rst
--------------------------------------
.. _titan_benchmarks_tutorial:

Running the Titan Benchmarks
----------------------------

This tutorial describes how to run the :ref:`Titan benchmarks <titan_benchmarks>` with VTR.

Integrating the Titan benchmarks into VTR
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The Titan benchmarks take up a large amount of disk space and are not distributed directly with VTR.

The Titan benchmarks can be automatically integrated into the VTR source tree by running the following from the root of the VTR source tree:

.. code-block:: console

    $ make get_titan_benchmarks

which downloads and extracts the benchmarks into the VTR source tree:

.. code-block:: console

    Warning: A typical Titan release is a ~1GB download, and uncompresses to ~10GB.
    Starting download in 15 seconds...
    Downloading http://www.eecg.utoronto.ca/~kmurray/titan/titan_release_1.1.0.tar.gz
    .....................................................................................................
    Downloading http://www.eecg.utoronto.ca/~kmurray/titan/titan_release_1.1.0.md5
    Verifying checksum
    OK
    Searching release for benchmarks and architectures...
    Extracting titan_release_1.1.0/benchmarks/titan23/sparcT2_core/netlists/sparcT2_core_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/sparcT2_core_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/LU230/netlists/LU230_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/LU230_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/segmentation/netlists/segmentation_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/segmentation_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/openCV/netlists/openCV_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/openCV_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/bitcoin_miner/netlists/bitcoin_miner_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/bitcoin_miner_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/sparcT1_chip2/netlists/sparcT1_chip2_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/sparcT1_chip2_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/mes_noc/netlists/mes_noc_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/mes_noc_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/bitonic_mesh/netlists/bitonic_mesh_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/bitonic_mesh_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/dart/netlists/dart_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/dart_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/cholesky_bdti/netlists/cholesky_bdti_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/cholesky_bdti_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/stereo_vision/netlists/stereo_vision_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/stereo_vision_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/neuron/netlists/neuron_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/neuron_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/gaussianblur/netlists/gaussianblur_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/gaussianblur_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/gsm_switch/netlists/gsm_switch_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/gsm_switch_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/sparcT1_core/netlists/sparcT1_core_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/sparcT1_core_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/des90/netlists/des90_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/des90_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/LU_Network/netlists/LU_Network_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/LU_Network_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/denoise/netlists/denoise_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/denoise_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/stap_qrd/netlists/stap_qrd_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/stap_qrd_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/directrf/netlists/directrf_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/directrf_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/SLAM_spheric/netlists/SLAM_spheric_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/SLAM_spheric_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/minres/netlists/minres_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/minres_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/benchmarks/titan23/cholesky_mc/netlists/cholesky_mc_stratixiv_arch_timing.blif to ./vtr_flow/benchmarks/titan_blif/cholesky_mc_stratixiv_arch_timing.blif
    Extracting titan_release_1.1.0/arch/stratixiv_arch.timing.no_pack_patterns.xml to ./vtr_flow/arch/titan/stratixiv_arch.timing.no_pack_patterns.xml
    Extracting titan_release_1.1.0/arch/stratixiv_arch.timing.xml to ./vtr_flow/arch/titan/stratixiv_arch.timing.xml
    Extracting titan_release_1.1.0/arch/stratixiv_arch.timing.no_directlink.xml to ./vtr_flow/arch/titan/stratixiv_arch.timing.no_directlink.xml
    Extracting titan_release_1.1.0/arch/stratixiv_arch.timing.no_chain.xml to ./vtr_flow/arch/titan/stratixiv_arch.timing.no_chain.xml
    Done
    Titan architectures: vtr_flow/arch/titan
    Titan benchmarks: vtr_flow/benchmarks/titan_blif

Once completed all the Titan benchmark BLIF netlists can be found under ``$VTR_ROOT/vtr_flow/benchmarks/titan_blif``, and the Titan architectures under ``$VTR_ROOT/vtr_flow/arch/titan``.

.. note:: :term:`$VTR_ROOT` corresponds to the root of the VTR source tree.

Running benchmarks manually
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Once the benchmarks have been integrated into VTR they can be run manually.

For example, the follow uses :ref:`VPR` to implement the ``neuron`` benchmark onto the ``startixiv_arch.timing.xml`` architecture at a :option:`channel width <vpr --route_chan_width>` of 300 tracks:

.. code-block:: console

    $ vpr $VTR_ROOT/vtr_flow/arch/titan/stratixiv_arch.timing.xml $VTR_ROOT/vtr_flow/benchmarks/titan_blif/neuron_stratixiv_arch_timing.blif --route_chan_width 300



utils/fasm.rst
--------------------------------------
.. _genfasm:

FPGA Assembly (FASM) Output Support
===================================

After VPR has generated a placed and routed design, the ``genfasm`` utility can
emit a FASM_ file to represent the design at a level detailed enough to allow generation of a bitstream to program a device. This FASM output file is enabled by FASM metadata encoded in the VPR
architecture definition and routing graph.  The output FASM file can be
converted into a bitstream format suitable to program the target architecture via architecture specific tooling. Current devices that can be programmed using the vpr + fasm flow include Lattice iCE40 and Xilinx Artix-7 devices, with work on more devices underway. More information on supported devices is available from the Symbiflow_ website and an overview of the flow for Artix-7 devices is described in IEEE Micro :cite:`murray_micro_symbiflow`.

.. _FASM: https://github.com/SymbiFlow/fasm
.. _Symbiflow: https://symbiflow.github.io

FASM metadata
-------------

The ``genfasm`` utility uses ``metadata`` blocks (see :ref:`arch_metadata`)
attached to the architecture definition and routing graph to emit FASM
features.  By adding FASM specific metadata to both the architecture
definition and the routing graph, a FASM file that represents the place and
routed design can be generated.

All metadata tags are ignored when packing, placing and routing.  After VPR has
been completed placement, ``genfasm`` utility loads the VPR output files
(.net, .place, .route) and then uses the FASM metadata to emit a FASM file.
The following metadata "keys" are recognized by ``genfasm``:

 * "fasm_prefix"
 * "fasm_features"
 * "fasm_type" and "fasm_lut"
 * "fasm_mux"
 * "fasm_params"

Invoking genfasm
----------------

``genfasm`` expects that place and route on the design is completed (e.g.
.net, .place, .route files are present), so ensure that routing is complete
before executing ``genfasm``.  ``genfasm`` should be invoked in the same
subdirectory as the routing output.  The output FASM file will be written to
``<blif root>.fasm``.

FASM prefixing
--------------

FASM feature names has structure through their prefixes.  In general the first
part of the FASM feature is the location of the feature, such as the name of
the tile the feature is located in, e.g. INT_L_X5Y6 or CLBLL_L_X10Y12.  The
next part is typically an identifier within the tile.  For example a CLBLL
tile has two slices, so the next part of the FASM feature name is the slice
identifier, e.g. SLICE_X0 or SLICE_X1.

Now consider the CLBLL_L pb_type.  This pb_type is repeated in the grid for
each tile of that type.  To allow one pb_type definition to be defined, the
"fasm_prefix" metadata tag is allowed to be attached at the layout level on
the <single> tag.  This enables the same pb_type to be used for all CLBLL_L
tiles, and the "fasm_prefix" is prepended to all FASM metadata within that
pb_type.  For example:

.. code-block:: xml

      <single priority="1" type="BLK_TI-CLBLL_L" x="35" y="51">
        <metadata>
          <meta name="fasm_prefix">CLBLL_L_X12Y100</meta>
        </metadata>
      </single>
      <single priority="1" type="BLK_TI-CLBLL_L" x="35" y="50">
        <metadata>
          <meta name="fasm_prefix">CLBLL_L_X12Y101</meta>
        </metadata>
      </single>

"fasm_prefix" tags can also be used within a pb_type to handle repeated
features.  For example in the CLB, there are 4 LUTs that can be described by
a common pb_type, except that the prefix changes for each.  For example,
consider the FF's within a CLB.  There are 8 FF's that share a common
structure, except for a prefix change.  "fasm_prefix" can be a space
separated list to assign prefixes to the index of the pb_type, rather than
needing to emit N copies of the pb_type with varying prefixes.

.. code-block:: xml

    <pb_type name="BEL_FF-FDSE_or_FDRE" num_pb="8">
      <input  name="D" num_pins="1"/>
      <input  name="CE" num_pins="1"/>
      <clock  name="C" num_pins="1"/>
      <input  name="SR" num_pins="1"/>
      <output name="Q" num_pins="1"/>
      <metadata>
        <meta name="fasm_prefix">AFF BFF CFF DFF A5FF B5FF C5FF D5FF</meta>
      </metadata>
    </pb_type>

Construction of the prefix
~~~~~~~~~~~~~~~~~~~~~~~~~~

"fasm_prefix" is accumulated throughout the structure of the architecture
definition.  Each "fasm_prefix" is joined together with a period ('.'), and
then a period is added after the prefix before the FASM feature name.


Simple FASM feature emissions
-----------------------------

In cases where a FASM feature needs to be emitted simply via use of a pb_type,
the "fasm_features" tag can be used.  If the pb_type (or mode) is selected,
then all "fasm_features" in the metadata will be emitted.  Multiple features
can be listed, whitespace separated.  Example:

.. code-block:: xml

    <metadata>
        <meta name="fasm_features">ZRST</meta>
    </metadata>

The other place that "fasm_features" is used heavily is on <edge> tags in the
routing graph.  If an edge is used in the final routed design, "genfasm" will
emit features attached to the edge.  Example:

.. code-block:: xml

    <edge sink_node="431195" src_node="418849" switch_id="0">
      <metadata>
        <meta name="fasm_features">HCLK_R_X58Y130.HCLK_LEAF_CLK_B_TOP4.HCLK_CK_BUFHCLK7 HCLK_R_X58Y130.ENABLE_BUFFER.HCLK_CK_BUFHCLK7</meta>
      </metadata>
    </edge>

In this example, when the routing graph connects node 418849 to 431195, two
FASM features will be emitted:

 * ``HCLK_R_X58Y130.HCLK_LEAF_CLK_B_TOP4.HCLK_CK_BUFHCLK7``
 * ``HCLK_R_X58Y130.ENABLE_BUFFER.HCLK_CK_BUFHCLK7``

Emitting LUTs
-------------

LUTs are a structure that is explicitly understood by VPR.  In order to emit
LUTs, two metadata keys must be used, "fasm_type" and "fasm_lut".  "fasm_type"
must be either "LUT" or "SPLIT_LUT".  The "fasm_type" modifies how the
"fasm_lut" key is interpreted.  If the pb_type that the metadata is attached
to has no "num_pb" or "num_pb" equals 1, then "fasm_type" can be "LUT".
"fasm_lut" is then the feature that represents the LUT table storage features,
example:

.. code-block:: xml

   <metadata>
     <meta name="fasm_type">LUT</meta>
     <meta name="fasm_lut">
       ALUT.INIT[63:0]
     </meta>
   </metadata>

FASM LUT metadata must be attached to the ``<pb_type>`` at or within the
``<mode>`` tag directly above the ``<pb_type>`` with ``blif_model=".names"``.
Do note that there is an implicit ``<mode>`` tag within intermediate
``<pb_type>`` when no explicit ``<mode>`` tag is present. The FASM LUT
metadata tags will not be recognized attached inside of ``<pb_type>``'s higher
above the leaf type.

When specifying a FASM features with more than one bit, explicitly specify the
bit range being set.  This is required because "genfasm" does not have access
to the actual bit database, and would otherwise not have the width of the
feature.

When "fasm_type" is "SPLIT_LUT", "fasm_lut" must specify both the feature that
represents the LUT table storage features and the pb_type path to the LUT
being specified.  Example:

.. code-block:: xml

   <metadata>
     <meta name="fasm_type">SPLIT_LUT</meta>
     <meta name="fasm_lut">
       ALUT.INIT[31:0] = BEL_LT-A5LUT[0]
       ALUT.INIT[63:32] = BEL_LT-A5LUT[1]
     </meta>
   </metadata>

In this case, the LUT in pb_type BEL_LT-A5LUT[0] will use INIT[31:0], and the
LUT in pb_type BEL_LT-A5LUT[1] will use INIT[63:32].

Within tile interconnect features
---------------------------------

When a tile has interconnect feature, e.g. output muxes, the "fasm_mux" tag
should be attached to the interconnect tag, likely the ``<direct>`` or
``<mux>`` tags.  From the perspective of genfasm, the ``<direct>`` and
``<mux>`` tags are equivalent.  The syntax for the "fasm_mux" newline
separated relationship between mux input wire names and FASM features.
Example:

.. code-block:: xml

    <mux name="D5FFMUX" input="BLK_IG-COMMON_SLICE.DX BLK_IG-COMMON_SLICE.DO5" output="BLK_BB-SLICE_FF.D5[3]" >
      <metadata>
        <meta name="fasm_mux">
          BLK_IG-COMMON_SLICE.DO5 : D5FFMUX.IN_A
          BLK_IG-COMMON_SLICE.DX : D5FFMUX.IN_B
        </meta>
      </metadata>
    </mux>

The above mux connects input BLK_IG-COMMON_SLICE.DX or BLK_IG-COMMON_SLICE.DO5
to BLK_BB-SLICE_FF.D5[3].  When VPR selects BLK_IG-COMMON_SLICE.DO5 for the
mux, "genfasm" will emit D5FFMUX.IN_A, etc.

There is not a requirement that all inputs result in a feature being set.
In cases where some mux selections result in no feature being set, use "NULL"
as the feature name.  Example:

.. code-block:: xml

    <mux name="CARRY_DI3" input="BLK_IG-COMMON_SLICE.DO5 BLK_IG-COMMON_SLICE.DX" output="BEL_BB-CARRY[2].DI" >
      <metadata>
        <meta name="fasm_mux">
          BLK_IG-COMMON_SLICE.DO5 : CARRY4.DCY0
          BLK_IG-COMMON_SLICE.DX : NULL
        </meta>
      </metadata>
    </mux>

The above examples all used the ``<mux>`` tag.  The "fasm_mux" metadata key
can also be used with the ``<direct>`` tag in the same way, example:

.. code-block:: xml

    <direct name="WA7"  input="BLK_IG-SLICEM.CX" output="BLK_IG-SLICEM_MODES.WA7">
      <metadata>
        <meta name="fasm_mux">
          BLK_IG-SLICEM.CX = WA7USED
        </meta>
      </metadata>
    </direct>

If multiple FASM features are required for a mux, they can be specified using
comma's as a seperator.  Example:

.. code-block:: xml

    <mux name="D5FFMUX" input="BLK_IG-COMMON_SLICE.DX BLK_IG-COMMON_SLICE.DO5" output="BLK_BB-SLICE_FF.D5[3]" >
      <metadata>
        <meta name="fasm_mux">
          BLK_IG-COMMON_SLICE.DO5 : D5FFMUX.IN_A
          BLK_IG-COMMON_SLICE.DX : D5FFMUX.IN_B, D5FF.OTHER_FEATURE
        </meta>
      </metadata>
    </mux>

Passing parameters through to the FASM Output
---------------------------------------------

In many cases there are parameters that need to be passed directly from the
input :ref:`vpr_eblif_file` to the FASM file.  These can be passed into a FASM
feature via the "fasm_params" key.  Note that care must be taken to have the
"fasm_params" metadata be attached to pb_type that the packer uses, the
pb_type with the blif_model= ".subckt".

The "fasm_params" value is a newline separated list of FASM features to eblif
parameters. Example:

.. code-block:: xml

  <metadata>
    <meta name="fasm_params">
      INIT[31:0] = INIT_00
      INIT[63:32] = INIT_01
    </meta>
  </metadata>

The FASM feature is on the left hand side of the equals.  When setting a
parameter with multiple bits, the bit range must be specified.  If the
parameter is a single bit, the bit range is not required, but can be supplied
for clarity.  The right hand side is the parameter name from eblif.  If the
parameter name is not found in the eblif, that FASM feature will not be
emitted.

No errors or warnings will be generated for unused parameters from eblif or
unused mappings between eblif parameters and FASM parameters to allow for
flexibility in the synthesis output.  This does mean it is important to check
spelling of the metadata, and create tests that the mapping is working as
expected.

Also note that "genfasm" will not accept "x" (unknown/don't care) or "z"
(high impedence) values in parameters.  Prior to emitting the eblif for place
and route, ensure that all parameters that will be mapped to FASM have a
valid "1" or "0".



utils/index.rst
--------------------------------------
.. _utils:

Utilities
---------

.. toctree::
   :maxdepth: 2

   fasm
   route_diag



utils/route_diag.rst
--------------------------------------
.. _route_diag:

Router Diagnosis Tool
=====================

The Router Diagnosis tool (``route_diag``) is an utility that helps developers understand the issues related to the routing phase of VPR.
Instead of running the whole routing step, ``route_diag`` performs one step of routing, aimed at analyzing specific connections between a SINK/SOURCE nodes pair.
Moreover, it is able also to profile all the possible paths of a given SOURCE node.

To correctly run the utility tool, the user needs to compile VTR with the ``VTR_ENABLE_DEBUG_LOGGING`` set to ON and found in the CMakeLists.txt_ configuration file.

.. _CMakeLists.txt: https://github.com/verilog-to-routing/vtr-verilog-to-routing/blob/01ff7e174d9d53753a2f981d7be0052b612b5874/CMakeLists.txt#L36

The tool is compiled with all the other targets when running the full build of VtR. It is also possible, though, to build the ``route_diag`` utility standalone, by running the following command:

.. code-block:: bash

  make route_diag

To use the Route Diagnosis tool, the users has different parameters at disposal:

.. option:: --sink_rr_node <int>

  Specifies the SINK RR NODE part of the pair that needs to be analyzed

.. option:: --source_rr_node <int>

  Specifies the SOURCE RR NODE part of the pair that needs to be analyzed

.. option:: --router_debug_sink_rr <int>

  Controls when router debugging is enabled for the specified sink RR.

  * For values >= 0, the value is taken as the sink RR Node ID for which to enable router debug output.
  * For values < 0, sink-based router debug output is disabled.

The Router Diagnosis tool must be provided at least with the RR GRAPH and the architecture description file to correctly function.



vpr/basic_flow.rst
--------------------------------------
Basic flow
==========

The Place and Route process in VPR consists of several steps:

- Packing (combinines primitives into complex blocks)
- Placement (places complex blocks within the FPGA grid)
- Routing (determines interconnections between blocks)
- Analysis (analyzes the implementation)

Each of these steps provides additional configuration options that can be used to customize the whole process.

Packing
-------

The packing algorithm tries to combine primitive netlist blocks (e.g. LUTs, FFs) into groups, called Complex Blocks (as specified in the :ref:`FPGA architecture file <arch_complex_blocks>`).
The results from the packing process are written into a ``.net`` file.
It contains a description of complex blocks with their inputs, outputs, used clocks and relations to other signals.
It can be useful in analyzing how VPR packs primitives together.

A detailed description of the ``.net`` file format can be found in the :ref:`vpr_pack_file` section.

Placement
---------

This step assigns a location to the Complex Blocks (produced by packing) with the FPGA grid, while optimizing for wirelength and timing.
The output from this step is written to the ``.place`` file, which contains the physical location of the blocks from the ``.net`` file.

The file has the following format:

.. code-block:: none

    block_name    x        y   subblock_number

where ``x`` and ``y`` are positions in the VPR grid and ``block_name`` comes from the ``.net`` file.

Example of a placing file:

.. code-block:: none

    Netlist_File: top.net Netlist_ID: SHA256:ce5217d251e04301759ee5a8f55f67c642de435b6c573148b67c19c5e054c1f9
    Array size: 149 x 158 logic blocks

    #block name	x	y	subblk	block number
    #----------	--	--	------	------------
    $auto$alumacc.cc:474:replace_alu$24.slice[1].carry4_full	53	32	0	#0
    $auto$alumacc.cc:474:replace_alu$24.slice[2].carry4_full	53	31	0	#1
    $auto$alumacc.cc:474:replace_alu$24.slice[3].carry4_full	53	30	0	#2
    $auto$alumacc.cc:474:replace_alu$24.slice[4].carry4_full	53	29	0	#3
    $auto$alumacc.cc:474:replace_alu$24.slice[5].carry4_full	53	28	0	#4
    $auto$alumacc.cc:474:replace_alu$24.slice[6].carry4_part	53	27	0	#5
    $auto$alumacc.cc:474:replace_alu$24.slice[0].carry4_1st_full	53	33	0	#6
    out:LD7		9	5	0	#7
    clk		42	26	0	#8
    $false		35	26	0	#9

A detailed description of the ``.place`` file format can be found in the :ref:`vpr_place_file` section.

Routing
-------

This step determines how to connect the placed Complex Blocks together, according to the netlist connectivity and the routing resources of the FPGA chip.
The router uses a Routing Resource (RR) Graph :cite:`betz_arch_cad` to represent the FPGA's available routing resources.
The RR graph can be created in two ways:

#. Automatically generated by VPR from the :ref:`FPGA architecture description <arch_reference>` :cite:`betz_automatic_generation_of_fpga_routing`, or
#. Loaded from an external :ref:`RR graph file <vpr_route_resource_file>`.

The output of routing is written into a ``.route`` file.
The file describes each connection from input to its output through different routing resources of the FPGA.
Each net starts with a ``SOURCE`` node and ends in a ``SINK`` node, potentially passing through complex block input/output pins (``IPIN``/``OPIN`` nodes) and horizontal/vertical routing wires (``CHANX``/``CHANY`` nodes).
The pair of numbers in round brackets provides information on the (x, y) resource location on the VPR grid.
The additional field provides information about the specific node.

An example routing file could look as follows:

.. code-block:: none

    Placement_File: top.place Placement_ID: SHA256:88d45f0bf7999e3f9331cdfd3497d0028be58ffa324a019254c2ae7b4f5bfa7a
    Array size: 149 x 158 logic blocks.

    Routing:

    Net 0 (counter[4])

    Node:	203972	SOURCE (53,32)  Class: 40  Switch: 0
    Node:	204095	  OPIN (53,32)  Pin: 40   BLK-TL-SLICEL.CQ[0] Switch: 189
    Node:	1027363	 CHANY (52,32)  Track: 165  Switch: 7
    Node:	601704	 CHANY (52,32)  Track: 240  Switch: 161
    Node:	955959	 CHANY (52,32) to (52,33)  Track: 90  Switch: 130
    Node:	955968	 CHANY (52,32)  Track: 238  Switch: 128
    Node:	955976	 CHANY (52,32)  Track: 230  Switch: 131
    Node:	601648	 CHANY (52,32)  Track: 268  Switch: 7
    Node:	1027319	 CHANY (52,32)  Track: 191  Switch: 183
    Node:	203982	  IPIN (53,32)  Pin: 1   BLK-TL-SLICEL.A2[0] Switch: 0
    Node:	203933	  SINK (53,32)  Class: 1  Switch: -1

   Net 1 ($auto$alumacc.cc:474:replace_alu$24.O[6])
   ...

A detailed description of the ``.route`` file format can be found in the :ref:`vpr_route_file` section.

Analysis
--------
This step analyzes the resulting implementation, producing information about:
 - Resource usage (e.g. block types, wiring)
 - Timing (e.g. critical path delays and timing paths)
 - Power (e.g. total power used, power broken down by blocks)

Note that VPR's analysis can be used independently of VPR's optimization stages, so long as the appropriate ``.net``/``.place``/``.route`` files are available.



vpr/command_line_usage.rst
--------------------------------------
Command-line Options
====================
.. program:: vpr

.. |des90_place| image:: https://www.verilogtorouting.org/img/des90_placement_macros.gif
    :width: 200px
    :alt: Placement

.. |des90_cpd| image:: https://www.verilogtorouting.org/img/des90_cpd.gif
    :width: 200px
    :alt: Critical Path

.. |des90_nets| image:: https://www.verilogtorouting.org/img/des90_nets.gif
    :width: 200px
    :alt: Wiring

.. |des90_routing| image:: https://www.verilogtorouting.org/img/des90_routing_util.gif
    :width: 200px
    :alt: Routing Usage

+---------------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+
| |des90_place|                         + |des90_cpd|                           | |des90_nets|                          + |des90_routing|                       +
|                                       +                                       |                                       +                                       +
| Placement                             + Critical Path                         | Logical Connections                   + Routing Utilization                   +
+---------------------------------------+---------------------------------------+---------------------------------------+---------------------------------------+

Basic Usage
-----------

At a minimum VPR requires two command-line arguments::

    vpr architecture circuit

where:

.. option:: architecture

    is an :ref:`FPGA architecture description file <fpga_architecture_description>`

.. option:: circuit

    is the technology mapped netlist in :ref:`BLIF format <vpr_blif_file>` to be implemented

VPR will then pack, place, and route the circuit onto the specified architecture.

By default VPR will perform a binary search routing to find the minimum channel width required to route the circuit.

Detailed Command-line Options
-----------------------------
VPR has a lot of options. Running :option:`vpr --help` will display all the available options and their usage information. 

.. option:: -h, --help

    Display help message then exit.
    
The options most people will be interested in are:

* :option:`--route_chan_width` (route at a fixed channel width), and
* :option:`--disp` (turn on/off graphics).

In general for the other options the defaults are fine, and only people looking at how different CAD algorithms perform will try many of them.
To understand what the more esoteric placer and router options actually do, see :cite:`betz_arch_cad` or download :cite:`betz_directional_bias_routing_arch,betz_biased_global_routing_tech_report,betz_vpr,marquardt_timing_driven_placement` from the author’s `web page <http://www.eecg.toronto.edu/~vaughn>`_.

In the following text, values in angle brackets e.g. ``<int>`` ``<float>`` ``<string>`` ``<file>``, should be replaced by the appropriate number, string, or file path.
Values in curly braces separated by vertical bars, e.g. ``{on | off}``, indicate all the permissible choices for an option.

.. _stage_options:

Stage Options
^^^^^^^^^^^^^
VPR runs all stages of (pack, place, route, and analysis) if none of :option:`--pack`, :option:`--place`, :option:`--route` or :option:`--analysis` are specified.

.. option:: --pack

    Run packing stage

    **Default:** ``off``

.. option:: --place

    Run placement stage

    **Default:** ``off``

.. option:: --analytical_place

    Run the analytical placement flow.
    This flows uses an integrated packing and placement algorithm which uses information from the primitive level to improve clustering and placement;
    as such, the :option:`--pack` and :option:`--place` options should not be set when this option is set.
    This flow requires that the device has a fixed size and some of the primitive blocks are fixed somewhere on the device grid.

    .. seealso:: See :ref:`Fixed FPGA Grid Layout <fixed_arch_grid_layout>` and :option:`--device` for how to fix the device size.

    .. seealso:: See :ref:`VPR Placement Constraints <placement_constraints>` for how to fix primitive blocks in a design to the device grid.

    .. warning::

        This analytical placement flow is experimental and under active development.

    **Default:** ``off``

.. option:: --route

    Run routing stage
    This also implies --analysis if routing was successful.

    **Default:** ``off``

.. option:: --analysis

    Run final analysis stage (e.g. timing, power).

    **Default:** ``off``

.. _graphics_options:

Graphics Options
^^^^^^^^^^^^^^^^

.. option:: --disp {on | off}

    Controls whether :ref:`VPR's interactive graphics <vpr_graphics>` are enabled.
    Graphics are very useful for inspecting and debugging the FPGA architecture and/or circuit implementation.

    **Default:** ``off``

.. option:: --auto <int>

    Can be 0, 1, or 2.
    This sets how often you must click Proceed to continue execution after viewing the graphics.
    The higher the number, the more infrequently the program will pause.

    **Default:** ``1``

.. option:: --save_graphics {on | off}

    If set to on, this option will save an image of the final placement and the final routing created by vpr to pdf files on disk, with no need for any user interaction. The files are named vpr_placement.pdf and vpr_routing.pdf.

    **Default:** ``off``

.. option:: --graphics_commands <string>

    A set of semi-colon seperated graphics commands.
    Graphics commands must be surrounded by quotation marks (e.g. --graphics_commands "save_graphics place.png;")

    * save_graphics <file>
         Saves graphics to the specified file (.png/.pdf/
         .svg). If <file> contains ``{i}``, it will be
         replaced with an integer which increments
         each time graphics is invoked.
    * set_macros <int>
         Sets the placement macro drawing state
    * set_nets <int>
         Sets the net drawing state
    * set_cpd <int>
         Sets the criticla path delay drawing state
    * set_routing_util <int>
         Sets the routing utilization drawing state
    * set_clip_routing_util <int>
         Sets whether routing utilization values are clipped to [0., 1.]. Useful when a consistent scale is needed across images
    * set_draw_block_outlines <int>
         Sets whether blocks have an outline drawn around them
    * set_draw_block_text <int>
         Sets whether blocks have label text drawn on them
    * set_draw_block_internals <int>
         Sets the level to which block internals are drawn
    * set_draw_net_max_fanout <int>
         Sets the maximum fanout for nets to be drawn (if fanout is beyond this value the net will not be drawn)
    * set_congestion <int>
         Sets the routing congestion drawing state
    * exit <int>
         Exits VPR with specified exit code

    Example:

    .. code-block:: none

        "save_graphics place.png; \
        set_nets 1; save_graphics nets1.png;\
        set_nets 2; save_graphics nets2.png; set_nets 0;\
        set_cpd 1; save_graphics cpd1.png; \
        set_cpd 3; save_graphics cpd3.png; set_cpd 0; \
        set_routing_util 5; save_graphics routing_util5.png; \
        set_routing_util 0; \
        set_congestion 1; save_graphics congestion1.png;"

    The above toggles various graphics settings (e.g. drawing nets, drawing critical path) and then saves the results to .png files.

    Note that drawing state is reset to its previous state after these commands are invoked.

    Like the interactive graphics :option`<--disp>` option, the :option:`--auto` option controls how often the commands specified with this option are invoked.

.. _general_options:

General Options
^^^^^^^^^^^^^^^
.. option:: --version

    Display version information then exit.

.. option:: --device <string>

    Specifies which device layout/floorplan to use from the architecture file.  Valid values are:

    * ``auto`` VPR uses the smallest device satisfying the circuit's resource requirements.  This option will use the ``<auto_layout>`` tag if it is present in the architecture file in order to construct the smallest FPGA that has sufficient resources to fit the design. If the ``<auto_layout>`` tag is not present, the ``auto`` option chooses the smallest device amongst all the architecture file's ``<fixed_layout>`` specifications into which the design can be packed.
    * Any string matching ``name`` attribute of a device layout defined with a ``<fixed_layout>`` tag in the :ref:`arch_grid_layout` section of the architecture file.

    If the value specified is neither ``auto`` nor matches the ``name`` attribute value of a ``<fixed_layout>`` tag, VPR issues an error.
       
    .. note:: If the only layout in the architecture file is a single device specified using ``<fixed_layout>``, it is recommended to always specify the ``--device`` option; this prevents the value ``--device auto`` from interfering with operations supported only for ``<fixed_layout>`` grids.

    **Default:** ``auto``

.. option:: -j, --num_workers <int>

    Controls how many parallel workers VPR may use:

    * ``1`` implies VPR will execute serially,
    * ``>1`` implies VPR may execute in parallel with up to the specified concurency
    * ``0`` implies VPR may execute with up to the maximum concurrency supported by the host machine

    If this option is not specified it may be set from the ``VPR_NUM_WORKERS`` environment variable; otherwise the default is used.

    .. note:: To compile VPR to allow the usage of parallel workers, ``libtbb-dev`` must be installed in the system.

    **Default:** ``1``

.. option:: --timing_analysis {on | off}

    Turn VPR timing analysis off.
    If it is off, you don’t have to specify the various timing analysis parameters in the architecture file.

    **Default:**  ``on``

.. option:: --echo_file {on | off}

    Generates echo files of key internal data structures.
    These files are generally used for debugging vpr, and typically end in ``.echo``

    **Default:** ``off``

.. option:: --verify_file_digests {on | off}

    Checks that any intermediate files loaded (e.g. previous packing/placement/routing) are consistent with the current netlist/architecture.

    If set to ``on`` will error if any files in the upstream dependancy have been modified.
    If set to ``off`` will warn if any files in the upstream dependancy have been modified.

    **Default:** ``on``

.. option:: --target_utilization <float>

    Sets the target device utilization.
    This corresponds to the maximum target fraction of device grid-tiles to be used.
    A value of 1.0 means the smallest device (which fits the circuit) will be used.

    **Default:** ``1.0``


.. option:: --constant_net_method {global | route}

    Specifies how constant nets (i.e. those driven to a constant value) are handled:

     * ``global``: Treat constant nets as globals (not routed)
     * ``route``: Treat constant nets as normal nets (routed)

     **Default:** ``global``

.. option:: --clock_modeling {ideal | route | dedicated_network}

    Specifies how clock nets are handled:

     * ``ideal``: Treat clock pins as ideal (i.e. no routing delays on clocks)
     * ``route``: Treat clock nets as normal nets (i.e. routed using inter-block routing)
     * ``dedicated_network``: Use the architectures dedicated clock network (experimental)

     **Default:** ``ideal``

.. option:: --two_stage_clock_routing {on | off}

    Routes clock nets in two stages using a dedicated clock network.

     * First stage: From the net source (e.g. an I/O pin) to a dedicated clock network root (e.g. center of chip)
     * Second stage: From the clock network root to net sinks.

    Note this option only works when specifying a clock architecture, see :ref:`Clock Architecture Format <clock_architecture_format>`; it does not work when reading a routing resource graph (i.e. :option:`--read_rr_graph`).

     **Default:** ``off``

.. option:: --exit_before_pack {on | off}

    Causes VPR to exit before packing starts (useful for statistics collection).

    **Default:** ``off``

.. option:: --strict_checks {on, off}

    Controls whether VPR enforces some consistency checks strictly (as errors) or treats them as warnings.

    Usually these checks indicate an issue with either the targetted architecture, or consistency issues with VPR's internal data structures/algorithms (possibly harming optimization quality).
    In specific circumstances on specific architectures these checks may be too restrictive and can be turned off.

    .. warning:: Exercise extreme caution when turning this option off -- be sure you completely understand why the issue is being flagged, and why it is OK to treat as a warning instead of an error.

    **Default:** ``on``

.. option:: --terminate_if_timing_fails {on, off}

    Controls whether VPR should terminate if timing is not met after routing.

    **Default:** ``off``

.. _filename_options:

Filename Options
^^^^^^^^^^^^^^^^
VPR by default appends .blif, .net, .place, and .route to the circuit name provided by the user, and looks for an SDC file in the working directory with the same name as the circuit.
Use the options below to override this default naming behaviour.

.. option:: --circuit_file <file>

    Path to technology mapped user circuit in :ref:`BLIF format <vpr_blif_file>`.

    .. note:: If specified the :option:`circuit` positional argument is treated as the circuit name.

    .. seealso:: :option:`--circuit_format`

.. option:: --circuit_format {auto | blif | eblif}

    File format of the input technology mapped user circuit.

    * ``auto``: File format inferred from file extension (e.g. ``.blif`` or ``.eblif``)
    * ``blif``: Strict :ref:`structural BLIF <vpr_blif_file>`
    * ``eblif``: Structural :ref:`BLIF with extensions <vpr_eblif_file>`

    **Default:** ``auto``

.. option:: --net_file <file>

    Path to packed user circuit in :ref:`net format <vpr_net_file>`.

    **Default:** :option:`circuit <circuit>`.net

.. option:: --place_file <file>

    Path to final :ref:`placement file <vpr_place_file>`.

    **Default:** :option:`circuit <circuit>`.place

.. option:: --route_file <file>

    Path to final :ref:`routing file <vpr_route_file>`.

    **Default:** :option:`circuit <circuit>`.route

.. option:: --sdc_file <file>

    Path to SDC timing constraints file.

    If no SDC file is found :ref:`default timing constraints <default_timing_constraints>` will be used.

    **Default:** :option:`circuit <circuit>`.sdc

.. option:: --write_rr_graph <file>

    Writes out the routing resource graph generated at the last stage of VPR in the :ref:`RR Graph file format <vpr_route_resource_file>`. The output can be read into VPR using :option:`--read_rr_graph`.

    <file> describes the filename for the generated routing resource graph. Accepted extensions are ``.xml`` and ``.bin`` to write the graph in XML or binary (Cap'n Proto) format.

.. option:: --read_rr_graph <file>

    Reads in the routing resource graph named <file> loads it for use during the placement and routing stages. Expects a file extension of either ``.xml`` or ``.bin``.

    The routing resource graph overthrows all the architecture definitions regarding switches, nodes, and edges. Other information such as grid information, block types, and segment information are matched with the architecture file to ensure accuracy.

    The file can be obtained through :option:`--write_rr_graph`.

    .. seealso:: :ref:`Routing Resource XML File <vpr_route_resource_file>`.

.. option:: --read_vpr_constraints <file>

    Reads the :ref:`VPR constraints <vpr_constraints>` that the flow must respect from the specified XML file.

.. option:: --write_vpr_constraints <file>

    Writes out new :ref:`floorplanning constraints <placement_constraints>` based on the current placement to the specified XML file.

.. option:: --read_router_lookahead <file>

    Reads the lookahead data from the specified file instead of computing it. Expects a file extension of either ``.capnp`` or ``.bin``.

.. option:: --write_router_lookahead <file>

    Writes the lookahead data to the specified file. Accepted file extensions are ``.capnp``, ``.bin``, and ``.csv``.

.. option:: --read_placement_delay_lookup <file>

    Reads the placement delay lookup from the specified file instead of computing it. Expects a file extension of either ``.capnp`` or ``.bin``.

.. option:: --write_placement_delay_lookup <file>

    Writes the placement delay lookup to the specified file. Expects a file extension of either ``.capnp`` or ``.bin``.
.. option:: --write_initial_place_file <file>

    Writes out the the placement chosen by the initial placement algorithm to the specified file.

.. option:: --outfile_prefix <string>

    Prefix for output files

.. _netlist_options:

Netlist Options
^^^^^^^^^^^^^^^
By default VPR will remove buffer LUTs, and iteratively sweep the netlist to remove unused primary inputs/outputs, nets and blocks, until nothing else can be removed.

.. option:: --absorb_buffer_luts {on | off}

    Controls whether LUTs programmed as wires (i.e. implementing logical identity) should be absorbed into the downstream logic.

    Usually buffer LUTS are introduced in BLIF circuits by upstream tools in order to rename signals (like ``assign`` statements in Verilog).
    Absorbing these buffers reduces the number of LUTs required to implement the circuit.

    Ocassionally buffer LUTs are inserted for other purposes, and this option can be used to preserve them.
    Disabling buffer absorption can also improve the matching between the input and post-synthesis netlist/SDF.

    **Default**: ``on``

.. option:: --const_gen_inference {none | comb | comb_seq}

    Controls how constant generators are inferred/detected in the input circuit.
    Constant generators and the signals they drive are not considered during timing analysis.

    * ``none``: No constant generator inference will occur. Any signals which are actually constants will be treated as non-constants.
    * ``comb``: VPR will infer constant generators from combinational blocks with no non-constant inputs (always safe).
    * ``comb_seq``: VPR will infer constant generators from combinational *and* sequential blocks with only constant inputs (usually safe).

    .. note:: In rare circumstances ``comb_seq`` could incorrectly identify certain blocks as constant generators.
              This would only occur if a sequential netlist primitive has an internal state which evolves *completely independently* of any data input (e.g. a hardened LFSR block, embedded thermal sensor).

    **Default**: ``comb_seq``

.. option:: --sweep_dangling_primary_ios {on | off}

    Controls whether the circuits dangling primary inputs and outputs (i.e. those who do not drive, or are not driven by anything) are swept and removed from the netlist.

    Disabling sweeping of primary inputs/outputs can improve the matching between the input and post-synthesis netlists.
    This is often useful when performing formal verification.

    .. seealso:: :option:`--sweep_constant_primary_outputs`

    **Default**: ``on``

.. option:: --sweep_dangling_nets {on | off}

    Controls whether dangling nets (i.e. those who do not drive, or are not driven by anything) are swept and removed from the netlist.

    **Default**: ``on``

.. option:: --sweep_dangling_blocks {on | off}

    Controls whether dangling blocks (i.e. those who do not drive anything) are swept and removed from the netlist.

    **Default**: ``on``

.. option:: --sweep_constant_primary_outputs {on | off}

    Controls whether primary outputs driven by constant values are swept and removed from the netlist.

    .. seealso:: :option:`--sweep_dangling_primary_ios`

    **Default**: ``off``

.. option:: --netlist_verbosity <int>

    Controls the verbosity of netlist processing (constant generator detection, swept netlist components).
    High values produce more detailed output.

    **Default**: ``1``

.. _packing_options:

Packing Options
^^^^^^^^^^^^^^^
AAPack is the packing algorithm built into VPR.
AAPack takes as input a technology-mapped blif netlist consisting of LUTs, flip-flops, memories, mulitpliers, etc and outputs a .net formatted netlist composed of more complex logic blocks.
The logic blocks available on the FPGA are specified through the FPGA architecture file.
For people not working on CAD, you can probably leave all the options to their default values.

.. option:: --connection_driven_clustering {on | off}

    Controls whether or not AAPack prioritizes the absorption of nets with fewer connections into a complex logic block over nets with more connections.

    **Default**: ``on``

.. option:: --allow_unrelated_clustering {on | off | auto}

    Controls whether primitives with no attraction to a cluster may be packed into it.

    Unrelated clustering can increase packing density (decreasing the number of blocks required to implement the circuit), but can significantly impact routability.

    When set to ``auto`` VPR automatically decides whether to enable unrelated clustring based on the targetted device and achieved packing density.

    **Default**:  ``auto``

.. option:: --alpha_clustering <float>

    A parameter that weights the optimization of timing vs area.

    A value of 0 focuses solely on area, a value of 1 focuses entirely on timing.

    **Default**: ``0.75``

.. option:: --beta_clustering <float>

    A tradeoff parameter that controls the optimization of smaller net absorption vs. the optimization of signal sharing.

    A value of 0 focuses solely on signal sharing, while a value of 1 focuses solely on absorbing smaller nets into a cluster.
    This option is meaningful only when connection_driven_clustering is on.

    **Default**:  ``0.9``

.. option:: --timing_driven_clustering {on|off}

    Controls whether or not to do timing driven clustering

    **Default**: ``on``

.. option:: --cluster_seed_type {blend | timing | max_inputs}

    Controls how the packer chooses the first primitive to place in a new cluster.

    ``timing`` means that the unclustered primitive with the most timing-critical connection is used as the seed.

    ``max_inputs`` means the unclustered primitive that has the most connected inputs is used as the seed.

    ``blend`` uses a weighted sum of timing criticality, the number of tightly coupled blocks connected to the primitive, and the number of its external inputs.

    ``max_pins`` selects primitives with the most number of pins (which may be used, or unused).

    ``max_input_pins`` selects primitives with the most number of input pins (which may be used, or unused).

    ``blend2`` An alternative blend formulation taking into account both used and unused pin counts, number of tightly coupled blocks and criticality.

    **Default**: ``blend2`` if timing_driven_clustering is on; ``max_inputs`` otherwise.

.. option:: --clustering_pin_feasibility_filter {on | off}

    Controls whether the pin counting feasibility filter is used during clustering.
    When enabled the clustering engine counts the number of available pins in groups/classes of mutually connected pins within a cluster.
    These counts are used to quickly filter out candidate primitives/atoms/molecules for which the cluster has insufficient pins to route (without performing a full routing).
    This reduces packing run-time.

    **Default:** ``on``

.. option:: --balance_block_type_utilization {on, off, auto}

    Controls how the packer selects the block type to which a primitive will be mapped if it can potentially map to multiple block types.

     * ``on``  : Try to balance block type utilization by picking the block type with the (currenty) lowest utilization.
     * ``off`` : Do not try to balance block type utilization
     * ``auto``: Dynamically enabled/disabled (based on density)

    **Default:** ``auto``

.. option:: --target_ext_pin_util { auto | <float> | <float>,<float> | <string>:<float> | <string>:<float>,<float> }

    Sets the external pin utilization target (fraction between 0.0 and 1.0) during clustering.
    This determines how many pin the clustering engine will aim to use in a given cluster before closing it and opening a new cluster.

    Setting this to ``1.0`` guides the packer to pack as densely as possible (i.e. it will keep adding molecules to the cluster until no more can fit).
    Setting this to a lower value will guide the packer to pack less densely, and instead creating more clusters.
    In the limit setting this to ``0.0`` will cause the packer to create a new cluster for each molecule.

    Typically packing less densely improves routability, at the cost of using more clusters.

    This option can take several different types of values:

    * ``auto`` VPR will automatically determine appropriate target utilizations.

    * ``<float>`` specifies the target input pin utilization for all block types.

        For example:

          * ``0.7`` specifies that all blocks should aim for 70% input pin utilization.

    * ``<float>,<float>`` specifies the target input and output pin utilizations respectively for all block types.

        For example:

          * ``0.7,0.9`` specifies that all blocks should aim for 70% input pin utilization, and 90% output pin utilization.

    * ``<string>:<float>`` and ``<string>:<float>,<float>`` specify the target pin utilizations for a specific block type (as above).

        For example:

          * ``clb:0.7`` specifies that only ``clb`` type blocks should aim for 70% input pin utilization.
          * ``clb:0.7,0.9`` specifies that only ``clb`` type blocks should aim for 70% input pin utilization, and 90% output pin utilization.

    .. note::

        If a pin utilization target is unspecified it defaults to 1.0 (i.e. 100% utilization).

        For example:

          * ``0.7`` leaves the output pin utilization unspecified, which is equivalent to ``0.7,1.0``.
          * ``clb:0.7,0.9`` leaves the pin utilizations for all other block types unspecified, so they will assume a default utilization of ``1.0,1.0``.

    This option can also take multiple space-separated values.
    For example::

        --target_ext_pin_util clb:0.5 dsp:0.9,0.7 0.8

    would specify that ``clb`` blocks use a target input pin utilization of 50%, ``dsp`` blocks use a targets of 90% and 70% for inputs and outputs respectively, and all other blocks use an input pin utilization target of 80%.

    .. note::

        This option is only a guideline.
        If a molecule  (e.g. a carry-chain with many inputs) would not otherwise fit into a cluster type at the specified target utilization the packer will fallback to using all pins (i.e. a target utilization of ``1.0``).

    .. note::

        This option requires :option:`--clustering_pin_feasibility_filter` to be enabled.

    **Default:** ``auto``


.. option:: --pack_prioritize_transitive_connectivity {on | off}

    Controls whether transitive connectivity is prioritized over high-fanout connectivity during packing.

    **Default:** ``on``

.. option:: --pack_high_fanout_threshold {auto | <int> | <string>:<int>}

    Defines the threshold for high fanout nets within the packer.

    This option can take several different types of values:

    * ``auto`` VPR will automatically determine appropriate thresholds.

    * ``<int>`` specifies the fanout threshold for all block types.

        For example:

          * ``64`` specifies that a threshold of 64 should be used for all blocks.

    * ``<string>:<float>`` specifies the the threshold for a specific block type.

        For example:

          * ``clb:16`` specifies that ``clb`` type blocks should use a threshold of 16.

    This option can also take multiple space-separated values.
    For example::

        --pack_high_fanout_threshold 128 clb:16

    would specify that ``clb`` blocks use a threshold of 16, while all other blocks (e.g. DSPs/RAMs) would use a threshold of 128.

    **Default:** ``auto``

.. option::  --pack_transitive_fanout_threshold <int>

    Packer transitive fanout threshold.

    **Default:** ``4``

.. option::  --pack_feasible_block_array_size <int>

    This value is used to determine the max size of the priority queue for candidates that pass the early filter legality test
    but not the more detailed routing filter.

    **Default:** ``30``

.. option:: --pack_verbosity <int>

    Controls the verbosity of clustering output.
    Larger values produce more detailed output, which may be useful for debugging architecture packing problems.

    **Default:** ``2``

.. option:: --write_block_usage <file>

    Writes out to the file under path <file> cluster-level block usage summary in machine
    readable (JSON or XML) or human readable (TXT) format. Format is selected
    based on the extension of <file>.

.. _placer_options:

Placer Options
^^^^^^^^^^^^^^
The placement engine in VPR places logic blocks using simulated annealing.
By default, the automatic annealing schedule is used :cite:`betz_arch_cad,betz_vpr`.
This schedule gathers statistics as the placement progresses, and uses them to determine how to update the temperature, when to exit, etc.
This schedule is generally superior to any user-specified schedule.
If any of init_t, exit_t or alpha_t is specified, the user schedule, with a fixed initial temperature, final temperature and temperature update factor is used.

.. seealso:: :ref:`timing_driven_placer_options`

.. option:: --seed <int>

    Sets the initial random seed used by the placer.

    **Default:** ``1``

.. option:: --enable_timing_computations {on | off}

    Controls whether or not the placement algorithm prints estimates of the circuit speed of the placement it generates.
    This setting affects statistics output only, not optimization behaviour.

    **Default:** ``on`` if timing-driven placement is specified, ``off`` otherwise.

.. option:: --inner_num <float>

    The number of moves attempted at each temperature in placement can be calculated from inner_num scaled with circuit size or device-circuit size as specified in ``place_effort_scaling``.

    Changing inner_num is the best way to change the speed/quality tradeoff of the placer, as it leaves the highly-efficient automatic annealing schedule on and simply changes the number of moves per temperature.

    Specifying ``-inner_num 10`` will slow the placer by a factor of 10 while typically improving placement quality only by 10% or less (depends on the architecture).
    Hence users more concerned with quality than CPU time may find this a more appropriate value of inner_num.

    **Default:** ``0.5``

.. option:: --place_effort_scaling {circuit | device_circuit}

    Controls how the number of placer moves level scales with circuit and device size:

    * ``circuit``: The number of moves attempted at each temperature is inner_num *  num_blocks^(4/3) in the circuit.
    * ``device_circuit``: The number of moves attempted at each temperature is inner_num * grid_size^(2/3) * num_blocks^(4/3) in the circuit.

    The number of blocks in a circuit is the number of pads plus the number of clbs.

    **Default:** ``circuit``

.. option:: --init_t <float>

    The starting temperature of the anneal for the manual annealing schedule.

    **Default:** ``100.0``

.. option:: --exit_t <float>

    The manual anneal will terminate when the temperature drops below the exit temperature.

    **Default:** ``0.01``

.. option:: --alpha_t <float>

    The temperature is updated by multiplying the old temperature by alpha_t when the manual annealing schedule is enabled.

    **Default:** ``0.8``

.. option:: --fix_pins {free | random}

    Controls how the placer handles I/O pads during placement.

    * ``free``: The placer can move I/O locations to optimize the placement.
    * ``random``: Fixes I/O pads to arbitrary locations and does not allow the placer to move them during the anneal (models the effect of poor board-level I/O constraints).

    Note: the fix_pins option also used to accept a third argument - a place file that specified where I/O pins should be placed. This argument is no longer accepted by         fix_pins. Instead, the fix_clusters option can now be used to lock down I/O pins.

    **Default:** ``free``.

.. option:: --fix_clusters {<file.place>}

    Controls how the placer handles blocks (of any type) during placement.

    * ``<file.place>``: A path to a file listing the desired location of blocks in the netlist.

    This place location file is in the same format as a :ref:`normal placement file <vpr_place_file>`, but does not require the first two lines which are normally at the top     of a placement file that specify the netlist file, netlist ID, and array size.

    **Default:** ````.

.. option:: --place_algorithm {bounding_box | criticality_timing | slack_timing}

    Controls the algorithm used by the placer.

    ``bounding_box`` Focuses purely on minimizing the bounding box wirelength of the circuit. Turns off timing analysis if specified.

    ``criticality_timing`` Focuses on minimizing both the wirelength and the connection timing costs (criticality * delay).

    ``slack_timing`` Focuses on improving the circuit slack values to reduce critical path delay.

    **Default:**  ``criticality_timing``

.. option:: --place_quench_algorithm {bounding_box | criticality_timing | slack_timing}

    Controls the algorithm used by the placer during placement quench.
    The algorithm options have identical functionality as the ones used by the option ``--place_algorithm``. If specified, it overrides the option ``--place_algorithm`` during placement quench.

    **Default:**  ``criticality_timing``

.. option:: --place_bounding_box_mode {auto_bb | cube_bb | per_layer_bb}

    Specifies the type of the wirelength estimator used during placement. For single layer architectures, cube_bb (a 3D bounding box) is always used (and is the same as per_layer_bb).
    For 3D architectures, cube_bb is appropriate if you can cross between layers at switch blocks, while if you can only cross between layers at output pins per_layer_bb (one bouding box per layer) is more accurate and appropriate.

    ``auto_bb``: The bounding box type is determined automatically based on the cross-layer connections.

    ``cube_bb``: ``cube_bb`` bounding box is used to estimate the wirelength.

    ``per_layer_bb``: ``per_layer_bb`` bounding box is used to estimate the wirelength

    **Default:** ``auto_bb``

.. option:: --place_chan_width <int>

    Tells VPR how many tracks a channel of relative width 1 is expected to need to complete routing of this circuit.
    VPR will then place the circuit only once, and repeatedly try routing the circuit as usual.

    **Default:** ``100``

.. option:: --place_rlim_escape <float>

    The fraction of moves which are allowed to ignore the region limit.
    For example, a value of 0.1 means 10% of moves are allowed to ignore the region limit.

    **Default:** ``0.0``

.. _dusty_sa_options:
Setting any of the following 5 options selects :ref:`Dusty's annealing schedule <dusty_sa>` .

.. option:: --alpha_min <float>

    The minimum (starting) update factor (alpha) used.
    Ranges between 0 and alpha_max.

    **Default:** ``0.2``

.. option:: --alpha_max <float>

    The maximum (stopping) update factor (alpha) used after which simulated annealing will complete.
    Ranges between alpha_min and 1.

    **Default:** ``0.9``

.. option:: --alpha_decay <float>

    The rate at which alpha will approach 1: alpha(n) = 1 - (1 - alpha(n-1)) * alpha_decay
    Ranges between 0 and 1.

    **Default:** ``0.7``

.. option:: --anneal_success_min <float>

   The minimum success ratio after which the temperature will reset to maintain the target success ratio.
   Ranges between 0 and anneal_success_target.

    **Default:** ``0.1``

.. option:: --anneal_success_target <float>

   The temperature after each reset is selected to keep this target success ratio.
   Ranges between anneal_success_target and 1.

    **Default:** ``0.25``

.. option:: --place_cost_exp <float>

    Wiring cost is divided by the average channel width over a net's bounding box
    taken to this exponent. Only impacts devices with different channel widths in 
    different directions or regions. 

    **Default:** ``1``

.. option:: --RL_agent_placement {on | off}

    Uses a Reinforcement Learning (RL) agent in choosing the appropiate move type in placement.
    It activates the RL agent placement instead of using a fixed probability for each move type.

    **Default:** ``on``

.. option:: --place_agent_multistate {on | off}

    Enable a multistate agent in the placement. A second state will be activated late in
    the annealing and in the Quench that includes all the timing driven directed moves.

    **Default:** ``on``

.. option:: --place_agent_algorithm {e_greedy | softmax}

    Controls which placement RL agent is used. 

    **Default:** ``softmax``

.. option:: --place_agent_epsilon <float>

    Placement RL agent's epsilon for the epsilon-greedy agent. Epsilon represents
    the percentage of exploration actions taken vs the exploitation ones.

    **Default:** ``0.3``

.. option:: --place_agent_gamma <float>

    Controls how quickly the agent's memory decays. Values between [0., 1.] specify
    the fraction of weight in the exponentially weighted reward average applied to moves
    which occured greater than moves_per_temp moves ago. Values < 0 cause the
    unweighted reward sample average to be used (all samples are weighted equally)

    **Default:** ``0.05``

.. option:: --place_reward_fun {basic | nonPenalizing_basic | runtime_aware | WLbiased_runtime_aware}

    The reward function used by the placement RL agent to learn the best action at each anneal stage. 

    .. note:: The latter two are only available for timing-driven placement. 
    
    **Default:** ``WLbiased_runtime_aware``

.. option:: --place_agent_space {move_type | move_block_type}

    The RL Agent exploration space can be either based on only move types or also consider different block types moved.

    **Default:** ``move_block_type``

.. option:: --placer_debug_block <int>
    
    .. note:: This option is likely only of interest to developers debugging the placement algorithm

    Controls which block the placer produces detailed debug information for. 
    
    If the block being moved has the same ID as the number assigned to this parameter, the placer will print debugging information about it.

    * For values >= 0, the value is the block ID for which detailed placer debug information should be produced.
    * For value == -1, detailed placer debug information is produced for all blocks.
    * For values < -1, no placer debug output is produced.

    .. warning:: VPR must have been compiled with `VTR_ENABLE_DEBUG_LOGGING` on to get any debug output from this option.

    **Default:** ``-2``

.. option:: --placer_debug_net <int>
    
    .. note:: This option is likely only of interest to developers debugging the placement algorithm

    Controls which net the placer produces detailed debug information for.

    If a net with the same ID assigned to this parameter is connected to the block that is being moved, the placer will print debugging information about it.

    * For values >= 0, the value is the net ID for which detailed placer debug information should be produced.
    * For value == -1, detailed placer debug information is produced for all nets.
    * For values < -1, no placer debug output is produced.

    .. warning:: VPR must have been compiled with `VTR_ENABLE_DEBUG_LOGGING` on to get any debug output from this option.

    **Default:** ``-2``


.. _timing_driven_placer_options:

Timing-Driven Placer Options
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
The following options are only valid when the placement engine is in timing-driven mode (timing-driven placement is used by default).

.. option:: --timing_tradeoff <float>

    Controls the trade-off between bounding box minimization and delay minimization in the placer.

    A value of 0 makes the placer focus completely on bounding box (wirelength) minimization, while a value of 1 makes the placer focus completely on timing optimization.

    **Default:**  ``0.5``

.. option:: --recompute_crit_iter <int>

    Controls how many temperature updates occur before the placer performs a timing analysis to update its estimate of the criticality of each connection.

    **Default:**  ``1``

.. option:: --inner_loop_recompute_divider <int>

    Controls how many times the placer performs a timing analysis to update its criticality estimates while at a single temperature.

    **Default:** ``0``

.. option:: --quench_recompute_divider <int>

    Controls how many times the placer performs a timing analysis to update its criticality estimates during a quench. 
    If unspecified, uses the value from --inner_loop_recompute_divider.

    **Default:** ``0``

.. option:: --td_place_exp_first <float>

    Controls how critical a connection is considered as a function of its slack, at the start of the anneal.

    If this value is 0, all connections are considered equally critical.
    If this value is large, connections with small slacks are considered much more critical than connections with small slacks.
    As the anneal progresses, the exponent used in the criticality computation gradually changes from its starting value of td_place_exp_first to its final value of :option:`--td_place_exp_last`.

    **Default:** ``1.0``

.. option:: --td_place_exp_last <float>

    Controls how critical a connection is considered as a function of its slack, at the end of the anneal.

    .. seealso:: :option:`--td_place_exp_first`

    **Default:** ``8.0``

.. option:: --place_delay_model {simple, delta, delta_override}

    Controls how the timing-driven placer estimates delays.

     * ``simple`` The placement delay estimator is built from the router lookahead. This takes less CPU time to build and it and still as accurate as the ``delta` model.
     * ``delta`` The router is used to profile delay from various locations in the grid for various differences in position.
     * ``delta_override`` Like ``delta`` but also includes special overrides to ensure effects of direct connects between blocks are accounted for.
       This is potentially more accurate but is more complex and depending on the architecture (e.g. number of direct connects) may increase place run-time.

    **Default:** ``simple``

.. option:: --place_delay_model_reducer {min, max, median, arithmean, geomean}

    When calculating delta delays for the placment delay model how are multiple values combined?

    **Default:** ``min``

.. option:: --place_delay_offset <float>

    A constant offset (in seconds) applied to the placer's delay model.

    **Default:** ``0.0``

.. option:: --place_delay_ramp_delta_threshold <float>

    The delta distance beyond which --place_delay_ramp is applied.
    Negative values disable the placer delay ramp.

    **Default:** ``-1``

.. option:: --place_delay_ramp_slope <float>

    The slope of the ramp (in seconds per grid tile) which is applied to the placer delay model for delta distance beyond :option:`--place_delay_ramp_delta_threshold`.

    **Default:** ``0.0e-9``

.. option:: --place_tsu_rel_margin <float>

    Specifies the scaling factor for cell setup times used by the placer.
    This effectively controls whether the placer should try to achieve extra margin on setup paths.
    For example a value of 1.1 corresponds to requesting 10%% setup margin.

    **Default:** ``1.0``

.. option:: --place_tsu_abs_margin <float>

    Specifies an absolute offest added to cell setup times used by the placer.
    This effectively controls whether the placer should try to achieve extra margin on setup paths.
    For example a value of 500e-12 corresponds to requesting an extra 500ps of setup margin.

    **Default:** ``0.0``

.. option:: --post_place_timing_report <file>

    Name of the post-placement timing report file to generate (not generated if unspecfied).


.. _noc_placement_options:

NoC Options
^^^^^^^^^^^^^^
The following options are only used when FPGA device and netlist contain a NoC router.  

.. option:: --noc {on | off}

    Enables a NoC-driven placer that optimizes the placement of routers on the NoC. Also, it enables an option in the graphical display that can be used to
    display the NoC on the FPGA.

    **Default:** ``off``

.. option:: --noc_flows_file <file>
    
    XML file containing the list of traffic flows within the NoC (communication between routers).

    .. note:: noc_flows_file are required to specify if NoC optimization is turned on (--noc on).

.. option:: --noc_routing_algorithm {xy_routing | bfs_routing | west_first_routing | north_last_routing | negative_first_routing | odd_even_routing}

    Controls the algorithm used by the NoC to route packets.
    
    * ``xy_routing`` Uses the direction oriented routing algorithm. This is recommended to be used with mesh NoC topologies.
    * ``bfs_routing`` Uses the breadth first search algorithm. The objective is to find a route that uses a minimum number of links. This algorithm is not guaranteed to generate deadlock-free traffic flow routes, but can be used with any NoC topology.
    * ``west_first_routing`` Uses the west-first routing algorithm. This is recommended to be used with mesh NoC topologies.
    * ``north_last_routing`` Uses the north-last routing algorithm. This is recommended to be used with mesh NoC topologies.
    * ``negative_first_routing`` Uses the negative-first routing algorithm. This is recommended to be used with mesh NoC topologies.
    * ``odd_even_routing`` Uses the odd-even routing algorithm. This is recommended to be used with mesh NoC topologies.

    **Default:** ``bfs_routing``

.. option:: --noc_placement_weighting <float>

    Controls the importance of the NoC placement parameters relative to timing and wirelength of the design.
    
    * ``noc_placement_weighting = 0`` means the placement is based solely on timing and wirelength.
    * ``noc_placement_weighting = 1`` means noc placement is considered equal to timing and wirelength.
    * ``noc_placement_weighting > 1`` means the placement is increasingly dominated by NoC parameters.
    
    **Default:** ``5.0``

.. option:: --noc_aggregate_bandwidth_weighting <float>

    Controls the importance of minimizing the NoC aggregate bandwidth. This value can be >=0, where 0 would mean the aggregate bandwidth has no relevance to placement.
    Other positive numbers specify the importance of minimizing the NoC aggregate bandwidth compared to other NoC-related cost terms.
    Weighting factors for NoC-related cost terms are normalized internally. Therefore, their absolute values are not important, and
    only their relative ratios determine the importance of each cost term.

    **Default:** ``0.38``

.. option:: --noc_latency_constraints_weighting <float>

    Controls the importance of meeting all the NoC traffic flow latency constraints. This value can be >=0, where 0 would mean latency constraints have no relevance to placement.
    Other positive numbers specify the importance of meeting latency constraints compared to other NoC-related cost terms.
    Weighting factors for NoC-related cost terms are normalized internally. Therefore, their absolute values are not important, and
    only their relative ratios determine the importance of each cost term.
    
    **Default:** ``0.6``

.. option:: --noc_latency_weighting <float>

    Controls the importance of reducing the latencies of the NoC traffic flows.
    This value can be >=0, where 0 would mean the latencies have no relevance to placement
    Other positive numbers specify the importance of minimizing aggregate latency compared to other NoC-related cost terms.
    Weighting factors for NoC-related cost terms are normalized internally. Therefore, their absolute values are not important, and
    only their relative ratios determine the importance of each cost term.
    
    **Default:** ``0.02``

.. option:: --noc_congestion_weighting <float>

    Controls the importance of reducing the congestion of the NoC links.
    This value can be >=0, where 0 would mean the congestion has no relevance to placement.
    Other positive numbers specify the importance of minimizing congestion compared to other NoC-related cost terms.
    Weighting factors for NoC-related cost terms are normalized internally. Therefore, their absolute values are not important, and
    only their relative ratios determine the importance of each cost term.

    **Default:** ``0.25``

.. option:: --noc_swap_percentage <float>

    Sets the minimum fraction of swaps attempted by the placer that are NoC blocks.
    This value is an integer ranging from [0-100]. 
    
    * ``0`` means NoC blocks will be moved at the same rate as other blocks. 
    * ``100`` means all swaps attempted by the placer are NoC router blocks.
    
    **Default:** ``0``

.. option:: --noc_placement_file_name <file>

    Name of the output file that contains the NoC placement information.

    **Default:** ``vpr_noc_placement_output.txt``

.. _router_options:

Router Options
^^^^^^^^^^^^^^
VPR uses a negotiated congestion algorithm (based on Pathfinder) to perform routing.

.. note:: By default the router performs a binary search to find the minimum routable channel width.  To route at a fixed channel width use :option:`--route_chan_width`.

.. seealso:: :ref:`timing_driven_router_options`

.. option:: --flat_routing {on | off}

    If this option is enabled, the *run-flat* router is used instead of the *two-stage* router.
    This means that during the routing stage, all nets, both intra- and inter-cluster, are routed directly from one primitive pin to another primitive pin.
    This increases routing time but can improve routing quality by re-arranging LUT inputs and exposing additional optimization opportunities in architectures with local intra-cluster routing that is not a full crossbar.

    **Default:** ``OFF`

.. option:: --max_router_iterations <int>

    The number of iterations of a Pathfinder-based router that will be executed before a circuit is declared unrouteable (if it hasn’t routed successfully yet) at a given channel width.

    *Speed-quality trade-off:* reducing this number can speed up the binary search for minimum channel width, but at the cost of some increase in final track count.
    This is most effective if -initial_pres_fac is simultaneously increased.
    Increase this number to make the router try harder to route heavily congested designs.

    **Default:** ``50``

.. option:: --first_iter_pres_fac <float>

    Similar to :option:`--initial_pres_fac`.
    This sets the present overuse penalty factor for the very first routing iteration.
    :option:`--initial_pres_fac` sets it for the second iteration.

    .. note:: A value of ``0.0`` causes congestion to be ignored on the first routing iteration.

    **Default:** ``0.0``

.. option:: --initial_pres_fac <float>

    Sets the starting value of the present overuse penalty factor.

    *Speed-quality trade-off:* increasing this number speeds up the router, at the cost of some increase in final track count.
    Values of 1000 or so are perfectly reasonable.

    **Default:** ``0.5``

.. option:: --pres_fac_mult <float>

    Sets the growth factor by which the present overuse penalty factor is multiplied after each router iteration.

    **Default:** ``1.3``

.. option:: --max_pres_fac <float>

    Sets the maximum present overuse penalty factor that can ever result during routing. Should always be less than 1e25 or so to prevent overflow. 
    Smaller values may help prevent circuitous routing in difficult routing problems, but may increase 
    the number of routing iterations needed and hence runtime.

    **Default:** ``1000.0``

.. option:: --acc_fac <float>

    Specifies the accumulated overuse factor (historical congestion cost factor).

    **Default:** ``1``

.. option:: --bb_factor <int>

    Sets the distance (in channels) outside of the bounding box of its pins a route can go.
    Larger numbers slow the router somewhat, but allow for a more exhaustive search of possible routes.

    **Default:** ``3``

.. option:: --base_cost_type {demand_only | delay_normalized | delay_normalized_length | delay_normalized_frequency | delay_normalized_length_frequency}

    Sets the basic cost of using a routing node (resource).

    * ``demand_only`` sets the basic cost of a node according to how much demand is expected for that type of node.

    * ``delay_normalized`` is similar to ``demand_only``, but normalizes all these basic costs to be of the same magnitude as the typical delay through a routing resource.

    * ``delay_normalized_length`` like ``delay_normalized``, but scaled by routing resource length.

    * ``delay_normalized_frequency`` like ``delay_normalized``, but scaled inversely by routing resource frequency.

    * ``delay_normalized_length_frequency`` like ``delay_normalized``, but scaled by routing resource length and scaled inversely by routing resource frequency.

    **Default:** ``delay_normalized_length``

.. option:: --bend_cost <float>

    The cost of a bend.
    Larger numbers will lead to routes with fewer bends, at the cost of some increase in track count.
    If only global routing is being performed, routes with fewer bends will be easier for a detailed router to subsequently route onto a segmented routing architecture.

    **Default:** ``1`` if global routing is being performed, ``0`` if combined global/detailed routing is being performed.

.. option:: --route_type {global | detailed}

    Specifies whether global routing or combined global and detailed routing should be performed.

    **Default:**  ``detailed`` (i.e. combined global and detailed routing)

.. option:: --route_chan_width <int>

    Tells VPR to route the circuit at the specified channel width.

    .. note:: If the channel width is >= 0, no binary search on channel capacity will be performed to find the minimum number of tracks required for routing. VPR simply reports whether or not the circuit will route at this channel width.

    **Default:** ``-1`` (perform binary search for minimum routable channel width)

.. option:: --min_route_chan_width_hint <int>

    Hint to the router what the minimum routable channel width is.

    The value provided is used to initialize the binary search for minimum channel width.
    A good hint may speed-up the binary search by avoiding time spent at congested channel widths which are not routable.

    The algorithm is robust to incorrect hints (i.e. it continues to binary search), so the hint does not need to be precise.

    This option may ocassionally produce a different minimum channel width due to the different initialization.

    .. seealso:: :option:`--verify_binary_search`

.. option:: --verify_binary_search {on | off}

    Force the router to check that the channel width determined by binary search is the minimum.

    The binary search ocassionally may not find the minimum channel width (e.g. due to router sub-optimality, or routing pattern issues at a particular channel width).

    This option attempts to verify the minimum by routing at successively lower channel widths until two consecutive routing failures are observed.

.. option:: --router_algorithm {timing_driven | parallel | parallel_decomp}

    Selects which router algorithm to use. 

    * ``timing_driven`` is the default single-threaded PathFinder algorithm.

    * ``parallel`` partitions the device to route non-overlapping nets in parallel. Use with the ``-j`` option to specify the number of threads.

    * ``parallel_decomp`` decomposes nets for aggressive parallelization :cite:`kosar2024parallel`. This imposes additional constraints and may result in worse QoR for difficult circuits.

    Note that both ``parallel`` and ``parallel_decomp`` are timing-driven routers.

    **Default:** ``timing_driven``

.. option:: --min_incremental_reroute_fanout <int>

    Incrementally re-route nets with fanout above the specified threshold.

    This attempts to re-use the legal (i.e. non-congested) parts of the routing tree for high fanout nets, with the aim of reducing router execution time.

    To disable, set value to a value higher than the largest fanout of any net.

    **Default:** ``16``

.. option:: --max_logged_overused_rr_nodes <int>

    Prints the information on overused RR nodes to the VPR log file after the each failed routing attempt.

    If the number of overused nodes is above the given threshold ``N``, then only the first ``N`` entries are printed to the logfile.

    **Default:** ``20``

.. option:: --generate_rr_node_overuse_report {on | off}

    Generates a detailed report on the overused RR nodes' information: **report_overused_nodes.rpt**.

    This report is generated only when the final routing attempt fails (i.e. the whole routing process has failed).

    In addition to the information that can be seen via ``--max_logged_overused_rr_nodes``, this report prints out all the net ids that are associated with each overused RR node. Also, this report does not place a threshold upon the number of RR nodes printed.

    **Default:** ``off``

.. option:: --write_timing_summary <file>

    Writes out to the file under path <file> final timing summary in machine
    readable (JSON or XML) or human readable (TXT) format. Format is selected
    based on the extension of <file>. The summary consists of parameters:

    * `cpd` - Final critical path delay (least slack) [ns]
    * `fmax` - Maximal frequency of the implemented circuit [MHz]
    * `swns` - setup Worst Negative Slack (sWNS) [ns]
    * `stns` - Setup Total Negative Slack (sTNS) [ns]

.. option:: --route_verbosity <int>

    Controls the verbosity of routing output.
    High values produce more detailed output, which can be useful for debugging or understanding the routing process.

    **Default**: ``1``

.. _timing_driven_router_options:

Timing-Driven Router Options
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
The following options are only valid when the router is in timing-driven mode (the default).

.. option:: --astar_fac <float>

    Sets how aggressive the directed search used by the timing-driven router is.

    Values between 1 and 2 are reasonable, with higher values trading some quality for reduced CPU time.

    **Default:** ``1.2``

.. option:: --astar_offset <float>

    Sets how aggressive the directed search used by the timing-driven router is.
    It is a subtractive adjustment to the lookahead heuristic.

    Values between 0 and 1e-9 are resonable; higher values may increase quality at the expense of run-time.

    **Default:** ``0.0``

.. option:: --router_profiler_astar_fac <float>
    
    Controls the directedness of the timing-driven router's exploration when doing router delay profiling of an architecture.
    The router delay profiling step is currently used to calculate the place delay matrix lookup.
    Values between 1 and 2 are resonable; higher values trade some quality for reduced run-time.

    **Default:** ``1.2``

.. option:: --max_criticality <float>

    Sets the maximum fraction of routing cost that can come from delay (vs. coming from routability) for any net.

    A value of 0 means no attention is paid to delay; a value of 1 means nets on the critical path pay no attention to congestion.

    **Default:** ``0.99``

.. option:: --criticality_exp <float>

    Controls the delay - routability tradeoff for nets as a function of their slack.

    If this value is 0, all nets are treated the same, regardless of their slack.
    If it is very large, only nets on the critical path will be routed with attention paid to delay. Other values produce more moderate tradeoffs.

    **Default:** ``1.0``

.. option:: --router_init_wirelength_abort_threshold <float>

    The first routing iteration wirelength abort threshold.
    If the first routing iteration uses more than this fraction of available wirelength routing is aborted.

    **Default:** ``0.85``

.. option:: --incremental_reroute_delay_ripup {on | off | auto}

    Controls whether incremental net routing will rip-up (and re-route) a critical connection for delay, even if the routing is legal.
    ``auto`` enables delay-based rip-up unless routability becomes a concern.

    **Default:** ``auto``

.. option:: --routing_failure_predictor {safe | aggressive | off}

    Controls how aggressive the router is at predicting when it will not be able to route successfully, and giving up early.
    Using this option can significantly reduce the runtime of a binary search for the minimum channel width.

    ``safe`` only declares failure when it is extremely unlikely a routing will succeed, given the amount of congestion existing in the design.

    ``aggressive`` can further reduce the CPU time for a binary search for the minimum channel width but can increase the minimum channel width by giving up on some routings that would succeed.

    ``off`` disables this feature, which can be useful if you suspect the predictor is declaring routing failure too quickly on your architecture.

    .. seealso:: :option:`--verify_binary_search`

    **Default:** ``safe``

.. option:: --routing_budgets_algorithm { disable | minimax | yoyo | scale_delay }

    .. warning:: Experimental

    Controls how the routing budgets are created. Routing budgets are used to guid VPR's routing algorithm to consider both short path and long path timing constraints :cite:`RCV_algorithm`.

    ``disable`` is used to disable the budget feature. This uses the default VPR and ignores hold time constraints.

    ``minimax`` sets the minimum and maximum budgets by distributing the long path and short path slacks depending on the the current delay values. This uses the Minimax-PERT algorithm :cite:`minimax_pert`.

    ``yoyo`` allocates budgets using minimax algorithm (as above), and enables hold slack resolution in the router using the Routing Cost Valleys (RCV) algorithm :cite:`RCV_algorithm`.

    ``scale_delay`` has the minimum budgets set to 0 and the maximum budgets is set to the delay of a net scaled by the pin criticality (net delay/pin criticality).

    **Default:** ``disable``

.. option:: --save_routing_per_iteration {on | off}

    Controls whether VPR saves the current routing to a file after each routing iteration.
    May be helpful for debugging.

    **Default:** ``off``

.. option:: --congested_routing_iteration_threshold CONGESTED_ROUTING_ITERATION_THRESHOLD

    Controls when the router enters a high effort mode to resolve lingering routing congestion.
    Value is the fraction of max_router_iterations beyond which the routing is deemed congested.

    **Default:** ``1.0`` (never)

.. option:: --route_bb_update {static, dynamic}

    Controls how the router's net bounding boxes are updated:

     * ``static`` : bounding boxes are never updated
     * ``dynamic``: bounding boxes are updated dynamically as routing progresses (may improve routability of congested designs)

     **Default:** ``dynamic``

.. option:: --router_high_fanout_threshold ROUTER_HIGH_FANOUT_THRESHOLD

    Specifies the net fanout beyond which a net is considered high fanout.
    Values less than zero disable special behaviour for high fanout nets.

    **Default:** ``64``

.. option:: --router_lookahead {classic, map}

    Controls what lookahead the router uses to calculate cost of completing a connection.

     * ``classic``: The classic VPR lookahead
     * ``map``: A more advanced lookahead which accounts for diverse wire types and their connectivity

     **Default:** ``map``

.. option:: --router_max_convergence_count <float>

    Controls how many times the router is allowed to converge to a legal routing before halting.
    If multiple legal solutions are found the best quality implementation is used.

    **Default:** ``1``

.. option:: --router_reconvergence_cpd_threshold <float>

    Specifies the minimum potential CPD improvement for which the router will continue to attempt re-convergent routing.

    For example, a value of 0.99 means the router will not give up on reconvergent routing if it thinks a > 1% CPD reduction is possible.

     **Default:** ``0.99``

.. option:: --router_initial_timing {all_critical | lookahead}

    Controls how criticality is determined at the start of the first routing iteration.

     * ``all_critical``: All connections are considered timing critical.
     * ``lookahead``: Connection criticalities are determined from timing analysis assuming (best-case) connection delays as estimated by the router's lookahead.

     **Default:** ``all_critical`` for the classic :option:`--router_lookahead`, otherwise ``lookahead``

.. option:: --router_update_lower_bound_delays {on | off}

    Controls whether the router updates lower bound connection delays after the 1st routing iteration.

    **Default:** ``on``

.. option:: --router_first_iter_timing_report <file>

    Name of the timing report file to generate after the first routing iteration completes (not generated if unspecfied).

.. option:: --router_debug_net <int>

    .. note:: This option is likely only of interest to developers debugging the routing algorithm

    Controls which net the router produces detailed debug information for.

    * For values >= 0, the value is the net ID for which detailed router debug information should be produced.
    * For value == -1, detailed router debug information is produced for all nets.
    * For values < -1, no router debug output is produced.

    .. warning:: VPR must have been compiled with `VTR_ENABLE_DEBUG_LOGGING` on to get any debug output from this option.

    **Default:** ``-2``

.. option:: --router_debug_sink_rr ROUTER_DEBUG_SINK_RR

    .. note:: This option is likely only of interest to developers debugging the routing algorithm

    Controls when router debugging is enabled for the specified sink RR.

     * For values >= 0, the value is taken as the sink RR Node ID for which to enable router debug output.
     * For values < 0, sink-based router debug output is disabled.

    .. warning:: VPR must have been compiled with `VTR_ENABLE_DEBUG_LOGGING` on to get any debug output from this option.

    **Default:** ``-2``

.. _analysis_options:

Analysis Options
^^^^^^^^^^^^^^^^

.. option:: --full_stats

    Print out some extra statistics about the circuit and its routing useful for wireability analysis.

    **Default:** off

.. option:: --gen_post_synthesis_netlist { on | off }

    Generates the Verilog and SDF files for the post-synthesized circuit.
    The Verilog file can be used to perform functional simulation and the SDF file enables timing simulation of the post-synthesized circuit.

    The Verilog file contains instantiated modules of the primitives in the circuit.
    Currently VPR can generate Verilog files for circuits that only contain LUTs, Flip Flops, IOs, Multipliers, and BRAMs.
    The Verilog description of these primitives are in the primitives.v file.
    To simulate the post-synthesized circuit, one must include the generated Verilog file and also the primitives.v Verilog file, in the simulation directory.

    .. seealso:: :ref:`timing_simulation_tutorial`

    If one wants to generate the post-synthesized Verilog file of a circuit that contains a primitive other than those mentioned above, he/she should contact the VTR team to have the source code updated.
    Furthermore to perform simulation on that circuit the Verilog description of that new primitive must be appended to the primitives.v file as a separate module.

    **Default:** ``off``

.. option:: --gen_post_implementation_merged_netlist { on | off }

    This option is based on ``--gen_post_synthesis_netlist``.
    The difference is that ``--gen_post_implementation_merged_netlist`` generates a single verilog file with merged top module multi-bit ports of the implemented circuit.
    The name of the file is ``<basename>_merged_post_implementation.v``

    **Default:** ``off``

.. option:: --post_synth_netlist_unconn_inputs { unconnected | nets | gnd | vcc }

    Controls how unconnected input cell ports are handled in the post-synthesis netlist

     * unconnected: leave unconnected
     * nets: connect each unconnected input pin to its own separate undriven net named: ``__vpr__unconn<ID>``, where ``<ID>`` is index assigned to this occurrence of unconnected port in design
     * gnd: tie all to ground (``1'b0``)
     * vcc: tie all to VCC (``1'b1``)

    **Default:** ``unconnected``

.. option:: --post_synth_netlist_unconn_outputs { unconnected | nets }

    Controls how unconnected output cell ports are handled in the post-synthesis netlist

     * unconnected: leave unconnected
     * nets: connect each unconnected output pin to its own separate undriven net named: ``__vpr__unconn<ID>``, where ``<ID>`` is index assigned to this occurrence of unconnected port in design

    **Default:** ``unconnected``

.. option:: --timing_report_npaths <int>

    Controls how many timing paths are reported.

    .. note:: The number of paths reported may be less than the specified value, if the circuit has fewer paths.

    **Default:** ``100``

.. option:: --timing_report_detail { netlist | aggregated | detailed }

    Controls the level of detail included in generated timing reports.

    We obtained the following results using the k6_frac_N10_frac_chain_mem32K_40nm.xml architecture and multiclock.blif circuit.

        * ``netlist``: Timing reports show only netlist primitive pins.

          For example:

            .. code-block:: none

                #Path 2
                Startpoint: FFC.Q[0] (.latch clocked by clk)
                Endpoint  : out:out1.outpad[0] (.output clocked by virtual_io_clock)
                Path Type : setup

                Point                                                             Incr      Path
                --------------------------------------------------------------------------------
                clock clk (rise edge)                                            0.000     0.000
                clock source latency                                             0.000     0.000
                clk.inpad[0] (.input)                                            0.000     0.000
                FFC.clk[0] (.latch)                                              0.042     0.042
                FFC.Q[0] (.latch) [clock-to-output]                              0.124     0.166
                out:out1.outpad[0] (.output)                                     0.550     0.717
                data arrival time                                                          0.717

                clock virtual_io_clock (rise edge)                               0.000     0.000
                clock source latency                                             0.000     0.000
                clock uncertainty                                                0.000     0.000
                output external delay                                            0.000     0.000
                data required time                                                         0.000
                --------------------------------------------------------------------------------
                data required time                                                         0.000
                data arrival time                                                         -0.717
                --------------------------------------------------------------------------------
                slack (VIOLATED)                                                          -0.717


        * ``aggregated``: Timing reports show netlist pins, and an aggregated summary of intra-block and inter-block routing delays.

          For example:

            .. code-block:: none

                #Path 2
                Startpoint: FFC.Q[0] (.latch at (3,3) clocked by clk)
                Endpoint  : out:out1.outpad[0] (.output at (3,4) clocked by virtual_io_clock)
                Path Type : setup

                Point                                                             Incr      Path
                --------------------------------------------------------------------------------
                clock clk (rise edge)                                            0.000     0.000
                clock source latency                                             0.000     0.000
                clk.inpad[0] (.input at (4,2))                                   0.000     0.000
                | (intra 'io' routing)                                           0.042     0.042
                | (inter-block routing)                                          0.000     0.042
                | (intra 'clb' routing)                                          0.000     0.042
                FFC.clk[0] (.latch at (3,3))                                     0.000     0.042
                | (primitive '.latch' Tcq_max)                                   0.124     0.166
                FFC.Q[0] (.latch at (3,3)) [clock-to-output]                     0.000     0.166
                | (intra 'clb' routing)                                          0.045     0.211
                | (inter-block routing)                                          0.491     0.703
                | (intra 'io' routing)                                           0.014     0.717
                out:out1.outpad[0] (.output at (3,4))                            0.000     0.717
                data arrival time                                                          0.717

                clock virtual_io_clock (rise edge)                               0.000     0.000
                clock source latency                                             0.000     0.000
                clock uncertainty                                                0.000     0.000
                output external delay                                            0.000     0.000
                data required time                                                         0.000
                --------------------------------------------------------------------------------
                data required time                                                         0.000
                data arrival time                                                         -0.717
                --------------------------------------------------------------------------------
                slack (VIOLATED)                                                          -0.717

            where each line prefixed with ``|`` (pipe character) represent a sub-delay of an edge within the timing graph.

            For instance:

            .. code-block:: none

                FFC.Q[0] (.latch at (3,3)) [clock-to-output]                     0.000     0.166
                | (intra 'clb' routing)                                          0.045     0.211
                | (inter-block routing)                                          0.491     0.703
                | (intra 'io' routing)                                           0.014     0.717
                out:out1.outpad[0] (.output at (3,4))                            0.000     0.717

            indicates that between the netlist pins ``FFC.Q[0]`` and ``out:out1.outpad[0]`` there are delays of:

              * ``45`` ps from the ``.latch`` output pin to an output pin of a ``clb`` block,
              * ``491`` ps through the general inter-block routing fabric, and
              * ``14`` ps from the input pin of a ``io`` block to ``.output``.

            Also note that a connection between two pins can be contained within the same ``clb`` block, and does not use the general inter-block routing network. As an example from a completely different circuit-architecture pair:

            .. code-block:: none

                n1168.out[0] (.names)                                            0.000     0.902
                | (intra 'clb' routing)                                          0.000     0.902
                top^finish_FF_NODE.D[0] (.latch)                                 0.000     0.902

        * ``detailed``: Like ``aggregated``, the timing reports show netlist pins, and an aggregated summary of intra-block. In addition, it includes a detailed breakdown of the inter-block routing delays.

          It is important to note that detailed timing report can only list the components of a non-global
          net, otherwise, it reports inter-block routing as well as an incremental delay of 0, just as in the
          aggregated and netlist reports.


          For example:

            .. code-block:: none

                #Path 2
                Startpoint: FFC.Q[0] (.latch at (3,3) clocked by clk)
                Endpoint  : out:out1.outpad[0] (.output at (3,4) clocked by virtual_io_clock)
                Path Type : setup

                Point                                                             Incr      Path
                --------------------------------------------------------------------------------
                clock clk (rise edge)                                            0.000     0.000
                clock source latency                                             0.000     0.000
                clk.inpad[0] (.input at (4,2))                                   0.000     0.000
                | (intra 'io' routing)                                           0.042     0.042
                | (inter-block routing:global net)                               0.000     0.042
                | (intra 'clb' routing)                                          0.000     0.042
                FFC.clk[0] (.latch at (3,3))                                     0.000     0.042
                | (primitive '.latch' Tcq_max)                                   0.124     0.166
                FFC.Q[0] (.latch at (3,3)) [clock-to-output]                     0.000     0.166
                | (intra 'clb' routing)                                          0.045     0.211
                | (OPIN:1479 side:TOP (3,3))                                     0.000     0.211
                | (CHANX:2073 unnamed_segment_0 length:1 (3,3)->(2,3))           0.095     0.306
                | (CHANY:2139 unnamed_segment_0 length:0 (1,3)->(1,3))           0.075     0.382
                | (CHANX:2040 unnamed_segment_0 length:1 (2,2)->(3,2))           0.095     0.476
                | (CHANY:2166 unnamed_segment_0 length:0 (2,3)->(2,3))           0.076     0.552
                | (CHANX:2076 unnamed_segment_0 length:0 (3,3)->(3,3))           0.078     0.630
                | (IPIN:1532 side:BOTTOM (3,4))                                  0.072     0.703
                | (intra 'io' routing)                                           0.014     0.717
                out:out1.outpad[0] (.output at (3,4))                            0.000     0.717
                data arrival time                                                          0.717

                clock virtual_io_clock (rise edge)                               0.000     0.000
                clock source latency                                             0.000     0.000
                clock uncertainty                                                0.000     0.000
                output external delay                                            0.000     0.000
                data required time                                                         0.000
                --------------------------------------------------------------------------------
                data required time                                                         0.000
                data arrival time                                                         -0.717
                --------------------------------------------------------------------------------
                slack (VIOLATED)                                                          -0.717

            where each line prefixed with ``|`` (pipe character) represent a sub-delay of an edge within the timing graph.
            In the detailed mode, the inter-block routing has now been replaced by the net components.

            For OPINS and IPINS, this is the format of the name:
            | (``ROUTING_RESOURCE_NODE_TYPE:ROUTING_RESOURCE_NODE_ID`` ``side:SIDE`` ``(START_COORDINATES)->(END_COORDINATES)``)

            For CHANX and CHANY, this is the format of the name:
            | (``ROUTING_RESOURCE_NODE_TYPE:ROUTING_RESOURCE_NODE_ID`` ``SEGMENT_NAME`` ``length:LENGTH`` ``(START_COORDINATES)->(END_COORDINATES)``)

            Here is an example of the breakdown:

            .. code-block:: none

                FFC.Q[0] (.latch at (3,3)) [clock-to-output]                     0.000     0.166
                | (intra 'clb' routing)                                          0.045     0.211
                | (OPIN:1479 side:TOP (3,3))                                     0.000     0.211
                | (CHANX:2073 unnamed_segment_0 length:1 (3,3)->(2,3))           0.095     0.306
                | (CHANY:2139 unnamed_segment_0 length:0 (1,3)->(1,3))           0.075     0.382
                | (CHANX:2040 unnamed_segment_0 length:1 (2,2)->(3,2))           0.095     0.476
                | (CHANY:2166 unnamed_segment_0 length:0 (2,3)->(2,3))           0.076     0.552
                | (CHANX:2076 unnamed_segment_0 length:0 (3,3)->(3,3))           0.078     0.630
                | (IPIN:1532 side:BOTTOM (3,4))                                  0.072     0.703
                | (intra 'io' routing)                                           0.014     0.717
                out:out1.outpad[0] (.output at (3,4))                            0.000     0.717

            indicates that between the netlist pins ``FFC.Q[0]`` and ``out:out1.outpad[0]`` there are delays of:

              * ``45`` ps from the ``.latch`` output pin to an output pin of a ``clb`` block,
              * ``0`` ps from the ``clb`` output pin to the ``CHANX:2073`` wire,
              * ``95`` ps from the ``CHANX:2073`` to the ``CHANY:2139`` wire,
              * ``75`` ps from the ``CHANY:2139`` to the ``CHANX:2040`` wore,
              * ``95`` ps from the ``CHANX:2040`` to the ``CHANY:2166`` wire,
              * ``76`` ps from the ``CHANY:2166`` to the ``CHANX:2076`` wire,
              * ``78`` ps from the ``CHANX:2076`` to the input pin of a ``io`` block,
              * ``14`` ps input pin of a ``io`` block to ``.output``.

            In the initial description we referred to the existence of global nets, which also occur in this net:

            .. code-block:: none

                clk.inpad[0] (.input at (4,2))                                   0.000     0.000
                | (intra 'io' routing)                                           0.042     0.042
                | (inter-block routing:global net)                               0.000     0.042
                | (intra 'clb' routing)                                          0.000     0.042
                FFC.clk[0] (.latch at (3,3))                                     0.000     0.042

            Global nets are unrouted nets, and their route trees happen to be null.

            Finally, is interesting to note that the consecutive channel components may not seem to connect. There are two types of occurences:

            1. The preceding channel's ending coordinates extend past the following channel's starting coordinates (example from a different path):

            .. code-block:: none

                | (chany:2113 unnamed_segment_0 length:2 (1, 3) -> (1, 1))       0.116     0.405
                | (chanx:2027 unnamed_segment_0 length:0 (1, 2) -> (1, 2))       0.078     0.482

            It is possible that by opening a switch between (1,2) to (1,1), CHANY:2113 actually only extends from (1,3) to (1,2).

            1. The preceding channel's ending coordinates have no relation to the following channel's starting coordinates.
               There is no logical contradiction, but for clarification, it is best to see an explanation of the VPR coordinate system.
               The path can also be visualized by VPR graphics, as an illustration of this point:

            .. _fig_path_2:

            .. figure:: path_2.*

             Illustration of Path #2 with insight into the coordinate system.

            :numref:`fig_path_2` shows the routing resources used in Path #2 and their locations on the FPGA.

            1. The signal emerges from near the top-right corner of the block to_FFC (OPIN:1479)  and joins the topmost horizontal segment of length 1 (CHANX:2073).

            2. The signal proceeds to the left, then connects to the outermost, blue vertical segment of length 0 (CHANY:2139).

            3. The signal continues downward and attaches to the horizontal segment of length 1 (CHANX:2040).

            4. Of the aforementioned horizontal segment, after travelling one linear unit to the right, the signal jumps on a vertical segment of length 0 (CHANY:2166).

            5. The signal travels upward and promptly connects to a horizontal segment of length 0 (CHANX:2076).

            6. This segment connects to the green destination io (3,4).

        * ``debug``: Like ``detailed``, but includes additional VPR internal debug information such as timing graph node IDs (``tnode``) and routing SOURCE/SINK nodes.

    **Default:** ``netlist``

.. option:: --echo_dot_timing_graph_node { string | int }

    Controls what subset of the timing graph is echoed to a GraphViz DOT file when :option:`vpr --echo_file` is enabled.

    Value can be a string (corresponding to a VPR atom netlist pin name), or an integer representing a timing graph node ID.
    Negative values mean the entire timing graph is dumped to the DOT file.

    **Default:** ``-1``

.. option:: --timing_report_skew { on | off }

    Controls whether clock skew timing reports are generated.

    **Default:** ``off``


.. _power_estimation_options:

Power Estimation Options
^^^^^^^^^^^^^^^^^^^^^^^^
The following options are used to enable power estimation in VPR.

.. seealso:: :ref:`power_estimation` for more details.

.. option:: --power

    Enable power estimation

    **Default:** ``off``

.. option:: --tech_properties <file>

    XML File containing properties of the CMOS technology (transistor capacitances, leakage currents, etc).
    These can be found at ``$VTR_ROOT/vtr_flow/tech/``, or can be created for a user-provided SPICE technology (see :ref:`power_estimation`).

.. option:: --activity_file <file>

    File containing signal activites for all of the nets in the circuit.  The file must be in the format::

        <net name1> <signal probability> <transition density>
        <net name2> <signal probability> <transition density>
        ...

    Instructions on generating this file are provided in :ref:`power_estimation`.

Server Mode Options
^^^^^^^^^^^^^^^^^^^^^^^^

If VPR is in server mode, it listens on a socket for commands from a client. Currently, this is used to enable interactive timing analysis and visualization of timing paths in the VPR UI under the control of a separate client.

The following options are used to enable server mode in VPR.

.. seealso:: :ref:`server_mode` for more details.

.. option:: --server

    Run in server mode. Accept single client application connection and respond to client requests

    **Default:** ``off``

.. option:: --port PORT

    Server port number.

    **Default:** ``60555``

.. seealso:: :ref:`interactive_path_analysis_client`

Command-line Auto Completion
----------------------------

To simplify using VPR on the command-line you can use the ``dev/vpr_bash_completion.sh`` script, which will enable TAB completion for VPR commandline arguments (based on the output of `vpr -h`).

Simply add:

.. code-block:: bash

    source $VTR_ROOT/dev/vpr_bash_completion.sh

to your ``.bashrc``. ``$VTR_ROOT`` refers to the root of the VTR source tree on your system.



vpr/debug_aids.rst
--------------------------------------
Debugging Aids
==============

.. program:: vpr

.. note:: This section is most relevant to developers modifying VPR

The ``report_timing.setup.rpt`` file lists details about the critical path of a circuit, and is very useful for determining why your circuit is so fast or so slow.

To access detailed echo files from VPR’s operation, use the command-line option :vpr:option:`--echo_file` ``on``.
After parsing the netlist and architecture files, VPR dumps out an image of its internal data structures into echo files (typically ending in ``.echo``).
These files can be examined to be sure that VPR is parsing the input files as you expect.

You ca visualize and control the placement move generator whenever the placement engine is paused in the UI. Run with graphics and VTR_ENABLE_DEBUG_LOGGONG enabled and set a breakpoint to stop placement. The new location of the moving block for each proposed move will be highlighted with GREEN and the old location will be highlighted with GOLD. The fanin and fanout blocks will also be highlighted. The move type, move outcome and delta cost will be printed in the status bar.
.. warning:: VPR must have been compiled with `VTR_ENABLE_DEBUG_LOGGING` on to get any debug output from this flag.   

If the preprocessor flag ``DEBUG`` is defined in ``vpr_types.h``, some additional sanity checks are performed during a run.
``DEBUG`` only slows execution by 1 to 2%.
The major sanity checks are always enabled, regardless of the state of ``DEBUG``.
Finally, if ``VERBOSE`` is set in vpr_types.h, a great deal of intermediate data will be printed to the screen as VPR runs.
If you set verbose, you may want to redirect screen output to a file.

The initial and final placement costs provide useful numbers for regression testing the netlist parsers and the placer, respectively.
VPR generates and prints out a routing serial number to allow easy regression testing of the router.

Finally, if you need to route an FPGA whose routing architecture cannot be described in VPR’s architecture description file, don’t despair!
The router, graphics, sanity checker, and statistics routines all work only with a graph that defines all the available routing resources in the FPGA and the permissible connections between them.
If you change the routines that build this graph (in ``rr_graph*.c``) so that they create a graph describing your FPGA, you should be able to route your FPGA.
If you want to read a text file describing the entire routing resource graph, call the ``dump_rr_graph`` subroutine.

Placer and Router Debugger
==========================
 
.. image:: https://www.verilogtorouting.org/img/debuggerWindow.png
    :align: center

Overview
~~~~~~~~~~~~~~~~~

It can be very useful to stop the program at a significant point and evaluate the circuit at that stage. This debugger allows setting breakpoints during placement and routing using a variety of variables and operations. For example the user can stop the placer after a certain number of perturbations, temperature changes, or when a specific block is moved. It can also stop after a net is routed in the routing process and other such scenarios. There are multiple ways to set and manipulate breakpoints which are all explained in detail below.

Adding a breakpoint
~~~~~~~~~~~~~~~~~~~

Currently the user is required to have graphics on in order to set breakpoints. By clocking the “Debug” button, the debugger window opens up and from there the user can enter integer values in the entry fields and set breakpoints. A more advanced option is using expressions which allows a wider variety of settings since the user can incorporate multiple variables and use boolean operators. This option is found by clicking the “Advanced” button in the debugger window. Using an expression is more accurate than the entry fields when setting multiple breakpoints.

Enabling/Disabling a breakpoint
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Enabling and disabling breakpoints are done using the checkboxes in front of each breakpoint in the breakpoint list. The breakpoint is enabled when the box is checked and disabled otherwise.

Deleting a breakpoint
~~~~~~~~~~~~~~~~~~~~~

Deleting a breakpoint is done using the trash can button in front of each breakpoint in the breakpoint list.

Reaching a breakpoint
~~~~~~~~~~~~~~~~~~~~~

Upon reaching a breakpoint, the program will stop, notify the user which breakpoint was encountered, and give a summary of the current variable values. This information is presented through a pop-up window and printed to the terminal as well.

Available Variables
~~~~~~~~~~~~~~~~~~~

.. image:: https://verilogtorouting.org/img/advancedWindow.png
    :align: center 

You can also find the variables’ list in the Advanced Settings Window, on the left.

**Placer Variables**

  * move_num: every placer perturbation counts as a move, so the user can stop the program after a certain number of moves. This breakpoint can be enabled through the entry field on the main debugger window or using an expression. It should be noted however, that using the entry field would proceed the specified number of moves. (as in the second example)
          * Ex. move _num == 33
          * Ex. move_num += 4
  * temp_count: every time the temperature is updated it counts as an increase to temp_count. This breakpoint can be enabled through the entry field on the main debugger window or using an expression. It should be noted however, that using the entry field would proceed the specified number of temperatures. (as in the second example)
          * Ex. temp_count == 5
          * Ex. temp_count += 5
  * from_block:  in every placer move one or more blocks are relocated. from_block specifies the first block that is relocated in every move; and a breakpoint of this type stops the program when the first block moved is the one indicated by the user. This breakpoint can be enabled through the entry field on the main debugger window or using an expression.
          * Ex. from_block == 83
  * in_blocks_affected: this variable allows you to stop after your specified block was moved. Unlike "from_block" which only checks the first block relocated in every move, in_blocks_affected looks through all the blocks whose locations were changed by that move. This breakpoint can only be enabled through the use of an expression.
          * Ex. in_blocks_affected == 83
    
**Router Variables**

  * router_iter: Every pass through the whole netlist (with each unrouted or poorly routed net being re-routed) counts as a router iteration. This breakpoint can be enabled through the entry field on the main debugger window or using an expression.
          * Ex. router_iter == 2
  * route_net_id: stops after the specified net is rerouted. This breakpoint can be enabled through the entry field on the main debugger window or using an expression.
          * route_net_id == 12
    
Available Operators
~~~~~~~~~~~~~~~~~~~

  * ==
          * Ex. temp_count == 2
  * >
          * Ex. move_num > 94
  * <
          * Ex. move_num < 94
  * >=
          * Ex. router_iter >=2
  * <=
          * Ex. router_iter <=2
  * &&
          * Ex. from_block == 83 && move_num > 72
  * ||
          * Ex. in_blocks_affected == 11 || temp_count == 9
  * +=
          * Ex. move_num += 8



vpr/dusty_sa.rst
--------------------------------------
.. _dusty_sa:

Dusty's Simulated Annealing Schedule
====================================

This simulated annealing schedule is designed to quickly characterize the search space and maintain a target success ratio (accepted moves.)

It starts at the minimum alpha (``--alpha_min``) to allow it to quickly find the target.

For each alpha, the temperature decays by a factor of alpha after each outer loop iteration.

The temperature before which the success ratio drops below the target (``--anneal_success_target``) is recorded; after hitting the minimum success ratio (``--anneal_success_min``), the temperature resets to a little before recorded temperature, and alpha parameter itself decays according to ``--alpha_decay``.

The effect of this is many fast, but slowing sweeps in temperature, focused where they can make the most effective progress. Unlike fixed and adaptive schedules that monotonically decrease temperature, this allows the global properties of the search space to affect the schedule.

In addition, move_lim (which controls the number of iterations in the inner loop) is scaled with the target success ratio over the current success ratio, which reduces the time to reach the target ratio.

The schedule terminates when the maximum alpha (``--alpha_max``) is reached. Termination is ensured by the narrowing range between the recorded upper temperature and the minimum success ratio, which will eventually cause alpha to reach its maximum.

This algorithm was inspired by Lester Ingber's adaptive simulated annealing algorithm [ASA93]_.

See ``update_state()`` in ``place.cpp`` for the algorithm details.

.. [ASA93] Ingber, Lester. "Adaptive simulated annealing (ASA)." Global optimization C-code, Caltech Alumni Association, Pasadena, CA (1993).



vpr/file_formats.rst
--------------------------------------
.. _vpr_file_formats:

File Formats
============
VPR consumes and produces several files representing the packing, placement, and routing results.

FPGA Architecture (.xml)
--------------------------
The target FPGA architecture is specified as an architecture file.
For details of this file format see :ref:`fpga_architecture_description`.

.. _blif_format:
.. _vpr_blif_file:

BLIF Netlist (.blif)
--------------------------
The technology mapped circuit to be implement on the target FPGA is specified as a Berkely Logic Interchange Format (BLIF) netlist.
The netlist must be flattened and consist of only primitives (e.g. ``.names``, ``.latch``, ``.subckt``).

For a detailed description of the BLIF file format see the :download:`BLIF Format Description <../../../libs/EXTERNAL/libblifparse/doc/blif.pdf>`.

Note that VPR supports only the structural subset of BLIF, and does not support the following BLIF features:

 * Subfile References (``.search``).
 * Finite State Machine Descriptions (``.start_kiss``, ``.end_kiss`` etc.).
 * Clock Constraints (``.cycle``, ``.clock_event``).
 * Delay Constraints (``.delay`` etc.).

Clock and delay constraints can be specified with an :ref:`SDC File <vpr_sdc_file>`.

.. note:: By default VPR assumes file with ``.blif`` are in structural BLIF format. The format can be controlled with :option:`vpr --circuit_format`.

Black Box Primitives
~~~~~~~~~~~~~~~~~~~~
Black-box architectural primitives (RAMs, Multipliers etc.) should be instantiated in the netlist using BLIF's ``.subckt`` directive.
The BLIF file should also contain a black-box ``.model`` definition which defines the input and outputs of each ``.subckt`` type.

VPR will check that blackbox ``.model``\s are consistent with the :ref:`<models> section <arch_models>` of the architecture file.

Unconnected Primitive Pins
~~~~~~~~~~~~~~~~~~~~~~~~~~
Unconnected primitive pins can be specified through several methods.

#. The ``unconn`` net (input pins only).

    VPR treats any **input pin** connected to a net named ``unconn`` as disconnected.

    For example:

    .. code-block:: none

        .names unconn out
        0 1

    specifies an inverter with no connected input.

    .. note:: ``unconn`` should only be used for **input pins**. It may cause name conflicts and create multi-driven nets if used with output pins.

#. Implicitly disconnected ``.subckt`` pins.

    For ``.subckt`` instantiations VPR treats unlisted primitive pins as implicitly disconnected.
    This works for both input and output pins.

    For example the following ``.subckt`` instantiations are equivalent:

    .. code-block:: none

        .subckt single_port_ram \
            clk=top^clk \
            data=top.memory_controller+memtroll^MULTI_PORT_MUX~8^MUX_2~546 \
            addr[0]=top.memory_controller+memtroll^MULTI_PORT_MUX~8^MUX_2~541 \
            addr[1]=top.memory_controller+memtroll^MULTI_PORT_MUX~8^MUX_2~542 \
            addr[2]=top.memory_controller+memtroll^MULTI_PORT_MUX~8^MUX_2~543 \
            addr[3]=top.memory_controller+memtroll^MULTI_PORT_MUX~8^MUX_2~544 \
            addr[4]=top.memory_controller+memtroll^MULTI_PORT_MUX~8^MUX_2~545 \
            addr[5]=unconn \
            addr[6]=unconn \
            addr[7]=unconn \
            addr[8]=unconn \
            addr[9]=unconn \
            addr[10]=unconn \
            addr[11]=unconn \
            addr[12]=unconn \
            addr[13]=unconn \
            addr[14]=unconn \
            we=top.memory_controller+memtroll^MULTI_PORT_MUX~8^MUX_2~554 \
            out=top.memory_controller+memtroll.single_port_ram+str^out~0

    .. code-block:: none

        .subckt single_port_ram \
            clk=top^clk \
            data=top.memory_controller+memtroll^MULTI_PORT_MUX~8^MUX_2~546 \
            addr[0]=top.memory_controller+memtroll^MULTI_PORT_MUX~8^MUX_2~541 \
            addr[1]=top.memory_controller+memtroll^MULTI_PORT_MUX~8^MUX_2~542 \
            addr[2]=top.memory_controller+memtroll^MULTI_PORT_MUX~8^MUX_2~543 \
            addr[3]=top.memory_controller+memtroll^MULTI_PORT_MUX~8^MUX_2~544 \
            addr[4]=top.memory_controller+memtroll^MULTI_PORT_MUX~8^MUX_2~545 \
            we=top.memory_controller+memtroll^MULTI_PORT_MUX~8^MUX_2~554 \
            out=top.memory_controller+memtroll.single_port_ram+str^out~0


#. Dummy nets with no sinks (output pins only)

    By default VPR sweeps away nets with no sinks (see :option:`vpr --sweep_dangling_nets`). As a result output pins can be left 'disconnected' by connecting them to dummy nets.

    For example:

    .. code-block:: none

        .names in dummy_net1
        0 1

    specifies an inverter with no connected output (provided ``dummy_net1`` is connected to no other pins).

    .. note:: This method requires that every disconnected output pin should be connected to a **uniquely named** dummy net.

BLIF File Format Example
~~~~~~~~~~~~~~~~~~~~~~~~
The following is an example BLIF file. It implements a 4-bit ripple-carry ``adder`` and some simple logic.

The main ``.model`` is named ``top``, and its input and output pins are listed using the ``.inputs`` and ``.outputs`` directives.

The 4-bit ripple-cary adder is built of 1-bit ``adder`` primitives which are instantiated using the ``.subckt`` directive.
Note that the adder primitive is defined as its own ``.model`` (which describes its pins), and is marked as ``.blackbox`` to indicate it is an architectural primitive.

The signal ``all_sum_high_comb`` is computed using combinational logic (``.names``) which ANDs all the sum bits together.

The ``.latch`` directive instantiates a rising-edge (``re``) latch (i.e. an edge-triggered Flip-Flop) clocked by ``clk``.
It takes in the combinational signal ``all_sum_high_comb`` and drives the primary output ``all_sum_high_reg``.

Also note that the last ``.subckt adder`` has it's ``cout`` output left implicitly disconnected.

.. code-block:: none

    .model top
    .inputs clk a[0] a[1] a[2] a[3] b[0] b[1] b[2] b[3]
    .outputs sum[0] sum[1] sum[2] sum[3] cout all_sum_high_reg

    .names gnd
     0

    .subckt adder a=a[0] b=b[0] cin=gnd    cout=cin[1]     sumout=sum[0]
    .subckt adder a=a[1] b=b[1] cin=cin[1] cout=cin[2]     sumout=sum[1]
    .subckt adder a=a[2] b=b[2] cin=cin[2] cout=cin[3]     sumout=sum[2]
    .subckt adder a=a[3] b=b[3] cin=cin[3]                 sumout=sum[3]

    .names sum[0] sum[1] sum[2] sum[3] all_sum_high_comb
    1111 1

    .latch all_sum_high_comb all_sum_high_reg re clk  0

    .end


    .model adder
    .inputs a b cin
    .outputs cout sumout
    .blackbox
    .end

.. _vpr_blif_naming_convention:

BLIF Naming Convention
~~~~~~~~~~~~~~~~~~~~~~
VPR follows a naming convention to refer to primitives and pins in the BLIF netlist.
These names appear in the :ref:`VPR GUI <vpr_graphics>`, in log and error messages, and in can be used elsewhere (e.g. in :ref:`SDC constraints <sdc_commands>`).

.. _vpr_blif_naming_convention_nets:

Net Names
^^^^^^^^^
The BLIF format uses explicit names to refer to nets.
These names are used directly as is by VPR (although some nets may be merged/removed by :ref:`netlist cleaning <netlist_options>`).

For example, the following netlist:

.. code-block:: none

    .model top
    .inputs a b
    .outputs c

    .names a b c
    11 1

    .end

contains nets named:

- ``a``
- ``b``
- ``c``

.. _vpr_blif_naming_convention_primitives:

Primitive Names
^^^^^^^^^^^^^^^
The standard BLIF format has no mechanism for specifying the names of primitives (e.g. ``.names``/``.latch``/``.subckt``).
As a result, tools processing BLIF follow a naming convention which generates unique names for each netlist primitive.

The VPR primitive naming convention is as follows:

+---------------+--------------------------+-------------------------+
| Primitive     | Drives at least one net? | Primitive Name          |
+===============+==========================+=========================+
| - ``.input``  | Yes                      | Name of first           |
| - ``.names``  |                          | driven net              |
| - ``.latch``  +--------------------------+-------------------------+
| - ``.subckt`` | No                       | Arbitrarily             |
|               |                          | generated (e.g.         |
|               |                          | ``unamed_instances_K``) |
+---------------+--------------------------+-------------------------+
| - ``.output`` | N/A                      | .output name            |
|               |                          | prefixed with           |
|               |                          | ``out:``                |
+---------------+--------------------------+-------------------------+

which ensures each netlist primitive is given a unique name.

For example, in the following:

.. code-block:: none

    .model top
    .inputs a b x y z clk
    .outputs c c_reg cout[0] sum[0]

    .names a b c
    11 1

    .latch c c_reg re clk 0

    .subckt adder a=x b=y cin=z cout=cout[0] sumout=sum[0]

    .end

    .model adder
    .inputs a b cin
    .outputs cout sumout
    .blackbox
    .end

- The circuit primary inputs (``.inputs``) are named: ``a``, ``b``, ``x``, ``y``, ``z``, ``clk``,
- The 2-LUT (``.names``) is named ``c``,
- The FF (``.latch``) is named ``c_reg``,
- The ``adder`` (``.subckt``) is named ``cout[0]`` (the name of the first net it drives), and
- The circuit primary outputs (``.outputs``) are named: ``out:c``, ``out:c_reg``, ``out:cout[0]``, ``out:sum[0]``.

.. seealso:: EBLIF's :ref:`.cname <vpr_eblif_cname>` extension, which allows explicit primitive names to be specified.


.. _vpr_blif_naming_convention_pins:

Pin Names
^^^^^^^^^
It is useful to be able to refer to particular pins in the netlist.
VPR uses the convention: ``<primitive_instance_name>.<pin_name>``.
Where ``<primitive_instance_name>`` is replaced with the netlist primitive name, and ``<pin_name>`` is the name of the relevant pin.

For example, the following ``adder``:

.. code-block:: none

    .subckt adder a=x b=y cin=z cout=cout[0] sumout=sum[0]

which has pin names:

- ``cout[0].a[0]`` (driven by net ``x``)
- ``cout[0].b[0]`` (driven by net ``y``)
- ``cout[0].cin[0]`` (driven by net ``z``)
- ``cout[0].cout[0]`` (drives net ``cout[0]``)
- ``cout[0].sumout[0]`` (drives net ``sum[0]``)

Since the primitive instance itself is named ``cout[0]`` :ref:`by convention <vpr_blif_naming_convention_primitives>`.


Built-in Primitive Pin Names
""""""""""""""""""""""""""""
The built-in primitives in BLIF (``.names``, ``.latch``) do not explicitly list the names of their input/output pins.
VPR uses the following convention:

+------------+---------+---------+
| Primitive  | Port    | Name    |
+============+=========+=========+
| ``.names`` | input   | ``in``  |
|            +---------+---------+
|            | output  | ``out`` |
+------------+---------+---------+
| ``.latch`` | input   | ``D``   |
|            +---------+---------+
|            | output  | ``Q``   |
|            +---------+---------+
|            | control | ``clk`` |
+------------+---------+---------+


Consider the following:

.. code-block:: none

    .names a b c d e f
    11111 1

    .latch g h re clk 0

The ``.names``' pin names are:

- ``f.in[0]`` (driven by net ``a``)
- ``f.in[1]`` (driven by net ``b``)
- ``f.in[2]`` (driven by net ``c``)
- ``f.in[3]`` (driven by net ``d``)
- ``f.in[4]`` (driven by net ``e``)
- ``f.out[0]`` (drives net ``f``)

and the ``.latch`` pin names are:

- ``h.D[0]`` (driven by net ``g``)
- ``h.Q[0]`` (drives net ``h``)
- ``h.clk[0]`` (driven by net ``clk``)

since the ``.names`` and ``.latch`` primitives are named ``f`` and ``h`` :ref:`by convention <vpr_blif_naming_convention_primitives>`.

.. note:: To support pins within multi-bit ports unambiguously, the bit index of the pin within its associated port is included in the pin name (for single-bit ports this will always be ``[0]``).

.. _vpr_eblif_file:

Extended BLIF (.eblif)
----------------------
VPR also supports several extentions to :ref:`structural BLIF <vpr_blif_file>` to address some of its limitations.

.. note:: By default VPR assumes file with ``.eblif`` are in extneded BLIF format. The format can be controlled with :option:`vpr --circuit_format`.

.conn
~~~~~
The ``.conn`` statement allows direct connections between two wires.

For example:

.. code-block:: none

    .model top
    .input a
    .output b

    #Direct connection
    .conn a b

    .end

specifies that 'a' and 'b' are direct connected together.
This is analogous to Verilog's ``assign b = a;``.

This avoids the insertion of a ``.names`` buffer which is required in standard BLIF, for example:

.. code-block:: none

    .model top
    .input a
    .output b

    #Buffer LUT required in standard BLIF
    .names a b
    1 1

    .end


.. _vpr_eblif_cname:

.cname
~~~~~~
The ``.cname`` statement allows names to be specified for BLIF primitives (e.g. ``.latch``, ``.names``, ``.subckt``).


.. note:: ``.cname`` statements apply to the previous primitive instantiation.

For example:

.. code-block:: none

    .names a b c
    11 1
    .cname my_and_gate

Would name of the above ``.names`` instance ``my_and_gate``.

.param
~~~~~~
The ``.param`` statement allows parameters (e.g. primitive modes) to be tagged on BLIF primitives.

.. note:: ``.param`` statements apply to the previous primitive instantiation.

Parameters can have one of the three available types. Type is inferred from the format in which a parameter is provided.

 * **string**
    Whenever a parameter value is quoted it is considered to be a string. BLIF parser does not allow escaped characters hence those are illegal and will cause syntax errors.

 * **binary word**
    Binary words are specified using strings of characters ``0`` and ``1``. No other characters are allowed. Number of characters denotes the word length.

 * **real number**
    Real numbers are stored as decimals where the dot ``.`` character separates the integer and fractional part. Presence of the dot character implies that the value is to be treated as a real number.

For example:

.. code-block:: none

    .subckt pll clk_in=gclk clk_out=pclk
    .param feedback "internal"
    .param multiplier 0.50
    .param power 001101

Would set the parameters ``feedback``, ``multiplier`` and ``power``  of the above ``pll`` ``.subckt`` to ``"internal"``, ``0.50`` and ``001101`` respectively.

.. warning:: Integers in notation other than binary (e.g. decimal, hexadecimal) are not supported. Occurrence of params with digits other than 1 and 0 for binary words, not quoted (strings) or not separated with dot ``.`` (real numbers) are considered to be illegal.

Interpretation of parameter values is out of scope of the BLIF format extension.

``.param`` statements propagate to ``<parameter>`` elements in the packed netlist.

Paramerer values propagate also to the post-route Verilog netlist, if it is generated. Strings and real numbers are passed directly while binary words are prepended with the ``<N>'b`` prefix where ``N`` denotes a binary word length.

.attr
~~~~~
The ``.attr`` statement allows attributes (e.g. source file/line) to be tagged on BLIF primitives.

.. note:: ``.attr`` statements apply to the previous primitive instantiation.

For example:

.. code-block:: none

    .latch a_and_b dff_q re clk 0
    .attr src my_design.v:42

Would set the attribute ``src`` of the above ``.latch`` to ``my_design.v:42``.

``.attr`` statements propagate to ``<attribute>`` elements in the packed netlist.

Extended BLIF File Format Example
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: none

    .model top
    .inputs a b clk
    .outputs o_dff

    .names a b a_and_b
    11 1
    .cname lut_a_and_b
    .param test_names_param "test_names_param_value"
    .attr test_names_attrib "test_names_param_attrib"

    .latch a_and_b dff_q re clk 0
    .cname my_dff
    .param test_latch_param "test_latch_param_value"
    .attr test_latch_attrib "test_latch_param_attrib"

    .conn dff_q o_dff

    .end

.. _vpr_sdc_file:

Timing Constraints (.sdc)
-------------------------
Timing constraints are specified using SDC syntax.
For a description of VPR's SDC support see :ref:`sdc_commands`.

.. note:: Use :option:`vpr --sdc_file` to specify the SDC file used by VPR.

Timing Constraints File Format Example
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
See :ref:`sdc_examples`.

.. _vpr_net_file:
.. _vpr_pack_file:

Packed Netlist Format (.net)
----------------------------
The circuit .net file is an xml file that describes a post-packed user circuit.
It represents the user netlist in terms of the complex logic blocks of the target architecture.
This file is generated from the packing stage and used as input to the placement stage in VPR.

The .net file is constructed hierarchically using ``block`` tags.
The top level ``block`` tag contains the I/Os and complex logic blocks used in the user circuit.
Each child ``block`` tag of this top level tag represents a single complex logic block inside the FPGA.
The ``block`` tags within a complex logic block tag describes, hierarchically, the clusters/modes/primitives used internally within that logic block.

A ``block`` tag has the following attributes:

 * ``name``
    A name to identify this component of the FPGA.
    This name can be completely arbitrary except in two situations.
    First, if this is a primitive (leaf) block that implements an atom in the input technology-mapped netlist (eg. LUT, FF, memory slice, etc), then the name of this block must match exactly with the name of the atom in that netlist so that one can later identify that mapping.
    Second, if this block is not used, then it should be named with the keyword open.
    In all other situations, the name is arbitrary.

 * ``instance``
    The phyiscal block in the FPGA architecture that the current block represents.
    Should be of format: architecture_instance_name[instance #].
    For example, the 5th index BLE in a CLB should have ``instance="ble[5]"``

 * ``mode``
    The mode the block is operating in.

A block connects to other blocks via pins which are organized based on a hierarchy.
All block tags contains the children tags: inputs, outputs, clocks.
Each of these tags in turn contain port tags.
Each port tag has an attribute name that matches with the name of a corresponding port in the FPGA architecture.
Within each port tag is a list of named connections where the first name corresponds to pin 0, the next to pin 1, and so forth.
The names of these connections use the following format:

#. Unused pins are identified with the keyword open.
#. The name of an input pin to a complex logic block is the same as the name of the net using that pin.
#. The name of an output pin of a primitve (leaf block) is the same as the name of the net using that pin.
#. The names of all other pins are specified by describing their immediate drivers.  This format is ``[name_of_immediate_driver_block].[port_name][pin#]->interconnect_name``.

For primitives with equivalent inputs VPR may rotate the input pins.
The resulting rotation is specified with the ``<port_rotation_map>`` tag.
For example, consider a netlist contains a 2-input LUT named ``c``, which is implemented in a 5-LUT:

.. code-block:: xml
    :caption: Example of ``<port_rotation_map>`` tag.
    :linenos:

    ...
    <block name="c" instance="lut[0]">
        <inputs>
            <port name="in">open open lut5.in[2]->direct:lut5  open lut5.in[4]->direct:lut5  </port>
            <port_rotation_map name="in">open open 1 open 0 </port_rotation_map>
        </inputs>
        <outputs>
            <port name="out">c </port>
        </outputs>
        <clocks>
        </clocks>
    </block>
    ...

In the original netlist the two LUT inputs were connected to pins at indicies 0 and 1 (the only input pins).
However during clustering the inputs were rotated, and those nets now connect to the pins at indicies 2 and 4 (line 4).
The ``<port_rotation_map>`` tag specified the port name it applies to (``name`` attribute), and its contents lists the pin indicies each pin in the port list is associated with in the original netlist (i.e. the pins ``lut5.in[2]->direct:lut5`` and ``lut5.in[4]->direct:lut5`` respectively correspond to indicies 1 and 0 in the original netlist).

.. note:: Use :option:`vpr --net_file` to override the default net file name.

Packing File Format Example
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following is an example of what a .net file would look like.
In this circuit there are 3 inputs (pa, pb, pc) and 4 outputs (out:pd, out:pe, out:pf, out:pg).
The io pad is set to inpad mode and is driven by the inpad:

.. code-block:: xml
    :caption: Example packed netlist file (trimmed for brevity).
    :linenos:

    <block name="b1.net" instance="FPGA_packed_netlist[0]">
        <inputs>
                pa pb pc
        </inputs>

        <outputs>
                out:pd out:pe out:pf out:pg
        </outputs>

        <clocks>
        </clocks>

        <block name="pa" instance="io[0]" mode="inpad">
                <inputs>
                        <port name="outpad">open </port>
                </inputs>

                <outputs>
                        <port name="inpad">inpad[0].inpad[0]->inpad  </port>
                </outputs>

                <clocks>
                        <port name="clock">open </port>
                </clocks>

                <block name="pa" instance="inpad[0]">
                        <inputs>
                        </inputs>

                        <outputs>
                                <port name="inpad">pa </port>
                        </outputs>

                        <clocks>
                        </clocks>

                        <attributes>
                                <attribute name="vccio">3.3</attribute>
                        </attributes>

                        <parameters>
                                <parameter name="iostandard">LVCMOS33</parameter>
                        </parameters>
                </block>
        </block>
    ...

.. note:: ``.net`` files may be outputted at two stages:
          - After packing is completed, the packing results will be outputted. The ``.net`` file can be loaded as an input for placer, router and analyzer. Note that the file may **not** represent the final packing results as the analyzer will apply synchronization between packing and routing results.
          - After analysis is completed, updated packing results will be outputted. This is due to that VPR router may swap pin mapping in packing results for optimizations. In such cases, packing results are synchronized with routing results. The outputted ``.net`` file will have a postfix of ``.post_routing`` as compared to the original packing results. It could happen that VPR router does not apply any pin swapping and the two ``.net`` files are the same. In both cases, the post-analysis ``.net`` file should be considered to be **the final packing results** for downstream tools, e.g., bitstream generator. Users may load the post-routing ``.net`` file in VPR's analysis flow to sign-off the final results.

.. warning:: Currently, the packing result synchronization is only applicable to input pins which may be remapped to different nets during routing optimization. If your architecture defines `link_instance_pin_xml_syntax_` equivalence for output pins, the packing results still mismatch the routing results!

.. _link_instance_pin_xml_syntax: https://docs.verilogtorouting.org/en/latest/arch/reference/#tag-%3Coutputname=

.. _vpr_place_file:

Placement File Format (.place)
------------------------------
The placement file format is used to specify the position of cluster-level blocks in an FPGA design. It includes information about the netlist and architecture files, the size of the logic block array, and the placement details of each block in the CLB netlist..

The first line of the placement file lists the netlist (.net) and architecture (.xml) files used to create this placement.
This information is used to ensure you are warned if you accidentally route this placement with a different architecture or netlist file later. 
The second line of the file gives the size of the logic block array used by this placement.

All subsequent lines follow this format:

    block_name    x    y    subblk    [layer_number]    [#block_number]

- **block_name**: Refers to either:
  - The name of a clustered block, as given in the input .net formatted netlist.
  - The name of a primitive within a clustered block.

- **x** and **y**: Represent the row and column in which the block is placed, respectively.

- **subblk**: Specifies which of several possible subtile locations in row **x** and column **y** contains this block, which is useful when the tile capacity is greater than 1. The subtile number should be in the range `0` to `(grid[i][j].capacity - 1)`. The subtile numbers for a particular **x, y** location do not have to be used in order.

- **layer_number**: Indicates the layer (or die) on which the block is placed. If omitted, the block is assumed to be placed on layer `0` (a single die system). In 3D FPGA architectures, multiple dies can be stacked, with the bottom die considered as layer `0`.

The placement files output by VPR also include (as a comment) an extra field: the id (number) of the block in the CLB netlist. This is the internal index used by VPR to identify a CLB level block -- it may be useful to know this index if you are modifying VPR and trying to debug something.

.. note:: The blocks in a placement file can be listed in any order.


.. note:: A `#` character on a line indicates that all text after the `#` to the end of a line is a comment.

.. _fig_fpga_coord_system:

.. figure:: fpga_coordinate_system.*

    FPGA co-ordinate system.

:numref:`fig_fpga_coord_system` shows the coordinate system used by VPR for a small 2 x 2 CLB FPGA.
The number of CLBs in the x and y directions are denoted by ``nx`` and ``ny``, respectively.
CLBs all go in the area with x between ``1`` and ``nx`` and y between ``1`` and ``ny``, inclusive.
All pads either have x equal to ``0`` or ``nx + 1`` or y equal to ``0`` or ``ny + 1``.

.. note:: Use :option:`vpr --place_file` to override the default place file name.

Placement File Format Example
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. code-block:: none
    :caption: Example 2D Placement File
    :linenos:

    Netlist file: xor5.net   Architecture file: sample.xml
    Array size: 2 x 2 logic blocks

    #block name x       y       subblk  block number
    #---------- --      --      ------- -----------
    a           0       1       0       #0  -- NB: block number is a comment.
    b           1       0       0       #1
    c           0       2       1       #2
    d           1       3       0       #3
    e           1       3       1       #4
    out:xor5    0       2       0       #5
    xor5        1       2       0       #6
    [1]         1       1       0       #7

.. code-block:: none
    :caption: Example 3D Placement File with Layer Column
    :linenos:

    Netlist file: xor5.net   Architecture file: sample.xml
    Array size: 2 x 2 logic blocks

    #block name x       y       subblk  layer  block number
    #---------- --      --      ------- ------ -----------
    a           0       1       0       0      #0  -- NB: block number is a comment.
    b           1       0       0       1      #1
    c           0       2       1       0      #2
    d           1       3       0       1      #3
    e           1       3       1       0      #4
    out:xor5    0       2       0       1      #5
    xor5        1       2       0       0      #6
    [1]         1       1       0       1      #7

.. _vpr_route_file:

Routing File Format (.route)
----------------------------
The first line of the routing file gives the array size, ``nx`` x ``ny``.
The remainder of the routing file lists the global or the detailed routing for each net, one by one.
Each routing begins with the word net, followed by the net index used internally by VPR to identify the net and, in brackets, the name of the net given in the netlist file.
The following lines define the routing of the net.
Each begins with a keyword that identifies a type of routing segment.
The possible keywords are ``SOURCE`` (the source of a certain output pin class), ``SINK`` (the sink of a certain input pin class), ``OPIN`` (output pin), ``IPIN`` (input pin), ``CHANX`` (horizontal channel), and ``CHANY`` (vertical channel).
Each routing begins on a ``SOURCE`` and ends on a ``SINK``.
In brackets after the keyword is the (x, y) location of this routing resource.
Finally, the pad number (if the ``SOURCE``, ``SINK``, ``IPIN`` or ``OPIN`` was on an I/O pad), pin number (if the ``IPIN`` or ``OPIN`` was on a clb), class number (if the ``SOURCE`` or ``SINK`` was on a clb) or track number (for ``CHANX`` or ``CHANY``) is listed -- whichever one is appropriate.
The meaning of these numbers should be fairly obvious in each case.
If we are attaching to a pad, the pad number given for a resource is the subblock number defining to which pad at location (x, y) we are attached.
See :numref:`fig_fpga_coord_system` for a diagram of the coordinate system used by VPR.
In a horizontal channel (``CHANX``) track ``0`` is the bottommost track, while in a vertical channel (``CHANY``) track ``0`` is the leftmost track.
Note that if only global routing was performed the track number for each of the ``CHANX`` and ``CHANY`` resources listed in the routing will be ``0``, as global routing does not assign tracks to the various nets.

For an N-pin net, we need N-1 distinct wiring “paths” to connect all the pins.
The first wiring path will always go from a ``SOURCE`` to a ``SINK``.
The routing segment listed immediately after the ``SINK`` is the part of the existing routing to which the new path attaches.

.. note:: It is important to realize that the first pin after a ``SINK`` is the connection into the already specified routing tree; when computing routing statistics be sure that you do not count the same segment several times by ignoring this fact.

.. note:: Use :option:`vpr --route_file` to override the default route file name.

Routing File Format Examples
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
An example routing for one net is listed below:

.. code-block:: none
    :caption: Example routing for a non-global net.
    :linenos:

    Net 5 (xor5)

    Node:  1   SOURCE (1,2)  Class: 1  Switch: 1       # Source for pins of class 1.
    Node:  2   OPIN (1,2)    Pin: 4    clb.O[12]  Switch:0   #Output pin the O port of clb block, pin number 12
    Node:  4   CHANX (1,1) to (4,1)  Track: 1  Switch: 1
    Node:  6   CHANX (4,1) to (7,1)  Track: 1  Switch: 1
    Node:  8   IPIN (7,1)  Pin: 0  clb.I[0]  Switch: 2
    Node:  9   SINK (7,1)  Class: 0  Switch: -1      # Sink for pins of class 0 on a clb.
    Node:  4   CHANX (7,1) to (10,1)  Track: 1  Switch: 1      # Note:  Connection to existing routing!
    Node:  5   CHANY (10,1) to (10,4)  Track: 1  Switch: 0
    Node:  4   CHANX (10,4) to (13,4)  Track: 1  Switch: 1
    Node:  10  CHANX (13,4) to (16,4)  Track: 1  Switch: 1
    Node:  11  IPIN (16,4)  Pad: 1  clb.I[1]  Switch: 2
    Node:  12  SINK (16,4)  Pad: 1  Switch: -1      # This sink is an output pad at (16,4), subblock 1.


Nets which are specified to be global in the netlist file (generally clocks) are not routed.
Instead, a list of the blocks (name and internal index) which this net must connect is printed out.
The location of each block and the class of the pin to which the net must connect at each block is also printed.
For clbs, the class is simply whatever class was specified for that pin in the architecture input file.
For pads the pinclass is always -1; since pads do not have logically-equivalent pins, pin classes are not needed.
An example listing for a global net is given below.

.. code-block:: none
    :caption: Example routing for a global net.
    :linenos:

    Net 146 (pclk): global net connecting:
    Block pclk (#146) at (1,0), pinclass -1
    Block pksi_17_ (#431) at (3,26), pinclass 2
    Block pksi_185_ (#432) at (5,48), pinclass 2
    Block n_n2879 (#433) at (49,23), pinclass 2

.. _vpr_route_resource_file:

Routing Resource Graph File Format (.xml)
-----------------------------------------
The routing resource graph (rr graph) file is an XML file that describes the routing resources within the FPGA.
VPR can generate a rr graph that matches your architecture specifications (from the architecture xml file), or it can read in an externally generated rr graph.
When this file is written by VPR, the rr graph written out is the rr graph generated before routing with a final channel width 
(even if multiple routings at different channel widths are performed during a binary search for the minimum channel width). 
When reading in rr graph from an external file, the rr graph is used during both the placement and routing phases of VPR.
The file is constructed using tags. The top level is the ``rr_graph`` tag.
This tag contains all the channel, switches, segments, block, grid, node, and edge information of the FPGA.
It is important to keep all the values as high precision as possible. Sensitive values include capacitance and Tdel. As default, these values are printed out with a precision of 30 digits.
Each of these sections are separated into separate tags as described below.

.. note:: Use :option:`vpr --read_rr_graph` to specify an RR graph file to be loaded.
.. note:: Use :option:`vpr --write_rr_graph` to specify where the RR graph should be written.

Top Level Tags
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The first tag in all rr graph files is the ``<rr_graph>`` tag that contains detailed subtags for each catagory in the rr graph.
Each tag has their subsequent subtags that describes one entity. For example, ``<segments>`` includes all the segments in the graph where each ``<segment>`` tag outlines one type of segment.

The ``rr_graph`` tag contains the following tags:

* ``<channels>``
        * ``<channel>``content``</channel>``
* ``<switches>``
        * ``<switch>``content``</switch>``
* ``<segments>``
        * ``<segment>``content``</segment>``
* ``<block_types>``
        * ``<block_type>``content``</block_type>``
* ``<grid>``
        * ``<grid_loc>``content``</grid_loc>``
* ``<rr_nodes>``
        * ``<node>``content``</node>``
* ``<rr_edges>``
        * ``<edge>``content``</edge>``

.. note:: The rr graph is based on the architecture, so more detailed description of each section of the rr graph can be found at :ref:`FPGA architecture description <fpga_architecture_description>`

Detailed Tag Information
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Channel
^^^^^^^

The channel information is contained within the ``channels`` subtag. This describes the minimum and maximum channel width within the architecture. Each ``channels`` tag has the following subtags:

.. rrgraph:tag:: <channel chan_width_max="int" x_min="int" y_min="int" x_max="int" y_max="int"/>

    This is a required subtag that contains information about the general channel width information. This stores the channel width between x or y directed channels.

    :req_param chan_width_max:
        Stores the maximum channel width value of x or y channels.

    :req_param x_min y_min x_max y_max:
        Stores the minimum and maximum value of x and y coordinate within the lists.

.. rrgraph:tag:: <x_list index="int" info="int"/>  <y_list index="int" info="int"/>

        These are a required subtags that lists the contents of an x_list and y_list array which stores the width of each channel. The x_list array size as large as the size of the y dimension of the FPGA itself while the y_list has the size of the x_dimension. This x_list tag is repeated for each index within the array.

    :req_param index:
        Describes the index within the array.

    :req_param info:
        The width of each channel. The minimum is one track per channel.
        The input and output channels are io_rat * maximum in interior tracks wide.
        The channel distributions read from the architecture file are scaled by a constant factor.

Switches
^^^^^^^^

A ``switches`` tag contains all the switches and its information within the FPGA. It should be noted that for values such as capacitance, Tdel, and sizing info all have high precision. This ensures a more accurate calculation when reading in the routing resource graph. Each switch tag has a ``switch`` subtag.

.. rrgraph:tag:: <switch id="int" name="unique_identifier" type="{mux|tristate|pass_gate|short|buffer}">

    :req_param id:
        A unique identifier for that type of switch.

    :req_param name:
        An optional general identifier for the switch.

    :req_param type:
        See :ref:`architecture switch description <arch_switches>`.

.. rrgraph:tag:: <timing R="float" cin="float" Cout="float" Tdel="float/>

        This optional subtag contains information used for timing analysis. Without it, the program assums all subtags to contain a value of 0.

    :opt_param R, Cin, Cout:
        The resistance, input capacitance and output capacitance of the switch.

    :opt_param Tdel:
        Switch's intrinsic delay. It can be outlined that the delay through an unloaded switch is Tdel + R * Cout.

.. rrgraph:tag:: <sizing mux_trans_size="int" buf_size="float"/>

        The sizing information contains all the information needed for area calculation.

    :req_param mux_trans_size:
        The area of each transistor in the segment's driving mux. This is measured in minimum width transistor units.

    :req_param buf_size:
        The area of the buffer. If this is set to zero, the area is calculated from the resistance.

Segments
^^^^^^^^

The ``segments`` tag contains all the segments and its information. Note again that the capacitance has a high decimal precision. Each segment is then enclosed in its own ``segment`` tag.

.. rrgraph:tag:: <segment id="int" name="unique_identifier">

    :req_param id:
        The index of this segment.

    :req_param name:
        The name of this segment.

.. rrgraph:tag:: <timing R_per_meter="float" C_per_meter="float">

        This optional tag defines the timing information of this segment.

    :opt_param R_per_meter, C_per_meter:
        The resistance and capacitance of a routing track, per unit logic block length.

Blocks
^^^^^^

The ``block_types`` tag outlines the information of a placeable complex logic block. This includes generation, pin classes, and pins within each block. Information here is checked to make sure it corresponds with the architecture. It contains the following subtags:

.. rrgraph:tag:: <block_type id="int" name="unique_identifier" width="int" height="int">

        This describes generation information about the block using the following attributes:

    :req_param id:
        The index of the type of the descriptor in the array. This is used for index referencing

    :req_param name:
        A unique identifier for this type of block. Note that an empty block type must be denoted ``"EMPTY"`` without the brackets ``<>`` to prevent breaking the xml format. Input and output blocks must be named "io". Other blocks can have any name.

    :req_param width, height:
        The width and height of a large block in grid tiles.

.. rrgraph:tag:: <pin_class type="pin_type">

        This optional subtag of ``block_type`` describes groups of pins in configurable logic blocks that share common properties.

    :req_param type:
        This describes whether the pin class is a driver or receiver. Valid inputs are ``OPEN``, ``OUTPUT``, and ``INPUT``.

.. rrgraph:tag:: <pin ptc="block_pin_index">name</pin>

        This required subtag of ``pin_class`` describes its pins.

    :req_param ptc:
        The index of the pin within the ``block_type``.

    :req_param name:
        Human readable pin name.

Grid
^^^^

The ``grid`` tag contains information about the grid of the FPGA. Information here is checked to make sure it corresponds with the architecture. Each grid tag has one subtag as outlined below:

.. rrgraph:tag:: <grid_loc x="int" y="int" block_type_id="int" width_offset="int" height_offset="int">

    :req_param x, y:
        The x and y  coordinate location of this grid tile.

    :req_param block_type_id:
        The index of the type of logic block that resides here.

    :req_param width_offset, height_offset:
        The number of grid tiles reserved based on the width and height of a block.

Nodes
^^^^^

The ``rr_nodes`` tag stores information about each node for the routing resource graph. These nodes describe each wire and each logic block pin as represented by nodes.

.. rrgraph:tag:: <node id="int" type="unique_type" direction="unique_direction" capacity="int">

    :req_param id:
        The index of the particular routing resource node

    :req_param type:
        Indicates whether the node is a wire or a logic block.
        Valid inputs for class types are { ``CHANX`` | ``CHANY`` | ``SOURCE`` | ``SINK`` | ``OPIN`` | ``IPIN`` }.
        Where ``CHANX`` and ``CHANY`` describe a horizontal and vertical channel.
        Sources and sinks describes where nets begin and end.
        ``OPIN`` represents an output pin and ``IPIN`` representd an input pin

    :opt_param direction:
        If the node represents a track (``CHANX`` or ``CHANY``), this field represents its direction as {``INC_DIR`` | ``DEC_DIR`` | ``BI_DIR``}.
        In other cases this attribute should not be specified.

    :req_param capacity:
        The number of routes that can use this node.

.. rrgraph:tag:: <loc xlow="int" ylow="int" xhigh="int" yhigh="int" side="{LEFT|RIGHT|TOP|BOTTOM}" ptc="int">

    Contains location information for this node. For pins or segments of length one, xlow = xhigh and ylow = yhigh.

    :req_param xlow, xhigh, ylow, yhigh:
        Integer coordinates of the ends of this routing source.

    :opt_param side:
        For ``IPIN`` and ``OPIN`` nodes specifies the side of the grid tile on which the node is located.
        Valid values are { ``LEFT`` | ``RIGHT`` | ``TOP`` | ``BOTTOM`` }.
        In other cases this attribute should not be specified.

    :req_param ptc:
        This is the pin, track, or class number that depends on the rr_node type.

.. rrgraph:tag:: <timing R="float" C="float">

    This optional subtag contains information used for timing analysis

    :req_param R:
        The resistance that goes through this node. This is only the metal resistance, it does not include the resistance of the switch that leads to another routing resource node.

    :req_param C:
        The total capacitance of this node. This includes the metal capacitance, input capacitance of all the switches hanging off the node, the output capacitance of all the switches to the node, and the connection box buffer capacitances that hangs off it.

.. rrgraph:tag:: <segment segment_id="int">

      This optional subtag describes the information of the segment that connects to the node.

    :req_param segment_id:
        This describes the index of the segment type. This value only applies to horizontal and vertical channel types. It can be left empty, or as -1 for other types of nodes.

Edges
^^^^^

The final subtag is the ``rr_edges`` tag that encloses information about all the edges between nodes. Each ``rr_edges`` tag contains multiple subtags:

.. rrgraph:tag:: <edge src_node="int" sink_node="int" switch_id="int"/>

    This subtag repeats every edge that connects nodes together in the graph.

    :req_param src_node, sink_node:
        The index for the source and sink node that this edge connects to.

    :req_param switch_id:
        The type of switch that connects the two nodes.

Node and Edge Metadata
^^^^^^^^^^^^^^^^^^^^^^

``metadata`` blocks (see :ref:`arch_metadata`) are supported under both ``node`` and ``edge`` tags.


Routing Resource Graph Format Example
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

An example of what a generated routing resource graph file would look like is shown below:

.. code-block:: xml
    :caption: Example of a routing resource graph in XML format
    :linenos:

    <rr_graph tool_name="vpr" tool_version="82a3c72" tool_comment="Based on my_arch.xml">
        <channels>
            <channel chan_width_max="2" x_min="2" y_min="2" x_max="2" y_max="2"/>
            <x_list index="1" info="5"/>
            <x_list index="2" info="5"/>
            <y_list index="1" info="5"/>
            <y_list index="2" info="5"/>
        </channels>
        <switches>
            <switch id="0" name="my_switch" buffered="1">
                <timing R="100" Cin="1233-12" Cout="123e-12" Tdel="1e-9"/>
                <sizing mux_trans_size="2.32" buf_size="23.54"/>
            </switch>
        </switches>
        <segments>
            <segment id="0" name="L4">
                <timing R_per_meter="201.7" C_per_meter="18.110e-15"/>
            </segment>
        </segments>
        <block_types>
            <block_type id="0" name="io" width="1" height="1">
                <pin_class type="input">
                    <pin ptc="0">DATIN[0]</pin>
                    <pin ptc="1">DATIN[1]</pin>
                    <pin ptc="2">DATIN[2]</pin>
                    <pin ptc="3">DATIN[3]</pin>
                </pin_class>
                <pin_class type="output">
                    <pin ptc="4">DATOUT[0]</pin>
                    <pin ptc="5">DATOUT[1]</pin>
                    <pin ptc="6">DATOUT[2]</pin>
                    <pin ptc="7">DATOUT[3]</pin>
                </pin_class>
            </block_type>
            <block_type id="1" name="buf" width="1" height="1">
                <pin_class type="input">
                    <pin ptc="0">IN</pin>
                </pin_class>
                <pin_class type="output">
                    <pin ptc="1">OUT</pin>
                </pin_class>
            </block_type>
        </block_types>
        <grid>
            <grid_loc x="0" y="0" block_type_id="0" width_offset="0" height_offset="0"/>
            <grid_loc x="1" y="0" block_type_id="1" width_offset="0" height_offset="0"/>
        </grid>
        <rr_nodes>
            <node id="0" type="SOURCE" direction="NONE" capacity="1">
                <loc xlow="0" ylow="0" xhigh="0" yhigh="0" ptc="0"/>
                <timing R="0" C="0"/>
            </node>
            <node id="1" type="CHANX" direction="INC" capacity="1">
                <loc xlow="0" ylow="0" xhigh="2" yhigh="0" ptc="0"/>
                <timing R="100" C="12e-12"/>
                <segment segment_id="0"/>
            </node>
        </rr_nodes>
        <rr_edges>
            <edge src_node="0" sink_node="1" switch_id="0"/>
            <edge src_node="1" sink_node="2" switch_id="0"/>
        </rr_edges>
    </rr_graph>

Binary Format (Cap'n Proto)
~~~~~~~~~~~~~~~~~~~~~~~~~~~

To aid in handling large graphs, rr_graph files can also be :ref:`saved in <filename_options>` a binary (Cap'n Proto) format. This will result in a smaller file and faster read/write times.

.. _end:


Network-on-Chip (NoC) Traffic Flows Format (.flows)
---------------------------------------------------

In order to co-optimize for the NoC placement VPR needs expected performance metrics of the NoC.
VPR defines the performance requirements of the NoC as traffic flows. A traffic flow is a one-way communication between two
logical routers in a design. The traffic flows provide the communications bandwidth and Quality of
Service (QoS) requirements. The traffic flows are application dependant and need to be supplied
externally by a user. The traffic flows file is an XML based file format which designers
can use to describe the traffic flows in a given application.

.. note:: Use :option:`vpr --noc_traffic_flows` to specify an NoC traffic flows file to be loaded.

Top Level Tags
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The first tag in all NoC traffic flow files is the ``<traffic_flows>`` tag that contains detailed subtags for each catagory in the NoC traffic flows.

The ``traffic_flows`` tag contains the following tags:

* ``<single_flow>``
        * ``<single_flow>``content``</single_flow>`` 

Detailed Tag Information
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Single Flow
^^^^^^^^^^^

A given traffic flow information is contained within the ``single_flow`` tag. There can be 0 or more single flow tags.
0 would indicate that an application does not have any traffic flows.

.. rrgraph:tag:: <channel src="logical_router_name" dst="logical_router_name" bandwidth="float" latency_cons="float" priority="int"/>

    :opt_param latency_cons:
        A floating point number which indicates the upper bound
        on the latency for a traffic flow. This is in units of seconds and is an optional attribute.
        If this attribute is not provided then the CAD tool will try to reduce the latency as much
        as possible.

    :opt_param priority:
        An integer which represents the relative importance of the traffic flow
        against all other traffic flows in an application. For example, a traffic flow with priority
        10 would be weighted ten times more than a traffic flow with priority 1. This is an
        optional attribute and by default all traffic flows have a priority of 1

    :req_param src:
        A string which represents a logical router name in an application.
        This logical router is the source endpoint for the traffic flow being described by the cor-
        responding single flow tag. The logical router name must match the name of the router
        as found in the clustered netlist; since this name assigned by the CAD tool, instead of
        having the designer go through the clustered netlist to retrieve the exact name we instead
        allow designers to use regex patters in the logical router name. For example, instead of
        ”noc_router_adapter_block:noc_router_layer1_mvm2:slave_tready_reg0” user could pro-
        vide ”.*noc_router_layer1_mvm2.*”. This allows users to provide the instance name for a given logical router
        module in the design. This is a required attribute.
    
    :req_param dst:
        A string which represents a logical router name in an application.
        This logical router is the deastination endpoint for the traffic flow being described by the cor-
        responding single flow tag. The logical router name must match the name of the router
        as found in the clustered netlist; since this name assigned by the CAD tool, instead of
        having the designer go through the clustered netlist to retrieve the exact name we instead
        allow designers to use regex patters in the logical router name. For example, instead of
        ”noc_router_adapter_block:noc_router_layer1_mvm3:slave_tready_reg0” user could pro-
        vide ”.*noc_router_layer1_mvm3.*”. This allows users to provide the instance name for a given logical router
        module in the design. This is a required attribute.

    :req_param bandwidth:
        A floating point number which indicates the data size in the
        traffic flow communication. This is in units of bits-per-second (bps) and is a required
        attribute.

NoC Traffic Flows File Example
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

An example of what a NoC traffic flows file looks like is shown below:

.. code-block:: xml
    :caption: Example of a NoC traffic flows file in XML format
    :linenos:

    <traffic_flows>
        <single_flow src="m0" dst="m1" bandwidth="2.3e9" latency_cons="3e-9"/>
        <single_flow src="m0" dst="m2" bandwidth="5e8"/>
        <single_flow src="ddr" dst="m0" bandwidth="1.3e8" priority=3/>
        <single_flow src="m3" dst="m2" bandwidth="4.8e9" latency_cons="5e-9" priority=2/>
    </traffic_flows>

Block types usage summary (.txt .xml or .json)
-----------------------------------------

Block types usage summary is a file written in human or machine readable format.
It describes types and the amount of cluster-level FPGA resources that are used
by implemented design. This file is generated after the placement step with
option: `--write_block_usage <filename>`. It can be saved as a human readable
text file or in XML or JSON file to provide machine readable output. Format is
selected based on the extension of the `<filename>`.

The summary consists of 4 parameters:

* `nets number` - the amount of created nets
* `blocks number` - sum of blocks used to implement the design
* `input pins` - sum of input pins
* `output pins` - sum of output pins

and a list of `block types` followed by the number of specific block types that
are used in the design.

TXT
~~~

Presents the information in human readable format, the same as in log output:

.. code-block:: none
    :caption: TXT format of block types usage summary
    :linenos:

    Netlist num_nets: <int>
    Netlist num_blocks: <int>
    Netlist <block_type_name_0> blocks: <int>
    Netlist <block_type_name_1> blocks: <int>
    ...
    Netlist <block_type_name_n> blocks: <int>
    Netlist inputs pins: <int>
    Netlist output pins: <int>

.. _end:

JSON
~~~~

One of two available machine readable formats. The information is written as follows:

.. code-block:: json
    :caption: JSON format of block types usage summary
    :linenos:

    {
      "num_nets": "<int>",
      "num_blocks": "<int>",
      "input_pins": "<int>",
      "output_pins": "<int>",
      "blocks": {
        "<block_type_name_0>": <int>,
        "<block_type_name_1>": <int>,
        ...
        "<block_type_name_n>": <int>
      }
    }

.. _end:

XML
~~~

Second machine readable format. The information is written as follows:

.. code-block:: xml
    :caption: XML format of block types usage summary
    :linenos:

    <?xml version="1.0" encoding="UTF-8"?>
    <block_usage_report>
      <nets num="<int>"></nets>
      <blocks num="<int>">
        <block type="<block_type_name_0>" usage="<int>"></block>
        <block type="<block_type_name_1>" usage="<int>"></block>
        ...
        <block type="<block_type_name_n>" usage="<int>"></block>
      </blocks>
      <input_pins num="<int>"></input_pins>
      <output_pins num="<int>"></output_pins>
    </block_usage_report>

.. _end:

Timing summary (.txt .xml or .json)
-----------------------------------------

Timing summary is a file written in human or machine readable format.
It describes final timing parameters of design implemented for the FPGA device.
This file is generated after the routing step with option: `--write_timing_summary <filename>`.
It can be saved as a human readable text file or in XML or JSON file to provide
machine readable output. Format is selected based on the extension of the `<filename>`.

The summary consists of 4 parameters:

* `Critical Path Delay (cpd) [ns]`
* `Max Circuit Frequency (Fmax) [MHz]`
* `setup Worst Negative Slack (sWNS) [ns]`
* `setup Total Negative Slack (sTNS) [ns]`

TXT
~~~

Presents the information in human readable format, the same as in log output:

.. code-block:: none
    :caption: TXT format of timing summary
    :linenos:

    Final critical path delay (least slack): <double> ns, Fmax: <double> MHz
    Final setup Worst Negative Slack (sWNS): <double> ns
    Final setup Total Negative Slack (sTNS): <double> ns

.. _end:

JSON
~~~~

One of two available machine readable formats. The information is written as follows:

.. code-block:: json
    :caption: JSON format of timing summary
    :linenos:

    {
      "cpd": <double>,
      "fmax": <double>,
      "swns": <double>,
      "stns": <double>
    }

.. _end:

XML
~~~

Second machine readable format. The information is written as follows:

.. code-block:: xml
    :caption: XML format of timing summary
    :linenos:

    <?xml version="1.0" encoding="UTF-8"?>
    <timing_summary_report>
      <cpd value="<double>" unit="ns" description="Final critical path delay"></nets>
      <fmax value="<double>" unit="MHz" description="Max circuit frequency"></fmax>
      <swns value="<double>" unit="ns" description="setup Worst Negative Slack (sWNS)"></swns>
      <stns value="<double>" unit="ns" description="setup Total Negative Slack (sTNS)"></stns>
    </block_usage_report>

.. _end:



vpr/global_routing_constraints.rst
--------------------------------------
Global Routing Constraints
==========================
.. _global_routing_constraints:

VPR allows users to designate specific nets in the input netlist as global and define the routing model for the global nets by utilizing a VPR constraints XML file. These routing constraints for global nets are specified inside the VPR constraints file in XML format, as described in the following section.  

A Global Routing Constraints File Example
------------------------------------------

.. code-block:: xml
    :caption: An example of a global routing constraints file in XML format.
    :linenos:

    <vpr_constraints tool_name="vpr">
        <global_route_constraints>
            <set_global_signal name="clock*" route_model="dedicated_network" network_name="clock_network"/>
        </global_route_constraints>
    </vpr_constraints>


Global Routing Constraints File Format
---------------------------------------
.. _global_routing_constraints_file_format:

.. vpr_constraints:tag:: <global_route_constraints>content</global_route_constraints>

    Content inside this tag contains a group of ``<set_global_signal>`` tags that specify the global nets and their assigned routing methods.

.. vpr_constraints:tag:: <set_global_signal name="string" route_model="{ideal|route|dedicated_network}" network_name="string"/>

    :req_param name: The name of the net to be assigned as global. Regular expressions are also accepted. 
    :req_param route_model: The route model for the specified net.
       
        * ``ideal``: The net is not routed. There would be no delay for the global net. 

        * ``route``: The net is routed similarly to other nets using generic routing resources.

        * ``dedicated_network``: The net will be routed through a dedicated clock network.

    :req_param network_name: The name of the clock network through which the net is routed. This parameter is required when ``route_model="dedicated_network"``.


Dedicated Clock Networks
--------------------------

Users can define a clock architecture in two ways. First, through the architecture description outlined in section :ref:`Clocks <clock_architecture>`. By using the ``<clocknetworks>`` tag, users can establish a single clock architecture with the fixed default name "clock_network". When routing a net through the dedicated network described with this tag, the network name must be set to ``"clock_network"``.

Alternatively, users can define a custom clock network architecture by inputting a custom resource routing graph. In this approach, users can specify various clock routing networks, such as a global clock network and multiple regional clock networks.  There are three main considerations for defining a clock network through a custom RR graph definition, as described in the following sections.

Virtual Sinks Definition for Clock Networks
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
For VPR to route global nets within defined clock networks, there needs to be a virtual sink node defined in the RR graph per each clock network. This virtual sink, which is of the type ``"SINK"``, must have incoming edges from all drive points of the clock network. The two-stage router used for global net routing will initially route to the virtual sink (which serves as the entry point of the clock network) in the first stage and then from the entry point to the actual sink of the net in the second stage.

To indicate that a node represents a clock network virtual sink, users can utilize the ``"clk_res_type"`` attribute on a node setting it to ``"VIRTUAL_SINK"``.

Distinguishing Between Clock Networks
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Given the support for multiple clock networks, VPR needs a way to distinguish between different virtual sinks belonging to various clock networks. This is achieved through the optional ``"name"`` attribute for the rr_node, accepting a string used as the clock network name. Therefore, when the ``"clk_res_type"`` is set to  ``"VIRTUAL_SINK"``, the attribute ``"name"`` becomes a requried parameter to enable VPR to determine which clock network the virtual sink belongs to. 

When specifying the network_name in a global routing constraints file for routing a global net through a desired clock network, as described in the :ref:`above <global_routing_constraints_file_format>` section, the name defined as an attribute in the virtual sink of the clock network should be used to reference that clock network.

Segment Definition for Clock Networks
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
The node types ``"CHANX"`` and ``"CHANY"`` can construct the clock routing architecture, similar to a generic routing network. However, to identify nodes that are part of a clock network, one can define unique segments for clock networks. To indicate that a segment defined is a clock network resource, users can use the optional attribute ``res_type="GCLK"``. Therefore, nodes with a segment ID of this defined segment are considered to be part of a clock network.

While VPR currently does not leverage this distinction of clock network resources, it is recommended to use the ``res_type="GCLK"`` attribute, as this preparation ensures compatibility for future support.


Example of RR Graph Definition for Clock Networks
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Below are snapshots of a sample RR graph that illustrate how to define a clock network. This example demonstrates the definition of a virtual sink node, a clock network segment, and a CHANY node that is part of the clock network. 

For the node with ``id="12746"``, the ``res_type="VIRTUAL_SINK"`` attribute marks it as the virtual sink node of a clock network named ``"global_network"``, as specified by the ``name`` attribute.

For the segment with ``id="1"``, the ``res_type="GCLK"`` attribute indicates that this segment is a clock network resource.

The ``"CHANY"`` node with the ``id="12746"`` has ``segment_id="1"``, which means this resource belongs to the clock network.

.. code-block:: xml
    :caption: Example snippets from an RR graph describing part of a clock network.
    :linenos:

        <rr_nodes>
            <!-- Definition of a virtual sink node for a clock network named "global_network" -->
            <node capacity="1" clk_res_type="VIRTUAL_SINK" id="12746" name="global_network" type="SINK">
                <loc layer="0" ptc="72" xhigh="6" xlow="6" yhigh="6" ylow="6"/>
                <timing C="0" R="0"/>
            </node>

            <!-- Definition of a CHANY node as part of the clock network -->
            <node capacity="1" direction="BI_DIR" id="12668" type="CHANY">
                <loc layer="0" ptc="20" xhigh="6" xlow="6" yhigh="6" ylow="6"/>
                <timing C="1.98240038e-13" R="50.4199982"/>
                <segment segment_id="1"/>
            </node>
            
            <!-- ... other nodes ... -->
        </rr_nodes>

        <segments>
            <!-- Definition of a clock network segment -->
            <segment id="1" name="spine1_drive" res_type="GCLK">
                <timing C_per_meter="2.06999995e-14" R_per_meter="50.4199982"/>
            </segment>

            <!-- ... other segments ... -->
        </segments>


vpr/graphics.rst
--------------------------------------
.. _vpr_graphics:

Graphics
========
VPR includes easy-to-use graphics for visualizing both the targetted FPGA architecture, and the circuit VPR has implemented on the architecture.

.. image:: https://www.verilogtorouting.org/img/des90_routing_util.gif
    :align: center

Enabling Graphics
-----------------

Compiling with Graphics Support
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The build system will attempt to build VPR with graphics support by default.

If all the required libraries are found the build system will report::

    -- EZGL: graphics enabled

If the required libraries are not found cmake will report::

    -- EZGL: graphics disabled

and list the missing libraries::

    -- EZGL: Failed to find required X11 library (on debian/ubuntu try 'sudo apt-get install libx11-dev' to install)
    -- EZGL: Failed to find required Xft library (on debian/ubuntu try 'sudo apt-get install libxft-dev' to install)
    -- EZGL: Failed to find required fontconfig library (on debian/ubuntu try 'sudo apt-get install fontconfig' to install)
    -- EZGL: Failed to find required cairo library (on debian/ubuntu try 'sudo apt-get install libcairo2-dev' to install)

Enabling Graphics at Run-time
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
When running VPR provide :option:`vpr --disp` ``on`` to enable graphics.

Saving Graphics at Run-time
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
When running VPR provide :option:`vpr --save_graphics` ``on`` to save an image of the final placement and the final routing created by vpr to pdf files on disk. The files are named vpr_placement.pdf and vpr_routing.pdf.

A graphical window will now pop up when you run VPR.

Navigation
----------
* Click on the **Zoom-Fit** button to get an over-encompassing view of the FPGA architecture.
* Click and drag with the left mouse button to pan the view, or scroll the mouse wheel to zoom in and out.
* Click on the **Window** button, then on the diagonally opposite corners of a box, to zoom in on a particular area.
* Click on **Save** under the **Misc.** tab to save the image on screen to PDF, PNG, or SVG file.
* **Done** tells VPR to continue with the next step in placing and routing the circuit.


.. note:: Menu buttons will be greyed out when they are not selectable (e.g. VPR is working).

Visualizing Placement
--------------------------------
By default VPR's graphics displays the FPGA floorplan (block grid) and current placement.

.. figure:: ../Images/Overall_view.png
    :align: center 
    :height: 300

    FPGA floorplan (block grid)    

If the **Placement Macros** drop down is set, any placement macros (e.g. carry chains, which require specific relative placements between some blocks) will be highlighted.

.. figure:: https://www.verilogtorouting.org/img/neuron_placement_macros.gif
    :align: center

    Placement with macros (carry chains) highlighted

Visualizing Netlist Connectivity
--------------------------------
The **Toggle Nets** drop-down list under the **Net Settings** tab toggles the nets in the circuit to be visible/invisible. Options include **Cluster Nets** and **Primitive Nets**.

.. figure:: ../Images/Net_Settings.png
    :align: center 
    :height: 200

    Toggle Nets drop-down under Net Settings tab

When a placement is being displayed, routing information is not yet known so nets are simply drawn as a “star;” that is, a straight line is drawn from the net source to each of its sinks.
Click on any clb in the display, and it will be highlighted in green, while its fanin and fanout are highlighted in blue and red, respectively.
Once a circuit has been routed the true path of each net will be shown.

.. figure:: https://www.verilogtorouting.org/img/des90_nets.gif
    :align: center

    Logical net connectivity during placement

If the nets routing are shown, click on a clb or pad to highlight its fanins and fanouts, or click on a pin or channel wire to highlight a whole net in magenta.
Multiple nets can be highlighted by pressing ctrl + mouse click.

Visualizing the Critical Path
-----------------------------
During placement and routing you can click on the **Crit. Path** drop-down menu under the **Misc.** tab to visualize the critical path.
Each stage between primitive pins is shown in a different colour.

.. figure:: ../Images/crit_path.png
    :align: center 
    :height: 200

    Crit. Path drop-down list under the Misc. tab

The **Crit. Path** drop-down will toggle through the various visualizations:

* During placement the critical path is shown only as flylines.
* During routing the critical path can be shown as both flylines and routed net connections.

.. figure:: https://www.verilogtorouting.org/img/des90_cpd.gif
    :align: center

    Critical Path flylines during placement and routing

Visualizing Routing Architecture
--------------------------------

When a routing is on screen, the **Routing Options** tab provides various options to gain more visual information.

.. figure:: ../Images/Routing_Options.png
    :align: center 
    :height: 300

    Routing Options

Clicking on **Toggle RR** lets you to choose between various views of the routing resources available in the FPGA.

.. figure:: https://github.com/verilog-to-routing/verilog-to-routing.github.io/raw/master/img/routing_arch.gif
    :align: center

    Routing Architecture Views

The routing resource view can be very useful in ensuring that you have correctly described your FPGA in the architecture description file -- if you see switches where they shouldn’t be or pins on the wrong side of a clb, your architecture description needs to be revised.

Wiring segments are drawn in black, input pins are drawn in sky blue, and output pins are drawn in pink.
Sinks are drawn in dark slate blue, and sources in plum.
Direct connections between output and input pins are shown in medium purple.
Connections from wiring segments to input pins are shown in sky blue, connections from output pins to wiring segments are shown in pink, and connections between wiring segments are shown in green.
The points at which wiring segments connect to clb pins (connection box switches) are marked with an ``x``.

Switch box connections will have buffers (triangles) or pass transistors (circles) drawn on top of them, depending on the type of switch each connection uses.
Clicking on a clb or pad will overlay the routing of all nets connected to that block on top of the drawing of the FPGA routing resources, and will label each of the pins on that block with its pin number.
Clicking on a routing resource will highlight it in magenta, and its fanouts will be highlighted in red and fanins in blue.
Multiple routing resources can be highlighted by pressing ctrl + mouse click.

Visualizing Routing Congestion
------------------------------
When a routing is shown on-screen, clicking on the **Congestion** drop-down menu under the **Routing Options** tab will show a heat map of any overused routing resources (wires or pins).
Lighter colours (e.g. yellow) correspond to highly overused resources, while darker colours (e.g. blue) correspond to lower overuse.
The overuse range shown at the bottom of the window.

.. figure:: https://www.verilogtorouting.org/img/bitcoin_congestion.gif
    :align: center

    Routing Congestion during placement and routing

Visualizing Routing Utilization
-------------------------------
When a routing is shown on-screen, clicking on the **Routing Util** drop-down menu will show a heat map of routing wire utilization (i.e. fraction of wires used in each channel).
Lighter colours (e.g. yellow) correspond to highly utilized channels, while darker colours (e.g. blue) correspond to lower utilization.

.. figure:: https://www.verilogtorouting.org/img/bitcoin_routing_util.gif
    :align: center

    Routing Utilization during placement and routing

Toggle Block Internal
-------------------------------
During placement and routing you can adjust the level of block detail you visualize by using the **Toggle Block Internal** option under the **Block Settings** tab. 

.. figure:: ../Images/Block_Settings.png
    :align: center 
    :height: 300

    Block Settings 

Each block can contain a number of flip flops (ff), look up tables (lut), and other primitives. The higher the number, the deeper into the hierarchy within the cluster level block you see. 

.. figure:: https://www.verilogtorouting.org/img/ToggleBlockInternal.gif
    :align: center

    Visualizing Block Internals

View Menu
-----------------------------
.. figure:: ../Images/view_menu.png
    :align: center 

    Items under view menu

The view menu is displayed when vpr is targeting a stacked multi-die architecture (more than 1 layer).
Layers are drawn in ascending order for many drawing features (e.g. blocks); that is layer 0 is drawn first, and (if visible), layer 1 is drawn on top of it etc.
The visibility and transparency of a layer can be changed, which will affect blocks, nets, routing, and critical path.
Cross-layer connections refer to connections that are in different layers. 

Button Description Table
------------------------
+-------------------+-------------------+------------------------------+------------------------------+
|      Buttons      |      Stages       |        Functionalities       |     Detailed Descriptions    |
+-------------------+-------------------+------------------------------+------------------------------+
| Blk Internal      | Placement/Routing | Controls depth of sub-blocks | Click multiple times to show |
|                   |                   | shown                        | more details; Click to reset |
|                   |                   |                              | when reached maximum level   |
|                   |                   |                              | of detail                    |
+-------------------+-------------------+------------------------------+------------------------------+
| Toggle Block      | Placement/Routing | Adjusts the level of         | Click multiple times to      |
| Internal          |                   | visualized block detail      | go deeper into the           |
|                   |                   |                              | hierarchy within the cluster |
|                   |                   |                              | level block                  |
|                   |                   |                              |                              |
+-------------------+-------------------+------------------------------+------------------------------+
| Blk Pin Util      | Placement/Routing | Visualizes block pin         | Click multiple times to      |
|                   |                   | utilization                  | visualize all block pin      |
|                   |                   |                              | utilization, input block pin |
|                   |                   |                              | utilization, or output block |
|                   |                   |                              | pin utilization              |
+-------------------+-------------------+------------------------------+------------------------------+
| Cong. Cost        | Routing           | Visualizes the congestion    |                              |
|                   |                   | costs of routing resouces    |                              |
|                   |                   |                              |                              |
|                   |                   |                              |                              |
+-------------------+-------------------+------------------------------+------------------------------+
| Congestion        | Routing           | Visualizes a heat map of     |                              |
|                   |                   | overused routing resources   |                              |
|                   |                   |                              |                              |
|                   |                   |                              |                              |
+-------------------+-------------------+------------------------------+------------------------------+
| Crit. Path        | Placement/Routing | Visualizes the critical path |                              |
|                   |                   | of the circuit               |                              |
|                   |                   |                              |                              |
|                   |                   |                              |                              |
+-------------------+-------------------+------------------------------+------------------------------+
| Draw Partitions   | Placement/Routing | Visualizes placement         |                              |
|                   |                   | constraints                  |                              |
|                   |                   |                              |                              |
|                   |                   |                              |                              |
+-------------------+-------------------+------------------------------+------------------------------+
| Place Macros      | Placement/Routing | Visualizes placement macros  |                              |
|                   |                   |                              |                              |
|                   |                   |                              |                              |
|                   |                   |                              |                              |
+-------------------+-------------------+------------------------------+------------------------------+
| Route BB          | Routing           | Visualizes net bounding      | Click multiple times to      |
|                   |                   | boxes one by one             | sequence through the net     |
|                   |                   |                              | being shown                  |
|                   |                   |                              |                              |
+-------------------+-------------------+------------------------------+------------------------------+
| Router Cost       | Routing           | Visualizes the router costs  |                              |
|                   |                   | of different routing         |                              |
|                   |                   | resources                    |                              |
|                   |                   |                              |                              |
+-------------------+-------------------+------------------------------+------------------------------+
| Routing Util      | Routing           | Visualizes routing channel   |                              |
|                   |                   | utilization with colors      |                              |
|                   |                   | indicating the fraction of   |                              |
|                   |                   | wires used within a channel  |                              |
+-------------------+-------------------+------------------------------+------------------------------+
| Toggle Nets       | Placement/Routing | Visualizes the nets in the   | Click multiple times to      |
|                   |                   | circuit                      | set the nets to be visible / |
|                   |                   |                              | invisible                    |
|                   |                   |                              |                              |
+-------------------+-------------------+------------------------------+------------------------------+
| Toggle RR         | Placement/Routing | Visualizes different views   | Click multiple times to      |
|                   |                   | of the routing resources     | switch between routing       |
|                   |                   |                              | resources available in the   |
|                   |                   |                              | FPGA                         |
+-------------------+-------------------+------------------------------+------------------------------+

Manual Moves
------------

The manual moves feature allows the user to specify the next move in placement. If the move is legal, blocks are swapped and the new move is shown on the architecture. 

.. figure:: ../Images/manual_move.png
    :align: center
    :height: 200

    Misc. Tab

To enable the feature, activate the **Manual Move** toggle button under the **Misc.** tab and press Done. Alternatively, the user can activate the **Manual Move** toggle button and click on the block to be moved.

.. figure:: https://www.verilogtorouting.org/img/draw_manual_moves_window.png
   :align: center

On the manual move window, the user can specify the Block ID/Block name of the block to move and the To location, with the x position, y position and subtile position. For the manual move to be valid:

- The To location requested by the user should be within the grid's dimensions.
- The block to be moved is found, valid and not fixed.
- The blocks to be swapped are compatible.
- The location choosen by the user is different from the block's current location.
  
If the manual move is legal, the cost summary window will display the delta cost, delta timing, delta bounding box cost and the placer's annealing decision that would result from this move. 

.. figure:: https://www.verilogtorouting.org/img/manual_move_cost_dialog.png
   :align: center

The user can Accept or Reject the manual move based on the values provided. If accepted the block's new location is shown. 




vpr/index.rst
--------------------------------------
.. _vpr:

VPR
===

VPR (Versatile Place and Route) is an open source academic CAD tool designed for the exploration of new FPGA architectures and CAD algorithms, at the packing, placement and routing phases of the CAD flow :cite:`betz_vpr,luu_vpr_5`.
Since its public introduction, VPR has been used extensively in many academic projects partly because it is robust, well documented, easy-to-use, and can flexibly target a range of architectures.

VPR takes, as input, a description of an FPGA architecture along with a technology-mapped user circuit.
It then performs packing, placement, and routing to map the circuit onto the FPGA.
The output of VPR includes the FPGA configuration needed to implement the circuit and statistics about the final mapped design (eg. critical path delay, area, etc).

.. |bitcoin_place| image:: https://www.verilogtorouting.org/img/bitcoin_placement_macros.gif
    :width: 300px
    :alt: Placement 

.. |bitcoin_cpd| image:: https://www.verilogtorouting.org/img/bitcoin_cpd.gif
    :width: 300px 
    :alt: Critical Path 

.. |bitcoin_nets| image:: https://www.verilogtorouting.org/img/bitcoin_nets.gif
    :width: 300px
    :alt: Wiring

.. |bitcoin_routing| image:: https://www.verilogtorouting.org/img/bitcoin_routing_util.gif
    :width: 300px 
    :alt: Routing Usage

+---------------------------------------+---------------------------------------+
| |bitcoin_place|                       + |bitcoin_cpd|                         +
|                                       +                                       +
| Placement (carry chains highlighted)  + Critical Path                         +
+---------------------------------------+---------------------------------------+
| |bitcoin_nets|                        + |bitcoin_routing|                     +
|                                       +                                       +
| Logical Connections                   + Routing Utilization                   +
+---------------------------------------+---------------------------------------+

**Motivation**

The study of FPGA CAD and architecture can be a challenging process partly because of the difficulty in conducting high quality experiments.
A quality CAD/architecture experiment requires realistic benchmarks, accurate architectural models, and robust CAD tools that can appropriately map the benchmark to the particular architecture in question.
This is a lot of work.
Fortunately, this work can be made easier if open source tools are available as a starting point.

The purpose of VPR is to make the packing, placement, and routing stages of the FPGA CAD flow robust and flexible so that it is easier for researchers to investigate future FPGAs.


.. toctree::
   :maxdepth: 2

   basic_flow
   command_line_usage

   graphics

   timing_constraints
   vpr_constraints
   sdc_commands

   file_formats
   debug_aids



vpr/placement_constraints.rst
--------------------------------------

Placement Constraints
======================
.. _placement_constraints:

VPR supports running flows with placement constraints. Placement constraints are set on primitives to lock them down to specified regions on the FPGA chip. For example, a user may use placement constraints to lock down pins to specific locations on the chip. Also, groups of primitives may be locked down to regions on the chip in CAD flows that use floorplanning or modular design, or to hand-place a timing critical piece.

The placement constraints should be specified by the user using an XML constraints file format, as described in the section below. When VPR is run with placement constraints, both the packing and placement flows are performed in such a way that the constraints are respected. The packing stage does not pack any primitives together that have conflicting floorplan constraints. The placement stage considers the floorplan constraints when choosing a location for each clustered block during initial placement, and does not move any block outside of its constraint boundaries during place moves.

A Placement Constraints File Example
------------------------------------

.. code-block:: xml
	:caption: An example of a placement constraints file in XML format.
	:linenos:

	<vpr_constraints tool_name="vpr">
		<partition_list>
			<partition name="Part0">
					<add_atom name_pattern="li354"/>
					<add_atom name_pattern="alu*"/> <!-- Regular expressions can be used to provide name patterns of the primitives to be added -->
					<add_atom name_pattern="n877"/>
					<add_region x_low="3" y_low="1" x_high="7" y_high="2"/> <!-- Two rectangular regions are specified, together describing an L-shaped region -->
					<add_region x_low="7" y_low="3" x_high="7" y_high="6"/>
			</partition>
			<partition name="Part1">
					<add_region x_low="3" y_low="3" x_high="7" y_high="7" subtile="0"/> <!-- One specific location is specified -->
					<add_atom name_pattern="n4917"/>
					<add_atom name_pattern="n6010"/>
			</partition>

            <partition name="Part2">
					<add_region x_low="3" y_low="3" x_high="85" y_high="85"/> <!-- When the layer is not explicitly specified, layer 0 is assumed. -->
                    <add_region x_low="8" y_low="5" x_high="142" y_high="29 layer_low="0" layer_high="1"/> <!-- In 3D architectures, the region can span across multiple layers. -->
                    <add_region x_low="6" y_low="55" x_high="50" y_high="129 layer_low="2" layer_high="2"/> <!-- If the region only covers a non-zero layer, both layer_low and layer_high must be set the same value. -->
					<add_atom name_pattern="n135"/>
					<add_atom name_pattern="n7016"/>
			</partition>
		</partition_list>
	</vpr_constraints>

.. _end:

Placement Constraints File Format
---------------------------------

VPR has a specific XML format which must be used when creating a placement constraints file. The purpose of this constraints file is to specify 

#. Which primitives are to have placement constraints
#. The regions on the FPGA chip to which those primitives must be constrained

The file is passed as an input to VPR when running with placement constraints. When the file is read in, its information is used during the packing and placement stages of VPR. The hierarchy of the file is set up as follows.

The top level tag is the ``<vpr_constraints>`` tag. This tag can contain one ``<partition_list>`` tag. The ``<partition_list>`` tag can be made up of an unbounded number of ``<partition>`` tags. The ``<partition>`` tags contains all of the detailed information of the placement constraints, and is described in detail below.

Partitions, Atoms, and Regions
------------------------------

Partition
^^^^^^^^^

A partition is made up of two components - a group of primitives (a.k.a. atoms) that must be constrained to the same area on the chip, and a set of one or more regions specifying where those primitives must be constrained. The information for each partition is contained within a ``<partition>`` tag, and the number of ``partition`` tags that the partition_list tag can contain is unbounded. 

:req_param name:
   A name for the partition.

:req_param add_atom:
   A tag used for adding an atom primitive to the partition.

:req_param add_region:
   A tag used for specifying a region for the partition.

Atom 
^^^^

An ``<add_atom>`` tag is used to add an atom that must be constrained to the partition. Each partition can contain any number of atoms from the circuit. The ``<add_atom>`` tag has the following attribute:

:req_param name_pattern:
   The name of the atom.

The ``name_pattern`` can be the exact name of the atom from the input atom netlist that was passed to VPR. It can also be a regular expression, in which case VPR will add all atoms from the netlist which have a portion of their name matching the regular expression to the partition. For example, if a module contains primitives named in the pattern of "alu[0]", "alu[1]", and "alu[2]", the regular expression "alu*" would add all of the primitives from that module.

Region
^^^^^^

An ``<add_region>`` tag is used to add a region to the partition. A ``region`` is a rectangular area or cubic volume
on the chip. A partition can contain any number of independent regions - the regions within one partition **must not**
overlap with each other (in order to ease processing when loading in the file).
An ``<add_region>`` tag has the following attributes.

:req_param x_low:
   The x value of the lower left point of the rectangle.

:req_param y_low:
   The y value of the lower left point of the rectangle.

:req_param x_high:
   The x value of the upper right point of the rectangle.

:req_param y_high:
   The y value of the upper right point of the rectangle.

:opt_param subtile:
   Each x, y location on the grid may contain multiple locations known as subtiles. This parameter is an optional value specifying the subtile location that the atom(s) of the partition shall be constrained to.

:opt_param layer_low:
    The lowest layer number that the region covers. The default value is 0.

:opt_param layer_high:
    The highest layer number that the region covers. The default value is 0.

The optional ``subtile`` attribute is commonly used when constraining an atom to a specific location on the chip (e.g. an exact I/O location). It is legal to use with larger regions, but uncommon.

In 2D architectures, ``layer_low`` and ``layer_high`` can be safely ignored as their default value is 0.
In 3D architectures, a region can span across multiple layers or be assigned to a specific layer.
For assigning a region to a specific non-zero layer, the user should set both ``layer_low`` and ``layer_high`` to the
desired layer number. If a layer range is to be covered by the region, the user set ``layer_low`` and ``layer_high`` to
different values.

If a user would like to specify an area on the chip with an unusual shape (e.g. L-shaped or T-shaped),
they can simply add multiple ``<add_region>`` tags to cover the area specified.

It is strongly recommended that different partitions do not overlap. The packing algorithm compares the number clustered
blocks and the number of physical blocks in a region to decide pack atoms inside a partition more aggressively when
there are not enough resources in a partition. Overlapping partitions causes some physical blocks to be counted in more
than one partition.












vpr/sdc_commands.rst
--------------------------------------
.. _sdc_commands:

SDC Commands
============
The following subset of SDC syntax is supported by VPR.

create_clock
------------
Creates a netlist or virtual clock.

Assigns a desired period (in nanoseconds) and waveform to one or more clocks in the netlist (if the ``–name`` option is omitted) or to a single virtual clock (used to constrain input and outputs to a clock external to the design).
Netlist clocks can be referred to using regular expressions, while the virtual clock name is taken as-is.

*Example Usage:*

.. code-block:: tcl

    #Create a netlist clock
    create_clock -period <float> <netlist clock list or regexes>

    #Create a virtual clock
    create_clock -period <float> -name <virtual clock name>

    #Create a netlist clock with custom waveform/duty-cycle
    create_clock -period <float> -waveform {rising_edge falling_edge} <netlist clock list or regexes>


Omitting the waveform creates a clock with a rising edge at 0 and a falling edge at the half period, and is equivalent to using ``-waveform {0 <period/2>}``.
Non-50% duty cycles are supported but behave no differently than 50% duty cycles, since falling edges are not used in analysis.
If a virtual clock is assigned using a create_clock command, it must be referenced elsewhere in a set_input_delay or set_output_delay constraint.

.. sdc:command:: create_clock

    .. sdc:option:: -period <float>

        Specifies the clock period.

        **Required:** Yes

    .. sdc:option:: -waveform {<float> <float>}

        Overrides the default clock waveform.

        The first value indicates the time the clock rises, the second the time the clock falls.

        **Required:** No

        **Default:** 50% duty cycle (i.e. ``-waveform {0 <period/2>}``).

    .. sdc:option:: -name <string>

        Creates a virtual clock with the specified name.

        **Required:** No

    .. sdc:option:: <netlist clock list or regexes>

        Creates a netlist clock

        **Required:** No

.. note:: One of :sdc:option:`-name` or :sdc:option:`<netlist clock list or regexes>` must be specified.

.. warning:: If a netlist clock is not specified with a :sdc:command:`create_clock` command, paths to and from that clock domain will not be analysed.


set_clock_groups
----------------
Specifies the relationship between groups of clocks.
May be used with netlist or virtual clocks in any combination.

Since VPR supports only the :sdc:option:`-exclusive` option, a :sdc:command:`set_clock_groups` constraint is equivalent to a :sdc:command:`set_false_path` constraint (see below) between each clock in one group and each clock in another.

For example, the following sets of commands are equivalent:

.. code-block:: tcl

    #Do not analyze any timing paths between clk and clk2, or between
    #clk and clk3
    set_clock_groups -exclusive -group {clk} -group {clk2 clk3}

and

.. code-block:: tcl

     set_false_path -from [get_clocks {clk}] -to [get_clocks {clk2 clk3}]
     set_false_path -from [get_clocks {clk2 clk3}] -to [get_clocks {clk}]

.. sdc:command:: set_clock_groups

    .. sdc:option:: -exclusive

        Indicates that paths between clock groups should not be analyzed.

        **Required:** Yes

        .. note:: VPR currently only supports exclusive clock groups


    .. sdc:option:: -group {<clock list or regexes>}

        Specifies a group of clocks.

        .. note:: At least 2 groups must be specified.

        **Required:** Yes


set_false_path
--------------
Cuts timing paths unidirectionally from each clock in :sdc:option:`-from` to each clock in :sdc:option:`–to`.
Otherwise equivalent to :sdc:command:`set_clock_groups`.

*Example Usage:*

.. code-block:: tcl

    #Do not analyze paths launched from clk and captured by clk2 or clk3
    set_false_path -from [get_clocks {clk}] -to [get_clocks {clk2 clk3}]

    #Do not analyze paths launched from clk2 or clk3 and captured by clk
    set_false_path -from [get_clocks {clk2 clk3}] -to [get_clocks {clk}]

.. note:: False paths are supported between entire clock domains, but *not* between individual registers.

.. sdc:command:: set_false_path

    .. sdc:option:: -from [get_clocks <clock list or regexes>]

        Specifies the source clock domain(s).

        **Required:** No

        **Default:** All clocks

    .. sdc:option:: -to [get_clocks <clock list or regexes>]

        Specifies the sink clock domain(s).

        **Required:** No

        **Default:** All clocks

set_max_delay/set_min_delay
---------------------------
Overrides the default setup (max) or hold (min) timing constraint calculated using the information from :sdc:command:`create_clock` with a user-specified delay.

*Example Usage:*

.. code-block:: tcl

    #Specify a maximum delay of 17 from input_clk to output_clk
    set_max_delay 17 -from [get_clocks {input_clk}] -to [get_clocks {output_clk}]

    #Specify a minimum delay of 2 from input_clk to output_clk
    set_min_delay 2 -from [get_clocks {input_clk}] -to [get_clocks {output_clk}]

.. note:: Max/Min delays are supported between entire clock domains, but *not* between individual netlist elements.

.. sdc:command:: set_max_delay/set_min_delay

    .. sdc:option:: <delay>

        The delay value to apply.

        **Required:** Yes

    .. sdc:option:: -from [get_clocks <clock list or regexes>]

        Specifies the source clock domain(s).

        **Required:** No

        **Default:** All clocks

    .. sdc:option:: -to [get_clocks <clock list or regexes>]

        Specifies the sink clock domain(s).

        **Required:** No

        **Default:** All clocks

set_multicycle_path
-------------------
Sets how many clock cycles elapse between the launch and capture edges for setup and hold checks.

The default the setup mutlicycle value is 1 (i.e. the capture setup check is performed against the edge one cycle after the launch edge).

The default hold multicycle is one less than the setup multicycle path (e.g. the capture hold check occurs in the same cycle as the launch edge for the default setup multicycle).

*Example Usage:*

.. code-block:: tcl

    #Create a 4 cycle setup check, and 0 cycle hold check from clkA to clkB
    set_multicycle_path -from [get_clocks {clkA}] -to [get_clocks {clkB}] 4

    #Create a 3 cycle setup check from clk to clk2
    # Note that this moves the default hold check to be 2 cycles
    set_multicycle_path -setup -from [get_clocks {clk}] -to [get_clocks {clk2}] 3

    #Create a 0 cycle hold check from clk to clk2
    # Note that this moves the default hold check back to it's original
    # position before the previous setup setup_multicycle_path was applied
    set_multicycle_path -hold -from [get_clocks {clk}] -to [get_clocks {clk2}] 2

    #Create a multicycle to a specific pin
    set_multicycle_path -to [get_pins {my_inst.in\[0\]}] 2

.. note:: Multicycles are supported between entire clock domains, and ending at specific registers.

.. sdc:command:: set_multicycle_path

    .. sdc:option:: -setup

        Indicates that the multicycle-path applies to setup analysis.

        **Required:** No

    .. sdc:option:: -hold

        Indicates that the multicycle-path applies to hold analysis.

        **Required:** No

    .. sdc:option:: -from [get_clocks <clock list or regexes>]

        Specifies the source clock domain(s).

        **Required:** No

        **Default:** All clocks

    .. sdc:option:: -to [get_clocks <clock list or regexes>]

        Specifies the sink clock domain(s).

        **Required:** No

        **Default:** All clocks

    .. sdc:option:: -to [get_pins <pin list or regexes>]

        Specifies the sink/capture netlist pins to which the multicycle is applied.

        .. seealso:: VPR's :ref:`pin naming convention <vpr_blif_naming_convention_pins>`.

        **Required:** No

    .. sdc:option:: <path_multiplier>

        The number of cycles that apply to the specified path(s).

        **Required:** Yes

.. note:: If neither :sdc:option:`-setup` nor :sdc:option:`-hold` the setup multicycle is set to ``path_multiplier`` and the hold multicycle offset to ``0``.

.. note:: Only a single -to option can be specified (either clocks or pins, but not both).


set_input_delay/set_output_delay
--------------------------------
Use ``set_input_delay`` if you want timing paths from input I/Os analyzed, and ``set_output_delay`` if you want timing paths to output I/Os analyzed.

.. note:: If these commands are not specified in your SDC, paths from and to I/Os will not be timing analyzed.

These commands constrain each I/O pad specified after ``get_ports`` to be timing-equivalent to a register clocked on the clock specified after ``-clock``.
This can be either a clock signal in your design or a virtual clock that does not exist in the design but which is used only to specify the timing of I/Os.

The specified delays are added to I/O timing paths and can be used to model board level delays.

For single-clock circuits, ``-clock`` can be wildcarded using ``*`` to refer to the single netlist clock, although this is not supported in standard SDC.
This allows a single SDC command to constrain I/Os in all single-clock circuits.

*Example Usage:*

.. code-block:: tcl

    #Set a maximum input delay of 0.5 (relative to input_clk) on
    #ports in1, in2 and in3
    set_input_delay -clock input_clk -max 0.5 [get_ports {in1 in2 in3}]

    #Set a minimum output delay of 1.0 (relative to output_clk) on
    #all ports matching starting with 'out*'
    set_output_delay -clock output_clk -min 1 [get_ports {out*}]

    #Set both the maximum and minimum output delay to 0.3 for all I/Os
    #in the design
    set_output_delay -clock clk2 0.3 [get_ports {*}]

.. sdc:command:: set_input_delay/set_output_delay

    .. sdc:option:: -clock <virtual or netlist clock>

        Specifies the virtual or netlist clock the delay is relative to.

        **Required:** Yes

    .. sdc:option:: -max

        Specifies that the delay value should be treated as the maximum delay.

        **Required:** No

    .. sdc:option:: -min

        Specifies that the delay value should be treated as the minimum delay.

        **Required:** No

    .. sdc:option:: <delay>

        Specifies the delay value to be applied

        **Required:** Yes

    .. sdc:option:: [get_ports {<I/O list or regexes>}]

        Specifies the port names or port name regex.

        **Required:** Yes

    .. note::

        If neither ``-min`` nor ``-max`` are specified the delay value is applied to both.

set_clock_uncertainty
---------------------
Sets the clock uncertainty between clock domains.
This is typically used to model uncertainty in the clock arrival times due to clock jitter.

*Example Usage:*

.. code-block:: tcl

    #Sets the clock uncertainty between all clock domain pairs to 0.025
    set_clock_uncertainty 0.025

    #Sets the clock uncertainty from 'clk' to all other clock domains to 0.05
    set_clock_uncertainty -from [get_clocks {clk}] 0.05

    #Sets the clock uncertainty from 'clk' to 'clk2' to 0.75
    set_clock_uncertainty -from [get_clocks {clk}]  -to [get_clocks {clk2}] 0.75

.. sdc:command:: set_clock_uncertainty

    .. sdc:option:: -from [get_clocks <clock list or regexes>]

        Specifies the source clock domain(s).

        **Required:** No

        **Default:** All clocks

    .. sdc:option:: -to [get_clocks <clock list or regexes>]

        Specifies the sink clock domain(s).

        **Required:** No

        **Default:** All clocks

    .. sdc:option:: -setup

        Specifies the clock uncertainty for setup analysis.

        **Required:** No

    .. sdc:option:: -hold

        Specifies the clock uncertainty for hold analysis.

        **Required:** No

    .. sdc:option:: <uncertainty>

        The clock uncertainty value between the from and to clocks.

        **Required:** Yes

    .. note::

        If neither ``-setup`` nor ``-hold`` are specified the uncertainty value is applied to both.

set_clock_latency
-----------------
Sets the latency of a clock.
VPR automatically calculates on-chip clock network delay, and so only source latency is supported.

Source clock latency corresponds to the delay from the true clock source (e.g. off-chip clock generator) to the on-chip clock definition point.

.. code-block:: tcl

    #Sets the source clock latency of 'clk' to 1.0
    set_clock_latency -source 1.0 [get_clocks {clk}]

.. sdc:command:: set_clock_latency

    .. sdc:option:: -source

        Specifies that the latency is the source latency.

        **Required:** Yes

    .. sdc:option:: -early

        Specifies that the latency applies to early paths.

        **Required:** No

    .. sdc:option:: -late

        Specifies that the latency applies to late paths.

        **Required:** No

    .. sdc:option:: <latency>

        The clock's latency.

        **Required:** Yes

    .. sdc:option:: [get_clocks <clock list or regexes>]

        Specifies the clock domain(s).

        **Required:** Yes

    .. note::

        If neither ``-early`` nor ``-late`` are specified the latency value is applied to both.

set_disable_timing
------------------
Disables timing between a pair of connected pins in the netlist.
This is typically used to manually break combinational loops.

.. code-block:: tcl

    #Disables the timing edge between the pins 'FFA.Q[0]' and 'to_FFD.in[0]' on
    set_disable_timing -from [get_pins {FFA.Q\\[0\\]}] -to [get_pins {to_FFD.in\\[0\\]}]


.. sdc:command:: set_disable_timing

    .. sdc:option:: -from [get_pins <pin list or regexes>]

        Specifies the source netlist pins.

        .. seealso:: VPR's :ref:`pin naming convention <vpr_blif_naming_convention_pins>`.

        **Required:** Yes

    .. sdc:option:: -to [get_pins <pin list or regexes>]

        Specifies the sink netlist pins.

        .. seealso:: VPR's :ref:`pin naming convention <vpr_blif_naming_convention_pins>`.

        **Required:** Yes
    
    .. note::

        Make sure to escape the characters in the regexes.


Special Characters
------------------
.. sdc:command:: # (comment), \\ (line continued), * (wildcard), {} (string escape)

    ``#`` starts a comment – everything remaining on this line will be ignored.

    ``\`` at the end of a line indicates that a command wraps to the next line.

    ``*`` is used in a ``get_clocks``/``get_ports`` command or at the end of ``create_clock`` to match all netlist clocks.
    Partial wildcarding (e.g. ``clk*`` to match ``clk`` and ``clk2``) is also supported.
    As mentioned above, ``*`` can be used in set_input_delay and set_output delay to refer to the netlist clock for single-clock circuits only, although this is not supported in standard SDC.

    ``{}`` escapes strings, e.g. ``{top^clk}`` matches a clock called ``top^clk``, while ``top^clk`` without braces gives an error because of the special ``^`` character.



.. _sdc_examples:

SDC Examples
-----------------
The following are sample SDC files for common non-default cases (assuming netlist clock domains ``clk`` and ``clk2``).


.. _sdc_example_A:

A
~~
Cut I/Os and analyse only register-to-register paths, including paths between clock domains; optimize to run as fast as possible.

.. code-block:: tcl

    create_clock -period 0 *


.. _sdc_example_B:

B
~~
Same as :ref:`sdc_example_A`, but with paths between clock domains cut.  Separate target frequencies are specified.

.. code-block:: tcl

    create_clock -period 2 clk
    create_clock -period 3 clk2
    set_clock_groups -exclusive -group {clk} -group {clk2}


.. _sdc_example_C:

C
~~
Same as :ref:`sdc_example_B`, but with paths to and from I/Os now analyzed.
This is the same as the multi-clock default, but with custom period constraints.

.. code-block:: tcl

    create_clock -period 2 clk
    create_clock -period 3 clk2
    create_clock -period 3.5 -name virtual_io_clock
    set_clock_groups -exclusive -group {clk} -group {clk2}
    set_input_delay -clock virtual_io_clock -max 0 [get_ports {*}]
    set_output_delay -clock virtual_io_clock -max 0 [get_ports {*}]



.. _sdc_example_D:

D
~~
Changing the phase between clocks, and accounting for delay through I/Os with set_input/output delay constraints.

.. code-block:: tcl

    #Custom waveform rising edge at 1.25, falling at 2.75
    create_clock -period 3 -waveform {1.25 2.75} clk
    create_clock -period 2 clk2
    create_clock -period 2.5 -name virtual_io_clock
    set_input_delay -clock virtual_io_clock -max 1 [get_ports {*}]
    set_output_delay -clock virtual_io_clock -max 0.5 [get_ports {*}]


.. _sdc_example_E:

E
~~
Sample using many supported SDC commands.  Inputs and outputs are constrained on separate virtual clocks.

.. code-block:: tcl

    create_clock -period 3 -waveform {1.25 2.75} clk
    create_clock -period 2 clk2
    create_clock -period 1 -name input_clk
    create_clock -period 0 -name output_clk
    set_clock_groups -exclusive -group input_clk -group clk2
    set_false_path -from [get_clocks {clk}] -to [get_clocks {output_clk}]
    set_max_delay 17 -from [get_clocks {input_clk}] -to [get_clocks {output_clk}]
    set_multicycle_path -setup -from [get_clocks {clk}] -to [get_clocks {clk2}] 3
    set_input_delay -clock input_clk -max 0.5 [get_ports {in1 in2 in3}]
    set_output_delay -clock output_clk -max 1 [get_ports {out*}]

.. _sdc_example_F:

F
~~
Sample using all remaining SDC commands.

.. code-block:: tcl
    
    create_clock -period 3 -waveform {1.25 2.75} clk 
    create_clock -period 2 clk2
    create_clock -period 1 -name input_clk
    create_clock -period 0 -name output_clk
    set_clock_latency -source 1.0 [get_clocks{clk}] 
    #if neither early nor late is specified then the latency applies to early paths
    set_clock_groups -exclusive -group input_clk -group clk2
    set_false_path -from [get_clocks{clk}] -to [get_clocks{output_clk}]
    set_input_delay -clock input_clk -max 0.5 [get_ports{in1 in2 in3}]
    set_output_delay -clock output_clk -min 1 [get_ports{out*}]
    set_max_delay 17 -from [get_clocks{input_clk}] -to [get_clocks{output_clk}]
    set_min_delay 2 -from [get_clocks{input_clk}] -to [get_clocks{output_clk}]
    set_multicycle_path -setup -from [get_clocks{clk}] -to [get_clocks{clk2}] 3 
    #For multicycle_path, if setup is specified then hold is also implicity specified
    set_clock_uncertainty -from [get_clocks{clk}] -to [get_clocks{clk2}] 0.75 
    #For set_clock_uncertainty, if neither setup nor hold is unspecified then uncertainty is applied to both
    set_disable_timing -from [get_pins {FFA.Q\\[0\\]}] -to [get_pins {to_FFD.in\\[0\\]}]




vpr/timing_constraints.rst
--------------------------------------
Timing Constraints
==================
VPR supports setting timing constraints using Synopsys Design Constraints (SDC), an industry-standard format for specifying timing constraints.

VPR's default timing constraints are explained in :ref:`default_timing_constraints`.
The subset of SDC supported by VPR is described in :ref:`sdc_commands`.
Additional SDC examples are shown in :ref:`sdc_examples`.

.. seealso:: The :ref:`Primitive Timing Modelling Tutorial <arch_model_timing_tutorial>` which covers how to describe the timing characteristics of architecture primitives.

.. _default_timing_constraints:

Default Timing Constraints
--------------------------
If no timing constriants are specified, VPR assumes default constraints based on the type of circuit being analyzed.

Combinational Circuits
~~~~~~~~~~~~~~~~~~~~~~
Constrain all I/Os on a virtual clock ``virtual_io_clock``, and optimize this clock to run as fast as possible.

*Equivalent SDC File:*

.. code-block:: tcl

    create_clock -period 0 -name virtual_io_clock
    set_input_delay -clock virtual_io_clock -max 0 [get_ports {*}]
    set_output_delay -clock virtual_io_clock -max 0 [get_ports {*}]

Single-Clock Circuits
~~~~~~~~~~~~~~~~~~~~~
Constrain all I/Os on the netlist clock, and optimize this clock to run as fast as possible.

*Equivalent SDC File:*

.. code-block:: tcl

    create_clock -period 0 *
    set_input_delay -clock * -max 0 [get_ports {*}]
    set_output_delay -clock * -max 0 [get_ports {*}]


Multi-Clock Circuits
~~~~~~~~~~~~~~~~~~~~~
Constrain all I/Os a virtual clock ``virtual_io_clock``.
Does not analyse paths between netlist clock domains, but analyses all paths from I/Os to any netlist domain.
Optimizes all clocks, including I/O clocks, to run as fast as possible.

.. warning:: By default VPR does not analyze paths between netlist clock domains.

*Equivalent SDC File:*

.. code-block:: tcl

    create_clock -period 0 *
    create_clock -period 0 -name virtual_io_clock
    set_clock_groups -exclusive -group {clk} -group {clk2}
    set_input_delay -clock virtual_io_clock -max 0 [get_ports {*}]
    set_output_delay -clock virtual_io_clock -max 0 [get_ports {*}]

Where ``clk`` and ``clk2`` are the netlist clocks in the design.
This is similarily extended if there are more than two netlist clocks.




vpr/vpr_constraints.rst
--------------------------------------
VPR Constraints
=========================
.. _vpr_constraints:

Users can specify placement and/or global routing constraints on all or part of a design through a constraints file in XML format, as shown in the format below. These constraints are optional and allow detailed control of the region on the chip in which parts of the design are placed, and of the routing of global nets through dedicated (usually clock) networks. 

.. code-block:: xml
   :caption: The overall format of a VPR constraints file
   :linenos:

   <vpr_constraints tool_name="vpr">
       <partition_list>
         <!-- Placement constraints are specified inside this tag -->
       </partition_list>
       <global_route_constraints>
         <!-- Global routing constraints are specified inside this tag -->
       </global_route_constraints>
   </vpr_constraints>

.. note:: Use the VPR option :option:`vpr --read_vpr_constraints` to specify the VPR constraints file that is to be loaded.

The top-level tag is the ``<vpr_constraints>`` tag. This tag can contain ``<partition_list>`` and ``<global_route_constraints>`` tags. The ``<partition_list>`` tag contains information related to placement constraints, while ``<global_route_constraints>`` contains information about global routing constraints. The details for each of these constraints are given in the respective sections :ref:`Placement Constraints <placement_constraints>` and :ref:`Global Route Constraints <global_routing_constraints>`.

.. toctree::
   :maxdepth: 2

   placement_constraints
   global_routing_constraints





vtr/benchmarks.rst
--------------------------------------
.. _benchmarks:

Benchmarks
==========

There are several sets of benchmark designs which can be used with VTR.

VTR Benchmarks
--------------
The VTR benchmarks :cite:`luu_vtr,luu_vtr_7` are a set of medium-sized benchmarks included with VTR.
They are fully compatible with the full VTR flow.
They are suitable for FPGA architecture research and medium-scale CAD research.



.. _table_vtr_benchmarks:

.. table:: The VTR 7.0 Benchmarks.

    ================    =================
    Benchmark           Domain
    ================    =================
    bgm                 Finance
    blob_merge          Image Processing
    boundtop            Ray Tracing
    ch_intrinsics       Memory Init
    diffeq1             Math
    diffeq2             Math
    LU8PEEng            Math
    LU32PEEng           Math
    mcml                Medical Physics
    mkDelayWorker32B    Packet Processing
    mkPktMerge          Packet Processing
    mkSMAdapter4B       Packet Processing
    or1200              Soft Processor
    raygentop           Ray Tracing
    sha                 Cryptography
    stereovision0       Computer Vision
    stereovision1       Computer Vision
    stereovision2       Computer Vision
    stereovision3       Computer Vision
    ================    =================

The VTR benchmarks are provided as Verilog under: ::

    $VTR_ROOT/vtr_flow/benchmarks/verilog

This provides full flexibility to modify and change how the designs are implemented (including the creation of new netlist primitives).

The VTR benchmarks are also included as pre-synthesized BLIF files under: ::

    $VTR_ROOT/vtr_flow/benchmarks/vtr_benchmarks_blif

.. _titan_benchmarks:

Titan Benchmarks
----------------
The Titan benchmarks are a set of large modern FPGA benchmarks compatible with Intel Stratix IV :cite:`murray_titan,murray_timing_driven_titan` and Stratix 10 :cite:`talaei_titan2` devices.
The pre-synthesized versions of these benchmarks are compatible with recent versions of VPR.

The Titan benchmarks are suitable for large-scale FPGA CAD research, and FPGA architecture research which does not require synthesizing new netlist primitives.

.. note:: The Titan benchmarks are not included with the VTR release (due to their size). However they can be downloaded and extracted by running ``make get_titan_benchmarks`` from the root of the VTR tree.  They can also be `downloaded manually <https://www.eecg.utoronto.ca/~vaughn/titan/>`_.

.. seealso:: :ref:`titan_benchmarks_tutorial`

Koios 2.0 Benchmarks
-----------------
The Koios benchmarks :cite:`koios_benchmarks` are a set of Deep Learning (DL) benchmarks.
They are suitable for DL related architecture and CAD research.
There are 40 designs that include several medium-sized benchmarks and some large benchmarks.
The designs target different network types (CNNs, RNNs, MLPs, RL) and layer types (fully-connected, convolution, activation, softmax, reduction, eltwise).
Some of the designs are generated from HLS tools as well.
These designs use many precisions including binary, different fixed point types int8/16/32, brain floating point (bfloat16), and IEEE half-precision floating point (fp16).

..  table_koios_benchmarks:

.. table:: The Koios Benchmarks.

    =================   ======================================
    Benchmark           Description
    =================   ======================================
    dla_like            Intel-DLA-like accelerator
    clstm_like          CLSTM-like accelerator
    deepfreeze          ARM FixyNN design
    tdarknet_like       Accelerator for Tiny Darknet
    bwave_like          Microsoft-Brainwave-like design
    lstm                LSTM engine
    bnn                 4-layer binary neural network
    lenet               Accelerator for LeNet-5
    dnnweaver           DNNWeaver accelerator
    tpu_like            Google-TPU-v1-like accelerator
    gemm_layer          20x20 matrix multiplication engine
    attention_layer     Transformer self-attention layer
    conv_layer          GEMM based convolution
    robot_rl            Robot+maze application
    reduction_layer     Add/max/min reduction tree
    spmv                Sparse matrix vector multiplication
    eltwise_layer       Matrix elementwise add/sub/mult
    softmax             Softmax classification layer
    conv_layer_hls      Sliding window convolution
    proxy               Proxy/synthetic benchmarks
    =================   ======================================

The Koios benchmarks are provided as Verilog (enabling full flexibility to modify and change how the designs are implemented) under: ::

    $VTR_ROOT/vtr_flow/benchmarks/verilog/koios

To use these benchmarks, please see the documentation in the README file at: https://github.com/verilog-to-routing/vtr-verilog-to-routing/tree/master/vtr_flow/benchmarks/verilog/koios


MCNC20 Benchmarks
-----------------
The MCNC benchmarks :cite:`mcnc_benchmarks` are a set of small and old (circa 1991) benchmarks.
They consist primarily of logic (i.e. LUTs) with few registers and no hard blocks.

.. warning::
    The MCNC20 benchmarks are not recommended for modern FPGA CAD and architecture research.
    Their small size and design style (e.g. few registers, no hard blocks) make them unrepresentative of modern FPGA usage.
    This can lead to misleading CAD and/or architecture conclusions.

The MCNC20 benchmarks included with VTR are available as ``.blif`` files under::

    $VTR_ROOT/vtr_flow/benchmarks/blif/

The versions used in the VPR 4.3 release, which were mapped to :math:`K`-input look-up tables using FlowMap :cite:`cong_flowmap`, are available under::

    $VTR_ROOT/vtr_flow/benchmarks/blif/<#>

where :math:`K=` ``<#>``.

.. _table_mcnc20_benchmarks:

.. table:: The MCNC20 benchmarks.

    =========   ========================================
    Benchmark   Approximate Number of Netlist Primitives
    =========   ========================================
    alu4         934
    apex2       1116
    apex4        916
    bigkey      1561
    clma        3754
    des         1199
    diffeq      1410
    dsip        1559
    elliptic    3535
    ex1010      2669
    ex5p         824
    frisc       3291
    misex3       842
    pdc         2879
    s298         732
    s38417      4888
    s38584.1    4726
    seq         1041
    spla        2278
    tseng       1583
    =========   ========================================

SymbiFlow Benchmarks
--------------------

SymbiFlow benchmarks are a set of small and medium sized tests to verify and test the SymbiFlow-generated
architectures, including primarily the Xilinx Artix-7 device families.

The tests are generated by nightly builds from the `symbiflow-arch-defs repository <https://github.com/SymbiFlow/symbiflow-arch-defs>`_, and uploaded to a Google Cloud Platform
from where they are fetched and executed in the VTR benchmarking suite.

The circuits are the following:

.. _table_symbiflow_benchmarks:

.. table:: The SymbiFlow benchmarks.

    =================   ==========================================================================
    Benchmark           Description
    =================   ==========================================================================
    picosoc @100 MHz    simple SoC with a picorv32 CPU running @100MHz
    picosoc @50MHz      simple SoC with a picorv32 CPU running @50MHz
    base-litex          LiteX-based SoC with a VexRiscv CPU booting into a BIOS only
    ddr-litex           LiteX-based SoC with a VexRiscv CPU and a DDR controller
    ddr-eth-litex       LiteX=based SoC with a VexRiscv CPU, a DDR controller and an Ethernet core
    linux-litex         LiteX-based SoC with a VexRiscv CPU capable of booting linux
    =================   ==========================================================================

The SymbiFlow benchmarks can be downloaded and extracted by running the following:

.. code-block:: bash

    cd $VTR_ROOT
    make get_symbiflow_benchmarks

Once downloaded and extracted, benchmarks are provided as post-synthesized blif files under: ::

    $VTR_ROOT/vtr_flow/benchmarks/symbiflow

.. _noc_benchmarks:

NoC Benchmarks
----------------
NoC benchmarks are composed of synthetic and MLP benchmarks and target NoC-enhanced FPGA architectures. Synthetic
benchmarks include a wide variety of traffic flow patters and are divided into two groups: 1) simple and 2) complex
benchmarks. As their names imply, simple benchmarks use very simple and small logic modules connected to NoC routers,
while complex benchmarks implement more complicated functionalities like encryption. These benchmarks do not come from
real application domains. On the other hand, MLP benchmarks include modules that perform matrix-vector multiplication
and move data. Pre-synthesized netlists for the synthetic benchmarks are added to VTR project, but MLP netlists should
be downloaded separately.

.. note:: The NoC MLP benchmarks are not included with the VTR release (due to their size). However they can be downloaded and extracted by running ``make get_noc_mlp_benchmarks`` from the root of the VTR tree.  They can also be `downloaded manually <https://www.eecg.utoronto.ca/~vaughn/titan/>`_.



vtr/cad_flow.rst
--------------------------------------
.. _vtr_cad_flow:

VTR CAD Flow
============

.. _fig_vtr_cad_flow:

.. figure:: vtr_flow_fig.*

    VTR CAD flow (and variants)

In the standard VTR Flow (:numref:`fig_vtr_cad_flow`), :ref:`parmys` converts a Verilog Hardware Destription Language (HDL) design into a flattened netlist consisting of logic gates, flip-flops, and blackboxes representing heterogeneous blocks (e.g. adders, multipliers, RAM slices).

Next, the :ref:`abc`  synthesis package is used to perform technology-independent logic optimization, and technology-maps the circuit into LUTs :cite:`abc_cite,pistorius_benchmarking_method_fpga_synthesis,cho_priority_cuts`.
The output of ABC is a :ref:`.blif format <blif_format>` netlist of LUTs, flip flops, and blackboxes.

:ref:`vpr` then packs this netlist into more coarse-grained logic blocks, places and then routes the circuit :cite:`betz_arch_cad,betz_phd,betz_directional_bias_routing_arch,betz_biased_global_routing_tech_report,betz_vpr,betz_cluster_based_logic_blocks,marquardt_timing_driven_packing,marquardt_timing_driven_placement,betz_automatic_generation_of_fpga_routing`.
Generating :ref:`output files <vpr_file_formats>` for each stage.
VPR will analyze the resulting implementation, producing various statistics such as the minimum number of tracks per channel required to successfully route, the total wirelength, circuit speed, area and power.
VPR can also produce a post-implementation netlist for simulation and formal verification.

CAD Flow Variations
-------------------

Titan CAD Flow
~~~~~~~~~~~~~~
The Titan CAD Flow :cite:`murray_titan,murray_timing_driven_titan, talaei_titan2` interfaces Intel's Quartus tool with VPR.
This allows designs requiring industrial strength language coverage and IP to be brought into VPR.

Other CAD Flow Variants
~~~~~~~~~~~~~~~~~~~~~~~
Many other CAD flow variations are possible.

For instance, it is possible to use other logic synthesis tools like Yosys :cite:`yosys_cite` to generate the design netlist.
One could also use logic optimizers and technology mappers other than ABC; just put the output netlist from your technology-mapper into .blif format and pass it into VPR.

It is also possible to use tools other than VPR to perform the different stages of the implementation.

For example, if the logic block you are interested in is not supported by VPR, your CAD flow can bypass VPR's packer by outputting a netlist of logic blocks in :ref:`.net format <vpr_net_file>`.
VPR can place and route netlists of any type of logic block -- you simply have to create the netlist and describe the logic block in the FPGA architecture description file.

Similarly, if you want only to route a placement produced by another CAD tool you can create a :ref:`.place file <vpr_place_file>`, and have VPR route this pre-existing placement.

If you only need to analyze an implementation produced by another tool, you can create a :ref:`.route file <vpr_route_file>`, and have VPR analyze the implementation, to produce area/delay/power results.

Finally, if your routing architecture is not supported by VPR's architecture generator, you can describe your routing architecture in an :ref:`rr_graph.xml file <vpr_route_resource_file>`, which can be loaded directly into VPR.

Bitstream Generation
--------------------
The technology mapped netlist and packing/placement/routing results produced by VPR contain the information needed to generate a device programming bitstreams.

VTR focuses on the core physical design optimization tools and evaluation capabilities for new architectures and does not directly support generating device programming bitstreams.
Bitstream generators can either ingest the implementation files directly or make use of VTR utilities to emit :ref:`FASM <genfasm>`.



vtr/get_vtr.rst
--------------------------------------
.. _get_vtr:

Get VTR
-----------

How to Cite
~~~~~~~~~~~

Citations are important in academia, as they ensure contributors receive credit for their efforts.
Therefore please use the following paper as a general citation whenever you use VTR:

    K. E. Murray, O. Petelin, S. Zhong, J. M. Wang, M. ElDafrawy, J.-P. Legault, E. Sha, A. G. Graham, J. Wu, M. J. P. Walker, H. Zeng, P. Patros, J. Luu, K. B. Kent and V. Betz "VTR 8: High Performance CAD and Customizable FPGA Architecture Modelling", ACM TRETS, 2020

Bibtex:

.. code-block:: none

    @article{vtr8,
      title={VTR 8: High Performance CAD and Customizable FPGA Architecture Modelling},
      author={Murray, Kevin E. and Petelin, Oleg and Zhong, Sheng and Wang, Jai Min and ElDafrawy, Mohamed and Legault, Jean-Philippe and Sha, Eugene and Graham, Aaron G. and Wu, Jean and Walker, Matthew J. P. and Zeng, Hanqing and Patros, Panagiotis and Luu, Jason and Kent, Kenneth B. and Betz, Vaughn},
      journal={ACM Trans. Reconfigurable Technol. Syst.},
      year={2020}
    }

We are always interested in how VTR is being used, so feel free email the `vtr-users <https://verilogtorouting.org/contact/>`_ list with how you are using VTR.

Download
~~~~~~~~

The official VTR release is available from:

    https://verilogtorouting.org/download

VTR Docker Image
~~~~~~~~~~~~~~~~
A docker image for VTR is available. This image provides all the required packages and python libraries required. However, this ease to compile and run comes at the cost of some runtime increase (<10%). To pull and run the docker image of latest VTR repository, you can run the following commands:

.. code-block:: bash

    > sudo docker pull mohamedelgammal/vtr-master:latest
    > sudo docker run -it mohamedelgammal/vtr-master:latest


Release
~~~~~~~

The VTR |version| release provides the following:

* benchmark circuits,
* sample FPGA architecture description files,
* the full CAD flow, and
* scripts to run that flow.

The FPGA CAD flow takes as input, a user circuit (coded in Verilog) and a description of the FPGA architecture.
The CAD flow then maps the circuit to the FPGA architecture to produce, as output, a placed-and-routed FPGA.
Here are some highlights of the |version| full release:

* Timing-driven logic synthesis, packing, placement, and routing with multi-clock support.

* Power Analysis

* Benchmark digital circuits consisting of real applications that contain both memories and multipliers.

  Seven of the 19 circuits contain more than 10,000 6-LUTs. The largest of which is just under 100,000 6-LUTs.

* Sample architecture files of a wide range of different FPGA architectures including:

    #. Timing annotated architectures
    #. Various fracturable LUTs (dual-output LUTs that can function as one large LUT or two smaller LUTs with some shared inputs)
    #. Various configurable embedded memories and multiplier hard blocks
    #. One architecture containing embedded floating-point cores, and
    #. One architecture with carry chains.

* A front-end Verilog elaborator that has support for hard blocks.

  This tool can automatically recognize when a memory or multiplier instantiated in a user circuit is too large for a target FPGA architecture.
  When this happens, the tool can automatically split that memory/multiplier into multiple smaller components (with some glue logic to tie the components together).
  This makes it easier to investigate different hard block architectures because one does not need to modify the Verilog if the circuit instantiates a memory/multiplier that is too large.

* Packing/Clustering support for FPGA logic blocks with widely varying functionality.

  This includes memories with configurable aspect ratios, multipliers blocks that can fracture into smaller multipliers, soft logic clusters that contain fracturable LUTs, custom interconnect within a logic block, and more.

* Ready-to-run scripts that guide a user through the complexities of building the tools as well as using the tools to map realistic circuits (written in Verilog) to FPGA architectures.

* Regression tests of experiments that we have conducted to help users error check and/or compare their work.

  Along with experiments for more conventional FPGAs, we also include an experiment that explores FPGAs with embedded floating-point cores investigated in :cite:`ho_floating_point_fpga` to illustrate the usage of the VTR framework to explore unconventional FPGA architectures.

Development Repository
~~~~~~~~~~~~~~~~~~~~~~
The development repository for the Verilog-to-Routing project is hosted at:

    https://github.com/verilog-to-routing/vtr-verilog-to-routing

Unlike the nicely packaged official releases the code in a constant state of flux.
You should expect that the tools are not always stable and that more work is needed to get the flow to run.



vtr/index.rst
--------------------------------------
.. _vtr:

VTR
===

.. image:: https://www.verilogtorouting.org/img/neuron_placement_macros.gif
    :width: 24%
    :alt: Placement 

.. image:: https://www.verilogtorouting.org/img/neuron_cpd.gif
    :width: 24% 
    :alt: Critical Path 

.. image:: https://www.verilogtorouting.org/img/neuron_nets.gif
    :width: 24%
    :alt: Wiring

.. image:: https://www.verilogtorouting.org/img/neuron_routing_util.gif
    :width: 24% 
    :alt: Routing Usage

The Verilog-to-Routing (VTR) project :cite:`luu_vtr,luu_vtr_7` is a world-wide collaborative effort to provide a open-source framework for conducting FPGA architecture and CAD research and development.
The VTR design flow takes as input a Verilog description of a digital circuit, and a description of the target FPGA architecture.

It then perfoms:

 * Elaboration & Synthesis (:ref:`odin_ii`)
 * Logic Optimization & Technology Mapping (:ref:`abc`)
 * Packing, Placement, Routing & Timing Analysis (:ref:`vpr`)

Generating FPGA speed and area results.

VTR also includes a set of benchmark designs known to work with the design flow.


.. toctree::
   :maxdepth: 2

   cad_flow
   get_vtr
   ../BUILDING
   optional_build_info
   running_vtr
   benchmarks
   power_estimation/index.rst
   server_mode/index.rst
   tasks
   run_vtr_flow
   run_vtr_task
   parse_vtr_flow
   parse_vtr_task
   parse_config
   pass_requirements
   python_libs/vtr






vtr/optional_build_info.md
--------------------------------------
# Optional Build Information #

This page contains additional information about the VTR build system, and how to build VTR on other OS platforms or with non-standard build options.  If you only need to the default features of VTR on a Debian/Ubuntu system, the previous [Building VTR](../BUILDING.md) page should be sufficient and you can skip this page.

## Dependencies

Most package and Python dependencies can be installed using the instructions on the previous [Building VTR](../BUILDING.md) page.  However, more detailed information is provided here.

### CMake

VTR uses [CMake](https://cmake.org) as it's build system. 

CMake provides a portable cross-platform build systems with many useful features.

For unix-like systems we provide a wrapper Makefile which supports the traditional `make` and `make clean` commands, but calls CMake behind the scenes.

### Tested Compilers
VTR requires a C++-14 compliant compiler.
It is tested against the default compilers of all Debian and Ubuntu releases within their standard support lifetime. Currently, those are the following:

* GCC/G++: 9, 10, 11, 12
* Clang/Clang++: 11, 12, 13, 14

Other compilers may work but are untested (your milage may vary).

### Package Dependencies

  * On Linux, the fastest way to set up all dependencies is to enter the commands listed in the VTR Quick Start [Environment Setup](https://docs.verilogtorouting.org/en/latest/quickstart/#environment-setup).
  
  * At minimum you will require:
    * A modern C++ compiler supporting C++14 (such as GCC >= 4.9 or clang >= 3.6)
	* cmake, make
	* bison, flex, pkg-config
  * Additional packages are required for the VPR GUI (Cairo, FreeType, libXft, libX11, fontconfig, libgtk-3-dev)
  * The scripts to run the entire VTR flow, as well as the regressions scripts, require Python3 and Python packages listed in the *requirements.txt* file.
  * Developers may also wish to install other packages (git, ctags, gdb, valgrind, clang-format-7)
  * To generate the documentation you will need Sphinx, Doxygen, and several Python packages.  The Python packages can be installed with the following command:

		pip install -r doc/requirements.txt

## Build Options

### Build Type
You can specify the build type by passing the `BUILD_TYPE` parameter.

For instance to create a debug build (no optimization and debug symbols):

```shell
#In the VTR root
$ make BUILD_TYPE=debug
...
[100%] Built target vpr
```


### Passing parameters to CMake 
You can also pass parameters to CMake.

For instance to set the CMake configuration variable `VTR_ENABLE_SANITIZE` on:

```shell
#In the VTR root
$ make CMAKE_PARAMS="-DVTR_ENABLE_SANITIZE=ON"
...
[100%] Built target vpr
```

Both the `BUILD_TYPE` and `CMAKE_PARAMS` can be specified concurrently:
```shell
#In the VTR root
$ make BUILD_TYPE=debug CMAKE_PARAMS="-DVTR_ENABLE_SANITIZE=ON"
...
[100%] Built target vpr
```


### Using CMake directly
You can also use cmake directly.

First create a build directory under the VTR root:

```shell
#In the VTR root
$ mkdir build
$ cd build

#Call cmake pointing to the directory containing the root CMakeLists.txt
$ cmake ..

#Build
$ make
```

#### Changing configuration on the command line
You can change the CMake configuration by passing command line parameters.

For instance to set the configuration to debug:

```shell
#In the build directory
$ cmake . -DCMAKE_BUILD_TYPE=debug

#Re-build
$ make
```

#### Changing configuration interactively with ccmake 
You can also use `ccmake` to to modify the build configuration.

```shell
#From the build directory
$ ccmake . #Make some configuration change

#Build
$ make
```

## Other platforms

CMake supports a variety of operating systems and can generate project files for a variety of build systems and IDEs.
While VTR is developed primarily on Linux, it should be possible to build on different platforms (your milage may vary).
See the [CMake documentation](https://cmake.org) for more details about using cmake and generating project files on other platforms and build systems (e.g. Eclipse, Microsoft Visual Studio).


### Nix 

Nix can be used to build VTR on other platforms, such as MacOS.

If you don't have [Nix](https://nixos.org/nix/), you can [get it](https://nixos.org/nix/download.html) with:

```shell
$ curl -L https://nixos.org/nix/install | sh
```

These commands will set up dependencies for Linux and MacOS and build VTR:

```shell
#In the VTR root
$ nix-shell dev/nix/shell.nix
$ make
```

### Microsoft Windows

*NOTE: VTR support on Microsoft Windows is considered experimental*

#### WSL

The [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/) (WSL), "lets developers run a GNU/Linux environment -- including most command-line tools, utilities, and applications -- directly on Windows, unmodified, without the overhead of a traditional virtual machine or dual-boot setup."

This is the recommended way to run VTR on Windows systems.

#### Cygwin ####
[Cygwin](https://www.cygwin.com/) provides a POSIX (i.e. unix-like) environment for Microsoft Windows.

From within the cygwin terminal follow the Unix-like build instructions listed above.

Note that the generated executables will rely upon Cygwin (e.g. `cygwin1.dll`) for POSIX compatibility.

#### Cross-compiling from Linux to Microsoft Windows with MinGW-W64 ####
It is possible to cross-compile from a Linux host system to generate Microsoft Windows executables using the [MinGW-W64](https://mingw-w64.org) compilers.
These can usually be installed with your Linux distribution's package manager (e.g. `sudo apt-get install mingw-w64` on Debian/Ubuntu).

Unlike Cygwin, MinGW executables will depend upon the standard Microsoft Visual C++ run-time.

To build VTR using MinGW:
```shell
#In the VTR root
$ mkdir build_win64
$ cd build_win64

#Run cmake specifying the toolchain file to setup the cross-compilation environment
$ cmake .. -DCMAKE_TOOLCHAIN_FILE ../cmake/toolchains/mingw-linux-cross-compile-to-windows.cmake

#Building will produce Windows executables
$ make
```

Note that by default the MS Windows target system will need to dynamically link to the `libgcc` and `libstdc++` DLLs.
These are usually found under /usr/lib/gcc on the Linux host machine.

See the [toolchain file](https://github.com/verilog-to-routing/vtr-verilog-to-routing/blob/master/cmake/toolchains/mingw-linux-cross-compile-to-windows.cmake) for more details.

#### Microsoft Visual Studio ####
CMake can generate a Microsft Visual Studio project, enabling VTR to be built with the Microsoft Visual C++ (MSVC) compiler.

##### Installing additional tools #####
VTR depends on some external unix-style tools during it's buid process; in particular the `flex` and `bison` parser generators.

One approach is to install these tools using [MSYS2](http://www.msys2.org/), which provides up-to-date versions of many unix tools for MS Windows.

To ensure CMake can find the `flex` and `bison` executables you must ensure that they are available on your system path.
For instance, if MSYS2 was installed to `C:\msys64` you would need to ensure that `C:\msys64\usr\bin` was included in the system PATH environment variable.

##### Generating the Visual Studio Project #####
CMake (e.g. the `cmake-gui`) can then be configured to generate the MSVC project.



vtr/parse_config.rst
--------------------------------------
.. _vtr_parse_config:

Parse Configuration
-------------------

A parse configuration file defines a set of values that will be searched for within the specified files.

Format
~~~~~~

The configuration file contains one line for each value to be searched for.
Each line contains a semicolon delimited tuple in the following format::

    <field_name>;<file_to_search_within>;<regex>;<default_value>

* ``<field_name>``: The name of the value to be searched for.

    This name is used when generating the output files of :ref:`parse_vtr_task` and :ref:`parse_vtr_flow`.

* ``<file_to_search_within>``: The name of the file that will be searched (vpr.out, parmys.out, etc.)

* ``<regex>``: A perl regular expression used to find the desired value.

    The regex must contain a single grouping ``(`` ``)`` which will contain the desired value to be recorded.

* ``<default_value>``: The default value for the given ``<field_name>`` if the ``<regex>`` does not match.

    If no ``<default_value>`` is specified the value ``-1`` is used.

Or an include directive to import parsing patterns from a separate file::

    %include "<filepath>"

* ``<filepath>`` is a file containing additional parse specifications which will be included in the current file.

Comments can be specified with ``#``. Anything following a ``#`` is ignored.

Example File
~~~~~~~~~~~~

The following is an example parse configuration file:

.. code-block:: none

    vpr_status;output.txt;vpr_status=(.*)
    vpr_seconds;output.txt;vpr_seconds=(\d+)
    width;vpr.out;Best routing used a channel width factor of (\d+)
    pack_time;vpr.out;Packing took (.*) seconds
    place_time;vpr.out;Placement took (.*) seconds
    route_time;vpr.out;Routing took (.*) seconds
    num_pre_packed_nets;vpr.out;Total Nets: (\d+)
    num_pre_packed_blocks;vpr.out;Total Blocks: (\d+)
    num_post_packed_nets;vpr.out;Netlist num_nets:\s*(\d+)
    num_clb;vpr.out;Netlist clb blocks:\s*(\d+)
    num_io;vpr.out;Netlist inputs pins:\s*(\d+)
    num_outputs;vpr.out;Netlist output pins:\s*(\d+)
    num_lut0;vpr.out;(\d+) LUTs of size 0
    num_lut1;vpr.out;(\d+) LUTs of size 1
    num_lut2;vpr.out;(\d+) LUTs of size 2
    num_lut3;vpr.out;(\d+) LUTs of size 3
    num_lut4;vpr.out;(\d+) LUTs of size 4
    num_lut5;vpr.out;(\d+) LUTs of size 5
    num_lut6;vpr.out;(\d+) LUTs of size 6
    unabsorb_ff;vpr.out;(\d+) FFs in input netlist not absorbable
    num_memories;vpr.out;Netlist memory blocks:\s*(\d+)
    num_mult;vpr.out;Netlist mult_36 blocks:\s*(\d+)
    equiv;abc.out;Networks are (equivalent)
    error;output.txt;error=(.*)

    %include "my_other_metrics.txt"     #Include metrics from the file 'my_other_metrics.txt'



vtr/parse_vtr_flow.rst
--------------------------------------
.. _parse_vtr_flow:

parse_vtr_flow
--------------
This script parses statistics generated by a single execution of the VTR flow.

.. note:: If the user is using the :ref:`vtr_tasks` framework, :ref:`parse_vtr_task` should be used.


The script is located at::

    $VTR_ROOT/vtr_flow/scripts/python_libs/vtr/parse_vtr_flow.py

.. program:: parse_vtr_flow.py

Usage
~~~~~
Typical usage is::

    parse_vtr_flow.py <parse_path> <parse_config_file>

where:

  * ``<parse_path>`` is the directory path that contains the files to be parsed (e.g. ``vpr.out``, ``parmys.out``, etc).
  * ``<parse_config_file>`` is the path to the :ref:`vtr_parse_config` file.

Output
~~~~~~
The script will produce no standard output.
A single file named ``parse_results.txt`` will be produced in the ``<parse_path>`` folder.
The file is tab delimited and contains two lines.
The first line is a list of field names that were searched for, and the second line contains the associated values.



vtr/parse_vtr_task.rst
--------------------------------------
.. _parse_vtr_task:

parse_vtr_task
--------------

This script is used to parse the output of one or more :ref:`vtr_tasks`.
The values that will be parsed are specified using a :ref:`vtr_parse_config` file, which is specified in the task configuration.

The script will always parse the results of the latest execution of the task.

The script is located at::

    $VTR_ROOT/vtr_flow/scripts/python_libs/vtr/parse_vtr_task.py

.. program:: parse_vtr_task.py

Usage
~~~~~

Typical usage is::

    parse_vtr_task.py <task_name1> <task_name2> ...

.. note:: At least one task must be specified, either directly as a parameter or through the :option:`-l` option.

Output
~~~~~~

By default this script produces no standard output.
A tab delimited file containing the parse results will be produced for each task.
The file will be located here::

    $VTR_ROOT/vtr_flow/tasks/<task_name>/run<#>/parse_results.txt

If the :option:`-check_golden` is used, the script will output one line for each task in the format::

    <task_name>...<status>

where ``<status>`` will be ``[Pass]``, ``[Fail]``, or ``[Error]``.

Detailed Command-line Options
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. option:: -l <task_list_file>

    A file containing a list of tasks to parse.
    Each task name should be on a separate line.

.. option:: -temp_dir <path>

    Alternate directory containing task results to parse (see run_vtr_task).

    **Default:** ``config/..`` for each task being parsed

    Specifies the parent directory for the output of a set of tasks, which will contain ``<task_name>/run<#>`` directories, as well as any generated parse results.
    
    A task folder or list with a config directory must still be specified when invoking the script.

.. option:: -create_golden

    The results will be stored as golden results.
    If previous golden results exist they will be overwritten.

    The golden results are located here::

        $VTR_ROOT/vtr_flow/tasks/<task_name>/config/golden_results.txt

.. option:: -check_golden

    The results will be compared to the golden results using the :ref:`vtr_pass_requirements` file specified in the task configuration.
    A ``Pass`` or ``Fail`` will be output for each task (see below).
    In order to compare against the golden results, they must already exist, and have the same architectures, circuits and parse fields, otherwise the script will report ``Error``.

    If the golden results are missing, or need to be updated, use the :option:`-create_golden` option.



vtr/pass_requirements.rst
--------------------------------------
.. _vtr_pass_requirements:

Pass Requirements
-----------------

The :ref:`parse_vtr_task` scripts allow you to compare an executed task to a *golden* reference result.
The comparison, which is performed when using the :option:`parse_vtr_task.py -check_golden` option, which reports either ``Pass`` or ``Fail``.
The requirements that must be met to qualify as a ``Pass`` are specified in the pass requirements file.

Task Configuration
~~~~~~~~~~~~~~~~~~
Tasks can be configured to use a specific pass requirements file using the **pass_requirements_file** keyword in the :ref:`vtr_tasks` configuration file.

File Location
~~~~~~~~~~~~~
All provided pass requirements files are located here::

    $VTR_ROOT/vtr_flow/parse/pass_requirements

Users can also create their own pass requirement files.

File Format
~~~~~~~~~~~
Each line of the file indicates a single metric, data type and allowable values in the following format::

    <metric>;<requirement>

* **<metric>**: The name of the metric.

* **<requirement>**: The metric's pass requirement.

    Valid requiremnt types are:

    * ``Equal()``: The metric value must exactly match the golden reference result.
    * ``Range(<min_ratio>,<max_ratio>)``: The metric value (normalized to the golden result) must be between ``<min_ratio>`` and ``<max_ratio>``.
    * ``RangeAbs(<min_ratio>,<max_ratio>,<abs_threshold>)``: The metric value (normalized to the golden result) must be between ``<min_ratio>`` and ``<max_ratio>``, *or* the metric's absolute value must be below ``<abs_threshold>``.

Or an include directive to import metrics from a separate file::

    %include "<filepath>"

* **<filepath>**: a relative path to another pass requirements file, whose metric pass requirements will be added to the current file.

In order for a ``Pass`` to be reported, **all** requirements must be met.
For this reason, all of the specified metrics must be included in the parse results (see :ref:`vtr_parse_config`).

Comments can be specified with ``#``. Anything following a ``#`` is ignored.

Example File
~~~~~~~~~~~~

.. code-block:: none

    vpr_status;Equal()                      #Pass if precisely equal
    vpr_seconds;RangeAbs(0.80,1.40,2)       #Pass if within -20%, or +40%, or absolute value less than 2
    num_pre_packed_nets;Range(0.90,1.10)    #Pass if withing +/-10%

    %include "routing_metrics.txt"          #Import all pass requirements from the file 'routing_metrics.txt'



vtr/run_vtr_flow.rst
--------------------------------------
.. _run_vtr_flow:

run_vtr_flow
---------------

This script runs the VTR flow for a single benchmark circuit and architecture file.

The script is located at::

    $VTR_ROOT/vtr_flow/scripts/run_vtr_flow.py

.. program:: run_vtr_flow.py

Basic Usage
~~~~~~~~~~~

At a minimum ``run_vtr_flow.py`` requires two command-line arguments::

    run_vtr_flow.py <circuit_file> <architecture_file>

where:

  * ``<circuit_file>`` is the circuit to be processed
  * ``<architecture_file>`` is the target :ref:`FPGA architecture <fpga_architecture_description>`


.. note::
    The script will create a ``./temp`` directory, unless otherwise specified with the :option:`-temp_dir` option.
    The circuit file and architecture file will be copied to the temporary directory.
    All stages of the flow will be run within this directory.
    Several intermediate files will be generated and deleted upon completion.
    **Users should ensure that no important files are kept in this directory as they may be deleted.**

Output
~~~~~~
The standard out of the script will produce a single line with the format::

    <architecture>/<circuit_name>...<status>

If execution completed successfully the status will be 'OK'. Otherwise, the status will indicate which stage of execution failed.

The script will also produce an output files (\*.out) for each stage, containing the standout output of the executable(s).

Advanced Usage
~~~~~~~~~~~~~~

Additional *optional* command arguments can also be passed to ``run_vtr_flow.py``::

    run_vtr_flow.py <circuit_file> <architecture_file> [<options>] [<vpr_options>]

where:

  * ``<options>`` are additional arguments passed to ``run_vtr_flow.py`` (described below),
  * ``<vpr_options>`` are any arguments not recognized by ``run_vtr_flow.py``. These will be forwarded to VPR.

For example::

   run_vtr_flow.py my_circuit.v my_arch.xml -track_memory_usage --pack --place

will run the VTR flow to map the circuit ``my_circuit.v`` onto the architecture ``my_arch.xml``; the arguments ``--pack`` and ``--place`` will be passed to VPR (since they are unrecognized arguments to ``run_vtr_flow.py``).
They will cause VPR to perform only :ref:`packing and placement <general_options>`.

.. code-block:: bash

    # Using the Yosys conventional Verilog parser
    ./run_vtr_flow <path/to/Verilog/File> <path/to/arch/file>

    # Using the Yosys-SystemVerilog plugin if installed, otherwise the Yosys conventional Verilog parser
    ./run_vtr_flow <path/to/SystemVerilog/File> <path/to/arch/file> -parser system-verilog

Running the VTR flow with the default configuration using the Yosys standalone front-end.
The parser for these runs is considered the Yosys conventional Verilog/SystemVerilog parser (i.e., ``read_verilog -sv``), as the parser is not explicitly specified.

.. code-block:: bash

    # Using the Yosys-SystemVerilog plugin if installed, otherwise the Yosys conventional Verilog parser
    ./run_vtr_flow <path/to/SystemVerilog/File> <path/to/arch/file> -parser system-verilog

    # Using the Surelog plugin if installed, otherwise failure on the unsupported file type
    ./run_vtr_flow <path/to/UHDM/File> <path/to/arch/file> -parser surelog

Running the default VTR flow using the Parmys standalone front-end.
The Yosys HDL parser is considered as Yosys-SystemVerilog plugin (i.e., ``read_systemverilog``) and Yosys UHDM plugin (i.e., ``read_uhdm``), respectively.
Utilizing Yosys plugins requires passing the ``-DYOSYS_F4PGA_PLUGINS=ON`` compile flag to build and install the plugins for the Parmys front-end.

.. code-block:: bash

    # Using the Parmys (Partial Mapper for Yosys) plugin as partial mapper
    ./run_vtr_flow <path/to/Verilog/File> <path/to/arch/file>

Will run the VTR flow (default configuration) with Yosys frontend using Parmys plugin as partial mapper. To utilize the Parmys plugin, the ``-DYOSYS_PARMYS_PLUGIN=ON`` compile flag should be passed while building the VTR project with Yosys as a frontend.

Detailed Command-line Options
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. note:: Any options not recognized by this script is forwarded to VPR.

.. option:: -starting_stage <stage>

    Start the VTR flow at the specified stage.

    Accepted values:

      * ``odin``
      * ``parmys``
      * ``abc``
      * ``scripts``
      * ``vpr``

    **Default:** ``parmys``

.. option:: -ending_stage <stage>

    End the VTR flow at the specified stage.


    Accepted values:

      * ``odin``
      * ``parmys``
      * ``abc``
      * ``scripts``
      * ``vpr``

    **Default:** ``vpr``

.. option:: -power

    Enables power estimation.

    See :ref:`power_estimation`

.. option:: -cmos_tech <file>

    CMOS technology XML file.

    See :ref:`power_technology_properties`

.. option:: -delete_intermediate_files

    Delete intermediate files (i.e. ``.dot``, ``.xml``, ``.rc``, etc)

.. option:: -delete_result_files

    Delete result files (i.e. VPR's ``.net``, ``.place``, ``.route`` outputs)

.. option:: -track_memory_usage

    Record peak memory usage and additional statistics for each stage.

    .. note::
        Requires ``/usr/bin/time -v`` command.
        Some operating systems do not report peak memory.

    **Default:** off

.. option:: -limit_memory_usage

    Kill benchmark if it is taking up too much memory to avoid slow disk swaps.

    .. note:: Requires ``ulimit -Sv`` command.

    **Default:** off
.. option:: -timeout <float>

    Maximum amount of time to spend on a single stage of a task in seconds.

    **Default:** 14 days

.. option:: -temp_dir <path>

    Temporary directory used for execution and intermediate files.
    The script will automatically create this directory if necessary.

    **Default:** ``./temp``

.. option:: -valgrind

    Run the flow with valgrind while using the following valgrind
    options:

        * --leak-check=full
        * --errors-for-leak-kinds=none
        * --error-exitcode=1
        * --track-origins=yes

.. option:: -min_hard_mult_size <int>

    Tells Parmys/ODIN II the minimum multiplier size that should be implemented
    using hard multiplier (if available). Smaller multipliers will be
    implemented using soft logic.

    **Default:** 3

.. option:: -min_hard_adder_size <int>

    Tells Parmys/ODIN II the minimum adder size that should be implemented
    using hard adders (if available). Smaller adders will be
    implemented using soft logic.

    **Default:** 1

.. option:: -adder_cin_global

    Tells Parmys/ODIN II to connect the first cin in an adder/subtractor chain
    to a global gnd/vdd net. Instead of creating a dummy adder to generate
    the input signal of the first cin port of the chain.

.. option:: -odin_xml <path_to_custom_xml>

    Tells VTR flow to use a custom ODIN II configuration value. The default
    behavior is to use the vtr_flow/misc/basic_odin_config_split.xml. 
    Instead, an alternative config file might be supplied; compare the 
    default and vtr_flow/misc/custom_odin_config_no_mults.xml for usage 
    scenarios. This option is needed for running the entire VTR flow with 
    additional parameters for ODIN II that are provided from within the 
    .xml file.

.. option:: -use_odin_simulation 
    
    Tells ODIN II to run simulation.

.. option:: -min_hard_mult_size <min_hard_mult_size>
    
    Tells Parmys/ODIN II the minimum multiplier size (in bits) to be implemented using hard multiplier.
    
    **Default:** 3

.. option:: -min_hard_adder_size <MIN_HARD_ADDER_SIZE>
    
    Tells Parmys/ODIN II the minimum adder size (in bits) that should be implemented using hard adder.
    
    **Default:** 1

.. option:: -top_module <TOP_MODULE>
    
    Specifies the name of the module in the design that should be considered as top

.. option:: -yosys_script <YOSYS_SCRIPT>
    
    Supplies Parmys(Yosys) with a .ys script file (similar to Tcl script), including the synthesis steps.
    
    **Default:** None

.. option:: -parser <PARSER>

    Specify a parser for the Yosys synthesizer [default (Verilog-2005), surelog (UHDM), system-verilog].
    The script uses the default conventional Verilog parser if this argument is not used.
    
    **Default:** default

.. note::

    Universal Hardware Data Model (UHDM) is a complete modeling of the IEEE SystemVerilog Object Model with VPI Interface, Elaborator, Serialization, Visitor and Listener.
    UHDM is used as a compiled interchange format in between SystemVerilog tools. Typical inputs to the UHDM flow are files with ``.v`` or ``.sv`` extensions.
    The ``system-verilog`` parser, which represents the ``read_systemverilog`` command, reads SystemVerilog files directly in Yosys.
    It executes Surelog with provided filenames and converts them (in memory) into UHDM file. Then, this UHDM file is converted into Yosys AST. `[Yosys-SystemVerilog] <https://github.com/antmicro/yosys-systemverilog#usage>`_
    On the other hand, the ``surelog`` parser, which uses the ``read_uhdm`` Yosys command, walks the design tree and converts its nodes into Yosys AST nodes using Surelog. `[UHDM-Yosys <https://github.com/chipsalliance/UHDM-integration-tests#uhdm-yosys>`_, `Surelog] <https://github.com/chipsalliance/Surelog#surelog>`_

.. note::

    Parmys is a Yosys plugin which provides intelligent partial mapping features (inference, binding, and hard/soft logic trade-offs) from Odin-II for Yosys. For more information on available paramters see the `Parmys <https://github.com/CAS-Atlantic/parmys-plugin.git>`_ plugin page.



vtr/run_vtr_task.rst
--------------------------------------
.. _run_vtr_task:

run_vtr_task
---------------
This script is used to execute one or more *tasks* (i.e. collections of benchmarks and architectures).

.. seealso:: See :ref:`vtr_tasks` for creation and configuration of tasks.

This script runs the VTR flow for a single benchmark circuit and architecture file.

The script is located at::

    $VTR_ROOT/vtr_flow/scripts/run_vtr_task.py

.. program:: run_vtr_task.py

Basic Usage
~~~~~~~~~~~

Typical usage is::

    run_vtr_task.py <task_name1> <task_name2> ...

.. note:: At least one task must be specified, either directly as a parameter or via the :option:`-l` options.

Output
~~~~~~
Each task will execute the script specified in the configuration file for every benchmark/circuit/option combination.
The standard output of the underlying script will be forwarded to the output of this script.

If golden results exist (see :ref:`parse_vtr_task`), they will be inspected for runtime and memory usage.

Detailed Command-line Options
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. option:: -s <script_param> ...

    Treat the remaining command line options as parameters to forward to the underlying script (e.g. :ref:`run_vtr_flow`).

.. option:: -j <N>

    Perform parallel execution using ``N`` threads.

    .. note::
        Only effective for ``-system local``

    .. warning::
        Large benchmarks will use very large amounts of memory (several to 10s of gigabytes).
        Because of this, parallel execution often saturates the physical memory, requiring the use of swap memory, which significantly slows execution.
        Be sure you have allocated a sufficiently large swap memory or errors may result.

.. option:: -l <task_list_file>

    A file containing a list of tasks to execute.

    Each task name should be on a separate line, e.g.::

        <task_name1>
        <task_name2>
        <task_name3>
        ...

.. option:: -temp_dir <path>

    Alternate directory for files generated by a set of tasks.
    The script will automatically create this directory if necessary.

    **Default:** ``config/..`` for each task being run

    Specifies the parent directory for the output of this set of tasks, which will contain ``<task_name>/run<#>`` directories, as well as any generated parse results.
    
    A task folder or list with a config directory must still be specified when invoking the script.

.. option:: -system {local | scripts}

   Controls how the actions (e.g. invocations of :ref:`run_vtr_flow`) are called.

   **Default:** ``local``

   * ``local``: Runs the flow invocations on the local machine (potentially in parallel with the ``-j`` option).

      Example:

      .. code-block:: console

         #From $VTR_ROOT/vtr_flow/tasks

         $ ../scripts/run_vtr_task.py regression_tests/vtr_reg_basic/basic_timing
         regression_tests/vtr_reg_basic/basic_timing: k6_N10_mem32K_40nm.xml/ch_intrinsics.v/common          OK              (took 2.24 seconds)
         regression_tests/vtr_reg_basic/basic_timing: k6_N10_mem32K_40nm.xml/diffeq1.v/common                OK              (took 10.94 seconds)

   * ``scripts``: Prints out all the generated script files (instead of calling them to run all the flow invocations).

      Example:

      .. code-block:: console

         #From $VTR_ROOT/vtr_flow/tasks

         $ ../scripts/run_vtr_task.py regression_tests/vtr_reg_basic/basic_timing -system scripts
         /project/trees/vtr/vtr_flow/tasks/regression_tests/vtr_reg_basic/basic_timing/run001/k6_N10_mem32K_40nm.xml/ch_intrinsics.v/common/vtr_flow.sh
         /project/trees/vtr/vtr_flow/tasks/regression_tests/vtr_reg_basic/basic_timing/run001/k6_N10_mem32K_40nm.xml/diffeq1.v/common/vtr_flow.sh

      Each generated script file (``vtr_flow.sh``) corresponds to a particular flow invocation generated by the task, and is located within its own directory.

      This list of scripts can be used to run flow invocations on different computing infrastructures (e.g. a compute cluster).

      **Using the output of -system scripts to run a task**

      An example of using the output would be:

      .. code-block:: console

         #From $VTR_ROOT/vtr_flow/tasks

         $ ../scripts/run_vtr_task.py regression_tests/vtr_reg_basic/basic_timing -system scripts | parallel -j4 'cd $(dirname {}) && {}'
         regression_tests/vtr_reg_basic/basic_timing: k6_N10_mem32K_40nm.xml/ch_intrinsics.v/common          OK              (took 2.11 seconds)
         regression_tests/vtr_reg_basic/basic_timing: k6_N10_mem32K_40nm.xml/diffeq1.v/common                OK              (took 10.94 seconds)

      where ``{}`` is a special variable interpretted by the ``parallel`` command to represent the input line (i.e. a script, see ``parallel``'s documentation for details).
      This will run the scripts generated by run_vtr_task.py in parallel (up to 4 at-a-time due to ``-j4``).
      Each script is invoked in the script's containing directory (``cd $(dirname {})``), which mimics the behaviour of ``-system local -j4``.

      .. note::
         While this example shows how the flow invocations could be run locally, similar techniques can be used to submit jobs to other compute infrastructures (e.g. a compute cluster)

      **Determining Resource Requirements**

      Often, when running in a cluster computing enviroment, it is useful to know what compute resources are required for each flow invocation.

      Each generated ``vtr_flow.sh`` scripts contains the expected run-time and memory use of each flow invocation (derived from golden reference results).
      These can be inspected to determine compute requirements:

      .. code-block:: console

         $ grep VTR_RUNTIME_ESTIMATE_SECONDS /project/trees/vtr/vtr_flow/tasks/regression_tests/vtr_reg_basic/basic_timing/run001/k6_N10_mem32K_40nm.xml/ch_intrinsics.v/common/vtr_flow.sh
         VTR_RUNTIME_ESTIMATE_SECONDS=2.96

         $ grep VTR_MEMORY_ESTIMATE_BYTES /project/trees/vtr/vtr_flow/tasks/regression_tests/vtr_reg_basic/basic_timing/run001/k6_N10_mem32K_40nm.xml/ch_intrinsics.v/common/vtr_flow.sh
         VTR_MEMORY_ESTIMATE_BYTES=63422464

      .. note::
         If the resource estimates are unkown they will be set to ``0``



vtr/running_vtr.rst
--------------------------------------
.. _running_vtr:

Running the VTR Flow
--------------------

VTR is a collection of tools that perform the full FPGA CAD flow from Verilog to routing.

The design flow consists of:

* :ref:`parmys` (Logic Synthesis & Partial Mapping)
* :ref:`abc` (Logic Optimization & Technology Mapping)
* :ref:`vpr` (Pack, Place & Route)

There is no single executable for the entire flow.

Instead, scripts are provided to allow the user to easily run the entire tool flow.
The following provides instructions on using these scripts to run VTR.

Running a Single Benchmark
~~~~~~~~~~~~~~~~~~~~~~~~~~
The :ref:`run_vtr_flow` script is provided to execute the VTR flow for a single benchmark and architecture.

.. note:: In the following :term:`$VTR_ROOT` means the root directory of the VTR source code tree.

.. code-block:: none

    $VTR_ROOT/vtr_flow/scripts/run_vtr_flow.py <circuit_file> <architecture_file>

It requires two arguments:

 * ``<circuit_file>`` A benchmark circuit, and
 * ``<architecture_file>`` an FPGA architecture file

Circuits can be found under::

    $VTR_ROOT/vtr_flow/benchmarks/

Architecture files can be found under::

    $VTR_ROOT/vtr_flow/arch/

The script can also be used to run parts of the VTR flow.

.. seealso:: :ref:`run_vtr_flow` for the detailed command line options of ``run_vtr_flow.py``.


Running Multiple Benchmarks & Architectures with Tasks
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
VTR also supports *tasks*, which manage the execution of the VTR flow for multiple benchmarks and architectures.
By default, tasks execute the :ref:`run_vtr_flow` for every circuit/architecture combination.

VTR provides a variety of standard tasks which can be found under::

    $VTR_ROOT/vtr_flow/tasks


Tasks can be executed using :ref:`run_vtr_task`::

    $VTR_ROOT/vtr_flow/scripts/run_vtr_task.py <task_name>

.. seealso:: :ref:`run_vtr_task` for the detailed command line options of ``run_vtr_task.py``.

.. seealso:: :ref:`vtr_tasks` for more information on creating, modifying and running tasks.


Extracting Information & Statistics
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
VTR can also extract useful information and statistics from executions of the flow such as area, speed tool execution time etc.

For single benchmarks :ref:`parse_vtr_flow` extrastics statistics from a single execution of the flow.

For a :ref:`Task <vtr_tasks>`, :ref:`parse_vtr_task` can be used to parse and assemble statistics for the entire task (i.e. multiple circuits and architectures).

For regression testing purposes these results can also be verified against a set of *golden* reference results.
See :ref:`parse_vtr_task` for details.



vtr/tasks.rst
--------------------------------------
.. _vtr_tasks:

Tasks
-----

Tasks provide a framework for running the VTR flow on multiple benchmarks, architectures and with multiple CAD tool parameters.

A task specifies a set of benchmark circuits, architectures and CAD tool parameters to be used.
By default, tasks execute the :ref:`run_vtr_flow` script for every circuit/architecture/CAD parameter combination.

Example Tasks
~~~~~~~~~~~~~
* ``basic_flow``: Runs the VTR flow mapping a simple Verilog circuit to an FPGA architecture.

* ``timing``: Runs the flagship VTR benchmarks on a comprehensive, realistic architecture file.

* ``timing_chain``: Same as ``timing`` but with carry chains.

* ``regression_mcnc``: Runs VTR on the historical MCNC benchmarks on a legacy architecture file. (Note: This is only useful for comparing to the past, it is not realistic in the modern world)

* ``regression_titan/titan_small``: Runs a small subset of the Titan benchmarks targetting a simplified Altera Stratix IV (commercial FPGA) architecture capture

* ``regression_fpu_hard_block_arch``: Custom hard FPU logic block architecture

Directory Layout
~~~~~~~~~~~~~~~~

All of VTR's included tasks are located here::

    $VTR_ROOT/vtr_flow/tasks

If users wishes to create their own task, they must do so in this location.

All tasks must contain a configuration file located here::

    $VTR_ROOT/vtr_flow/tasks/<task_name>/config/config.txt


:numref:`fig_vtr_tasks_file_layout` illustrates the directory layout for a VTR task.
Every time the task is run a new ``run<#>`` directory is created to store the output files, where ``<#>`` is the smallest integer to make the run directory name unique.

The symbolic link ``latest`` will point to the most recent ``run<#>`` directory.

.. _fig_vtr_tasks_file_layout:

.. figure:: vtr_task_fig.*
    :width: 75%

    Task directory layout.

Creating a New Task
~~~~~~~~~~~~~~~~~~~

#. Create the folder ``$VTR_ROOT/vtr_flow/tasks/<task_name>``
#. Create the folder ``$VTR_ROOT/vtr_flow/tasks/<task_name>/config``
#. Create and configure the file ``$VTR_ROOT/vtr_flow/tasks/<task_name>/config/config.txt``


Task Configuration File
~~~~~~~~~~~~~~~~~~~~~~~
The task configuration file contains key/value pairs separated by the ``=`` character.
Comment line are indicted using the ``#`` symbol.

Example configuration file:

.. code-block:: none

    # Path to directory of circuits to use
    circuits_dir=benchmarks/verilog

    # Path to directory of architectures to use
    archs_dir=arch/timing

    # Add circuits to list to sweep
    circuit_list_add=ch_intrinsics.v
    circuit_list_add=diffeq1.v

    # Add architectures to list to sweep
    arch_list_add=k6_N10_memSize16384_memData64_stratix4_based_timing_sparse.xml

    # Parse info and how to parse
    parse_file=vpr_standard.txt

.. note::

    :ref:`run_vtr_task` will invoke the script (default :ref:`run_vtr_flow`) for the cartesian product of circuits, architectures and script parameters specified in the config file.

Required Fields
~~~~~~~~~~~~~~~

* **circuit_dir**: Directory path of the benchmark circuits.

    Absolute path or relative to ``$VTR_ROOT/vtr_flow/``.

* **arch_dir**: Directory path of the architecture XML files.

    Absolute path or relative to ``$VTR_ROOT/vtr_flow/``.

* **circuit_list_add**: Name of a benchmark circuit file.

    Use multiple lines to add multiple circuits.

* **arch_list_add**: Name of an architecture XML file.

    Use multiple lines to add multiple architectures.

* **parse_file**: :ref:`vtr_parse_config` file used for parsing and extracting the statistics.

    Absolute path or relative to ``$VTR_ROOT/vtr_flow/parse/parse_config``.

Optional Fields
~~~~~~~~~~~~~~~

* **script_path**: Script to run for each architecture/circuit combination.

    Absolute path or relative to ``$VTR_ROOT/vtr_flow/scripts/`` or ``$VTR_ROOT/vtr_flow/tasks/<task_name>/config/``)

    **Default:** :ref:`run_vtr_flow`

    Users can set this option to use their own script instead of the default.
    The circuit path will be provided as the first argument, and architecture path as the second argument to the user script.

* **script_params_common**: Common parameters to be passed to all script invocations.

    This can be used, for example, to run partial VTR flows.

    **Default:** none

* **script_params**: Alias for `script_params_common`

* **script_params_list_add**: Adds a set of command-line arguments

    Multiple `script_params_list_add` can be provided which are addded to the cartesian product of configurations to be evaluated.

* **sdc_dir**: Directory path to benchmark SDC files.

    Absolute path or relative to ``$VTR_ROOT/vtr_flow/``.

    If provided, each benchmark will look for a similarly named SDC file.

    For instance with ``circuit_list_add=my_circuit.v`` or ``circuit_list_add=my_circuit.blif``, the flow would look for an SDC file named ``my_circuit.sdc`` within the specified ``sdc_dir``.

* **includes_dir**:  Directory path to benchmark _include_ files

    Absolute path or relative to ``$VTR_ROOT/vtr_flow/``.

    Note: Multiple `includes_dir` are NOT allowed in a task config file.

* **include_list_add**: A path to an `include` file, which is relative to `includes_dir`
    
    Multiple `include_list_add` can be provided.

    `include` files could act as the top module complementary, like definitions, memory initialization files, macros or sub-modules.

    Note: Only `include` files, written in supported HDLs by each frontend, are synthesized. The others are only copied to the destination folder.
    
    Note: `include` files will be shared among all benchmark circuits in the task config file. 

* **pass_requirements_file**: :ref:`vtr_pass_requirements` file.

    Absolute path or relative to ``$VTR_ROOT/vtr_flow/parse/pass_requirements/`` or ``$VTR_ROOT/vtr_flow/tasks/<task_name>/config/``

    **Default:** none



power_estimation/index.rst
--------------------------------------
.. _power_estimation:

Power Estimation
================

VTR provides transistor-level dynamic and static power estimates for a given architecture and circuit.

:numref:`fig_power_estimation_flow` illustrates how power estimation is performed in the VTR flow.
The actual power estimation is performed within the :ref:`VPR` executable; however, additional files must be provided.
In addition to the circuit and architecture files, power estimation requires files detailing the signal activities and technology properties.

:ref:`vtr_running_power` details how to run power estimation for VTR.
:ref:`power_support_tools` provides details on the supporting tools that are used to generate the signal activities and technology properties files.
:ref:`power_arch_modeling` provides details about how the tool models architectures, including different modelling methods and options.
:ref:`power_advanced_usage` provides more advanced configuration options.


.. _fig_power_estimation_flow:

.. figure:: power_flow.*

    Power Estimation in the VTR Flow

.. _vtr_running_power:

Running VTR with Power Estimation
---------------------------------

VTR Flow
~~~~~~~~

The easiest way to run the VTR flow is to use the :ref:`run_vtr_flow` script.

In order to perform power estimation, you must add the following options:

  * :option:`run_vtr_flow.py -power`
  * :option:`run_vtr_flow.py -cmos_tech` ``<cmos_tech_properties_file>``

The CMOS technology properties file is an XML file that contains relevant process-dependent information needed for power estimation.
XML files for 22nm, 45nm, and 130nm PTM models can be found here::

$VTR_ROOT/vtrflow/tech/*

See :ref:`power_technology_properties` for information on how to generate an XML file for your own SPICE technology model.

In this mode, the VTR will run ODIN->ABC->ACE->VPR. The ACE stage is additional and specific to this power estimation flow. Using run_vtr_flow.py will automatically run ACE 2.0 to generate activity information and a new BLIF file (see ::ref:`power_ace` for details).

The final power estimates will be available in file named <circuit_name>.power in the result directory.

Here is an example command:

.. code-block:: 
    $VTR_ROOT/vtr_flow/scripts/run_vtr_flow.py ../benchmarks/verilog/diffeq1.v ../arch/timing/k6_frac_N10_frac_chain_depop50_mem32K_40nm.xml -power -cmos_tech ../tech/PTM_45nm/45nm.xml -temp_dir power_try_45nm


VPR
~~~

Power estimation can also be run directly from VPR with the following (all required) options:

* :option:`vpr --power`: Enables power estimation.
* :option:`vpr --activity_file` ``<activities.act>``: The activity file, produce by ACE 2.0, or another tool.
* :option:`vpr --tech_properties` ``<tech_properties.xml>``: The technology properties file.

Power estimation requires an activity file, which can be generated as described in :ref:`power_ace`.

.. _power_support_tools:

Supporting Tools
----------------

.. _power_technology_properties:

Technology Properties
~~~~~~~~~~~~~~~~~~~~~

Power estimation requires information detailing the properties of the CMOS technology.
This information, which includes transistor capacitances, leakage currents, etc. is included in an ``.xml`` file, and provided as a parameter to VPR.
This XML file is generated using a script which automatically runs HSPICE, performs multiple circuit simulations, and extract the necessary values.

Some of these technology XML files are included with the release, and are located here::

    $VTR_ROOT/vtr_flow/tech/*

If the user wishes to use a different CMOS technology file, they must run the following script:

.. note:: HSPICE must be available on the users path

.. code-block:: none

    $VTR_ROOT/vtr_flow/scripts/generate_cmos_tech_data.pl <tech_file> <tech_size> <vdd> <temp>


where:

    * ``<tech_file>``: Is a SPICE technology file, containing a ``pmos`` and ``nmos`` models.

    * ``<tech_size>``: The technology size, in meters.

        **Example:**

        A 90nm technology would have the value ``90e-9``.

    * ``<vdd>``: Supply voltage in Volts.

    * ``<temp>``: Operating temperature, in Celcius.


.. _power_ace:

ACE 2.0 Activity Estimation
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Power estimation requires activity information for the entire netlist.
This ativity information consists of two values:

#. *The Signal Probability*, :math:`P_1`, is the long-term probability that a signal is logic-high.

   **Example:**

   A clock signal with a 50% duty cycle will have :math:`P_1(clk) = 0.5`.

#. *The Transition Density* (or switching activity), :math:`A_S`, is the average number of times the signal will switch during each clock cycle.

   **Example:**

   A clock has :math:`A_S(clk)=2`.

The default tool used to perform activity estimation in VTR is ACE 2.0 :cite:`lamoureux_activity_estimation`.
This tool was originally designed to work with the (now obsolete) Berkeley SIS tool
ACE 2.0 was modifed to use ABC, and is included in the VTR package here::

    $VTR_ROOT/ace2

The tool can be run using the following command-line arguments::

    $VTR_ROOT/ace2/ace -b <abc.blif> -c <clock_name> -o <activities.act> -n <new.blif>

where

    * ``<abc.blif>``: Is the input BLIF file produced by ABC.
    * ``<clock_name>``: Is the name of the clock in the input BLIF file
    * ``<activities.act>``: Is the activity file to be created.
    * ``<new.blif>``: The new BLIF file.

        This will be functionally identical in function to the ABC blif; however, since ABC does not maintain internal node names, a new BLIF must be produced with node names that match the activity file. This blif file is fed to the subsequent parts of the flow (to VPR). If a user is using run_vtr_flow.py (which will run ACE 2.0 underneath if the options mentioned earlier like -power are used), then the flow will copy this ACE2 generated blif file (<circuit_name>.ace.blif) to <circuit_name>.pre-vpr.blif and then launch VPR with this new file.

User’s may with to use their own activity estimation tool.
The produced activity file must contain one line for each net in the BLIF file, in the following format::

    <net name> <signal probability> <transistion density>

.. _power_arch_modeling:

Architecture Modelling
----------------------

The following section describes the architectural assumptions made by the power model, and the related parameters in the architecture file.

Complex Blocks
~~~~~~~~~~~~~~

The VTR architecture description language supports a hierarchichal description of blocks. In the architecture file, each block is described as a ``pb_type``, which may includes one or more children of type ``pb_type``, and interconnect structures to connect them.

The power estimation algorithm traverses this hierarchy recursively, and performs power estimation for each ``pb_type``.
The power model supports multiple power estimation methods, and the user specifies the desired method in the architecture file:

.. code-block:: xml

    <pb_type>
        <power method="<estimation-method>"/>
    </pb_type>

The following is a list of valid estimation methods.
Detailed descriptions of each type are provided in the following sections.
The methods are listed in order from most accurate to least accurate.

#. ``specify-size``: Detailed transistor level modelleling.

   The user supplies all buffer sizes and wire-lengths.
   Any not provided by the user are ignored.

#. ``auto-size``: Detailed transistor level modelleling.

   The user can supply buffer sizes and wire-lengths; however, they will be automatically inserted when not provided.

#. ``pin-toggle``: Higher-level modelling.

   The user specifies energy per toggle of the pins.
   Static power provided as an absolute.

#. ``C-internal``: Higher-level modelling.

   The user supplies the internal capacitance of the block.
   Static power provided as an absolute.

#. ``absolute``: Highest-level modelling.

   The user supplies both dynamic and static power as absolutes.

Other methods of estimation:

#. ``ignore``: The power of the ``pb_type`` is ignored, including any children.

#. ``sum-of-children``: Power of ``pb_type`` is solely the sum of all children ``pb_types``.

    Interconnect between the ``pb_type`` and its children is ignored.

.. note:: If no estimation method is provided, it is inherited from the parent ``pb_type``.

.. note:: If the top-level ``pb_type`` has no estimation method, ``auto-size`` is assumed.


``specify-size``
""""""""""""""""
This estimation method provides a detailed transistor level modelling of CLBs, and will provide the most accurate power estimations.
For each ``pb_type``, power estimation accounts for the following components (see :numref:`fig_power_sample_block`).

    * Interconnect multiplexers

    * Buffers and wire capacitances

    * Child ``pb_types``


.. _fig_power_sample_block:

.. figure:: power_sample_clb.*

    Sample Block

**Multiplexers:**
Interconnect multiplexers are modelled as 2-level pass-transistor multiplexers, comprised of minimum-size NMOS transistors.
Their size is determined automatically from the ``<interconnect/>`` structures in the architecture description file.

**Buffers and Wires:**
Buffers and wire capacitances are not defined in the architecture file, and must be explicitly added by the user.
They are assigned on a per port basis using the following construct:

.. code-block:: xml

    <pb_type>
        <input name="my_input" num_pins="1">
            <power ...options.../>
        </input>
    </pb_type>

The wire and buffer attributes can be set using the following options.
If no options are set, it is assumed that the wire capacitance is zero, and there are no buffers present.
Keep in mind that the port construct allows for multiple pins per port.
These attributes will be applied to each pin in the port.
If necessary, the user can seperate a port into multiple ports with different wire/buffer properties.

* ``wire_capacitance=1.0e-15``: The absolute capacitance of the wire, in Farads.

* ``wire_length=1.0e-7``: The absolute length of the wire, in meters.

    The local interconnect capacitance option must be specified, as described in :ref:`power_local_interconnect_capacitance`.

* ``wire_length=auto``: The wirelength is automatically sized. See :ref:`power_local_wire_auto_sizing`.

* ``buffer_size=2.0``: The size of the buffer at this pin. See for more :ref:`power_buffer_sizing` information.

* ``buffer_size=auto``: The size of the buffer is automatically sized, assuming it drives the above wire capacitance and a single multiplexer. See :ref:`power_buffer_sizing` for more information.

**Primitives:**
For all child ``pb_types``, the algorithm performs a recursive call.
Eventually ``pb_types`` will be reached that have no children.
These are primitives, such as flip-flops, LUTs, or other hard-blocks.
The power model includes functions to perform transistor-level power estimation for flip-flops and LUTs (Note: the power model doesn't, by default, include power estimation for single-bit adders that are commonly found in logic blocks of modern FPGAs).
If the user wishes to use a design with other primitive types (memories, multipliers, etc), they must provide an equivalent function.
If the user makes such a function, the ``power_usage_primitive`` function should be modified to call it.
Alternatively, these blocks can be configured to use higher-level power estimation methods.

``auto-size``
""""""""""""""""
This estimation method also performs detailed transistor-level modelling.
It is almost identical to the ``specify-size`` method described above.
The only difference is that the local wire capacitance and buffers are automatically inserted for all pins, when necessary.
This is equivalent to using the ``specify-size`` method with the ``wire_length=auto`` and ``buffer_size=auto`` options for every port.

.. note:: **This is the default power estimation method.**

Although not as accurate as user-provided buffer and wire sizes, it is capable of automatically capturing trends in power dissipation as architectures are modified.

``pin-toggle``
""""""""""""""""
This method allows users to specify the dynamic power of a block in terms of the energy per toggle (in Joules) of each input, output or clock pin for the ``pb_type``.
The static power is provided as an absolute (in Watts).
This is done using the following construct:

.. code-block:: xml

    <pb_type>
        ...
        <power method="pin-toggle">
            <port name="A" energy_per_toggle="1.0e-12"/>
            <port name="B[3:2]" energy_per_toggle="1.0e-12"/>
            <port name="C" energy_per_toggle="1.0e-12" scaled_by_static_porb="en1"/>
            <port name="D" energy_per_toggle="1.0e-12" scaled_by_static_porb_n="en2"/>
            <static_power power_per_instance="1.0e-6"/>
        </power>
    </pb_type>

Keep in mind that the port construct allows for multiple pins per port.
Unless an subset index is provided, the energy per toggle will be applied to each pin in the port.
The energy per toggle can be scaled by another signal using the ``scaled_by_static_prob``.
For example, you could scale the energy of a memory block by the read enable pin.
If the read enable were high 80% of the time, then the energy would be scaled by the :math:`signal\_probability`, 0.8.
Alternatively ``scaled_by_static_prob_n`` can be used for active low signals, and the energy will be scaled by :math:`(1-signal\_probability)`.

This method does not perform any transistor-level estimations; the entire power estimation is performed using the above values.
It is assumed that the power usage specified here includes power of all child ``pb_types``.
No further recursive power estimation will be performed.

``C-internal``
""""""""""""""""
This method allows the users to specify the dynamic power of a block in terms of the internal capacitance of the block.
The activity will be averaged across all of the input pins, and will be supplied with the internal capacitance to the standard equation:

.. math::
    P_{dyn}=\frac{1}{2}\alpha CV^2.

Again, the static power is provided as an absolute (in Watts).
This is done using the following construct:

.. code-block:: xml

    <pb_type>
        <power method="c-internal">
            <dynamic_power C_internal="1.0e-16"/>
            <static_power power_per_instance="1.0e-16"/>
        </power>
    </pb_type>

It is assumed that the power usage specified here includes power of all child ``pb_types``.
No further recursive power estimation will be performed.

``absolute``
""""""""""""""""
This method is the most basic power estimation method, and allows users to specify both the dynamic and static power of a block as absolute
values (in Watts).
This is done using the following construct:

.. code-block:: xml

    <pb_type>
        <power method="absolute">
            <dynamic_power power_per_instance="1.0e-16"/>
            <static_power power_per_instance="1.0e-16"/>
        </power>
    </pb_type>

It is assumed that the power usage specified here includes power of all child ``pb_types``.
No further recursive power estimation will be performed.

Global Routing
~~~~~~~~~~~~~~

Global routing consists of switch boxes and input connection boxes.

Switch Boxes
""""""""""""""""

Switch boxes are modelled as the following components (:numref:`fig_power_sb`):

#. Multiplexer
#. Buffer
#. Wire capacitance

.. _fig_power_sb:

.. figure:: power_sb.*

    Switch Box

**Multiplexer:**
The multiplexer is modelled as 2-level pass-transistor multiplexer, comprised of minimum-size NMOS transistors.
The number of inputs to the multiplexer is automatically determined.

**Buffer:**
The buffer is a multistage CMOS buffer.
The buffer size is determined based upon output capacitance provided in the architecture file:

.. code-block:: xml

    <switchlist>
        <switch type="mux" ... C_out="1.0e-16"/>
    </switchlist>

The user may override this method by providing the buffer size as shown below:

.. code-block:: xml

    <switchlist>
        <switch type="mux" ... power_buf_size="16"/>
    </switchlist>

The size is the drive strength of the buffer, relative to a minimum-sized inverter.

Input Connection Boxes
""""""""""""""""""""""

Input connection boxes are modelled as the following components (:numref:`fig_power_cb`):

* One buffer per routing track, sized to drive the load of all input multiplexers to which the buffer is connected (For buffer sizing see :ref:`power_buffer_sizing`).

* One multiplexer per block input pin, sized according to the number of routing tracks that connect to the pin.

.. _fig_power_cb:

.. figure:: power_cb.*

    Connection Box

Clock Network
~~~~~~~~~~~~~

The clock network modelled is a four quadrant spine and rib design, as illustrated in :numref:`fig_power_clock_network`.
At this time, the power model only supports a single clock.
The model assumes that the entire spine and rib clock network will contain buffers separated in distance by the length of a grid tile.
The buffer sizes and wire capacitances are specified in the architecture file using the following construct:

.. code-block:: xml

    <clocks>
        <clock ... clock_options ... />
    </clocks>

The following clock options are supported:

* ``C_wire=1e-16``: The absolute capacitance, in fards, of the wire between each clock buffer.

* ``C_wire_per_m=1e-12``: The wire capacitance, in fards per m.

    The capacitance is calculated using an automatically determined wirelength, based on the area of a tile in the FPGA.

* ``buffer_size=2.0``: The size of each clock buffer.

    This can be replaced with the ``auto`` keyword.
    See :ref:`power_buffer_sizing` for more information on buffer sizing.



.. _fig_power_clock_network:

.. figure:: power_clock_network.*

    The clock network. Squares represent CLBs, and the wires represent the clock network.


.. _power_advanced_usage:

Other Architecture Options & Techniques
---------------------------------------

.. _power_local_wire_auto_sizing:

Local Wire Auto-Sizing
~~~~~~~~~~~~~~~~~~~~~~

Due to the significant user effort required to provide local buffer and wire sizes, we developed an algorithm to estimate them automatically.
This algorithm recursively calculates the area of all entities within a CLB, which consists of the area of primitives and the area of local interconnect multiplexers.
If an architecture uses new primitives in CLBs, it should include a function that returns the transistor count.
This function should be called from within ``power_count_transistors_primitive()``.

In order to determine the wire length that connects a parent entity to its children, the following assumptions are made:

*  Assumption 1:
    All components (CLB entities, multiplexers, crossbars) are assumed to be contained in a square-shaped area.

*  Assumption 2:
    All wires connecting a parent entity to its child pass through the *interconnect square*, which is the sum area of all interconnect multiplexers belonging to the parent entity.

:numref:`fig_power_local_interconnect` provides an illustration of a parent entity connected to its child entities, containing one of each interconnect type (direct, many-to-1, and complete).
In this figure, the square on the left represents the area used by the transistors of the interconnect multiplexers.
It is assumed that all connections from parent to child will pass through this area.
Real wire lengths could me more or less than this estimate; some pins in the parent may be directly adjacent to child entities, or they may have to traverse a distance greater than just the interconnect area.
Unfortuantely, a more rigorous estimation would require some information about the transistor layout.

.. _fig_power_local_interconnect:

.. figure:: power_local_interconnect.*

    Local interconnect wirelength.



.. _table_power_inerconnect_wire_cap:

.. table:: Local interconnect wirelength and capacitance. :math:`C_{inv}` is the input capacitance of a minimum-sized inverter.

    ==============================  ===========================================  =======================
    Connection from Entity Pin to:  Estimated Wirelength                         Transistor Capacitance
    ==============================  ===========================================  =======================
    Direct (Input or Output)        :math:`0.5 \cdot L_{interc}`                 0
    Many-to-1 (Input or Output)     :math:`0.5 \cdot L_{interc}`                 :math:`C_{INV}`
    Complete *m:n* (Input)          :math:`0.5 \cdot L_{interc} + L_{crossbar}`  :math:`n \cdot C_{INV}`
    Complete *m:n* (Output)         :math:`0.5 \cdot L_{interc}`                 :math:`C_{INV}`
    ==============================  ===========================================  =======================

:numref:`table_power_inerconnect_wire_cap` details how local wire lengths are determined as a function of entity and interconnect areas.
It is assumed that each wire connecting a pin of a ``pb_type`` to an interconnect structure is of length :math:`0.5 \cdot L_{interc}`.
In reality, this length depends on the actual transistor layout, and may be much larger or much smaller than the estimated value.
If desired, the user can override the 0.5 constant in the architecture file:

.. code-block:: xml

    <architecture>
        <power>
            <local_interconnect factor="0.5"/>
        </power>
    </architecture>


.. _power_buffer_sizing:

Buffer Sizing
~~~~~~~~~~~~~

In the power estimator, a buffer size refers to the size of the final stage of multi-stage buffer (if small, only a single stage is used).
The specified size is the :math:`\frac{W}{L}` of the NMOS transistor.
The PMOS transistor will automatically be sized larger.
Generally, buffers are sized depending on the load capacitance, using the following equation:

.. math::

       \text{Buffer Size} = \frac{1}{2 \cdot f_{LE}} * \frac{C_{Load}}{C_{INV}}

In this equation, :math:`C_{INV}` is the input capacitance of a minimum-sized inverter, and :math:`f_{LE}` is the logical effort factor.
The logical effort factor is the gain between stages of the multi-stage buffer, which by default is 4 (minimal delay).
The term :math:`(2\cdot f_{LE})` is used so that the ratio of the final stage to the driven capacitance is smaller.
This produces a much lower-area, lower-power buffer that is still close to the optimal delay, more representative of common design practises.
The logical effort factor can be modified in the architecture file:

.. code-block:: xml

    <architecture>
        <power>
            <buffers logical_effor_factor="4"/>
        </power>
    </architecture>

.. _power_local_interconnect_capacitance:

Local Interconnect Capacitance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If using the ``auto-size`` or ``wire-length`` options (:ref:`power_arch_modeling`), the local interconnect capacitance must be specified.
This is specified in the units of Farads/meter.

.. code-block:: xml

    <architecture>
        <power>
            <local_interconnect C_wire="2.5e-15"/>
        </power>
    </architecture>




python_libs/vtr.rst
--------------------------------------
.. _python_libs/vtr:

VTR Flow Python library
-----------------------
The VTR flow can be imported and implemented as a python library. Below are the descriptions of the useful functions. 

VTR flow
========
.. automodule:: vtr.flow
    :members: run

Parmys
=======

.. automodule:: vtr.parmys.parmys
    :members:

ODIN II
=======

.. automodule:: vtr.odin.odin
    :members: 

ABC
===

.. automodule:: vtr.abc.abc
    :members: run, run_lec

ACE
===

.. automodule:: vtr.ace.ace
    :members: 

VPR
===

.. automodule:: vtr.vpr.vpr
    :members: 

.. toctree::
   :maxdepth: 2
   :caption: Contents:


server_mode/index.rst
--------------------------------------
.. _server_mode:

Server Mode
================

If VPR is in server mode, it listens on a socket for commands from a client. Currently, this is used to enable interactive timing analysis and visualization of timing paths in the VPR UI under the control of a separate client.
VPR provides the ability to run in server mode using the following command-line arguments.

.. code-block:: none

  --server --port_num 60555

Server mode may only accept a single client application connection and respond to two types of requests: **get critical path report** and **highlight selected critical path elements**.

Communication telegram
-------------------------

Telegram consists of two parts: a fixed-size **telegram header** and a variable-size **telegram body**.
The telegram header contains helper information required to properly extract the telegram body sequence from the data flow.

.. _fig_comm_telegram_structure:

.. figure:: comm_telegram_structure.*

    Communication telegram structure.

.. note:: The telegram body itself could be compressed with zlib to minimize the amount of data transferred over the socket.
  This compression is applied to the response of the 'get critical path report' request. The compressor ID byte in the telegram header signals whether the telegram body is compressed.
  When the compressor ID is null, the telegram body is not compressed. If the compressor ID is 'z', it means the body is compressed with zlib.

.. note:: The checksum field contains the telegram body checksum. This checksum is used to validate the consistency of the telegram body during the dispatching phase.
  If checksums are mismatched, the telegram is considered invalid and is skipped in processing.


.. _fig_comm_telegram_body_structure:

.. figure:: comm_telegram_body_structure.*

    Communication telegram body structure.

    **telegram body** is a flat **JSON** structure.

    **CMD** could have following integer values:

    - 0 - command id for **get critical path**
    - 1 - command id for **highlight selected path elements**

    JOB_ID is a unique ID for a task. It is used to associate the request with the response by matching the same JOB_ID. Each new client request should increment the JOB_ID value; otherwise, it will not be clear which request the current response belongs to.


Get critical path timing report example
---------------------------------------

  Let's take a look at an example of a request timing report telegram body with the following options:

  - path_num = 1
  - path_type = "setup"
  - details_level = "netlist"
  - is_flat_routing = false

  .. code-block:: json
      :caption: telegram body **REQUEST** example
      :linenos:

      {
        "JOB_ID": "1",
        "CMD": "0",
        "OPTIONS": "int:path_num:1;string:path_type:setup;string:details_level:netlist;bool:is_flat_routing:0"
      }

  **path_type** could have following string values:

  - "setup"
  - "hold"

  **details_level** could have following string values:

  - "netlist"
  - "aggregated"
  - "detailed"
  - "debug"

  Response will look like:

  .. code-block:: json
      :caption: telegram body **RESPONSE** example
      :linenos:

      {
        "JOB_ID": "1",
        "CMD": "0",
        "OPTIONS": "int:path_num:1;string:path_type:setup;string:details_level:netlist;bool:is_flat_routing:0",
        "DATA": "
          #Timing report of worst 1 path(s)
          # Unit scale: 1e-09 seconds
          # Output precision: 3

          #Path 1
          Startpoint: count[1].Q[0] (dffsre clocked by clk)
          Endpoint  : count[13].D[0] (dffsre clocked by clk)
          Path Type : setup

          Point                                                                       Incr      Path
          ------------------------------------------------------------------------------------------
          clock clk (rise edge)                                                      0.000     0.000
          clock source latency                                                       0.000     0.000
          clk.inpad[0] (.input)                                                      0.000     0.000
          count[1].C[0] (dffsre)                                                     0.715     0.715
          count[1].Q[0] (dffsre) [clock-to-output]                                   0.286     1.001
          count_adder_carry_p_cout[2].p[0] (adder_carry)                             0.573     1.574
          count_adder_carry_p_cout[2].cout[0] (adder_carry)                          0.068     1.642
          count_adder_carry_p_cout[3].cin[0] (adder_carry)                           0.043     1.685
          count_adder_carry_p_cout[3].cout[0] (adder_carry)                          0.070     1.755
          count_adder_carry_p_cout[4].cin[0] (adder_carry)                           0.053     1.808
          count_adder_carry_p_cout[4].cout[0] (adder_carry)                          0.070     1.877
          count_adder_carry_p_cout[5].cin[0] (adder_carry)                           0.043     1.921
          count_adder_carry_p_cout[5].cout[0] (adder_carry)                          0.070     1.990
          count_adder_carry_p_cout[6].cin[0] (adder_carry)                           0.053     2.043
          count_adder_carry_p_cout[6].cout[0] (adder_carry)                          0.070     2.113
          count_adder_carry_p_cout[7].cin[0] (adder_carry)                           0.043     2.156
          count_adder_carry_p_cout[7].cout[0] (adder_carry)                          0.070     2.226
          count_adder_carry_p_cout[8].cin[0] (adder_carry)                           0.053     2.279
          count_adder_carry_p_cout[8].cout[0] (adder_carry)                          0.070     2.348
          count_adder_carry_p_cout[9].cin[0] (adder_carry)                           0.043     2.391
          count_adder_carry_p_cout[9].cout[0] (adder_carry)                          0.070     2.461
          count_adder_carry_p_cout[10].cin[0] (adder_carry)                          0.053     2.514
          count_adder_carry_p_cout[10].cout[0] (adder_carry)                         0.070     2.584
          count_adder_carry_p_cout[11].cin[0] (adder_carry)                          0.043     2.627
          count_adder_carry_p_cout[11].cout[0] (adder_carry)                         0.070     2.696
          count_adder_carry_p_cout[12].cin[0] (adder_carry)                          0.053     2.749
          count_adder_carry_p_cout[12].cout[0] (adder_carry)                         0.070     2.819
          count_adder_carry_p_cout[13].cin[0] (adder_carry)                          0.043     2.862
          count_adder_carry_p_cout[13].cout[0] (adder_carry)                         0.070     2.932
          count_adder_carry_p_cout[14].cin[0] (adder_carry)                          0.053     2.985
          count_adder_carry_p_cout[14].sumout[0] (adder_carry)                       0.040     3.025
          count_dffsre_Q_D[13].in[0] (.names)                                        0.564     3.589
          count_dffsre_Q_D[13].out[0] (.names)                                       0.228     3.818
          count[13].D[0] (dffsre)                                                    0.000     3.818
          data arrival time                                                                    3.818

          clock clk (rise edge)                                                      0.000     0.000
          clock source latency                                                       0.000     0.000
          clk.inpad[0] (.input)                                                      0.000     0.000
          count[13].C[0] (dffsre)                                                    0.715     0.715
          clock uncertainty                                                          0.000     0.715
          cell setup time                                                           -0.057     0.659
          data required time                                                                   0.659
          ------------------------------------------------------------------------------------------
          data required time                                                                   0.659
          data arrival time                                                                   -3.818
          ------------------------------------------------------------------------------------------
          slack (VIOLATED)                                                                    -3.159


          #End of timing report
          #RPT METADATA:
          path_index/clock_launch_path_elements_num/arrival_path_elements_num
          0/2/30
      ",
        "STATUS": "1"
      }

Draw selected critical path elements example
--------------------------------------------

  Let's take a look at an example of a request timing report telegram body with the following options:

  - path_elements = path 0 and it's sub-elements 10,11,12,13,14,15,20,21,22,23,24,25
  - high_light_mode = "crit path flylines delays"
  - draw_path_contour = 1

  .. code-block:: json
      :caption: telegram body **REQUEST** example
      :linenos:

      {
        "JOB_ID": "2",
        "CMD": "1",
        "OPTIONS": "string:path_elements:0#10,11,12,13,14,15,20,21,22,23,24,25;string:high_light_mode:crit path flylines delays;bool:draw_path_contour:1"
      }

  **high_light_mode** could have following string values:

  - "crit path flylines"
  - "crit path flylines delays"
  - "crit path routing"
  - "crit path routing delays"

  Response will look like:

  .. code-block:: json
      :caption: telegram body **RESPONSE** example
      :linenos:

      {
        "JOB_ID": "2",
        "CMD": "1",
        "OPTIONS": "string:path_elements:0#10,11,12,13,14,15,20,21,22,23,24,25;string:high_light_mode:crit path flylines delays;bool:draw_path_contour:1",
        "DATA": "",
        "STATUS": "1"
      }

  .. note:: If status is not 1, the field ***DATA*** contains error string.

`The interactive path analysis (IPA) client <https://github.com/w0lek/IPAClient>`_ is useful for viewing timing paths, and to test VPR's server mode functionality.

.. _interactive_path_analysis_client:

Interactive Path Analysis Client (IPA)
====================================================

The interactive path analysis (IPA) client connects to VPR's server mode and allows interactive visualization of timing paths and their placement and routing. This client application is called **IPAClient** and can also be used to test VPR's server mode functionality. It is available in a public Git repository:  
`https://github.com/w0lek/IPAClient <https://github.com/w0lek/IPAClient>`_.

This UI application is designed to generate requests for VPR in server mode and display the responses in a readable format, acting as a result viewer.

More details on how to build and use **IPAClient** can be found in the `README <https://github.com/w0lek/IPAClient/blob/main/README.md>`_.






libs/README.md
--------------------------------------
This directory contains common utility libraries used by VTR.

Libraries included under the EXTERNAL/ directory are developed outside of the VTR repository and are included for convienience.

EXTERNAL libaries should not be modified in the VTR source tree because it will diverge from their mainline implementations.



EXTERNAL/README.md
--------------------------------------
This folder contains libraries developed outside of the VTR repository.

Some are simply checked in manually (e.g. libpugixml), while others are kept in sync using the 'git subtree' command (see README.developers.md in the repository root for usage).

These libraries should NOT BE MODIFIED since they would diverge from their mainline versions, making it difficult to update in the future.



capnproto/README.md
--------------------------------------
[![Quick Tests](https://github.com/capnproto/capnproto/workflows/Quick%20Tests/badge.svg?branch=master&event=push)](https://github.com/capnproto/capnproto/actions?query=workflow%3A%22Quick+Tests%22)
[![Release Tests](https://github.com/capnproto/capnproto/workflows/Release%20Tests/badge.svg?branch=master&event=push)](https://github.com/capnproto/capnproto/actions?query=workflow%3A%22Release+Tests%22)

<img src='http://kentonv.github.io/capnproto/images/infinity-times-faster.png' style='width:334px; height:306px; float: right;'>

Cap'n Proto is an insanely fast data interchange format and capability-based RPC system. Think
JSON, except binary. Or think [Protocol Buffers](https://github.com/google/protobuf), except faster.
In fact, in benchmarks, Cap'n Proto is INFINITY TIMES faster than Protocol Buffers.

[Read more...](http://kentonv.github.io/capnproto/)



capnproto/RELEASE-PROCESS.md
--------------------------------------
How to release
==============

**Developing**

* First, develop some new features to release!  As you do, make sure to keep the documentation
  up-to-date.

**Testing**

* Run `super-test.sh` on as many platforms as you have available.  Remember that you can easily run
  on any machine available through ssh using `./super-test.sh remote [hostname]`.  Also run in
  Clang mode.  (If you are Kenton and running from Kenton's home machine and network, use
  `./mega-test.py mega-test.cfg` to run on all supported compilers and platforms.)

* Manually test Windows/MSVC -- unfortunately this can't be automated by super-test.sh.

* Manually run the pointer fuzz tests under Valgrind. This will take 40-80 minutes.

      valgrind ./capnp-test -fcapnp/fuzz-test.c++

* Manually run the AFL fuzz tests by running `afl-fuzz.sh`. There are three test cases, and ideally each should run for 24 hours or more.

**Documenting**

* Write a blog post discussing what is new, placing it in doc/_posts.

* Run jekyll locally and review the blog post and docs.

**Releasing**

* Check out the master branch in a fresh directory.  Do NOT use your regular repo, as the release
  script commits changes and if anything goes wrong you'll probably want to trash the whole thing
  without pushing.  DO NOT git clone the repo from an existing local repo -- check it out directly
  from github.  Otherwise, when it pushes its changes back, they'll only be pushed back to your
  local repo.

* Run `./release.sh candidate`.  This creates a new release branch, updates the version number to
  `-rc1`, builds release tarballs, copies them to the current directory, then switches back to the
  master branch and bumps the version number there.  After asking for final confirmation, it will
  upload the tarball to S3 and push all changes back to github.

* Install your release candidates on your local machine, as if you were a user.

* Go to `c++/samples` in the git repo and run `./test.sh`.  It will try to build against your
  installed copy.

* Post the release candidates somewhere public and then send links to the mailing list for people
  to test.  Wait a bit for bug reports.

* If there are any problems, fix them in master and start a new release candidate by running
  `./release.sh candidate <commit>...` from the release branch.  This will cherry-pick the specified
  commits into the release branch and create a new candidate.  Repeat until all problems are fixed.
  Be sure that any such fixes include tests or process changes so that they don't happen again.

* You should now be ready for an official release.  Run `./release.sh final`.  This will remove the
  "-rcN" suffix from the version number, update the version number shown on the downloads page,
  build the final release package, and -- after final confirmation -- upload the binary, push
  changes to git, and publish the new documentation.

* Submit the newly-published blog post to news sites and social media as you see fit.

* If problems are discovered in the release, fix them in master and run
  `./release.sh candidate <commit>...` in the release branch to start a new micro release.  The
  script automatically sees that the current branch's version no longer contains `-rc`, so it starts
  a new branch.  Repeat the rest of the process above.  If you decide to write a blog post (not
  always necessary), do it in the master branch and cherry-pick it.



capnproto/style-guide.md
--------------------------------------
# KJ Style

This document describes how to write C++ code in KJ style. It may be compared to the [Google C++ Style Guide](http://google-styleguide.googlecode.com/svn/trunk/cppguide.html).

KJ style is used by KJ (obviously), [Cap'n Proto](https://capnproto.org), [Sandstorm.io](https://sandstorm.io), and possibly other projects. When submitting code to these projects, you should follow this guide.

**Table of Contents**

- [Rule #1: There are no rules](#rule-1-there-are-no-rules)
- [Design Philosophy](#design-philosophy)
  - [Value Types vs. Resource Types](#value-types-vs-resource-types)
  - [RAII (Resource Acquisition Is Initialization)](#raii-resource-acquisition-is-initialization)
  - [Ownership](#ownership)
  - [No Singletons](#no-singletons)
  - [Exceptions](#exceptions)
  - [Threads vs. Event Loops](#threads-vs-event-loops)
  - [Lazy input validation](#lazy-input-validation)
  - [Premature optimization fallacy](#premature-optimization-fallacy)
  - [Text is always UTF-8](#text-is-always-utf-8)
- [C++ usage](#c-usage)
  - [Use C++11 (or later)](#use-c11-or-later)
  - [Heap allocation](#heap-allocation)
  - [Pointers, references](#pointers-references)
  - [Constness](#constness)
  - [Inheritance](#inheritance)
  - [Exceptions Usage](#exceptions-usage)
  - [Template Metaprogramming](#template-metaprogramming)
  - [Global Constructors](#global-constructors)
  - [`dynamic_cast`](#dynamic_cast)
  - [Use of Standard libraries](#use-of-standard-libraries)
  - [Compiler warnings](#compiler-warnings)
  - [Tools](#tools)
- [Irrelevant formatting rules](#irrelevant-formatting-rules)
  - [Naming](#naming)
  - [Spacing and bracing](#spacing-and-bracing)
  - [Comments](#comments)
  - [File templates](#file-templates)

## Rule #1: There are no rules

This guide contains suggestions, not rules.

If you wish to submit code to a project following KJ style, you should follow the guide so long as there is no good reason not to. You should not break rules just because you feel like it -- consistency is important for future maintainability. But, if you have a good, pragmatic reason to break a rule, do it. Do not ask permission. Just do it.

## Design Philosophy

This section contains guidelines on software design that aren't necessarily C++-specific (though KJ's preferences here are obviously influenced by C++).

### Value Types vs. Resource Types

There are two kinds of types: values and resources. Value types are simple data structures; they serve no purpose except to represent pure data. Resource types represent live objects with state and behavior, and often represent resources external to the program.

* Value types make sense to copy (though they don't necessarily have copy constructors). Resource types are not copyable.
* Value types always have move constructors (and sometimes copy constructors). Resource types are not movable; if ownership transfer is needed, the resource must be allocated on the heap.
* Value types almost always have implicit destructors. Resource types may have an explicit destructor.
* Value types should only be compared by value, not identity. Resource types can only be compared by identity.
* Value types make sense to serialize. Resource types fundamentally cannot be serialized.
* Value types rarely use inheritance and never have virtual methods. Resource types commonly do.
* Value types generally use templates for polymorphism. Resource types generally use virtual methods / abstract interfaces.
* You might even say that value types are used in functional programming style while resource types are used in object-oriented style.

In Cap'n Proto there is a very clear distinction between values and resources: interfaces are resource types whereas everything else is a value.

### RAII (Resource Acquisition Is Initialization)

KJ code is RAII-strict. Whenever it is the case that "this block of code cannot exit cleanly without performing operation X", then X *must* be performed in a destructor, so that X will happen regardless of how the block is exited (including by exception).

Use the macros `KJ_DEFER`, `KJ_ON_SCOPE_SUCCESS`, and `KJ_ON_SCOPE_FAILURE` to easily specify some code that must be executed on exit from the current scope, without the need to define a whole class with a destructor.

Be careful when writing complicated destructors. If a destructor performs multiple cleanup actions, you generally need to make sure that the latter actions occur even if the former ones throw an exception. For this reason, a destructor should generally perform no more than one cleanup action. If you need to clean up multiple things, have your class contain multiple members representing the different things that need cleanup, each with its own destructor. This way, if one member's destructor throws, the others still run.

### Ownership

Every object has an "owner". The owner may be another object, or it may be a stack frame (which is in turn owned by its parent stack frame, and so on up to the top frame, which is owned by the thread, which itself is an object which is owned by something).

The owner decides when to destroy an object. If the owner itself is destroyed, everything it owns must be transitively destroyed. This should be accomplished through RAII style.

The owner specifies the lifetime of the object and how the object may be accessed. This specification may be through documented convention or actually enforced through the type system; the latter is preferred when possible.

An object can never own itself, including transitively.

When declaring a pointer to an object which is owned by the scope, always use `kj::Own<T>`. Regular C++ pointers and references always point to objects that are *not* owned.

When passing a regular C++ pointer or reference as a parameter or return value of a function, care must be taken to document assumptions about the lifetime of the object. In the absence of documentation, make the following assumptions:

* A pointer or reference passed as a constructor parameter must remain valid for the lifetime of the constructed object.
* A pointer or reference passed as a function or method parameter must remain valid until the function returns. In the case that the function returns a promise, then the object must remain live until the promise completes or is canceled.
* A pointer or reference returned by a method remains valid at least until the object whose method was called is destroyed.
* A pointer or reference returned by a stand-alone function likely refers to content of one of the function's parameters, and remains valid until that parameter is destroyed.

Note that ownership isn't just about memory management -- it matters even in languages that implement garbage collection! Unless an object is 100% immutable, you need to keep track of who is allowed to modify it, and that generally requires declaring an owner. Moreover, even with GC, resource types commonly need `close()` method that acts very much like a C++ destructor, leading to all the same considerations. It is therefore completely wrong to believe garbage collection absolves you of thinking about ownership -- and this misconception commonly leads to huge problems in large-scale systems written in GC languages.

#### Reference Counting

Reference counting is allowed, in which case an object will have multiple owners.

When using reference counting, care must be taken to ensure that there is a clear contract between all owners about how the object shall be accessed. In general, this should mean one of the following:

* Reference-counted value types should be immutable.
* Reference-counted resource types should have an interface which clearly specifies how multiple clients should coordinate.

Care must also be taken to avoid cyclic references (which would constitute self-ownership, and would cause a memory leak). Think carefully about what the object ownership graph looks like.

Avoid reference counting when it is not absolutely necessary.

Keep in mind that atomic (thread-safe) reference counting can be extremely slow. Consider non-atomic reference counting if it is feasible under your threading philosophy (under KJ's philosophy, non-atomic reference counting is OK).

### No Singletons

A "singleton" is any mutable object or value that is globally accessible. "Globally accessible" means that the object is declared as a global variable or static member variable, or that the object can be found by following pointers from such variables.

Never use singletons. Singletons cause invisible and unexpected dependencies between components of your software that appear unrelated. Worse, the assumption that "there should only be one of this object per process" is almost always wrong, but its wrongness only becomes apparent after so much code uses the singleton that it is infeasible to change. Singleton interfaces often turn into unusable monstrosities in an attempt to work around the fact that they should never have been a singleton in the first place.

See ["Singletons Considered Harmful"](http://www.object-oriented-security.org/lets-argue/singletons) for a complete discussion.

#### Global registries are singletons

An all-too-common-pattern in modular frameworks is to design a way to register named components via global-scope macros. For example:

    // BAD BAD BAD
    REGISTER_PLUGIN("foo", fooEntryPoint);

This global registry is a singleton, and has many of the same problems as singletons. Don't do this. Again, see ["Singletons Considered Harmful"](http://www.object-oriented-security.org/lets-argue/singletons) for discussion.

#### What to do instead

High-level code (such as your `main()` function) should explicitly initialize the components the program needs. If component Foo depends on component Bar, then Foo's constructor should take a pointer to Bar as a parameter; the high-level code can then point each component at its dependencies explicitly.

For example, instead of a global registry, have high-level code construct a registry object and explicitly call some `register()` method to register each component that should be available through it. This way, when you read your `main()` function it's easy to see what components your program is using.

#### Working around OS singletons

Unfortunately, operating system APIs are traditionally singleton-heavy. The most obvious example is, of course, the filesystem.

In order to use these APIs while avoiding the problems of singletons, try to encapsulate OS singletons inside non-singleton interfaces as early on as possible in your program. For example, you might define an abstract interface called `Directory` with an implementation `DiskDirectory` representing a directory on disk. In your `main()` function, create two `DiskDirectory`s representing the root directory and the current working directory. From then on, have all of your code operate in terms of `Directory`. Pass the original `DiskDirectory` pointers into the components that need it.

### Exceptions

An exception represents something that "should never happen", assuming everything is working as expected. Of course, things that "should never happen" in fact happen all the time. But, a program should never be written in such a way that it _expects_ an exception under normal circumstances.

Put another way, exceptions are a way to achieve _fault tolerance_. Throwing an exception is a less-disruptive alternative to aborting the process. Exceptions are a _logistical_ construct, as opposed to a semantic one: an exception should never be part of your "business logic".

For example, exceptions may indicate conditions like:

* Logistics of software development:
  * There is a bug in the code.
  * The requested method is not implemented.
* Logistics of software usage:
  * There is an error in the program's configuration.
  * The input is invalid.
* Logistics of distributed systems:
  * A network connection was reset.
  * An optimistic transaction was aborted due to concurrent modification.
* Logistics of physical computation:
  * The system's resources are exhausted (e.g. out of memory, out of disk space).
  * The system is overloaded and must reject some requests to avoid long queues.

#### Business logic should never catch

If you find that callers of your interface need to catch and handle certain kinds of exceptions in order to operate correctly, then you must change your interface (or overload it) such that those conditions can be handled without an exception ever being thrown. For example, if you have a method `Own<File> open(StringPtr name)` that opens a file, you may also want to offer `Maybe<Own<File>> openIfExists(StringPtr name)` that returns null rather than throwing an exception if the file is not found. (But you should probably keep `open()` as well, for the convenience of the common case where the caller will just throw an exception anyway.)

Note that with this exception philosophy, Java-style "checked exceptions" (exceptions which are explicitly declared to be thrown by an interface) make no sense.

#### How to handle an exception

In framework and logistical code, you may catch exceptions and try to handle them. Given the nature of exceptions, though, there are only a few things that are reasonable to do when receiving an exception:

* On network disconnect or transaction failures, back up and start over from the beginning (restore connections and state, redo operations).
* On resources exhausted / overloaded, retry again later, with exponential back-off.
* On unimplemented methods, retry with a different implementation strategy, if there is one.
* When no better option is available, report the problem to a human (the user and/or the developer).

#### Exceptions can happen anywhere (including destructors)

Any piece of code may contain a bug. Therefore, an exception can happen anywhere. This includes destructors. It doesn't matter how much you argue that destructors should not throw exceptions, because that is equivalent to arguing that code should not have bugs. We all wish our code never had bugs, but nevertheless it happens.

Unfortunately, C++ made the awful decision that an exception thrown from a destructor that itself is called during stack unwind due to some other exception should cause the process to abort. This is an error in the language specification. Apparently, the committee could not agree on any other behavior, so they chose the worst possible behavior.

If exceptions are merely a means to fault tolerance, then it is perfectly clear what should happen in the case that a second exception is thrown while unwinding due to a first: the second exception should merely be discarded, or perhaps attached to the first as a supplementary note. The catching code usually does not care about the exception details anyway; it's just going to report that something went wrong, then maybe try to continue executing other, unrelated parts of the program. In fact, in most cases discarding the secondary exception makes sense, because it is often simply a side-effect of the fact that the code didn't complete normally, and so provides no useful additional information.

Alas, C++ is what it is. So, in KJ, we work around the problem in a couple ways:

* `kj::UnwindDetector` may be used to detect when a destructor is called during unwind and squelch secondary exceptions.
* The `KJ_ASSERT` family of macros -- from which most exceptions are thrown in the first place -- implement a concept of "recoverable" exceptions, where it is safe to continue execution without throwing in cases where throwing would be bad. Assert macros in destructors must always be recoverable.

#### Allowing `-fno-exceptions`

KJ and Cap'n Proto are designed to function even when compiled with `-fno-exceptions`. In this case, throwing an exception behaves differently depending on whether the exception is "fatal" or "recoverable". Fatal exceptions abort the process. On a recoverable exception, on the other hand, execution continues normally, perhaps after replacing invalid data with some safe default. The exception itself is stored in a thread-local variable where code up the stack can check for it later on.

This compromise is made only so that C++ applications which eschew exceptions are still able to use Cap'n Proto. We do NOT recommend disabling exceptions if you have a choice. Moreover, code following this style guide (other than KJ and Cap'n Proto) is not required to be `-fno-exceptions`-safe, and in fact we recommend against it.

### Threads vs. Event Loops

Threads are hard, and synchronization between threads is slow. Even "lock-free" data structures usually require atomic operations, which are costly, and such algorithms are notoriously difficult to get right. Fine-grained synchronization will therefore be expensive at best and highly unstable at worst.

KJ instead prefers event loop concurrency. In this model, each event callback is effectively a transaction; it does not need to worry about concurrent modification within the body of the function.

Multiple threads may exist, but each one has its own event loop and is treated as sort of a lightweight process with shared memory. Every object in the process either belongs to a specific thread (who is allowed to read and modify it) or is transitively immutable (in which case all threads can safely read it concurrently). Threads communicate through asynchronous message-passing. In fact, the only big difference between KJ-style threads compared to using separate processes is that threads may transfer ownership of in-memory objects as part of a message send.

Note that with hardware transactional memory, it may become possible to execute a single event loop across multiple CPU cores while behaving equivalently to a single thread, by executing each event callback as a hardware transaction. If so, this will be implemented as part of KJ's event loop machinery, transparently to apps.

### Lazy input validation

As we all know, you should always validate your input.

But, when should you validate it? There are two plausible answers:

* Upfront, on receipt.
* Lazily, on use.

Upfront validation occasionally makes sense for the purpose of easier debugging of problems: if an error is reported earlier, it's easier to find where it came from.

However, upfront validation has some big problems.

* It is inefficient, as it requires a redundant pass over the data. Lazy validation, in contrast, occurs at a time when you have already loaded the data for the purpose of using it. Extra passes are often cache-unfriendly and/or entail redundant I/O operations.

* It encourages people to skip validation at time of use, on the assumption that it was already validated earlier. This is dangerous, as it entails a non-local assumption. E.g. are you really sure that there is no way to insert data into your database without having validated it? Are you really sure that the data hasn't been corrupted? Are you really sure that your code will never be called in a new situation where validation hasn't happened? Are you sure the data cannot have been modified between validation and use? In practice, you should be validating your input at time of use _even if_ you know it has already been checked previously.

* The biggest problem: Upfront validation tends not to match actual usage, because the validation site is far away from the usage site. Over time, as the usage code changes, the validator can easily get out-of-sync. Note that this could mean the code itself is out-of-sync, or it could be that running servers are out-of-sync, because they have different update schedules. Or, the validator may be written with incorrect assumptions in the first place. The consequences of this can be severe. Protocol Buffers' concept of "required fields" is essentially an upfront validation check that [has been responsible for outages of Google Search, GMail, and others](https://capnproto.org/faq.html#how-do-i-make-a-field-required-like-in-protocol-buffers).

We recommend, therefore, that validation occur at time of use. Code should be written to be tolerant of validation failures. For example, most code dealing with UTF-8 text should treat it as a blob of bytes, not worrying about invalid byte sequences. When you actually need to decode the code points -- such as to display them -- you should do something reasonable with invalid sequences -- such as display the Unicode replacement character.

With that said, when storing data in a database long-term, it can make sense to perform an additional validation check at time of storage, in order to more directly notify the caller that their input was invalid. This validation should be considered optional, since the data will be validated again when it is read from storage and used.

### Premature optimization fallacy

_"We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil."_ -- Donald Knuth

_"The improvement in speed from Example 2 to Example 2a is only about 12%, and many people would pronounce that insignificant. The conventional wisdom shared by many of today’s software engineers calls for ignoring efficiency in the small; but I believe this is simply an overreaction to the abuses they see being practiced by penny-wise- and-pound-foolish programmers, who can’t debug or maintain their “optimized” programs. In established engineering disciplines a 12% improvement, easily obtained, is never considered marginal; and I believe the same viewpoint should prevail in software engineering. Of course I wouldn’t bother making such optimizations on a one-shot job, but when it’s a question of preparing quality programs, I don’t want to restrict myself to tools that deny me such efficiencies."_ -- Donald Knuth, **in the same paper**.

(Credit: [Stop Misquoting Donald Knuth!](http://www.joshbarczak.com/blog/?p=580))

You should not obsess over optimization or write unmaintainable code for the sake of speed.

However, you _should_ be thinking about efficiency of all the code you write. When writing efficient code is not much harder and not much uglier than inefficient code, you should be writing efficient code. If the efficient approach to a problem would take _much_ longer than the inefficient way then go ahead and code the inefficient way first, but in many cases it's not that stark. Rewriting your code later is _much_ more expensive than writing it correctly the first time, because by then you'll have lost context.

You should be constantly aware of whether the code you are writing is low-level (called frequently) or high-level (called infrequently). You should consider optimizations relative to the code's level. In low-level code, optimizations like avoiding heap allocations may make sense. In high-level code you should not worry about heap, but you may still want to think about expensive operations like disk I/O or contacting remote servers (things that low-level code should never do in the first place, of course).

Programmers who ignore efficiency until they have no choice inevitably end up shipping slow, bloated software. Their code's speed is always pushing the boundary of bearability, because they only do anything about it when it becomes unbearable. But programs which are "bearable" can still make users intensely unhappy due to their slowness.

### Text is always UTF-8

Always encode text as UTF-8. Always assume text is encoded as UTF-8.

(Do not, however, assume text is _valid_ UTF-8; see the section on lazy validation.)

Do not write code that tries to distinguish characters. Unless you are writing code to render text to a display, you probably don't care about characters. Besides, Unicode itself contains code points which act as modifiers to previous characters; it's futile for you to handle these. Most code only really cares about bytes.

Note that even parsers for machine-readable text-based languages (config languages, programming languages, other DSLs) do not really care about "characters" in the Unicode sense because such languages are almost always pure-ASCII. They may allow arbitrary UTF-8 in, say, string literals, but only ASCII code points have any special meaning to the language. Therefore, they still only care about bytes (since ASCII characters are single-byte, and multi-byte UTF-8 codepoints never contain individual bytes in the ASCII range).

## C++ usage

This section contains guidelines for usage of C++ language features.

### Use C++11 (or later)

C++11 completely transformed the way the C++ language is used. New code should take heavy advantage of the new features, especially rvalue references (move semantics) and lambda expressions.

KJ requires C++11. Application code (not used as a library) may consider requiring C++14, or even requiring a specific compiler and tracking the latest language features implemented by it.

### Heap allocation

* Never write `new` or `delete` explicitly. Use `kj::heap` to allocate single objects or `kj::heapArray` for arrays; these return "owned" pointers (`kj::Own<T>` or `kj::Array<T>`, respectively) which enforce RAII/ownership semantics. You may transfer ownership of these pointers via move semantics, but otherwise the objects will be automatically deleted when they go out of scope. This makes memory leaks very rare in KJ code.
* Only allocate objects on the heap when you actually need to be able to move them. Otherwise, avoid a heap allocation by declaring the object directly on the stack or as a member of some other object.
* If a class's copy constructor would require memory allocation, consider providing a `clone()` method instead and deleting the copy constructor. Allocation in implicit copies is a common source of death-by-1000-cuts performance problems. `kj::String`, for example, is movable but not copyable.

### Pointers, references

* Pointers and references always point to things that are owned by someone else. Take care to think about the lifetime of that object compared to the lifetime of the pointer.
* Always use `kj::ArrayPtr<T>` rather than `T*` to point to an array.
* Always use `kj::StringPtr` rather than `const char*` to point to a NUL-terminated string.
* Always use `kj::Maybe<T&>` rather than `T*` when a pointer can be null. This forces the user to check for null-ness.
* In other cases, prefer references over pointers. Note, though, that members of an assignable type cannot be references, so you'll need to use pointers in that case (darn).

**Rationale:** There is an argument that says that references should always be const and pointers mutable, because then when you see `foo(&bar)` you know that the function modifies `bar`. This is a nice theory, but in practice real C++ code is rarely so consistent that you can use this as a real signal. We prefer references because they make it unambiguous that the value cannot be null.

### Constness

* Treat `const`-ness as transitive. So, if you have a const instance of a struct which in turn contains a pointer (or reference), treat that pointer as pointing to const even if it is not declared as such. To enforce this, copyable classes which contain pointer fields should declare their copy constructor as `T(T& other)` rather than `T(const T& other)` (and similarly for assignment operators) in order to prevent escalating a transitively-const pointer to non-const via copy. You may inherit `kj::DisallowConstCopy` to force the implicit copy constructor and assignment operator to be declared this way.
* Try to treat const/non-const pointers like shared/exclusive locks. So, when a new const pointer to an object is created, all other pointers should also be considered const at least until the new pointer is destroyed. When a new non-const pointer is created, all other pointers should be considered not dereferenceable until the non-const pointer is destroyed. In theory, these rules help keep different objects from interfering with each other by modifying some third object in incompatible ways. Note that these rules are (as I understand it) enforceable by the Rust type system.
* `const` methods are safe to call on the same object from multiple threads simultaneously. Conversely, it is unsafe to call a non-`const` method if any other thread might be calling methods on that object concurrently. Note that KJ defines synchronization primitives including `kj::Mutex` which integrate nicely with this rule.

### Inheritance

A class is either an interface or an implementation. Interfaces have no fields. Implementations have no non-final virtual methods. You should not mix these: a class with state should never have virtual methods, as this leads to fragile base class syndrome.

Interfaces should NOT declare a destructor, because:

* That destructor is never called anyway (because we don't use `delete`, and `kj::Own` has a different mechanism for dispatching the destructor).
* Declaring destructors for interfaces is tedious.
* If you declare a destructor but do not declare it `noexcept(false)`, C++11 will (regrettably) decide that it is `noexcept` and that all derived classes must also have a `noexcept` destructor, which is wrong. (See the exceptions philosophy section for discussion on exceptions in destructors.)

Multiple inheritance is allowed and encouraged, keeping in mind that you are usually inheriting interfaces.

You should think carefully about whether to use virtual inheritance; it's not often needed, and it is relatively inefficient, but in complex inheritance hierarchies it becomes critical.

Implementation inheritance (that is, inheriting an implementation class) is allowed as a way to compose classes without requiring extra allocations. For example, Cap'n Proto's `capnp::InputStreamMessageReader` implements the `capnp::MessageReader` interface by reading from a `kj::InputStream`, which is itself an interface. One implementation of `kj::InputStream` is `kj::FdInputStream`, which reads from a unix file descriptor. As a convenience, Cap'n Proto defines `capnp::StreamFdMessageReader` which multiply-inherits `capnp::InputStreamMessageReader` and `kj::FdInputStream` -- that is, it inherits two implementations, and even inherits the latter privately. Many style guides would consider this taboo. The benefit, though, is that people can declare this composed class on the stack as one unit, with no heap allocation, and end up with something that they can directly treat as a `capnp::MessageReader`; any other solution would lose one of these benefits.

### Exceptions Usage

KJ's exception philosophy is described earlier in this document. Here we describe only how to actually use exceptions in code.

Never use `throw` explicitly. Almost all exceptions should originate from the `KJ_ASSERT`, `KJ_REQUIRE`, and `KJ_SYSCALL` macros (see `kj/debug.h`). These macros allow you to easily attach useful debug information to the exception message without spending time on string formatting.

Never declare anything `noexcept`. As explained in the philosophy section, whether you like it or not, bugs can happen anywhere and therefore exceptions can happen anywhere. `noexcept` causes the process to abort on exceptions. Aborting is _never_ the right answer.

Explicit destructors must always be declared `noexcept(false)`, to work around C++11's regrettable decision that destructors should be `noexcept` by default. In destructors, always use `kj::UnwindDetector` or make all your asserts recoverable in order to ensure that an exception is not thrown during unwind.

Do not fret too much about recovering into a perfectly consistent state after every exception. That's not the point. The point is to be able to recover at all -- to _improve_ reliability, but not to make it perfect. So, write your code to do a reasonable thing in most cases.

For example, if you are implementing a data structure like a vector, do not worry about whether move constructors might throw. In practice, it is extraordinarily rare for move constructors to contain any code that could throw. So just assume they don't. Do NOT do what the C++ standard library does and require that all move constructors be explicitly `noexcept`, because people will not remember to mark their move constructors `noexcept`, and you'll just be creating a huge headache for everyone with _no practical benefit_.

### Template Metaprogramming

#### Reducing Verbosity

Before C++11, it was common practice to write "template functions" in the form of a templated struct which contained a single member representing the output of the function. For example, you might see `std::is_integral<int>::value` to check if `int` is integral. This pattern is excessively verbose, especially when composed into complex expressions.

In C++11, we can do better. Where before you would have declared a struct named `Foo<T>` with a single member as described above, in C++11 you should:

1. Define the struct as before, but with the name `Foo_<T>`.
2. Define a template `Foo<T>` which directly aliases the single member of `Foo_<T>`. If the output is a type, use a template `using`, whereas if the output is a value, use a `constexpr` function.

Example:

    template <typename T> struct IsConst_ { static constexpr bool value = false; };
    template <typename T> struct IsConst_<const T> { static constexpr bool value = true; };
    template <typename T> constexpr bool isConst() { return IsConst_<T>::value; }
    // Return true if T is const.

Or:

    template <typename T> struct UnConst_ { typedef T Type; };
    template <typename T> struct UnConst_<const T> { typedef T Type; };
    template <typename T> using UnConst = typename UnConst_<T>::Type;
    // If T is const, return the underlying non-const type.
    // Otherwise, just return T.

Now people can use your template metafunction without the pesky `::Type` or `::value` suffix.

#### Other hints

* To explicitly disable a template under certain circumstances, bind an unnamed template parameter to `kj::EnableIf`:

        template <typename T, typename = kj::EnableIf(!isConst<T>())>
        void mutate(T& ptr);
        // T must not be const.

* Say you're writing a template type with a constructor function like so:

        template <typename T>
        Wrapper<T> makeWrapper(T&& inner);
        // Wraps `inner` and returns the wrapper.
  
  Should `inner` be taken by reference or by value here? Both might be useful, depending on the use case. The right answer is actually to support both: if the input is an lvalue, take it by reference, but if it's an rvalue, take it by value (move). And as it turns out, if you write your declaration exactly as shown above, this is exactly what you get, because if the input is an lvalue, `T` will implicitly bind to a reference type, whereas if the input is an rvalue or rvalue reference, T will not be a reference.
  
  In general, you should assume KJ code in this pattern uses this rule, so if you are passing in an lvalue but don't actually want it wrapped by reference, wrap it in `kj::mv()`.

* Never use function or method pointers. Prefer templating across functors (like STL does), or for non-templates use `kj::Function` (which will handle this for you).

### Global Constructors

Do not declare global or static variables with dynamic constructors. Global constructors disproportionately hurt startup time because they force code to be paged in before it is really needed. They also are usually only needed by singletons, which you should not be using in general (see philosophy section).

You can have global constants of non-trivial class type as long as they are declared `constexpr`. If you want to declare complex data structures as constants, try to declare all the pieces as separate globals that reference each other, so that nothing has to be heap allocated and everything can be `constexpr`.

Use Clang's `-Wglobal-constructors` warning to catch mistakes.

### `dynamic_cast`

Do not use `dynamic_cast` as a way to implement polymorphism. That is, do not write long blocks of if/else statements each trying to cast an object to a different derived class to handle in a different way. Instead, extend the base class's interface to cover the functionality you need.

With that said, `dynamic_cast` is not always bad. It is fine to use `dynamic_cast` for "logistical" improvements, such as optimization. As a rule of thumb, imagine if `dynamic_cast` were replaced with a function that always returned null. Would your code still be correct (if, perhaps, slower, or with less detailed logging, etc.)? If so, then your use of `dynamic_cast` is fine.

#### `-fno-rtti`

The KJ and Cap'n Proto libraries are designed to function correctly when compiled with `-fno-rtti`. To that end, `kj::dynamicCastIfAvailable` is a version of `dynamic_cast` that, when compiled with `-fno-rtti`, always returns null, and KJ and Cap'n Proto code always uses this version.

We do NOT recommend disabling RTTI in your own code.

### Lambdas

Lamba capture lists must never use `=` to specify "capture all by value", because this makes it hard to review the capture list for possible lifetime issues.

Capture lists *may* use `&` ("capture all by reference") but *only* in cases where it is known that the lambda will not outlive the current stack frame. In fact, they generally *should* use `&` in this case, to make clear that there are no lifetime issues to think about.

### Use of Standard libraries

#### C++ Standard Library

The C++ standard library is old and full of a lot of cruft. Many APIs are designed in pre-C++11 styles that are no longer ideal. Mistakes like giving copy constructors to objects that own heap space (because in the absence of move semantics, it was needed for usability) and atomically-reference-counted strings (intended as an optimization to avoid so much heap copying, but actually a pessimization) are now baked into the library and cannot change. The `iostream` library was designed before anyone knew how to write good C++ code and is absolutely awful by today's standards. Some parts of the library, such as `<chrono>`, are over-engineered, designed by committees more interested in theoretical perfection than practicality. To add insult to injury, the library's naming style does not distinguish types from values.

For these reasons and others, KJ aims to be a replacement for the C++ standard libraries.

It is not there yet. As of this writing, the biggest missing piece is that KJ provides no implementation of maps or sets, nor a `sort()` function.

We recommend that KJ code use KJ APIs where available, falling back to C++ standard types when necessary. To avoid breaking clients later, avoid including C++ standard library headers from other headers; only include them from source files.

All users of the KJ library should familiarize themselves at least with the declarations in the following files, as you will use them all the time:

* `kj/common.h`
* `kj/memory.h`
* `kj/array.h`
* `kj/string.h`
* `kj/vector.h`
* `kj/debug.h`

#### C Library

As a general rule of thumb, C library functions documented in man section 3 should be treated with skepticism.

Do not use the C standard I/O functions -- your code should never contain `FILE*`. For formatting strings, `kj::str()` is much safer and easier than `sprintf()`. For debug logging, `KJ_DBG()` will produce more information with fewer keystrokes compared to `printf()`. For parsing, KJ's parser combinator library is cleaner and more powerful than `scanf()`. `fread()` and `fwrite()` imply buffering that you usually don't want; use `kj/io.h` instead, or raw file descriptors.

### Compiler warnings

Use the following warning settings with Clang or GCC:

* `-Wall -Wextra`: Enable most warnings.
* `-Wglobal-constructors`: (Clang-only) This catches global variables with constructors, which KJ style disallows (see above). You will, however, want to disable this warning in tests, since test frameworks use global constructors and are excepted from the style rule.
* `-Wno-sign-compare`: While comparison between signed and unsigned values could be a serious bug, we find that in practice this warning is almost always spurious.
* `-Wno-unused-parameter`: This warning is always spurious. I have never seen it find a real bug. Worse, it encourages people to delete parameter names which harms readability.

For development builds, `-Werror` should also be enabled. However, this should not be on by default in open source code as not everyone uses the same compiler or compiler version and different compiler versions often produce different warnings.

### Tools

We use:

* Clang for compiling.
* `KJ_DBG()` for simple debugging.
* Valgrind for complicated debugging.
* [Ekam](https://github.com/capnproto/ekam) for a build system.
* Git for version control.

## Irrelevant formatting rules

Many style guides dwell on formatting. We mention it only because it's vaguely nice to have some formatting consistency, but know that this section is the *least* relevant section of the document.

As a code reviewer, when you see a violation of formatting rules, think carefully about whether or not it really matters that you point it out. If you believe the author may be unfamiliar with the rules, it may be worth letting them know to read this document, if only so that they can try to be consistent in the future. However, it is NOT worth the time to comment on every misplaced whitespace. As long as the code is readable, move on.

### Naming

* Type names: `TitleCase`
* Variable, member, function, and method names: `camelCase`
* Constant and enumerant names: `CAPITAL_WITH_UNDERSCORES`
* Macro names: `CAPITAL_WITH_UNDERSCORES`, with an appropriate project-specific prefix like `KJ_` or `CAPNP_`.
* Namespaces: `oneword`. Namespaces should be kept short, because you'll have to type them a lot. The name of KJ itself was chosen for the sole purpose of making the namespace easy to type (while still being sufficiently unique). Use a nested namespace called `_` to contain package-private declarations.
* Files: `module-name.c++`, `module-name.h`, `module-name-test.c++`

**Rationale:** There has never been broad agreement on C++ naming style. The closest we have is the C++ standard library. Unfortunately, the C++ standard library made the awful decision of naming types and values in the same style, losing a highly useful visual cue that makes programming more pleasant, and preventing variables from being named after their type (which in many contexts is perfectly appropriate).

Meanwhile, the Java style, which KJ emulates, has been broadly adopted to varying degrees in other languages, from JavaScript to Haskell. Using a similar style in KJ code makes it less jarring to switch between C++ and those other languages. Being consistent with JavaScript is especially useful because it is the one language that everyone pretty much has to use, due to its use in the web platform.

There has also never been any agreement on C++ file extensions, for some reason. The extension `.c++`, though not widely used, is accepted by all reasonable tools and is clearly the most precise choice.

### Spacing and bracing

* Indents are two spaces.
* Never use tabs.
* Maximum line length is 100 characters.
* Indent continuation lines for braced init lists by two spaces.
* Indent all other continuation lines by four spaces.
* Alternatively, line up continuation lines with previous lines if it makes them easier to read.
* Place a space between a keyword and an open parenthesis, e.g.: `if (foo)`
* Do not place a space between a function name and an open parenthesis, e.g.: `foo(bar)`
* Place an opening brace at the end of the statement which initiates the block, not on its own line.
* Place a closing brace on a new line indented the same as the parent block. If there is post-brace code related to the block (e.g. `else` or `while`), place it on the same line as the closing brace.
* Always place braces around a block *unless* the block is so short that it can actually go on the same line as the introductory `if` or `while`, e.g.: `if (done) return;`.
* `case` statements are indented within the `switch`, and their following blocks are **further** indented (so the actual statements in a case are indented four spaces more than the `switch`).
* `public:`, `private:`, and `protected:` are reverse-indented by one stop.
* Statements inside a `namespace` are **not** indented unless the namespace is a short block that is just forward-declaring things at the top of a file.
* Set your editor to strip trailing whitespace on save, otherwise other people who use this setting will see spurious diffs when they edit a file after you.

<br>

    if (foo) {
      bar();
    } else if (baz) {
      qux(quux);
    } else {
      corge();
    }

    if (done) return;

    switch (grault) {
      case GARPLY:
        print("mew");
        break;
      case WALDO: {  // note: needs braces due to variable
        Location location = findWaldo();
        print(location);
        break;
      }
    }

<br>

    namespace external {
      class Forward;
      class Declarations;
      namespace nested {
        class More;
      }
    }

    namespace myproj {

    class Fred {
    public:
      Fred();
      ~Fred();
    private:
      int plugh;
    };

    }  // namespace myproj

**Rationale:** Code which is inconsistently or sloppily formatted gives the impression that the author is not observant or simply doesn't care about quality, and annoys other people trying to read your code.

Other than that, there is absolutely no good reason to space things one way or another.

### Comments

* Always use line comments (`//`). Never use block comments (`/**/`).

  **Rationale:** Block comments don't nest. Block comments tend to be harder to re-arrange, whereas a group of line comments can be moved easily. Also, typing `*` is just way harder than typing `/` so why would you want to?

* Write comments that add useful information that the reader might not already know. Do NOT write comments which say things that are already blatantly obvious from the code. For example, for a function `void frob(Bar bar)`, do not write a comment `// Frobs the Bar.`; that's already obvious. It's better to have no comment.

* Doc comments go **after** the declaration. If the declaration starts a block, the doc comment should go inside the block at the top. A group of related declarations can have a single group doc comment after the last one as long as there are no black lines between the declarations.

        int foo();
        // This is documentation for foo().

        class Bar {
          // This is documentation for Bar.
        public:
          Bar();

          inline int baz() { return 5; }
          inline int qux() { return 6; }
          // This is documentation for baz() and qux().
        };

  **Rationale:** When you start reading a doc comment, the first thing you want to know is *what the heck is being documented*. Having to scroll down through a long comment to see the declaration, then back up to read the docs, is bad. Sometimes, people actually repeat the declaration at the top of the comment just so that it's visible. This is silly. Let's just put the comment after the declaration.

* TODO comments are of the form `// TODO(type): description`, where `type` is one of:
  * `now`: Do before next `git push`.
  * `soon`: Do before next stable release.
  * `someday`: A feature that might be nice to have some day, but no urgency.
  * `perf`: Possible performance enhancement.
  * `security`: Possible security concern. (Used for low-priority issues. Obviously, code with serious security problems should never be written in the first place.)
  * `cleanup`: An improvement to maintainability with no user-visible effects.
  * `port`: Things to do when porting to a new platform.
  * `test`: Something that needs better testing.
  * `msvc`: Something to revisit when the next, hopefully less-broken version of Microsoft Visual Studio becomes available.
  * others: Additional TODO types may be defined for use in certain contexts.

  **Rationale:** Google's guide suggests that TODOs should bear the name of their author ("the person to ask for more information about the comment"), but in practice there's no particular reason why knowing the author is more useful for TODOs than for any other comment (or, indeed, code), and anyway that's what `git blame` is for. Meanwhile, having TODOs classified by type allows for useful searches, so that e.g. release scripts can error out if release-blocking TODOs are present.

### File templates

Generally, a "module" should consist of three files: `module.h`, `module.c++`, and `module-test.c++`. One or more of these can be omitted if it would otherwise be empty. Use the following templates when creating new files.

Headers:

    // Project Name - Project brief description
    // Copyright (c) 2015 Primary Author and contributors
    //
    // Licensed under the Whatever License blah blah no warranties.

    #pragma once
    // Documentation for file.

    #include <kj/common.h>

    namespace myproject {

    // declarations

    namespace _ {  // private

    // private declarations

    }  // namespace _ (private)

    }  // namespace myproject

Source code:

    // Project Name - Project brief description
    // Copyright (c) 2015 Primary Author and contributors
    //
    // Licensed under the Whatever License blah blah no warranties.

    #include "this-module.h"
    #include <other-module.h>

    namespace myproject {

    // definitions

    }  // namespace myproject

Test:

    // Project Name - Project brief description
    // Copyright (c) 2015 Primary Author and contributors
    //
    // Licensed under the Whatever License blah blah no warranties.

    #include "this-module.h"
    #include <other-module.h>

    namespace myproject {
    namespace {

    // KJ_TESTs

    }  // namespace
    }  // namespace myproject

Note that in both the source and test files, you should *always* include the corresponding header first, in order to ensure that it is self-contained (does not secretly require including some other header before it).



doc/README.md
--------------------------------------
# Cap'n Proto Documentation

This directory contains the "source code" for the Cap'n Proto web site.

The site is built with [Jekyll](http://jekyllrb.com/), which depends on Ruby. 
Start by installing ruby1.9.1-dev. On Debian-based operating systems:

    sudo apt-get install ruby-dev

Then install Jekyll 3.8.1 (Jekyll 4.x will NOT work due as they removed Pygments support):

    sudo gem install jekyll -v 3.8.1
    sudo gem install pygments.rb

Now install Pygments and SetupTools to be able to install the CapnProto lexer.
On Debian based operating systems:

    sudo apt-get install python-pygments python-setuptools

Next, install the custom Pygments syntax highlighter:

    cd _plugins
    sudo python capnp_lexer.py install
    cd ..

Now you can launch a local server:

    jekyll _3.8.1_ serve --watch

Edit, test, commit.

If you have permission, after you've pushed your changes back to github, you can make your changes live by running:

    ./push-site.sh

Otherwise, send a pull request and let someone else actually push the new site.



doc/capnp-tool.md
--------------------------------------
---
layout: page
title: The capnp Tool
---

# The `capnp` Tool

Cap'n Proto comes with a command-line tool called `capnp` intended to aid development and
debugging.  This tool can be used to:

* Compile Cap'n Proto schemas to produce source code in multiple languages.
* Generate unique type IDs.
* Decode Cap'n Proto messages to human-readable text.
* Encode text representations of Cap'n Proto messages to binary.
* Evaluate and extract constants defined in Cap'n Proto schemas.

This page summarizes the functionality.  A complete reference on the command's usage can be
found by typing:

    capnp help

## Compiling Schemas

    capnp compile -oc++ myschema.capnp

This generates files `myschema.capnp.h` and `myschema.capnp.c++` which contain C++ source code
corresponding to the types defined in `myschema.capnp`.  Options exist to control output location
and import paths.

The above example generates C++ code, but the tool is able to generate output in any language
for which a plugin is available.  Compiler plugins are just regular programs named
`capnpc-language`.  For example, the above command runs `capnpc-c++`.  [More on how to write
compiler plugins](otherlang.html#how-to-write-compiler-plugins).

Note that some Cap'n Proto implementations (especially for interpreted languages) do not require
generating source code.

## Decoding Messages

    capnp decode myschema.capnp MyType < message.bin > message.txt

`capnp decode` reads a binary Cap'n Proto message from standard input and decodes it to a
human-readable text format (specifically, the format used for specifying constants and default
values in [the schema language](language.html)).  By default it
expects an unpacked message, but you can decode a
[packed](encoding.html#packing) message with the `--packed` flag.

## Encoding Messages

    capnp encode myschema.capnp MyType < message.txt > message.bin

`capnp encode` is the opposite of `capnp decode`: it takes a text-format message on stdin and
encodes it to binary (possibly [packed](encoding.html#packing),
with the `--packed` flag).

This is mainly useful for debugging purposes, to build test data or to apply tweaks to data
decoded with `capnp decode`.  You should not rely on `capnp encode` for encoding data written
and maintained in text format long-term -- instead, use `capnp eval`, which is much more powerful.

## Evaluating Constants

    capnp eval myschema.capnp myConstant

This prints the value of `myConstant`, a [const](language.html#constants) declaration, after
applying variable substitution.  It can also output the value in binary format (`--binary` or
`--packed`).

At first glance, this may seem no more interesting than `capnp encode`:  the syntax used to define
constants in schema files is the same as the format accepted by `capnp encode`, right?  There is,
however, a big difference:  constants in schema files may be defined in terms of other constants,
which may even be imported from other files.

As a result, `capnp eval` is a great basis for implementing config files.  For example, a large
company might maintain a production server that serves dozens of clients and needs configuration
information about each one.  Rather than maintaining the config as one enormous file, it can be
written as several separate files with a master file that imports the rest.

Such a configuration should be compiled to binary format using `capnp eval` before deployment,
in order to verify that there are no errors and to make deployment easier and faster.  While you
could technically ship the text configs to production and have the servers parse them directly
(e.g. with `capnp::SchemaParser`), encoding before deployment is more efficient and robust.



doc/cxx.md
--------------------------------------
---
layout: page
title: C++ Serialization
---

# C++ Serialization

The Cap'n Proto C++ runtime implementation provides an easy-to-use interface for manipulating
messages backed by fast pointer arithmetic.  This page discusses the serialization layer of
the runtime; see [C++ RPC](cxxrpc.html) for information about the RPC layer.

## Example Usage

For the Cap'n Proto definition:

{% highlight capnp %}
struct Person {
  id @0 :UInt32;
  name @1 :Text;
  email @2 :Text;
  phones @3 :List(PhoneNumber);

  struct PhoneNumber {
    number @0 :Text;
    type @1 :Type;

    enum Type {
      mobile @0;
      home @1;
      work @2;
    }
  }

  employment :union {
    unemployed @4 :Void;
    employer @5 :Text;
    school @6 :Text;
    selfEmployed @7 :Void;
    # We assume that a person is only one of these.
  }
}

struct AddressBook {
  people @0 :List(Person);
}
{% endhighlight %}

You might write code like:

{% highlight c++ %}
#include "addressbook.capnp.h"
#include <capnp/message.h>
#include <capnp/serialize-packed.h>
#include <iostream>

void writeAddressBook(int fd) {
  ::capnp::MallocMessageBuilder message;

  AddressBook::Builder addressBook = message.initRoot<AddressBook>();
  ::capnp::List<Person>::Builder people = addressBook.initPeople(2);

  Person::Builder alice = people[0];
  alice.setId(123);
  alice.setName("Alice");
  alice.setEmail("alice@example.com");
  // Type shown for explanation purposes; normally you'd use auto.
  ::capnp::List<Person::PhoneNumber>::Builder alicePhones =
      alice.initPhones(1);
  alicePhones[0].setNumber("555-1212");
  alicePhones[0].setType(Person::PhoneNumber::Type::MOBILE);
  alice.getEmployment().setSchool("MIT");

  Person::Builder bob = people[1];
  bob.setId(456);
  bob.setName("Bob");
  bob.setEmail("bob@example.com");
  auto bobPhones = bob.initPhones(2);
  bobPhones[0].setNumber("555-4567");
  bobPhones[0].setType(Person::PhoneNumber::Type::HOME);
  bobPhones[1].setNumber("555-7654");
  bobPhones[1].setType(Person::PhoneNumber::Type::WORK);
  bob.getEmployment().setUnemployed();

  writePackedMessageToFd(fd, message);
}

void printAddressBook(int fd) {
  ::capnp::PackedFdMessageReader message(fd);

  AddressBook::Reader addressBook = message.getRoot<AddressBook>();

  for (Person::Reader person : addressBook.getPeople()) {
    std::cout << person.getName().cStr() << ": "
              << person.getEmail().cStr() << std::endl;
    for (Person::PhoneNumber::Reader phone: person.getPhones()) {
      const char* typeName = "UNKNOWN";
      switch (phone.getType()) {
        case Person::PhoneNumber::Type::MOBILE: typeName = "mobile"; break;
        case Person::PhoneNumber::Type::HOME: typeName = "home"; break;
        case Person::PhoneNumber::Type::WORK: typeName = "work"; break;
      }
      std::cout << "  " << typeName << " phone: "
                << phone.getNumber().cStr() << std::endl;
    }
    Person::Employment::Reader employment = person.getEmployment();
    switch (employment.which()) {
      case Person::Employment::UNEMPLOYED:
        std::cout << "  unemployed" << std::endl;
        break;
      case Person::Employment::EMPLOYER:
        std::cout << "  employer: "
                  << employment.getEmployer().cStr() << std::endl;
        break;
      case Person::Employment::SCHOOL:
        std::cout << "  student at: "
                  << employment.getSchool().cStr() << std::endl;
        break;
      case Person::Employment::SELF_EMPLOYED:
        std::cout << "  self-employed" << std::endl;
        break;
    }
  }
}
{% endhighlight %}

## C++ Feature Usage:  C++11, Exceptions

This implementation makes use of C++11 features.  If you are using GCC, you will need at least
version 4.7 to compile Cap'n Proto.  If you are using Clang, you will need at least version 3.2.
These compilers required the flag `-std=c++11` to enable C++11 features -- your code which
`#include`s Cap'n Proto headers will need to be compiled with this flag.  Other compilers have not
been tested at this time.

This implementation prefers to handle errors using exceptions.  Exceptions are only used in
circumstances that should never occur in normal operation.  For example, exceptions are thrown
on assertion failures (indicating bugs in the code), network failures, and invalid input.
Exceptions thrown by Cap'n Proto are never part of the interface and never need to be caught in
correct usage.  The purpose of throwing exceptions is to allow higher-level code a chance to
recover from unexpected circumstances without disrupting other work happening in the same process.
For example, a server that handles requests from multiple clients should, on exception, return an
error to the client that caused the exception and close that connection, but should continue
handling other connections normally.

When Cap'n Proto code might throw an exception from a destructor, it first checks
`std::uncaught_exception()` to ensure that this is safe.  If another exception is already active,
the new exception is assumed to be a side-effect of the main exception, and is either silently
swallowed or reported on a side channel.

In recognition of the fact that some teams prefer not to use exceptions, and that even enabling
exceptions in the compiler introduces overhead, Cap'n Proto allows you to disable them entirely
by registering your own exception callback.  The callback will be called in place of throwing an
exception.  The callback may abort the process, and is required to do so in certain circumstances
(when a fatal bug is detected).  If the callback returns normally, Cap'n Proto will attempt
to continue by inventing "safe" values.  This will lead to garbage output, but at least the program
will not crash.  Your exception callback should set some sort of a flag indicating that an error
occurred, and somewhere up the stack you should check for that flag and cancel the operation.
See the header `kj/exception.h` for details on how to register an exception callback.

## KJ Library

Cap'n Proto is built on top of a basic utility library called KJ.  The two were actually developed
together -- KJ is simply the stuff which is not specific to Cap'n Proto serialization, and may be
useful to others independently of Cap'n Proto.  For now, the two are distributed together.  The
name "KJ" has no particular meaning; it was chosen to be short and easy-to-type.

As of v0.3, KJ is distributed with Cap'n Proto but built as a separate library.  You may need
to explicitly link against libraries:  `-lcapnp -lkj`

## Generating Code

To generate C++ code from your `.capnp` [interface definition](language.html), run:

    capnp compile -oc++ myproto.capnp

This will create `myproto.capnp.h` and `myproto.capnp.c++` in the same directory as `myproto.capnp`.

To use this code in your app, you must link against both `libcapnp` and `libkj`.  If you use
`pkg-config`, Cap'n Proto provides the `capnp` module to simplify discovery of compiler and linker
flags.

If you use [RPC](cxxrpc.html) (i.e., your schema defines [interfaces](language.html#interfaces)),
then you will additionally need to link against `libcapnp-rpc` and `libkj-async`, or use the
`capnp-rpc` `pkg-config` module.

### Setting a Namespace

You probably want your generated types to live in a C++ namespace.  You will need to import
`/capnp/c++.capnp` and use the `namespace` annotation it defines:

{% highlight capnp %}
using Cxx = import "/capnp/c++.capnp";
$Cxx.namespace("foo::bar::baz");
{% endhighlight %}

Note that `capnp/c++.capnp` is installed in `$PREFIX/include` (`/usr/local/include` by default)
when you install the C++ runtime.  The `capnp` tool automatically searches `/usr/include` and
`/usr/local/include` for imports that start with a `/`, so it should "just work".  If you installed
somewhere else, you may need to add it to the search path with the `-I` flag to `capnp compile`,
which works much like the compiler flag of the same name.

## Types

### Primitive Types

Primitive types map to the obvious C++ types:

* `Bool` -> `bool`
* `IntNN` -> `intNN_t`
* `UIntNN` -> `uintNN_t`
* `Float32` -> `float`
* `Float64` -> `double`
* `Void` -> `::capnp::Void` (An empty struct; its only value is `::capnp::VOID`)

### Structs

For each struct `Foo` in your interface, a C++ type named `Foo` generated.  This type itself is
really just a namespace; it contains two important inner classes:  `Reader` and `Builder`.

`Reader` represents a read-only instance of `Foo` while `Builder` represents a writable instance
(usually, one that you are building).  Both classes behave like pointers, in that you can pass them
by value and they do not own the underlying data that they operate on.  In other words,
`Foo::Builder` is like a pointer to a `Foo` while `Foo::Reader` is like a const pointer to a `Foo`.

For every field `bar` defined in `Foo`, `Foo::Reader` has a method `getBar()`.  For primitive types,
`get` just returns the type, but for structs, lists, and blobs, it returns a `Reader` for the
type.

{% highlight c++ %}
// Example Reader methods:

// myPrimitiveField @0 :Int32;
int32_t getMyPrimitiveField();

// myTextField @1 :Text;
::capnp::Text::Reader getMyTextField();
// (Note that Text::Reader may be implicitly cast to const char* and
// std::string.)

// myStructField @2 :MyStruct;
MyStruct::Reader getMyStructField();

// myListField @3 :List(Float64);
::capnp::List<double> getMyListField();
{% endhighlight %}

`Foo::Builder`, meanwhile, has several methods for each field `bar`:

* `getBar()`:  For primitives, returns the value.  For composites, returns a Builder for the
  composite.  If a composite field has not been initialized (i.e. this is the first time it has
  been accessed), it will be initialized to a copy of the field's default value before returning.
* `setBar(x)`:  For primitives, sets the value to x.  For composites, sets the value to a deep copy
  of x, which must be a Reader for the type.
* `initBar(n)`:  Only for lists and blobs.  Sets the field to a newly-allocated list or blob
  of size n and returns a Builder for it.  The elements of the list are initialized to their empty
  state (zero for numbers, default values for structs).
* `initBar()`:  Only for structs.  Sets the field to a newly-allocated struct and returns a
  Builder for it.  Note that the newly-allocated struct is initialized to the default value for
  the struct's _type_ (i.e., all-zero) rather than the default value for the field `bar` (if it
  has one).
* `hasBar()`:  Only for pointer fields (e.g. structs, lists, blobs).  Returns true if the pointer
  has been initialized (non-null).  (This method is also available on readers.)
* `adoptBar(x)`:  Only for pointer fields.  Adopts the orphaned object x, linking it into the field
  `bar` without copying.  See the section on orphans.
* `disownBar()`:  Disowns the value pointed to by `bar`, setting the pointer to null and returning
  its previous value as an orphan.  See the section on orphans.

{% highlight c++ %}
// Example Builder methods:

// myPrimitiveField @0 :Int32;
int32_t getMyPrimitiveField();
void setMyPrimitiveField(int32_t value);

// myTextField @1 :Text;
::capnp::Text::Builder getMyTextField();
void setMyTextField(::capnp::Text::Reader value);
::capnp::Text::Builder initMyTextField(size_t size);
// (Note that Text::Reader is implicitly constructable from const char*
// and std::string, and Text::Builder can be implicitly cast to
// these types.)

// myStructField @2 :MyStruct;
MyStruct::Builder getMyStructField();
void setMyStructField(MyStruct::Reader value);
MyStruct::Builder initMyStructField();

// myListField @3 :List(Float64);
::capnp::List<double>::Builder getMyListField();
void setMyListField(::capnp::List<double>::Reader value);
::capnp::List<double>::Builder initMyListField(size_t size);
{% endhighlight %}

### Groups

Groups look a lot like a combination of a nested type and a field of that type, except that you
cannot set, adopt, or disown a group -- you can only get and init it.

### Unions

A named union (as opposed to an unnamed one) works just like a group, except with some additions:

* For each field `foo`, the union reader and builder have a method `isFoo()` which returns true
  if `foo` is the currently-set field in the union.
* The union reader and builder also have a method `which()` that returns an enum value indicating
  which field is currently set.
* Calling the set, init, or adopt accessors for a field makes it the currently-set field.
* Calling the get or disown accessors on a field that isn't currently set will throw an
  exception in debug mode or return garbage when `NDEBUG` is defined.

Unnamed unions differ from named unions only in that the accessor methods from the union's members
are added directly to the containing type's reader and builder, rather than generating a nested
type.

See the [example](#example-usage) at the top of the page for an example of unions.

### Lists

Lists are represented by the type `capnp::List<T>`, where `T` is any of the primitive types,
any Cap'n Proto user-defined type, `capnp::Text`, `capnp::Data`, or `capnp::List<U>`
(to form a list of lists).

The type `List<T>` itself is not instantiatable, but has two inner classes: `Reader` and `Builder`.
As with structs, these types behave like pointers to read-only and read-write data, respectively.

Both `Reader` and `Builder` implement `size()`, `operator[]`, `begin()`, and `end()`, as good C++
containers should.  Note, though, that `operator[]` is read-only -- you cannot use it to assign
the element, because that would require returning a reference, which is impossible because the
underlying data may not be in your CPU's native format (e.g., wrong byte order).  Instead, to
assign an element of a list, you must use `builder.set(index, value)`.

For `List<Foo>` where `Foo` is a non-primitive type, the type returned by `operator[]` and
`iterator::operator*()` is `Foo::Reader` (for `List<Foo>::Reader`) or `Foo::Builder`
(for `List<Foo>::Builder`).  The builder's `set` method takes a `Foo::Reader` as its second
parameter.

For lists of lists or lists of blobs, the builder also has a method `init(index, size)` which sets
the element at the given index to a newly-allocated value with the given size and returns a builder
for it.  Struct lists do not have an `init` method because all elements are initialized to empty
values when the list is created.

### Enums

Cap'n Proto enums become C++11 "enum classes".  That means they behave like any other enum, but
the enum's values are scoped within the type.  E.g. for an enum `Foo` with value `bar`, you must
refer to the value as `Foo::BAR`.

To match prevaling C++ style, an enum's value names are converted to UPPERCASE_WITH_UNDERSCORES
(whereas in the schema language you'd write them in camelCase).

Keep in mind when writing `switch` blocks that an enum read off the wire may have a numeric
value that is not listed in its definition.  This may be the case if the sender is using a newer
version of the protocol, or if the message is corrupt or malicious.  In C++11, enums are allowed
to have any value that is within the range of their base type, which for Cap'n Proto enums is
`uint16_t`.

### Blobs (Text and Data)

Blobs are manipulated using the classes `capnp::Text` and `capnp::Data`.  These classes are,
again, just containers for inner classes `Reader` and `Builder`.  These classes are iterable and
implement `size()` and `operator[]` methods.  `Builder::operator[]` even returns a reference
(unlike with `List<T>`).  `Text::Reader` additionally has a method `cStr()` which returns a
NUL-terminated `const char*`.

As a special convenience, if you are using GCC 4.8+ or Clang, `Text::Reader` (and its underlying
type, `kj::StringPtr`) can be implicitly converted to and from `std::string` format.  This is
accomplished without actually `#include`ing `<string>`, since some clients do not want to rely
on this rather-bulky header.  In fact, any class which defines a `.c_str()` method will be
implicitly convertible in this way.  Unfortunately, this trick doesn't work on GCC 4.7.

### Interfaces

[Interfaces (RPC) have their own page.](cxxrpc.html)

### Generics

[Generic types](language.html#generic-types) become templates in C++. The outer type (the one whose
name matches the schema declaration's name) is templatized; the inner `Reader` and `Builder` types
are not, because they inherit the parameters from the outer type. Similarly, template parameters
should refer to outer types, not `Reader` or `Builder` types.

For example, given:

{% highlight capnp %}
struct Map(Key, Value) {
  entries @0 :List(Entry);
  struct Entry {
    key @0 :Key;
    value @1 :Value;
  }
}

struct People {
  byName @0 :Map(Text, Person);
  # Maps names to Person instances.
}
{% endhighlight %}

You might write code like:

{% highlight c++ %}
void processPeople(People::Reader people) {
  Map<Text, Person>::Reader reader = people.getByName();
  capnp::List<Map<Text, Person>::Entry>::Reader entries =
      reader.getEntries()
  for (auto entry: entries) {
    processPerson(entry);
  }
}
{% endhighlight %}

Note that all template parameters will be specified with a default value of `AnyPointer`.
Therefore, the type `Map<>` is equivalent to `Map<capnp::AnyPointer, capnp::AnyPointer>`.

### Constants

Constants are exposed with their names converted to UPPERCASE_WITH_UNDERSCORES naming style
(whereas in the schema language you’d write them in camelCase).  Primitive constants are just
`constexpr` values.  Pointer-type constants (e.g. structs, lists, and blobs) are represented
using a proxy object that can be converted to the relevant `Reader` type, either implicitly or
using the unary `*` or `->` operators.

## Messages and I/O

To create a new message, you must start by creating a `capnp::MessageBuilder`
(`capnp/message.h`).  This is an abstract type which you can implement yourself, but most users
will want to use `capnp::MallocMessageBuilder`.  Once your message is constructed, write it to
a file descriptor with `capnp::writeMessageToFd(fd, builder)` (`capnp/serialize.h`) or
`capnp::writePackedMessageToFd(fd, builder)` (`capnp/serialize-packed.h`).

To read a message, you must create a `capnp::MessageReader`, which is another abstract type.
Implementations are specific to the data source.  You can use `capnp::StreamFdMessageReader`
(`capnp/serialize.h`) or `capnp::PackedFdMessageReader` (`capnp/serialize-packed.h`)
to read from file descriptors; both take the file descriptor as a constructor argument.

Note that if your stream contains additional data after the message, `PackedFdMessageReader` may
accidentally read some of that data, since it does buffered I/O.  To make this work correctly, you
will need to set up a multi-use buffered stream.  Buffered I/O may also be a good idea with
`StreamFdMessageReader` and also when writing, for performance reasons.  See `capnp/io.h` for
details.

There is an [example](#example-usage) of all this at the beginning of this page.

### Using mmap

Cap'n Proto can be used together with `mmap()` (or Win32's `MapViewOfFile()`) for extremely fast
reads, especially when you only need to use a subset of the data in the file.  Currently,
Cap'n Proto is not well-suited for _writing_ via `mmap()`, only reading, but this is only because
we have not yet invented a mutable segment framing format -- the underlying design should
eventually work for both.

To take advantage of `mmap()` at read time, write your file in regular serialized (but NOT packed)
format -- that is, use `writeMessageToFd()`, _not_ `writePackedMessageToFd()`.  Now, `mmap()` in
the entire file, and then pass the mapped memory to the constructor of
`capnp::FlatArrayMessageReader` (defined in `capnp/serialize.h`).  That's it.  You can use the
reader just like a normal `StreamFdMessageReader`.  The operating system will automatically page
in data from disk as you read it.

`mmap()` works best when reading from flash media, or when the file is already hot in cache.
It works less well with slow rotating disks.  Here, disk seeks make random access relatively
expensive.  Also, if I/O throughput is your bottleneck, then the fact that mmaped data cannot
be packed or compressed may hurt you.  However, it all depends on what fraction of the file you're
actually reading -- if you only pull one field out of one deeply-nested struct in a huge tree, it
may still be a win.  The only way to know for sure is to do benchmarks!  (But be careful to make
sure your benchmark is actually interacting with disk and not cache.)

## Dynamic Reflection

Sometimes you want to write generic code that operates on arbitrary types, iterating over the
fields or looking them up by name.  For example, you might want to write code that encodes
arbitrary Cap'n Proto types in JSON format.  This requires something like "reflection", but C++
does not offer reflection.  Also, you might even want to operate on types that aren't compiled
into the binary at all, but only discovered at runtime.

The C++ API supports inspecting schemas at runtime via the interface defined in
`capnp/schema.h`, and dynamically reading and writing instances of arbitrary types via
`capnp/dynamic.h`.  Here's the example from the beginning of this file rewritten in terms
of the dynamic API:

{% highlight c++ %}
#include "addressbook.capnp.h"
#include <capnp/message.h>
#include <capnp/serialize-packed.h>
#include <iostream>
#include <capnp/schema.h>
#include <capnp/dynamic.h>

using ::capnp::DynamicValue;
using ::capnp::DynamicStruct;
using ::capnp::DynamicEnum;
using ::capnp::DynamicList;
using ::capnp::List;
using ::capnp::Schema;
using ::capnp::StructSchema;
using ::capnp::EnumSchema;

using ::capnp::Void;
using ::capnp::Text;
using ::capnp::MallocMessageBuilder;
using ::capnp::PackedFdMessageReader;

void dynamicWriteAddressBook(int fd, StructSchema schema) {
  // Write a message using the dynamic API to set each
  // field by text name.  This isn't something you'd
  // normally want to do; it's just for illustration.

  MallocMessageBuilder message;

  // Types shown for explanation purposes; normally you'd
  // use auto.
  DynamicStruct::Builder addressBook =
      message.initRoot<DynamicStruct>(schema);

  DynamicList::Builder people =
      addressBook.init("people", 2).as<DynamicList>();

  DynamicStruct::Builder alice =
      people[0].as<DynamicStruct>();
  alice.set("id", 123);
  alice.set("name", "Alice");
  alice.set("email", "alice@example.com");
  auto alicePhones = alice.init("phones", 1).as<DynamicList>();
  auto phone0 = alicePhones[0].as<DynamicStruct>();
  phone0.set("number", "555-1212");
  phone0.set("type", "mobile");
  alice.get("employment").as<DynamicStruct>()
       .set("school", "MIT");

  auto bob = people[1].as<DynamicStruct>();
  bob.set("id", 456);
  bob.set("name", "Bob");
  bob.set("email", "bob@example.com");

  // Some magic:  We can convert a dynamic sub-value back to
  // the native type with as<T>()!
  List<Person::PhoneNumber>::Builder bobPhones =
      bob.init("phones", 2).as<List<Person::PhoneNumber>>();
  bobPhones[0].setNumber("555-4567");
  bobPhones[0].setType(Person::PhoneNumber::Type::HOME);
  bobPhones[1].setNumber("555-7654");
  bobPhones[1].setType(Person::PhoneNumber::Type::WORK);
  bob.get("employment").as<DynamicStruct>()
     .set("unemployed", ::capnp::VOID);

  writePackedMessageToFd(fd, message);
}

void dynamicPrintValue(DynamicValue::Reader value) {
  // Print an arbitrary message via the dynamic API by
  // iterating over the schema.  Look at the handling
  // of STRUCT in particular.

  switch (value.getType()) {
    case DynamicValue::VOID:
      std::cout << "";
      break;
    case DynamicValue::BOOL:
      std::cout << (value.as<bool>() ? "true" : "false");
      break;
    case DynamicValue::INT:
      std::cout << value.as<int64_t>();
      break;
    case DynamicValue::UINT:
      std::cout << value.as<uint64_t>();
      break;
    case DynamicValue::FLOAT:
      std::cout << value.as<double>();
      break;
    case DynamicValue::TEXT:
      std::cout << '\"' << value.as<Text>().cStr() << '\"';
      break;
    case DynamicValue::LIST: {
      std::cout << "[";
      bool first = true;
      for (auto element: value.as<DynamicList>()) {
        if (first) {
          first = false;
        } else {
          std::cout << ", ";
        }
        dynamicPrintValue(element);
      }
      std::cout << "]";
      break;
    }
    case DynamicValue::ENUM: {
      auto enumValue = value.as<DynamicEnum>();
      KJ_IF_MAYBE(enumerant, enumValue.getEnumerant()) {
        std::cout <<
            enumerant->getProto().getName().cStr();
      } else {
        // Unknown enum value; output raw number.
        std::cout << enumValue.getRaw();
      }
      break;
    }
    case DynamicValue::STRUCT: {
      std::cout << "(";
      auto structValue = value.as<DynamicStruct>();
      bool first = true;
      for (auto field: structValue.getSchema().getFields()) {
        if (!structValue.has(field)) continue;
        if (first) {
          first = false;
        } else {
          std::cout << ", ";
        }
        std::cout << field.getProto().getName().cStr()
                  << " = ";
        dynamicPrintValue(structValue.get(field));
      }
      std::cout << ")";
      break;
    }
    default:
      // There are other types, we aren't handling them.
      std::cout << "?";
      break;
  }
}

void dynamicPrintMessage(int fd, StructSchema schema) {
  PackedFdMessageReader message(fd);
  dynamicPrintValue(message.getRoot<DynamicStruct>(schema));
  std::cout << std::endl;
}
{% endhighlight %}

Notes about the dynamic API:

* You can implicitly cast any compiled Cap'n Proto struct reader/builder type directly to
  `DynamicStruct::Reader`/`DynamicStruct::Builder`.  Similarly with `List<T>` and `DynamicList`,
  and even enum types and `DynamicEnum`.  Finally, all valid Cap'n Proto field types may be
  implicitly converted to `DynamicValue`.

* You can load schemas dynamically at runtime using `SchemaLoader` (`capnp/schema-loader.h`) and
  use the Dynamic API to manipulate objects of these types.  `MessageBuilder` and `MessageReader`
  have methods for accessing the message root using a dynamic schema.

* While `SchemaLoader` loads binary schemas, you can also parse directly from text using
  `SchemaParser` (`capnp/schema-parser.h`).  However, this requires linking against `libcapnpc`
  (in addition to `libcapnp` and `libkj`) -- this code is bulky and not terribly efficient.  If
  you can arrange to use only binary schemas at runtime, you'll be better off.

* Unlike with Protobufs, there is no "global registry" of compiled-in types.  To get the schema
  for a compiled-in type, use `capnp::Schema::from<MyType>()`.

* Unlike with Protobufs, the overhead of supporting reflection is small.  Generated `.capnp.c++`
  files contain only some embedded const data structures describing the schema, no code at all,
  and the runtime library support code is relatively small.  Moreover, if you do not use the
  dynamic API or the schema API, you do not even need to link their implementations into your
  executable.

* The dynamic API performs type checks at runtime.  In case of error, it will throw an exception.
  If you compile with `-fno-exceptions`, it will crash instead.  Correct usage of the API should
  never throw, but bugs happen.  Enabling and catching exceptions will make your code more robust.

* Loading user-provided schemas has security implications: it greatly increases the attack
  surface of the Cap'n Proto library.  In particular, it is easy for an attacker to trigger
  exceptions.  To protect yourself, you are strongly advised to enable exceptions and catch them.

## Orphans

An "orphan" is a Cap'n Proto object that is disconnected from the message structure.  That is,
it is not the root of a message, and there is no other Cap'n Proto object holding a pointer to it.
Thus, it has no parents.  Orphans are an advanced feature that can help avoid copies and make it
easier to use Cap'n Proto objects as part of your application's internal state.  Typical
applications probably won't use orphans.

The class `capnp::Orphan<T>` (defined in `<capnp/orphan.h>`) represents a pointer to an orphaned
object of type `T`.  `T` can be any struct type, `List<T>`, `Text`, or `Data`.  E.g.
`capnp::Orphan<Person>` would be an orphaned `Person` structure.  `Orphan<T>` is a move-only class,
similar to `std::unique_ptr<T>`.  This prevents two different objects from adopting the same
orphan, which would result in an invalid message.

An orphan can be "adopted" by another object to link it into the message structure.  Conversely,
an object can "disown" one of its pointers, causing the pointed-to object to become an orphan.
Every pointer-typed field `foo` provides builder methods `adoptFoo()` and `disownFoo()` for these
purposes.  Again, these methods use C++11 move semantics.  To use them, you will need to be
familiar with `std::move()` (or the equivalent but shorter-named `kj::mv()`).

Even though an orphan is unlinked from the message tree, it still resides inside memory allocated
for a particular message (i.e. a particular `MessageBuilder`).  An orphan can only be adopted by
objects that live in the same message.  To move objects between messages, you must perform a copy.
If the message is serialized while an `Orphan<T>` living within it still exists, the orphan's
content will be part of the serialized message, but the only way the receiver could find it is by
investigating the raw message; the Cap'n Proto API provides no way to detect or read it.

To construct an orphan from scratch (without having some other object disown it), you need an
`Orphanage`, which is essentially an orphan factory associated with some message.  You can get one
by calling the `MessageBuilder`'s `getOrphanage()` method, or by calling the static method
`Orphanage::getForMessageContaining(builder)` and passing it any struct or list builder.

Note that when an `Orphan<T>` goes out-of-scope without being adopted, the underlying memory that
it occupied is overwritten with zeros.  If you use packed serialization, these zeros will take very
little bandwidth on the wire, but will still waste memory on the sending and receiving ends.
Generally, you should avoid allocating message objects that won't be used, or if you cannot avoid
it, arrange to copy the entire message over to a new `MessageBuilder` before serializing, since
only the reachable objects will be copied.

## Reference

The runtime library contains lots of useful features not described on this page.  For now, the
best reference is the header files.  See:

    capnp/list.h
    capnp/blob.h
    capnp/message.h
    capnp/serialize.h
    capnp/serialize-packed.h
    capnp/schema.h
    capnp/schema-loader.h
    capnp/dynamic.h

## Tips and Best Practices

Here are some tips for using the C++ Cap'n Proto runtime most effectively:

* Accessor methods for primitive (non-pointer) fields are fast and inline.  They should be just
  as fast as accessing a struct field through a pointer.

* Accessor methods for pointer fields, on the other hand, are not inline, as they need to validate
  the pointer.  If you intend to access the same pointer multiple times, it is a good idea to
  save the value to a local variable to avoid repeating this work.  This is generally not a
  problem given C++11's `auto`.

  Example:

      // BAD
      frob(foo.getBar().getBaz(),
           foo.getBar().getQux(),
           foo.getBar().getCorge());

      // GOOD
      auto bar = foo.getBar();
      frob(bar.getBaz(), bar.getQux(), bar.getCorge());

  It is especially important to use this style when reading messages, for another reason:  as
  described under the "security tips" section, below, every time you `get` a pointer, Cap'n Proto
  increments a counter by the size of the target object.  If that counter hits a pre-defined limit,
  an exception is thrown (or a default value is returned, if exceptions are disabled), to prevent
  a malicious client from sending your server into an infinite loop with a specially-crafted
  message.  If you repeatedly `get` the same object, you are repeatedly counting the same bytes,
  and so you may hit the limit prematurely.  (Since Cap'n Proto readers are backed directly by
  the underlying message buffer and do not have anywhere else to store per-object information, it
  is impossible to remember whether you've seen a particular object already.)

* Internally, all pointer fields start out "null", even if they have default values.  When you have
  a pointer field `foo` and you call `getFoo()` on the containing struct's `Reader`, if the field
  is "null", you will receive a reader for that field's default value.  This reader is backed by
  read-only memory; nothing is allocated.  However, when you call `get` on a _builder_, and the
  field is null, then the implementation must make a _copy_ of the default value to return to you.
  Thus, you've caused the field to become non-null, just by "reading" it.  On the other hand, if
  you call `init` on that field, you are explicitly replacing whatever value is already there
  (null or not) with a newly-allocated instance, and that newly-allocated instance is _not_ a
  copy of the field's default value, but just a completely-uninitialized instance of the
  appropriate type.

* It is possible to receive a struct value constructed from a newer version of the protocol than
  the one your binary was built with, and that struct might have extra fields that you don't know
  about.  The Cap'n Proto implementation tries to avoid discarding this extra data.  If you copy
  the struct from one message to another (e.g. by calling a set() method on a parent object), the
  extra fields will be preserved.  This makes it possible to build proxies that receive messages
  and forward them on without having to rebuild the proxy every time a new field is added.  You
  must be careful, however:  in some cases, it's not possible to retain the extra fields, because
  they need to be copied into a space that is allocated before the expected content is known.
  In particular, lists of structs are represented as a flat array, not as an array of pointers.
  Therefore, all memory for all structs in the list must be allocated upfront.  Hence, copying
  a struct value from another message into an element of a list will truncate the value.  Because
  of this, the setter method for struct lists is called `setWithCaveats()` rather than just `set()`.

* Messages are built in "arena" or "region" style:  each object is allocated sequentially in
  memory, until there is no more room in the segment, in which case a new segment is allocated,
  and objects continue to be allocated sequentially in that segment.  This design is what makes
  Cap'n Proto possible at all, and it is very fast compared to other allocation strategies.
  However, it has the disadvantage that if you allocate an object and then discard it, that memory
  is lost.  In fact, the empty space will still become part of the serialized message, even though
  it is unreachable.  The implementation will try to zero it out, so at least it should pack well,
  but it's still better to avoid this situation.  Some ways that this can happen include:
  * If you `init` a field that is already initialized, the previous value is discarded.
  * If you create an orphan that is never adopted into the message tree.
  * If you use `adoptWithCaveats` to adopt an orphaned struct into a struct list, then a shallow
    copy is necessary, since the struct list requires that its elements are sequential in memory.
    The previous copy of the struct is discarded (although child objects are transferred properly).
  * If you copy a struct value from another message using a `set` method, the copy will have the
    same size as the original.  However, the original could have been built with an older version
    of the protocol which lacked some fields compared to the version your program was built with.
    If you subsequently `get` that struct, the implementation will be forced to allocate a new
    (shallow) copy which is large enough to hold all known fields, and the old copy will be
    discarded.  Child objects will be transferred over without being copied -- though they might
    suffer from the same problem if you `get` them later on.
  Sometimes, avoiding these problems is too inconvenient.  Fortunately, it's also possible to
  clean up the mess after-the-fact:  if you copy the whole message tree into a fresh
  `MessageBuilder`, only the reachable objects will be copied, leaving out all of the unreachable
  dead space.

  In the future, Cap'n Proto may be improved such that it can re-use dead space in a message.
  However, this will only improve things, not fix them entirely: fragmentation could still leave
  dead space.

### Build Tips

* If you are worried about the binary footprint of the Cap'n Proto library, consider statically
  linking with the `--gc-sections` linker flag.  This will allow the linker to drop pieces of the
  library that you do not actually use.  For example, many users do not use the dynamic schema and
  reflection APIs, which contribute a large fraction of the Cap'n Proto library's overall
  footprint.  Keep in mind that if you ever stringify a Cap'n Proto type, the stringification code
  depends on the dynamic API; consider only using stringification in debug builds.

  If you are dynamically linking against the system's shared copy of `libcapnp`, don't worry about
  its binary size.  Remember that only the code which you actually use will be paged into RAM, and
  those pages are shared with other applications on the system.

  Also remember to strip your binary.  In particular, `libcapnpc` (the schema parser) has
  excessively large symbol names caused by its use of template-based parser combinators.  Stripping
  the binary greatly reduces its size.

* The Cap'n Proto library has lots of debug-only asserts that are removed if you `#define NDEBUG`,
  including in headers.  If you care at all about performance, you should compile your production
  binaries with the `-DNDEBUG` compiler flag.  In fact, if Cap'n Proto detects that you have
  optimization enabled but have not defined `NDEBUG`, it will define it for you (with a warning),
  unless you define `DEBUG` or `KJ_DEBUG` to explicitly request debugging.

### Security Tips

Cap'n Proto has not yet undergone security review.  It most likely has some vulnerabilities.  You
should not attempt to decode Cap'n Proto messages from sources you don't trust at this time.

However, assuming the Cap'n Proto implementation hardens up eventually, then the following security
tips will apply.

* It is highly recommended that you enable exceptions.  When compiled with `-fno-exceptions`,
  Cap'n Proto categorizes exceptions into "fatal" and "recoverable" varieties.  Fatal exceptions
  cause the server to crash, while recoverable exceptions are handled by logging an error and
  returning a "safe" garbage value.  Fatal is preferred in cases where it's unclear what kind of
  garbage value would constitute "safe".  The more of the library you use, the higher the chance
  that you will leave yourself open to the possibility that an attacker could trigger a fatal
  exception somewhere.  If you enable exceptions, then you can catch the exception instead of
  crashing, and return an error just to the attacker rather than to everyone using your server.

  Basic parsing of Cap'n Proto messages shouldn't ever trigger fatal exceptions (assuming the
  implementation is not buggy).  However, the dynamic API -- especially if you are loading schemas
  controlled by the attacker -- is much more exception-happy.  If you cannot use exceptions, then
  you are advised to avoid the dynamic API when dealing with untrusted data.

* If you need to process schemas from untrusted sources, take them in binary format, not text.
  The text parser is a much larger attack surface and not designed to be secure.  For instance,
  as of this writing, it is trivial to deadlock the parser by simply writing a constant whose value
  depends on itself.

* Cap'n Proto automatically applies two artificial limits on messages for security reasons:
  a limit on nesting dept, and a limit on total bytes traversed.

  * The nesting depth limit is designed to prevent stack overflow when handling a deeply-nested
    recursive type, and defaults to 64.  If your types aren't recursive, it is highly unlikely
    that you would ever hit this limit, and even if they are recursive, it's still unlikely.

  * The traversal limit is designed to defend against maliciously-crafted messages which use
    pointer cycles or overlapping objects to make a message appear much larger than it looks off
    the wire.  While cycles and overlapping objects are illegal, they are hard to detect reliably.
    Instead, Cap'n Proto places a limit on how many bytes worth of objects you can _dereference_
    before it throws an exception.  This limit is assessed every time you follow a pointer.  By
    default, the limit is 64MiB (this may change in the future).  `StreamFdMessageReader` will
    actually reject upfront any message which is larger than the traversal limit, even before you
    start reading it.

    If you need to write your code in such a way that you might frequently re-read the same
    pointers, instead of increasing the traversal limit to the point where it is no longer useful,
    consider simply copying the message into a new `MallocMessageBuilder` before starting.  Then,
    the traversal limit will be enforced only during the copy.  There is no traversal limit on
    objects once they live in a `MessageBuilder`, even if you use `.asReader()` to convert a
    particular object's builder to the corresponding reader type.

  Both limits may be increased using `capnp::ReaderOptions`, defined in `capnp/message.h`.

* Remember that enums on the wire may have a numeric value that does not match any value defined
  in the schema.  Your `switch()` statements must always have a safe default case.

## Lessons Learned from Protocol Buffers

The author of Cap'n Proto's C++ implementation also wrote (in the past) version 2 of Google's
Protocol Buffers.  As a result, Cap'n Proto's implementation benefits from a number of lessons
learned the hard way:

* Protobuf generated code is enormous due to the parsing and serializing code generated for every
  class.  This actually poses a significant problem in practice -- there exist server binaries
  containing literally hundreds of megabytes of compiled protobuf code.  Cap'n Proto generated code,
  on the other hand, is almost entirely inlined accessors.  The only things that go into `.capnp.o`
  files are default values for pointer fields (if needed, which is rare) and the encoded schema
  (just the raw bytes of a Cap'n-Proto-encoded schema structure).  The latter could even be removed
  if you don't use dynamic reflection.

* The C++ Protobuf implementation used lots of dynamic initialization code (that runs before
  `main()`) to do things like register types in global tables.  This proved problematic for
  programs which linked in lots of protocols but needed to start up quickly.  Cap'n Proto does not
  use any dynamic initializers anywhere, period.

* The C++ Protobuf implementation makes heavy use of STL in its interface and implementation.
  The proliferation of template instantiations gives the Protobuf runtime library a large footprint,
  and using STL in the interface can lead to weird ABI problems and slow compiles.  Cap'n Proto
  does not use any STL containers in its interface and makes sparing use in its implementation.
  As a result, the Cap'n Proto runtime library is smaller, and code that uses it compiles quickly.

* The in-memory representation of messages in Protobuf-C++ involves many heap objects.  Each
  message (struct) is an object, each non-primitive repeated field allocates an array of pointers
  to more objects, and each string may actually add two heap objects.  Cap'n Proto by its nature
  uses arena allocation, so the entire message is allocated in a few contiguous segments.  This
  means Cap'n Proto spends very little time allocating memory, stores messages more compactly, and
  avoids memory fragmentation.

* Related to the last point, Protobuf-C++ relies heavily on object reuse for performance.
  Building or parsing into a newly-allocated Protobuf object is significantly slower than using
  an existing one.  However, the memory usage of a Protobuf object will tend to grow the more times
  it is reused, particularly if it is used to parse messages of many different "shapes", so the
  objects need to be deleted and re-allocated from time to time.  All this makes tuning Protobufs
  fairly tedious.  In contrast, enabling memory reuse with Cap'n Proto is as simple as providing
  a byte buffer to use as scratch space when you build or read in a message.  Provide enough scratch
  space to hold the entire message and Cap'n Proto won't allocate any memory.  Or don't -- since
  Cap'n Proto doesn't do much allocation in the first place, the benefits of scratch space are
  small.



doc/cxxrpc.md
--------------------------------------
---
layout: page
title: C++ RPC
---

# C++ RPC

The Cap'n Proto C++ RPC layer sits on top of the [serialization layer](cxx.html) and implements
the [RPC protocol](rpc.html).

## Current Status

As of version 0.4, Cap'n Proto's C++ RPC implementation is a [Level 1](rpc.html#protocol-features)
implementation.  Persistent capabilities, three-way introductions, and distributed equality are
not yet implemented.

## Sample Code

The [Calculator example](https://github.com/capnproto/capnproto/tree/master/c++/samples) implements
a fully-functional Cap'n Proto client and server.

## KJ Concurrency Framework

RPC naturally requires a notion of concurrency.  Unfortunately,
[all concurrency models suck](https://web.archive.org/web/20170718202612/https://plus.google.com/+KentonVarda/posts/D95XKtB5DhK).

Cap'n Proto's RPC is based on the [KJ library](cxx.html#kj-library)'s event-driven concurrency
framework.  The core of the KJ asynchronous framework (events, promises, callbacks) is defined in
`kj/async.h`, with I/O interfaces (streams, sockets, networks) defined in `kj/async-io.h`.

### Event Loop Concurrency

KJ's concurrency model is based on event loops.  While multiple threads are allowed, each thread
must have its own event loop.  KJ discourages fine-grained interaction between threads as
synchronization is expensive and error-prone.  Instead, threads are encouraged to communicate
through Cap'n Proto RPC.

KJ's event loop model bears a lot of similarity to the JavaScript concurrency model.  Experienced
JavaScript hackers -- especially node.js hackers -- will feel right at home.

_As of version 0.4, the only supported way to communicate between threads is over pipes or
socketpairs.  This will be improved in future versions.  For now, just set up an RPC connection
over that socketpair.  :)_

### Promises

Function calls that do I/O must do so asynchronously, and must return a "promise" for the
result.  Promises -- also known as "futures" in some systems -- are placeholders for the results
of operations that have not yet completed.  When the operation completes, we say that the promise
"resolves" to a value, or is "fulfilled".  A promise can also be "rejected", which means an
exception occurred.

{% highlight c++ %}
// Example promise-based interfaces.

kj::Promise<kj::String> fetchHttp(kj::StringPtr url);
// Asynchronously fetches an HTTP document and returns
// the content as a string.

kj::Promise<void> sendEmail(kj::StringPtr address,
    kj::StringPtr title, kj::StringPtr body);
// Sends an e-mail to the given address with the given title
// and body.  The returned promise resolves (to nothing) when
// the message has been successfully sent.
{% endhighlight %}

As you will see, KJ promises are very similar to the evolving JavaScript promise standard, and
much of the [wisdom around it](https://www.google.com/search?q=javascript+promises) can be directly
applied to KJ promises.

### Callbacks

If you want to do something with the result of a promise, you must first wait for it to complete.
This is normally done by registering a callback to execute on completion.  Luckily, C++11 just
introduced lambdas, which makes this far more pleasant than it would have been a few years ago!

{% highlight c++ %}
kj::Promise<kj::String> contentPromise =
    fetchHttp("http://example.com");

kj::Promise<int> lineCountPromise =
    contentPromise.then([](kj::String&& content) {
  return countChars(content, '\n');
});
{% endhighlight %}

The callback passed to `then()` takes the promised result as its parameter and returns a new value.
`then()` itself returns a new promise for that value which the callback will eventually return.
If the callback itself returns a promise, then `then()` actually returns a promise for the
resolution of the latter promise -- that is, `Promise<Promise<T>>` is automatically reduced to
`Promise<T>`.

Note that `then()` consumes the original promise:  you can only call `then()` once.  This is true
of all of the methods of `Promise`.  The only way to consume a promise in multiple places is to
first "fork" it with the `fork()` method, which we don't get into here.  Relatedly, promises
are linear types, which means they have move constructors but not copy constructors.

### Error Propagation

`then()` takes an optional second parameter for handling errors.  Think of this like a `catch`
block.

{% highlight c++ %}
kj::Promise<int> lineCountPromise =
    promise.then([](kj::String&& content) {
  return countChars(content, '\n');
}, [](kj::Exception&& exception) {
  // Error!  Pretend the document was empty.
  return 0;
});
{% endhighlight %}

Note that the KJ framework coerces all exceptions to `kj::Exception` -- the exception's description
(as returned by `what()`) will be retained, but any type-specific information is lost.  Under KJ
exception philosophy, exceptions always represent an error that should not occur under normal
operation, and the only purpose of exceptions is to make software fault-tolerant.  In particular,
the only reasonable ways to handle an exception are to try again, tell a human, and/or propagate
to the caller.  To that end, `kj::Exception` contains information useful for reporting purposes
and to help decide if trying again is reasonable, but typed exception hierarchies are not useful
and not supported.

It is recommended that Cap'n Proto code use the assertion macros in `kj/debug.h` to throw
exceptions rather than use the C++ `throw` keyword.  These macros make it easy to add useful
debug information to an exception and generally play nicely with the KJ framework.  In fact, you
can even use these macros -- and propagate exceptions through promises -- if you compile your code
with exceptions disabled.  See the headers for more information.

### Waiting

It is illegal for code running in an event callback to wait, since this would stall the event loop.
However, if you are the one responsible for starting the event loop in the first place, then KJ
makes it easy to say "run the event loop until this promise resolves, then return the result".

{% highlight c++ %}
kj::EventLoop loop;
kj::WaitScope waitScope(loop);

kj::Promise<kj::String> contentPromise =
    fetchHttp("http://example.com");

kj::String content = contentPromise.wait(waitScope);

int lineCount = countChars(content, '\n');
{% endhighlight %}

Using `wait()` is common in high-level client-side code.  On the other hand, it is almost never
used in servers.

### Cancellation

If you discard a `Promise` without calling any of its methods, the operation it was waiting for
is canceled, because the `Promise` itself owns that operation.  This means than any pending
callbacks simply won't be executed.  If you need explicit notification when a promise is canceled,
you can use its `attach()` method to attach an object with a destructor -- the destructor will be
called when the promise either completes or is canceled.

### Lazy Execution

Callbacks registered with `.then()` which aren't themselves asynchronous (i.e. they return a value,
not a promise) by default won't execute unless the result is actually used -- they are executed
"lazily". This allows the runtime to optimize by combining a series of .then() callbacks into one.

To force a `.then()` callback to execute as soon as its input is available, do one of the
following:

* Add it to a `kj::TaskSet` -- this is usually the best choice. You can cancel all tasks in the set
  by destroying the `TaskSet`.
* `.wait()` on it -- but this only works in a top-level wait scope, typically your program's main
  function.
* Call `.eagerlyEvaluate()` on it. This returns a new `Promise`. You can cancel the task by
  destroying this `Promise` (without otherwise consuming it).
* `.detach()` it. **WARNING:** `.detach()` is dangerous because there is no way to cancel a promise
  once it has been detached. This can make it impossible to safely tear down the execution
  environment, e.g. if the callback has captured references to other objects. It is therefore
  recommended to avoid `.detach()` except in carefully-controlled circumstances.

### Other Features

KJ supports a number of primitive operations that can be performed on promises.  The complete API
is documented directly in the `kj/async.h` header.  Additionally, see the `kj/async-io.h` header
for APIs for performing basic network I/O -- although Cap'n Proto RPC users typically won't need
to use these APIs directly.

## Generated Code

Imagine the following interface:

{% highlight capnp %}
interface Directory {
  create @0 (name :Text) -> (file :File);
  open @1 (name :Text) -> (file :File);
  remove @2 (name :Text);
}
{% endhighlight %}

`capnp compile` will generate code that looks like this (edited for readability):

{% highlight c++ %}
struct Directory {
  Directory() = delete;

  class Client;
  class Server;

  struct CreateParams;
  struct CreateResults;
  struct OpenParams;
  struct OpenResults;
  struct RemoveParams;
  struct RemoveResults;
  // Each of these is equivalent to what would be generated for
  // a Cap'n Proto struct with one field for each parameter /
  // result.
};

class Directory::Client
    : public virtual capnp::Capability::Client {
public:
  Client(std::nullptr_t);
  Client(kj::Own<Directory::Server> server);
  Client(kj::Promise<Client> promise);
  Client(kj::Exception exception);

  capnp::Request<CreateParams, CreateResults> createRequest();
  capnp::Request<OpenParams, OpenResults> openRequest();
  capnp::Request<RemoveParams, RemoveResults> removeRequest();
};

class Directory::Server
    : public virtual capnp::Capability::Server {
protected:
  typedef capnp::CallContext<CreateParams, CreateResults> CreateContext;
  typedef capnp::CallContext<OpenParams, OpenResults> OpenContext;
  typedef capnp::CallContext<RemoveParams, RemoveResults> RemoveContext;
  // Convenience typedefs.

  virtual kj::Promise<void> create(CreateContext context);
  virtual kj::Promise<void> open(OpenContext context);
  virtual kj::Promise<void> remove(RemoveContext context);
  // Methods for you to implement.
};
{% endhighlight %}

### Clients

The generated `Client` type represents a reference to a remote `Server`.  `Client`s are
pass-by-value types that use reference counting under the hood.  (Warning:  For performance
reasons, the reference counting used by `Client`s is not thread-safe, so you must not copy a
`Client` to another thread, unless you do it by means of an inter-thread RPC.)

A `Client` can be implicitly constructed from any of:

* A `kj::Own<Server>`, which takes ownership of the server object and creates a client that
  calls it.  (You can get a `kj::Own<T>` to a newly-allocated heap object using
  `kj::heap<T>(constructorParams)`; see `kj/memory.h`.)
* A `kj::Promise<Client>`, which creates a client whose methods first wait for the promise to
  resolve, then forward the call to the resulting client.
* A `kj::Exception`, which creates a client whose methods always throw that exception.
* `nullptr`, which creates a client whose methods always throw.  This is meant to be used to
  initialize variables that will be initialized to a real value later on.

For each interface method `foo()`, the `Client` has a method `fooRequest()` which creates a new
request to call `foo()`.  The returned `capnp::Request` object has methods equivalent to a
`Builder` for the parameter struct (`FooParams`), with the addition of a method `send()`.
`send()` sends the RPC and returns a `capnp::RemotePromise<FooResults>`.

This `RemotePromise` is equivalent to `kj::Promise<capnp::Response<FooResults>>`, but also has
methods that allow pipelining.  Namely:

* For each interface-typed result, it has a getter method which returns a `Client` of that type.
  Calling this client will send a pipelined call to the server.
* For each struct-typed result, it has a getter method which returns an object containing pipeline
  getters for that struct's fields.

In other words, the `RemotePromise` effectively implements a subset of the eventual results'
`Reader` interface -- one that only allows access to interfaces and sub-structs.

The `RemotePromise` eventually resolves to `capnp::Response<FooResults>`, which behaves like a
`Reader` for the result struct except that it also owns the result message.

{% highlight c++ %}
Directory::Client dir = ...;

// Create a new request for the `open()` method.
auto request = dir.openRequest();
request.setName("foo");

// Send the request.
auto promise = request.send();

// Make a pipelined request.
auto promise2 = promise.getFile().getSizeRequest().send();

// Wait for the full results.
auto promise3 = promise2.then(
    [](capnp::Response<File::GetSizeResults>&& response) {
  cout << "File size is: " << response.getSize() << endl;
});
{% endhighlight %}

For [generic methods](language.html#generic-methods), the `fooRequest()` method will be a template;
you must explicitly specify type parameters.

### Servers

The generated `Server` type is an abstract interface which may be subclassed to implement a
capability.  Each method takes a `context` argument and returns a `kj::Promise<void>` which
resolves when the call is finished.  The parameter and result structures are accessed through the
context -- `context.getParams()` returns a `Reader` for the parameters, and `context.getResults()`
returns a `Builder` for the results.  The context also has methods for controlling RPC logistics,
such as cancellation -- see `capnp::CallContext` in `capnp/capability.h` for details.

Accessing the results through the context (rather than by returning them) is unintuitive, but
necessary because the underlying RPC transport needs to have control over where the results are
allocated.  For example, a zero-copy shared memory transport would need to allocate the results in
the shared memory segment.  Hence, the method implementation cannot just create its own
`MessageBuilder`.

{% highlight c++ %}
class DirectoryImpl final: public Directory::Server {
public:
  kj::Promise<void> open(OpenContext context) override {
    auto iter = files.find(context.getParams().getName());

    // Throw an exception if not found.
    KJ_REQUIRE(iter != files.end(), "File not found.");

    context.getResults().setFile(iter->second);

    return kj::READY_NOW;
  }

  // Any method which we don't implement will simply throw
  // an exception by default.

private:
  std::map<kj::StringPtr, File::Client> files;
};
{% endhighlight %}

On the server side, [generic methods](language.html#generic-methods) are NOT templates. Instead,
the generated code is exactly as if all of the generic parameters were bound to `AnyPointer`. The
server generally does not get to know exactly what type the client requested; it must be designed
to be correct for any parameterization.

## Initializing RPC

Cap'n Proto makes it easy to start up an RPC client or server using the  "EZ RPC" classes,
defined in `capnp/ez-rpc.h`.  These classes get you up and running quickly, but they hide a lot
of details that power users will likely want to manipulate.  Check out the comments in `ez-rpc.h`
to understand exactly what you get and what you miss.  For the purpose of this overview, we'll
show you how to use EZ RPC to get started.

### Starting a client

A client should typically look like this:

{% highlight c++ %}
#include <capnp/ez-rpc.h>
#include "my-interface.capnp.h"
#include <iostream>

int main(int argc, const char* argv[]) {
  // We expect one argument specifying the server address.
  if (argc != 2) {
    std::cerr << "usage: " << argv[0] << " HOST[:PORT]" << std::endl;
    return 1;
  }

  // Set up the EzRpcClient, connecting to the server on port
  // 5923 unless a different port was specified by the user.
  capnp::EzRpcClient client(argv[1], 5923);
  auto& waitScope = client.getWaitScope();

  // Request the bootstrap capability from the server.
  MyInterface::Client cap = client.getMain<MyInterface>();

  // Make a call to the capability.
  auto request = cap.fooRequest();
  request.setParam(123);
  auto promise = request.send();

  // Wait for the result.  This is the only line that blocks.
  auto response = promise.wait(waitScope);

  // All done.
  std::cout << response.getResult() << std::endl;
  return 0;
}
{% endhighlight %}

Note that for the connect address, Cap'n Proto supports DNS host names as well as IPv4 and IPv6
addresses.  Additionally, a Unix domain socket can be specified as `unix:` followed by a path name,
and an abstract Unix domain socket can be specified as `unix-abstract:` followed by an identifier.

For a more complete example, see the
[calculator client sample](https://github.com/capnproto/capnproto/tree/master/c++/samples/calculator-client.c++).

### Starting a server

A server might look something like this:

{% highlight c++ %}
#include <capnp/ez-rpc.h>
#include "my-interface-impl.h"
#include <iostream>

int main(int argc, const char* argv[]) {
  // We expect one argument specifying the address to which
  // to bind and accept connections.
  if (argc != 2) {
    std::cerr << "usage: " << argv[0] << " ADDRESS[:PORT]"
              << std::endl;
    return 1;
  }

  // Set up the EzRpcServer, binding to port 5923 unless a
  // different port was specified by the user.  Note that the
  // first parameter here can be any "Client" object or anything
  // that can implicitly cast to a "Client" object.  You can even
  // re-export a capability imported from another server.
  capnp::EzRpcServer server(kj::heap<MyInterfaceImpl>(), argv[1], 5923);
  auto& waitScope = server.getWaitScope();

  // Run forever, accepting connections and handling requests.
  kj::NEVER_DONE.wait(waitScope);
}
{% endhighlight %}

Note that for the bind address, Cap'n Proto supports DNS host names as well as IPv4 and IPv6
addresses.  The special address `*` can be used to bind to the same port on all local IPv4 and
IPv6 interfaces.  Additionally, a Unix domain socket can be specified as `unix:` followed by a
path name, and an abstract Unix domain socket can be specified as `unix-abstract:` followed by
an identifier.

For a more complete example, see the
[calculator server sample](https://github.com/capnproto/capnproto/tree/master/c++/samples/calculator-server.c++).

## Debugging

If you've written a server and you want to connect to it to issue some calls for debugging, perhaps
interactively, the easiest way to do it is to use [pycapnp](http://jparyani.github.io/pycapnp/).
We have decided not to add RPC functionality to the `capnp` command-line tool because pycapnp is
better than anything we might provide.



doc/encoding.md
--------------------------------------
---
layout: page
title: Encoding Spec
---

# Encoding Spec

## Organization

### 64-bit Words

For the purpose of Cap'n Proto, a "word" is defined as 8 bytes, or 64 bits.  Since alignment of
data is important, all objects (structs, lists, and blobs) are aligned to word boundaries, and
sizes are usually expressed in terms of words.  (Primitive values are aligned to a multiple of
their size within a struct or list.)

### Messages

The unit of communication in Cap'n Proto is a "message".  A message is a tree of objects, with
the root always being a struct.

Physically, messages may be split into several "segments", each of which is a flat blob of bytes.
Typically, a segment must be loaded into a contiguous block of memory before it can be accessed,
so that the relative pointers within the segment can be followed quickly.  However, when a message
has multiple segments, it does not matter where those segments are located in memory relative to
each other; inter-segment pointers are encoded differently, as we'll see later.

Ideally, every message would have only one segment.  However, there are a few reasons why splitting
a message into multiple segments may be convenient:

* It can be difficult to predict how large a message might be until you start writing it, and you
  can't start writing it until you have a segment to write to.  If it turns out the segment you
  allocated isn't big enough, you can allocate additional segments without the need to relocate the
  data you've already written.
* Allocating excessively large blocks of memory can make life difficult for memory allocators,
  especially on 32-bit systems with limited address space.

The first word of the first segment of the message is always a pointer pointing to the message's
root struct.

### Objects

Each segment in a message contains a series of objects.  For the purpose of Cap'n Proto, an "object"
is any value which may have a pointer pointing to it.  Pointers can only point to the beginning of
objects, not into the middle, and no more than one pointer can point at each object.  Thus, objects
and the pointers connecting them form a tree, not a graph.  An object is itself composed of
primitive data values and pointers, in a layout that depends on the kind of object.

At the moment, there are three kinds of objects:  structs, lists, and far-pointer landing pads.
Blobs might also be considered to be a kind of object, but are encoded identically to lists of
bytes.

## Value Encoding

### Primitive Values

The built-in primitive types are encoded as follows:

* `Void`:  Not encoded at all.  It has only one possible value thus carries no information.
* `Bool`:  One bit.  1 = true, 0 = false.
* Integers:  Encoded in little-endian format.  Signed integers use two's complement.
* Floating-points:  Encoded in little-endian IEEE-754 format.

Primitive types must always be aligned to a multiple of their size.  Note that since the size of
a `Bool` is one bit, this means eight `Bool` values can be encoded in a single byte -- this differs
from C++, where the `bool` type takes a whole byte.

### Enums

Enums are encoded the same as `UInt16`.

## Object Encoding

### Blobs

The built-in blob types are encoded as follows:

* `Data`:  Encoded as a pointer, identical to `List(UInt8)`.
* `Text`:  Like `Data`, but the content must be valid UTF-8, and the last byte of the content must
  be zero.  The encoding allows bytes other than the last to be zero, but some applications
  (especially ones written in languages that use NUL-terminated strings) may truncate at the first
  zero.  If a particular text field is explicitly intended to support zero bytes, it should
  document this, but otherwise senders should assume that zero bytes are not allowed to be safe.
  Note that the NUL terminator is included in the size sent on the wire, but the runtime library
  should not count it in any size reported to the application.

### Structs

A struct value is encoded as a pointer to its content.  The content is split into two sections:
data and pointers, with the pointer section appearing immediately after the data section.  This
split allows structs to be traversed (e.g., copied) without knowing their type.

A struct pointer looks like this:

    lsb                      struct pointer                       msb
    +-+-----------------------------+---------------+---------------+
    |A|             B               |       C       |       D       |
    +-+-----------------------------+---------------+---------------+

    A (2 bits) = 0, to indicate that this is a struct pointer.
    B (30 bits) = Offset, in words, from the end of the pointer to the
        start of the struct's data section.  Signed.
    C (16 bits) = Size of the struct's data section, in words.
    D (16 bits) = Size of the struct's pointer section, in words.

Fields are positioned within the struct according to an algorithm with the following principles:

* The position of each field depends only on its definition and the definitions of lower-numbered
  fields, never on the definitions of higher-numbered fields.  This ensures backwards-compatibility
  when new fields are added.
* Due to alignment requirements, fields in the data section may be separated by padding.  However,
  later-numbered fields may be positioned into the padding left between earlier-numbered fields.
  Because of this, a struct will never contain more than 63 bits of padding.  Since objects are
  rounded up to a whole number of words anyway, padding never ends up wasting space.
* Unions and groups need not occupy contiguous memory.  Indeed, they may have to be split into
  multiple slots if new fields are added later on.

Field offsets are computed by the Cap'n Proto compiler.  The precise algorithm is too complicated
to describe here, but you need not implement it yourself, as the compiler can produce a compiled
schema format which includes offset information.

#### Default Values

A default struct is always all-zeros.  To achieve this, fields in the data section are stored xor'd
with their defined default values.  An all-zero pointer is considered "null"; accessor methods
for pointer fields check for null and return a pointer to their default value in this case.

There are several reasons why this is desirable:

* Cap'n Proto messages are often "packed" with a simple compression algorithm that deflates
  zero-value bytes.
* Newly-allocated structs only need to be zero-initialized, which is fast and requires no knowledge
  of the struct type except its size.
* If a newly-added field is placed in space that was previously padding, messages written by old
  binaries that do not know about this field will still have its default value set correctly --
  because it is always zero.

#### Zero-sized structs.

As stated above, a pointer whose bits are all zero is considered a null pointer, *not* a struct of
zero size. To encode a struct of zero size, set A, C, and D to zero, and set B (the offset) to -1.

**Historical explanation:** A null pointer is intended to be treated as equivalent to the field's
default value. Early on, it was thought that a zero-sized struct was a suitable synonym for
null, since interpreting an empty struct as any struct type results in a struct whose fields are
all default-valued. So, the pointer encoding was designed such that a zero-sized struct's pointer
would be all-zero, so that it could conveniently be overloaded to mean "null".

However, it turns out there are two important differences between a zero-sized struct and a null
pointer. First, applications often check for null explicitly when implementing optional fields.
Second, an empty struct is technically equivalent to the default value for the struct *type*,
whereas a null pointer is equivalent to the default value for the particular *field*. These are
not necessarily the same.

It therefore became necessary to find a different encoding for zero-sized structs. Since the
struct has zero size, the pointer's offset can validly point to any location so long as it is
in-bounds. Since an offset of -1 points to the beginning of the pointer itself, it is known to
be in-bounds. So, we use an offset of -1 when the struct has zero size.

### Lists

A list value is encoded as a pointer to a flat array of values.

    lsb                       list pointer                        msb
    +-+-----------------------------+--+----------------------------+
    |A|             B               |C |             D              |
    +-+-----------------------------+--+----------------------------+

    A (2 bits) = 1, to indicate that this is a list pointer.
    B (30 bits) = Offset, in words, from the end of the pointer to the
        start of the first element of the list.  Signed.
    C (3 bits) = Size of each element:
        0 = 0 (e.g. List(Void))
        1 = 1 bit
        2 = 1 byte
        3 = 2 bytes
        4 = 4 bytes
        5 = 8 bytes (non-pointer)
        6 = 8 bytes (pointer)
        7 = composite (see below)
    D (29 bits) = Size of the list:
        when C <> 7: Number of elements in the list.
        when C = 7: Number of words in the list, not counting the tag word
        (see below).

The pointed-to values are tightly-packed.  In particular, `Bool`s are packed bit-by-bit in
little-endian order (the first bit is the least-significant bit of the first byte).

When C = 7, the elements of the list are fixed-width composite values -- usually, structs.  In
this case, the list content is prefixed by a "tag" word that describes each individual element.
The tag has the same layout as a struct pointer, except that the pointer offset (B) instead
indicates the number of elements in the list.  Meanwhile, section (D) of the list pointer -- which
normally would store this element count -- instead stores the total number of _words_ in the list
(not counting the tag word).  The reason we store a word count in the pointer rather than an element
count is to ensure that the extents of the list's location can always be determined by inspecting
the pointer alone, without having to look at the tag; this may allow more-efficient prefetching in
some use cases.  The reason we don't store struct lists as a list of pointers is because doing so
would take significantly more space (an extra pointer per element) and may be less cache-friendly.

In the future, we could consider implementing matrixes using the "composite" element type, with the
elements being fixed-size lists rather than structs.  In this case, the tag would look like a list
pointer rather than a struct pointer.  As of this writing, no such feature has been implemented.

A struct list must always be written using C = 7. However, a list of any element size (except
C = 1, i.e. 1-bit) may be *decoded* as a struct list, with each element being interpreted as being
a prefix of the struct data. For instance, a list of 2-byte values (C = 3) can be decoded as a
struct list where each struct has 2 bytes in their "data" section (and an empty pointer section). A
list of pointer values (C = 6) can be decoded as a struct list where each struct has a pointer
section with one pointer (and an empty data section). The purpose of this rule is to make it
possible to upgrade a list of primitives to a list of structs, as described under the
[protocol evolution rules](language.html#evolving-your-protocol).
(We make a special exception that boolean lists cannot be upgraded in this way due to the
unreasonable implementation burden.) Note that even though struct lists can be decoded from any
element size (except C = 1), it is NOT permitted to encode a struct list using any type other than
C = 7 because doing so would interfere with the [canonicalization algorithm](#canonicalization).

### Inter-Segment Pointers

When a pointer needs to point to a different segment, offsets no longer work.  We instead encode
the pointer as a "far pointer", which looks like this:

    lsb                        far pointer                        msb
    +-+-+---------------------------+-------------------------------+
    |A|B|            C              |               D               |
    +-+-+---------------------------+-------------------------------+

    A (2 bits) = 2, to indicate that this is a far pointer.
    B (1 bit) = 0 if the landing pad is one word, 1 if it is two words.
        See explanation below.
    C (29 bits) = Offset, in words, from the start of the target segment
        to the location of the far-pointer landing-pad within that
        segment.  Unsigned.
    D (32 bits) = ID of the target segment.  (Segments are numbered
        sequentially starting from zero.)

If B == 0, then the "landing pad" of a far pointer is normally just another pointer, which in turn
points to the actual object.

If B == 1, then the "landing pad" is itself another far pointer that is interpreted differently:
This far pointer (which always has B = 0) points to the start of the object's _content_, located in
some other segment.  The landing pad is itself immediately followed by a tag word.  The tag word
looks exactly like an intra-segment pointer to the target object would look, except that the offset
is always zero.

The reason for the convoluted double-far convention is to make it possible to form a new pointer
to an object in a segment that is full.  If you can't allocate even one word in the segment where
the target resides, then you will need to allocate a landing pad in some other segment, and use
this double-far approach.  This should be exceedingly rare in practice since pointers are normally
set to point to new objects, not existing ones.

### Capabilities (Interfaces)

When using Cap'n Proto for [RPC](rpc.html), every message has an associated "capability table"
which is a flat list of all capabilities present in the message body.  The details of what this
table contains and where it is stored are the responsibility of the RPC system; in some cases, the
table may not even be part of the message content.

A capability pointer, then, simply contains an index into the separate capability table.

    lsb                    capability pointer                     msb
    +-+-----------------------------+-------------------------------+
    |A|              B              |               C               |
    +-+-----------------------------+-------------------------------+

    A (2 bits) = 3, to indicate that this is an "other" pointer.
    B (30 bits) = 0, to indicate that this is a capability pointer.
        (All other values are reserved for future use.)
    C (32 bits) = Index of the capability in the message's capability
        table.

In [rpc.capnp](https://github.com/capnproto/capnproto/blob/master/c++/src/capnp/rpc.capnp), the
capability table is encoded as a list of `CapDescriptors`, appearing along-side the message content
in the `Payload` struct.  However, some use cases may call for different approaches.  A message
that is built and consumed within the same process need not encode the capability table at all
(it can just keep the table as a separate array).  A message that is going to be stored to disk
would need to store a table of `SturdyRef`s instead of `CapDescriptor`s.

## Serialization Over a Stream

When transmitting a message, the segments must be framed in some way, i.e. to communicate the
number of segments and their sizes before communicating the actual data.  The best framing approach
may differ depending on the medium -- for example, messages read via `mmap` or shared memory may
call for a different approach than messages sent over a socket or a pipe.  Cap'n Proto does not
attempt to specify a framing format for every situation.  However, since byte streams are by far
the most common transmission medium, Cap'n Proto does define and implement a recommended framing
format for them.

When transmitting over a stream, the following should be sent.  All integers are unsigned and
little-endian.

* (4 bytes) The number of segments, minus one (since there is always at least one segment).
* (N * 4 bytes) The size of each segment, in words.
* (0 or 4 bytes) Padding up to the next word boundary.
* The content of each segment, in order.

### Packing

For cases where bandwidth usage matters, Cap'n Proto defines a simple compression scheme called
"packing".  This scheme is based on the observation that Cap'n Proto messages contain lots of
zero bytes: padding bytes, unset fields, and high-order bytes of small-valued integers.

In packed format, each word of the message is reduced to a tag byte followed by zero to eight
content bytes.  The bits of the tag byte correspond to the bytes of the unpacked word, with the
least-significant bit corresponding to the first byte.  Each zero bit indicates that the
corresponding byte is zero.  The non-zero bytes are packed following the tag.

For example, here is some typical Cap'n Proto data (a struct pointer (offset = 2, data size = 3,
pointer count = 2) followed by a text pointer (offset = 6, length = 53)) and its packed form:

    unpacked (hex):  08 00 00 00 03 00 02 00   19 00 00 00 aa 01 00 00
    packed (hex):  51 08 03 02   31 19 aa 01

In addition to the above, there are two tag values which are treated specially:  0x00 and 0xff.

* 0x00:  The tag is followed by a single byte which indicates a count of consecutive zero-valued
  words, minus 1.  E.g. if the tag 0x00 is followed by 0x05, the sequence unpacks to 6 words of
  zero.

  Or, put another way: the tag is first decoded as if it were not special.  Since none of the bits
  are set, it is followed by no bytes and expands to a word full of zeros.  After that, the next
  byte is interpreted as a count of _additional_ words that are also all-zero.

* 0xff:  The tag is followed by the bytes of the word (as if it weren't special), but after those
  bytes is another byte with value N.  Following that byte is N unpacked words that should be copied
  directly.  These unpacked words may or may not contain zeros -- it is up to the compressor to
  decide when to end the unpacked span and return to packing each word.  The purpose of this rule
  is to minimize the impact of packing on data that doesn't contain any zeros -- in particular,
  long text blobs.  Because of this rule, the worst-case space overhead of packing is 2 bytes per
  2 KiB of input (256 words = 2KiB).

Examples:

    unpacked (hex):  00 (x 32 bytes)
    packed (hex):  00 03

    unpacked (hex):  8a (x 32 bytes)
    packed (hex):  ff 8a (x 8 bytes) 03 8a (x 24 bytes)

Notice that both of the special cases begin by treating the tag as if it weren't special.  This
is intentionally designed to make encoding faster:  you can compute the tag value and encode the
bytes in a single pass through the input word.  Only after you've finished with that word do you
need to check whether the tag ended up being 0x00 or 0xff.

It is possible to write both an encoder and a decoder which only branch at the end of each word,
and only to handle the two special tags.  It is not necessary to branch on every byte.  See the
C++ reference implementation for an example.

Packing is normally applied on top of the standard stream framing described in the previous
section.

### Compression

When Cap'n Proto messages may contain repetitive data (especially, large text blobs), it makes sense
to apply a standard compression algorithm in addition to packing. When CPU time is scarce, we
recommend [LZ4 compression](https://code.google.com/p/lz4/). Otherwise, [zlib](http://www.zlib.net)
is slower but will compress more.

## Canonicalization

Cap'n Proto messages have a well-defined canonical form. Cap'n Proto encoders are NOT required to
output messages in canonical form, and in fact they will almost never do so by default. However,
it is possible to write code which canonicalizes a Cap'n Proto message without knowing its schema.

A canonical Cap'n Proto message must adhere to the following rules:

* The object tree must be encoded in preorder (with respect to the order of the pointers within
  each object).
* The message must be encoded as a single segment. (When signing or hashing a canonical Cap'n Proto
  message, the segment table shall not be included, because it would be redundant.)
* Trailing zero-valued words in a struct's data or pointer segments must be truncated. Since zero
  represents a default value, this does not change the struct's meaning. This rule is important
  to ensure that adding a new field to a struct does not affect the canonical encoding of messages
  that do not set that field.
* Similarly, for a struct list, if a trailing word in a section of all structs in the list is zero,
  then it must be truncated from all structs in the list. (All structs in a struct list must have
  equal sizes, hence a trailing zero can only be removed if it is zero in all elements.)
* Any struct pointer pointing to a zero-sized struct should have an
  offset of -1.
  * Note that this applies _only_ to structs; other zero-sized values should have offsets
    allocated in preorder, as normal.
* Canonical messages are not packed. However, packing can still be applied for transmission
  purposes; the message must simply be unpacked before checking signatures.

Note that Cap'n Proto 0.5 introduced the rule that struct lists must always be encoded using
C = 7 in the [list pointer](#lists). Prior versions of Cap'n Proto allowed struct lists to be
encoded using any element size, so that small structs could be compacted to take less than a word
per element, and many encoders in fact implemented this. Unfortunately, this "optimization" made
canonicalization impossible without knowing the schema, which is a significant obstacle. Therefore,
the rules have been changed in 0.5, but data written by previous versions may not be possible to
canonicalize.

## Security Considerations

A naive implementation of a Cap'n Proto reader may be vulnerable to attacks based on various kinds
of malicious input. Implementations MUST guard against these.

### Pointer Validation

Cap'n Proto readers must validate pointers, e.g. to check that the target object is within the
bounds of its segment. To avoid an upfront scan of the message (which would defeat Cap'n Proto's
O(1) parsing performance), validation should occur lazily when the getter method for a pointer is
called, throwing an exception or returning a default value if the pointer is invalid.

### Amplification attack

A message containing cyclic (or even just overlapping) pointers can cause the reader to go into
an infinite loop while traversing the content.

To defend against this, as the application traverses the message, each time a pointer is
dereferenced, a counter should be incremented by the size of the data to which it points.  If this
counter goes over some limit, an error should be raised, and/or default values should be returned. We call this limit the "traversal limit" (or, sometimes, the "read limit").

The C++ implementation currently defaults to a limit of 64MiB, but allows the caller to set a
different limit if desired. Another reasonable strategy is to set the limit to some multiple of
the original message size; however, most applications should place limits on overall message sizes
anyway, so it makes sense to have one check cover both.

**List amplification:** A list of `Void` values or zero-size structs can have a very large element count while taking constant space on the wire. If the receiving application expects a list of structs, it will see these zero-sized elements as valid structs set to their default values. If it iterates through the list processing each element, it could spend a large amount of CPU time or other resources despite the message being small. To defend against this, the "traversal limit" should count a list of zero-sized elements as if each element were one word instead. This rule was introduced in the C++ implementation in [commit 1048706](https://github.com/capnproto/capnproto/commit/104870608fde3c698483fdef6b97f093fc15685d).

### Stack overflow DoS attack

A message with deeply-nested objects can cause a stack overflow in typical code which processes
messages recursively.

To defend against this, as the application traverses the message, the pointer depth should be
tracked. If it goes over some limit, an error should be raised.  The C++ implementation currently
defaults to a limit of 64 pointers, but allows the caller to set a different limit.



doc/faq.md
--------------------------------------
---
layout: page
title: FAQ
---

# FAQ

## Design

### Isn't I/O bandwidth more important than CPU usage?  Is Cap'n Proto barking up the wrong tree?

It depends.  What is your use case?

Are you communicating between two processes on the same machine?  If so, you have unlimited
bandwidth, and you should be entirely concerned with CPU.

Are you communicating between two machines within the same datacenter?  If so, it's unlikely that
you will saturate your network connection before your CPU.  Possible, but unlikely.

Are you communicating across the general internet?  In that case, bandwidth is probably your main
concern.  Luckily, Cap'n Proto lets you choose to enable "packing" in this case, achieving similar
encoding size to Protocol Buffers while still being faster.  And you can always add extra
compression on top of that.

### Have you considered building the RPC system on ZeroMQ?

ZeroMQ (and its successor, Nanomsg) is a powerful technology for distributed computing.  Its
design focuses on scenarios involving lots of stateless, fault-tolerant worker processes
communicating via various patterns, such as request/response, produce/consume, and
publish/subscribe.  For big data processing where armies of stateless nodes make sense, pairing
Cap'n Proto with ZeroMQ would be an excellent choice -- and this is easy to do today, as ZeroMQ
is entirely serialization-agnostic.

That said, Cap'n Proto RPC takes a very different approach.  Cap'n Proto's model focuses on
stateful servers interacting in complex, object-oriented ways.  The model is better suited to
tasks involving applications with many heterogeneous components and interactions between
mutually-distrusting parties.  Requests and responses can go in any direction.  Objects have
state and so two calls to the same object had best go to the same machine.  Load balancing and
fault tolerance is pushed up the stack, because without a large pool of homogeneous work there's
just no way to make them transparent at a low level.

Put concretely, you might build a search engine indexing pipeline on ZeroMQ, but an online
interactive spreadsheet editor would be better built on Cap'n Proto RPC.

(Actually, a distributed programming framework providing similar features to ZeroMQ could itself be
built on top of Cap'n Proto RPC.)

### Aren't messages that contain pointers a huge security problem?

Not at all.  Cap'n Proto bounds-checks each pointer when it is read and throws an exception or
returns a safe dummy value (your choice) if the pointer is out-of-bounds.

### So it's not that you've eliminated parsing, you've just moved it to happen lazily?

No.  Compared to Protobuf decoding, the time spent validating pointers while traversing a Cap'n
Proto message is negligible.

### I think I heard somewhere that capability-based security doesn't work?

This was a popular myth in security circles way back in the 80's and 90's, based on an incomplete
understanding of what capabilities are and how to use them effectively.  Read
[Capability Myths Demolished](http://zesty.ca/capmyths/usenix.pdf).  (No really, read it;
it's awesome.)

## Usage

### How do I make a field "required", like in Protocol Buffers?

You don't.  You may find this surprising, but the "required" keyword in Protocol Buffers turned
out to be a horrible mistake.

For background, in protocol buffers, a field could be marked "required" to indicate that parsing
should fail if the sender forgot to set the field before sending the message.  Required fields were
encoded exactly the same as optional ones; the only difference was the extra validation.

The problem with this is, validation is sometimes more subtle than that.  Sometimes, different
applications -- or different parts of the same application, or different versions of the same
application -- place different requirements on the same protocol.  An application may want to
pass around partially-complete messages internally.  A particular field that used to be required
might become optional.  A new use case might call for almost exactly the same message type, minus
one field, at which point it may make more sense to reuse the type than to define a new one.

A field declared required, unfortunately, is required everywhere.  The validation is baked into
the parser, and there's nothing you can do about it.  Nothing, that is, except change the field
from "required" to "optional".  But that's where the _real_ problems start.

Imagine a production environment in which two servers, Alice and Bob, exchange messages through a
message bus infrastructure running on a big corporate network.  The message bus parses each message
just to examine the envelope and decide how to route it, without paying attention to any other
content.  Often, messages from various applications are batched together and then split up again
downstream.

Now, at some point, Alice's developers decide that one of the fields in a deeply-nested message
commonly sent to Bob has become obsolete.  To clean things up, they decide to remove it, so they
change the field from "required" to "optional".  The developers aren't idiots, so they realize that
Bob needs to be updated as well.  They make the changes to Bob, and just to be thorough they
run an integration test with Alice and Bob running in a test environment.  The test environment
is always running the latest build of the message bus, but that's irrelevant anyway because the
message bus doesn't actually care about message contents; it only does routing.  Protocols are
modified all the time without updating the message bus.

Satisfied with their testing, the devs push a new version of Alice to prod.  Immediately,
everything breaks.  And by "everything" I don't just mean Alice and Bob.  Completely unrelated
servers are getting strange errors or failing to receive messages.  The whole data center has
ground to a halt and the sysadmins are running around with their hair on fire.

What happened?  Well, the message bus running in prod was still an older build from before the
protocol change.  And even though the message bus doesn't care about message content, it _does_
need to parse every message just to read the envelope.  And the protobuf parser checks the _entire_
message for missing required fields.  So when Alice stopped sending that newly-optional field, the
whole message failed to parse, envelope and all.  And to make matters worse, any other messages
that happened to be in the same batch _also_ failed to parse, causing errors in seemingly-unrelated
systems that share the bus.

Things like this have actually happened.  At Google.  Many times.

The right answer is for applications to do validation as-needed in application-level code.  If you
want to detect when a client fails to set a particular field, give the field an invalid default
value and then check for that value on the server.  Low-level infrastructure that doesn't care
about message content should not validate it at all.

Oh, and also, Cap'n Proto doesn't have any parsing step during which to check for required
fields.  :)

### How do I make a field optional?

Cap'n Proto has no notion of "optional" fields.

A primitive field always takes space on the wire whether you set it or not (although default-valued
fields will be compressed away if you enable packing).  Such a field can be made semantically
optional by placing it in a union with a `Void` field:

{% highlight capnp %}
union {
  age @0 :Int32;
  ageUnknown @1 :Void;
}
{% endhighlight %}

However, this field still takes space on the wire, and in fact takes an extra 16 bits of space
for the union tag.  A better approach may be to give the field a bogus default value and interpret
that value to mean "not present".

Pointer fields are a bit different.  They start out "null", and you can check for nullness using
the `hasFoo()` accessor.  You could use a null pointer to mean "not present".  Note, though, that
calling `getFoo()` on a null pointer returns the default value, which is indistinguishable from a
legitimate value, so checking `hasFoo()` is in fact the _only_ way to detect nullness.

### How do I resize a list?

Unfortunately, you can't.  You have to know the size of your list upfront, before you initialize
any of the elements.  This is an annoying side effect of arena allocation, which is a fundamental
part of Cap'n Proto's design:  in order to avoid making a copy later, all of the pieces of the
message must be allocated in a tightly-packed segment of memory, with each new piece being added
to the end.  If a previously-allocated piece is discarded, it leaves a hole, which wastes space.
Since Cap'n Proto lists are flat arrays, the only way to resize a list would be to discard the
existing list and allocate a new one, which would thus necessarily waste space.

In theory, a more complicated memory allocation algorithm could attempt to reuse the "holes" left
behind by discarded message pieces.  However, it would be hard to make sure any new data inserted
into the space is exactly the right size.  Fragmentation would result.  And the allocator would
have to do a lot of extra bookkeeping that could be expensive.  This would be sad, as arena
allocation is supposed to be cheap!

The only solution is to temporarily place your data into some other data structure (an
`std::vector`, perhaps) until you know how many elements you have, then allocate the list and copy.
On the bright side, you probably aren't losing much performance this way -- using vectors already
involves making copies every time the backing array grows.  It's just annoying to code.

Keep in mind that you can use [orphans](cxx.html#orphans) to allocate sub-objects before you have
a place to put them.  But, also note that you cannot allocate elements of a struct list as orphans
and then put them together as a list later, because struct lists are encoded as a flat array of
struct values, not an array of pointers to struct values.  You can, however, allocate any inner
objects embedded within those structs as orphans.

## Security

### Is Cap'n Proto secure?

What is your threat model?

### Sorry. Can Cap'n Proto be used to deserialize malicious messages?

Cap'n Proto's serialization layer is designed to be safe against malicious input. The Cap'n Proto implementation should never segfault, corrupt memory, leak secrets, execute attacker-specified code, consume excessive resources, etc. as a result of any sequence of input bytes. Moreover, the API is carefully designed to avoid putting app developers into situations where it is easy to write insecure code -- we consider it a bug in Cap'n Proto if apps commonly misuse it in a way that is a security problem.

With all that said, Cap'n Proto's C++ reference implementation has not yet undergone a formal security review. It may have bugs.

### Is it safe to use Cap'n Proto RPC with a malicious peer?

Cap'n Proto's RPC layer is explicitly designed to be useful for interactions between mutually-distrusting parties. Its capability-based security model makes it easy to express complex interactions securely.

At this time, the RPC layer is not robust against resource exhaustion attacks, possibly allowing denials of service.

### Is Cap'n Proto encrypted?

Cap'n Proto may be layered on top of an existing encrypted transport, such as TLS, but at this time it is the application's responsibility to add this layer. We plan to integrate this into the Cap'n Proto library proper in the future.

### How do I report security bugs?

Please email [kenton@cloudflare.com](mailto:kenton@cloudflare.com).

## Sandstorm

### How does Cap'n Proto relate to Sandstorm.io?

[Sandstorm.io](https://sandstorm.io) is an Open Source project and startup founded by Kenton, the author of Cap'n Proto. Cap'n Proto was developed by Sandstorm the company and heavily used in Sandstorm the project. Sandstorm ceased most operations in 2017 and formally dissolved as a company in 2022, but the open source project continues to be developed by the community.

### How does Sandstorm use Cap'n Proto?

See [this Sandstorm blog post](https://blog.sandstorm.io/news/2014-12-15-capnproto-0.5.html).

## Cloudflare

### How does Cap'n Proto relate to Cloudflare?

[Cloudflare Workers](https://workers.dev) is a next-generation cloud application platform. Kenton, the author of Cap'n Proto, is the lead engineer on the Workers project. Workers heavily uses Cap'n Proto in its implementation, and the Cloudflare Workers team are now the primarily developers and maintainers of Cap'n Proto's primary C++ implementation.

### How does Cloudflare use Cap'n Proto?

The Cloudflare Workers runtime is built on Cap'n Proto and it's associated C++ toolkit library, KJ. Cap'n Proto is used for a variety of things, such as communication between sandbox processes and their supervisors, as well between machines and datacenters, especially in the implementation of [Durable Objects](https://blog.cloudflare.com/introducing-workers-durable-objects/).

Cloudflare has also [long used Cap'n Proto in its logging pipeline](http://www.thedotpost.com/2015/06/john-graham-cumming-i-got-10-trillion-problems-but-logging-aint-one) and [developed the Lua implementation of Cap'n Proto](https://blog.cloudflare.com/introducing-lua-capnproto-better-serialization-in-lua/) -- both of these actually predate Kenton joining the company.



doc/index.md
--------------------------------------
---
layout: page
title: Introduction
---

# Introduction

<img src='images/infinity-times-faster.png' style='width:334px; height:306px; float: right;'>

Cap'n Proto is an insanely fast data interchange format and capability-based RPC system. Think
JSON, except binary. Or think [Protocol Buffers](https://github.com/protocolbuffers/protobuf), except faster.
In fact, in benchmarks, Cap'n Proto is INFINITY TIMES faster than Protocol Buffers.

This benchmark is, of course, unfair. It is only measuring the time to encode and decode a message
in memory. Cap'n Proto gets a perfect score because _there is no encoding/decoding step_. The Cap'n
Proto encoding is appropriate both as a data interchange format and an in-memory representation, so
once your structure is built, you can simply write the bytes straight out to disk!

**_But doesn't that mean the encoding is platform-specific?_**

NO! The encoding is defined byte-for-byte independent of any platform. However, it is designed to
be efficiently manipulated on common modern CPUs. Data is arranged like a compiler would arrange a
struct -- with fixed widths, fixed offsets, and proper alignment. Variable-sized elements are
embedded as pointers. Pointers are offset-based rather than absolute so that messages are
position-independent. Integers use little-endian byte order because most CPUs are little-endian,
and even big-endian CPUs usually have instructions for reading little-endian data.

**_Doesn't that make backwards-compatibility hard?_**

Not at all! New fields are always added to the end of a struct (or replace padding space), so
existing field positions are unchanged. The recipient simply needs to do a bounds check when
reading each field. Fields are numbered in the order in which they were added, so Cap'n Proto
always knows how to arrange them for backwards-compatibility.

**_Won't fixed-width integers, unset optional fields, and padding waste space on the wire?_**

Yes. However, since all these extra bytes are zeros, when bandwidth matters, we can apply an
extremely fast Cap'n-Proto-specific compression scheme to remove them. Cap'n Proto calls this
"packing" the message; it achieves similar (better, even) message sizes to protobuf encoding, and
it's still faster.

When bandwidth really matters, you should apply general-purpose compression, like
[zlib](http://www.zlib.net/) or [LZ4](https://github.com/Cyan4973/lz4), regardless of your
encoding format.

**_Isn't this all horribly insecure?_**

No no no! To be clear, we're NOT just casting a buffer pointer to a struct pointer and calling it a day.

Cap'n Proto generates classes with accessor methods that you use to traverse the message. These accessors validate pointers before following them. If a pointer is invalid (e.g. out-of-bounds), the library can throw an exception or simply replace the value with a default / empty object (your choice).

Thus, Cap'n Proto checks the structural integrity of the message just like any other serialization protocol would. And, just like any other protocol, it is up to the app to check the validity of the content.

Cap'n Proto was built to be used in [Sandstorm.io](https://sandstorm.io), and is now heavily used in [Cloudflare Workers](https://workers.dev), two environments where security is a major concern. Cap'n Proto has undergone fuzzing and expert security review. Our response to security issues was once described by security guru Ben Laurie as ["the most awesome response I've ever had."](https://twitter.com/BenLaurie/status/575079375307153409) (Please report all security issues to [kenton@cloudflare.com](mailto:kenton@cloudflare.com).)

**_Are there other advantages?_**

Glad you asked!

* **Incremental reads:** It is easy to start processing a Cap'n Proto message before you have
  received all of it since outer objects appear entirely before inner objects (as opposed to most
  encodings, where outer objects encompass inner objects).
* **Random access:** You can read just one field of a message without parsing the whole thing.
* **mmap:** Read a large Cap'n Proto file by memory-mapping it. The OS won't even read in the
  parts that you don't access.
* **Inter-language communication:** Calling C++ code from, say, Java or Python tends to be painful
  or slow. With Cap'n Proto, the two languages can easily operate on the same in-memory data
  structure.
* **Inter-process communication:** Multiple processes running on the same machine can share a
  Cap'n Proto message via shared memory. No need to pipe data through the kernel. Calling another
  process can be just as fast and easy as calling another thread.
* **Arena allocation:** Manipulating Protobuf objects tends to be bogged down by memory
  allocation, unless you are very careful about object reuse. Cap'n Proto objects are always
  allocated in an "arena" or "region" style, which is faster and promotes cache locality.
* **Tiny generated code:** Protobuf generates dedicated parsing and serialization code for every
  message type, and this code tends to be enormous. Cap'n Proto generated code is smaller by an
  order of magnitude or more.  In fact, usually it's no more than some inline accessor methods!
* **Tiny runtime library:** Due to the simplicity of the Cap'n Proto format, the runtime library
  can be much smaller.
* **Time-traveling RPC:** Cap'n Proto features an RPC system that implements [time travel](rpc.html)
  such that call results are returned to the client before the request even arrives at the server!

<a href="rpc.html"><img src='images/time-travel.png' style='max-width:639px'></a>


**_Why do you pick on Protocol Buffers so much?_**

Because it's easy to pick on myself. :) I, Kenton Varda, was the primary author of Protocol Buffers
version 2, which is the version that Google released open source. Cap'n Proto is the result of
years of experience working on Protobufs, listening to user feedback, and thinking about how
things could be done better.

Note that I no longer work for Google. Cap'n Proto is not, and never has been, affiliated with Google.

**_OK, how do I get started?_**

To install Cap'n Proto, head over to the [installation page](install.html).  If you'd like to help
hack on Cap'n Proto, such as by writing bindings in other languages, let us know on the
[discussion group](https://groups.google.com/group/capnproto).  If you'd like to receive e-mail
updates about future releases, add yourself to the
[announcement list](https://groups.google.com/group/capnproto-announce).

{% include buttons.html %}



doc/install.md
--------------------------------------
---
layout: page
title: Installation
---

# Installation: Tools and C++ Runtime

<div style="float: right"><a class="groups_link" style="color: #fff"
href="https://groups.google.com/group/capnproto-announce">Get Notified of Updates</a></div>

The Cap'n Proto tools, including the compiler (which takes `.capnp` files and generates source code
for them), are written in C++.  Therefore, you must install the C++ package even if your actual
development language is something else.

This package is licensed under the [MIT License](http://opensource.org/licenses/MIT).

## Prerequisites

### Supported Compilers

Cap'n Proto makes extensive use of C++14 language features. As a result, it requires a relatively
new version of a well-supported compiler. The minimum versions are:

* GCC 7.0
* Clang 6.0
* Visual C++ 2019

If your system's default compiler is older that the above, you will need to install a newer
compiler and set the `CXX` environment variable before trying to build Cap'n Proto. For example,
after installing GCC 7, you could set `CXX=g++-7` to use this compiler.

### Supported Operating Systems

In theory, Cap'n Proto should work on any POSIX platform supporting one of the above compilers,
as well as on Windows. We test every Cap'n Proto release on the following platforms:

* Android
* Linux
* Mac OS X
* Windows - MinGW-w64
* Windows - Visual C++

**Windows users:** Cap'n Proto requires Visual Studio 2019 or newer. All features
of Cap'n Proto -- including serialization, dynamic API, RPC, and schema parser -- are now supported.

**Mac OS X users:** You should use the latest Xcode with the Xcode command-line
tools (Xcode menu > Preferences > Downloads).  Alternatively, the command-line tools
package from [Apple](https://developer.apple.com/downloads/) or compiler builds from
[Macports](http://www.macports.org/), [Fink](http://www.finkproject.org/), or
[Homebrew](http://brew.sh/) are reported to work.

## Installation: Unix

**From Release Tarball**

You may download and install the release version of Cap'n Proto like so:

<pre><code>curl -O <a href="https://capnproto.org/capnproto-c++-1.0.2.tar.gz">https://capnproto.org/capnproto-c++-1.0.2.tar.gz</a>
tar zxf capnproto-c++-1.0.2.tar.gz
cd capnproto-c++-1.0.2
./configure
make -j6 check
sudo make install</code></pre>

This will install `capnp`, the Cap'n Proto command-line tool.  It will also install `libcapnp`,
`libcapnpc`, and `libkj` in `/usr/local/lib` and headers in `/usr/local/include/capnp` and
`/usr/local/include/kj`.

**From Package Managers**

Some package managers include Cap'n Proto packages.

Note: These packages are not maintained by us and are sometimes not up to date with the latest Cap'n Proto release.

* Debian / Ubuntu: `apt-get install capnproto`
* Arch Linux: `sudo pacman -S capnproto`
* Homebrew (OSX): `brew install capnp`

**From Git**

If you download directly from Git, you will need to have the GNU autotools --
[autoconf](http://www.gnu.org/software/autoconf/),
[automake](http://www.gnu.org/software/automake/), and
[libtool](http://www.gnu.org/software/libtool/) -- installed.

    git clone -b master https://github.com/capnproto/capnproto.git
    cd capnproto/c++
    autoreconf -i
    ./configure
    make -j6 check
    sudo make install

## Installation: Windows

**From Release Zip**

1. Download Cap'n Proto Win32 build:

   <pre><a href="https://capnproto.org/capnproto-c++-win32-1.0.2.zip">https://capnproto.org/capnproto-c++-win32-1.0.2.zip</a></pre>

2. Find `capnp.exe`, `capnpc-c++.exe`, and `capnpc-capnp.exe` under `capnproto-tools-win32-1.0.2` in
   the zip and copy them somewhere.

3. If your `.capnp` files will import any of the `.capnp` files provided by the core project, or
   if you use the `stream` keyword (which implicitly imports `capnp/stream.capnp`), then you need
   to put those files somewhere where the capnp compiler can find them. To do this, copy the
   directory `capnproto-c++-1.0.2/src` to the location of your choice, then make sure to pass the
   flag `-I <that location>` to `capnp` when you run it.

If you don't care about C++ support, you can stop here. The compiler exe can be used with plugins
provided by projects implementing Cap'n Proto in other languages.

If you want to use Cap'n Proto in C++ with Visual Studio, do the following:

1. Make sure that you are using Visual Studio 2019 or newer, with all updates installed. Cap'n
   Proto uses C++14 language features that did not work in previous versions of Visual Studio,
   and the updates include many bug fixes that Cap'n Proto requires.

2. Install [CMake](http://www.cmake.org/) version 3.16 or later.

3. Use CMake to generate Visual Studio project files under `capnproto-c++-1.0.2` in the zip file.
   You can use the CMake UI for this or run this shell command:

       cmake -G "Visual Studio 16 2019"

3. Open the "Cap'n Proto" solution in Visual Studio.

4. Adjust the project build options (e.g., choice of C++ runtime library, enable/disable exceptions
   and RTTI) to match the options of the project in which you plan to use Cap'n Proto.

5. Build the solution (`ALL_BUILD`).

6. Build the `INSTALL` project to copy the compiled libraries, tools, and header files into
   `CMAKE_INSTALL_PREFIX`.

   Alternatively, find the compiled `.lib` files in the build directory under
   `src/{capnp,kj}/{Debug,Release}` and place them somewhere where your project can link against them.
   Also add the `src` directory to your search path for `#include`s, or copy all the headers to your
   project's include directory.

Cap'n Proto can also be built with MinGW or Cygwin, using the Unix/autotools build instructions.

**From Git**

The C++ sources are located under `c++` directory in the git repository. The build instructions are
otherwise the same as for the release zip.




doc/language.md
--------------------------------------
---
layout: page
title: Schema Language
---

# Schema Language

Like Protocol Buffers and Thrift (but unlike JSON or MessagePack), Cap'n Proto messages are
strongly-typed and not self-describing. You must define your message structure in a special
language, then invoke the Cap'n Proto compiler (`capnp compile`) to generate source code to
manipulate that message type in your desired language.

For example:

{% highlight capnp %}
@0xdbb9ad1f14bf0b36;  # unique file ID, generated by `capnp id`

struct Person {
  name @0 :Text;
  birthdate @3 :Date;

  email @1 :Text;
  phones @2 :List(PhoneNumber);

  struct PhoneNumber {
    number @0 :Text;
    type @1 :Type;

    enum Type {
      mobile @0;
      home @1;
      work @2;
    }
  }
}

struct Date {
  year @0 :Int16;
  month @1 :UInt8;
  day @2 :UInt8;
}
{% endhighlight %}

Some notes:

* Types come after names. The name is by far the most important thing to see, especially when
  quickly skimming, so we put it up front where it is most visible.  Sorry, C got it wrong.
* The `@N` annotations show how the protocol evolved over time, so that the system can make sure
  to maintain compatibility with older versions. Fields (and enumerants, and interface methods)
  must be numbered consecutively starting from zero in the order in which they were added. In this
  example, it looks like the `birthdate` field was added to the `Person` structure recently -- its
  number is higher than the `email` and `phones` fields. Unlike Protobufs, you cannot skip numbers
  when defining fields -- but there was never any reason to do so anyway.

## Language Reference

### Comments

Comments are indicated by hash signs and extend to the end of the line:

{% highlight capnp %}
# This is a comment.
{% endhighlight %}

Comments meant as documentation should appear _after_ the declaration, either on the same line, or
on a subsequent line. Doc comments for aggregate definitions should appear on the line after the
opening brace.

{% highlight capnp %}
struct Date {
  # A standard Gregorian calendar date.

  year @0 :Int16;
  # The year.  Must include the century.
  # Negative value indicates BC.

  month @1 :UInt8;   # Month number, 1-12.
  day @2 :UInt8;     # Day number, 1-30.
}
{% endhighlight %}

Placing the comment _after_ the declaration rather than before makes the code more readable,
especially when doc comments grow long. You almost always need to see the declaration before you
can start reading the comment.

### Built-in Types

The following types are automatically defined:

* **Void:** `Void`
* **Boolean:** `Bool`
* **Integers:** `Int8`, `Int16`, `Int32`, `Int64`
* **Unsigned integers:** `UInt8`, `UInt16`, `UInt32`, `UInt64`
* **Floating-point:** `Float32`, `Float64`
* **Blobs:** `Text`, `Data`
* **Lists:** `List(T)`

Notes:

* The `Void` type has exactly one possible value, and thus can be encoded in zero bits. It is
  rarely used, but can be useful as a union member.
* `Text` is always UTF-8 encoded and NUL-terminated.
* `Data` is a completely arbitrary sequence of bytes.
* `List` is a parameterized type, where the parameter is the element type. For example,
  `List(Int32)`, `List(Person)`, and `List(List(Text))` are all valid.

### Structs

A struct has a set of named, typed fields, numbered consecutively starting from zero.

{% highlight capnp %}
struct Person {
  name @0 :Text;
  email @1 :Text;
}
{% endhighlight %}

Fields can have default values:

{% highlight capnp %}
foo @0 :Int32 = 123;
bar @1 :Text = "blah";
baz @2 :List(Bool) = [ true, false, false, true ];
qux @3 :Person = (name = "Bob", email = "bob@example.com");
corge @4 :Void = void;
grault @5 :Data = 0x"a1 40 33";
{% endhighlight %}

### Unions

A union is two or more fields of a struct which are stored in the same location. Only one of
these fields can be set at a time, and a separate tag is maintained to track which one is
currently set. Unlike in C, unions are not types, they are simply properties of fields, therefore
union declarations do not look like types.

{% highlight capnp %}
struct Person {
  # ...

  employment :union {
    # We assume that a person is only one of these.
    unemployed @4 :Void;
    employer @5 :Company;
    school @6 :School;
    selfEmployed @7 :Void;
  }
}
{% endhighlight %}

Additionally, unions can be unnamed.  Each struct can contain no more than one unnamed union.  Use
unnamed unions in cases where you would struggle to think of an appropriate name for the union,
because the union represents the main body of the struct.

{% highlight capnp %}
struct Shape {
  area @0 :Float64;

  union {
    circle @1 :Float64;      # radius
    square @2 :Float64;      # width
  }
}
{% endhighlight %}

Notes:

* Unions members are numbered in the same number space as fields of the containing struct.
  Remember that the purpose of the numbers is to indicate the evolution order of the
  struct. The system needs to know when the union fields were declared relative to the non-union
  fields.

* Notice that we used the "useless" `Void` type here. We don't have any extra information to store
  for the `unemployed` or `selfEmployed` cases, but we still want the union to distinguish these
  states from others.

* By default, when a struct is initialized, the lowest-numbered field in the union is "set".  If
  you do not want any field set by default, simply declare a field called "unset" and make it the
  lowest-numbered field.

* You can move an existing field into a new union without breaking compatibility with existing
  data, as long as all of the other fields in the union are new.  Since the existing field is
  necessarily the lowest-numbered in the union, it will be the union's default field.

**Wait, why aren't unions first-class types?**

Requiring unions to be declared inside a struct, rather than living as free-standing types, has
some important advantages:

* If unions were first-class types, then union members would clearly have to be numbered separately
  from the containing type's fields.  This means that the compiler, when deciding how to position
  the union in its containing struct, would have to conservatively assume that any kind of new
  field might be added to the union in the future.  To support this, all unions would have to
  be allocated as separate objects embedded by pointer, wasting space.

* A free-standing union would be a liability for protocol evolution, because no additional data
  can be attached to it later on.  Consider, for example, a type which represents a parser token.
  This type is naturally a union: it may be a keyword, identifier, numeric literal, quoted string,
  etc.  So the author defines it as a union, and the type is used widely.  Later on, the developer
  wants to attach information to the token indicating its line and column number in the source
  file.  Unfortunately, this is impossible without updating all users of the type, because the new
  information ought to apply to _all_ token instances, not just specific members of the union.  On
  the other hand, if unions must be embedded within structs, it is always possible to add new
  fields to the struct later on.

* When evolving a protocol it is common to discover that some existing field really should have
  been enclosed in a union, because new fields being added are mutually exclusive with it.  With
  Cap'n Proto's unions, it is actually possible to "retroactively unionize" such a field without
  changing its layout.  This allows you to continue being able to read old data without wasting
  space when writing new data.  This is only possible when unions are declared within their
  containing struct.

Cap'n Proto's unconventional approach to unions provides these advantages without any real down
side:  where you would conventionally define a free-standing union type, in Cap'n Proto you
may simply define a struct type that contains only that union (probably unnamed), and you have
achieved the same effect.  Thus, aside from being slightly unintuitive, it is strictly superior.

### Groups

A group is a set of fields that are encapsulated in their own scope.

{% highlight capnp %}
struct Person {
  # ...

  address :group {
    # Note:  This is a terrible way to use groups, and meant
    #        only to demonstrate the syntax.
    houseNumber @8 :UInt32;
    street @9 :Text;
    city @10 :Text;
    country @11 :Text;
  }
}
{% endhighlight %}

Interface-wise, the above group behaves as if you had defined a nested struct called `Address` and
then a field `address :Address`.  However, a group is _not_ a separate object from its containing
struct: the fields are numbered in the same space as the containing struct's fields, and are laid
out exactly the same as if they hadn't been grouped at all.  Essentially, a group is just a
namespace.

Groups on their own (as in the above example) are useless, almost as much so as the `Void` type.
They become interesting when used together with unions.

{% highlight capnp %}
struct Shape {
  area @0 :Float64;

  union {
    circle :group {
      radius @1 :Float64;
    }
    rectangle :group {
      width @2 :Float64;
      height @3 :Float64;
    }
  }
}
{% endhighlight %}

There are two main reason to use groups with unions:

1. They are often more self-documenting.  Notice that `radius` is now a member of `circle`, so
   we don't need a comment to explain that the value of `circle` is its radius.
2. You can add additional members later on, without breaking compatibility.  Notice how we upgraded
   `square` to `rectangle` above, adding a `height` field.  This definition is actually
   wire-compatible with the previous version of the `Shape` example from the "union" section
   (aside from the fact that `height` will always be zero when reading old data -- hey, it's not
   a perfect example).  In real-world use, it is common to realize after the fact that you need to
   add some information to a struct that only applies when one particular union field is set.
   Without the ability to upgrade to a group, you would have to define the new field separately,
   and have it waste space when not relevant.

Note that a named union is actually exactly equivalent to a named group containing an unnamed
union.

**Wait, weren't groups considered a misfeature in Protobufs?  Why did you do this again?**

They are useful in unions, which Protobufs did not have.  Meanwhile, you cannot have a "repeated
group" in Cap'n Proto, which was the case that got into the most trouble with Protobufs.

### Dynamically-typed Fields

A struct may have a field with type `AnyPointer`.  This field's value can be of any pointer type --
i.e. any struct, interface, list, or blob.  This is essentially like a `void*` in C.

See also [generics](#generic-types).

### Enums

An enum is a type with a small finite set of symbolic values.

{% highlight capnp %}
enum Rfc3092Variable {
  foo @0;
  bar @1;
  baz @2;
  qux @3;
  # ...
}
{% endhighlight %}

Like fields, enumerants must be numbered sequentially starting from zero. In languages where
enums have numeric values, these numbers will be used, but in general Cap'n Proto enums should not
be considered numeric.

### Interfaces

An interface has a collection of methods, each of which takes some parameters and return some
results.  Like struct fields, methods are numbered.  Interfaces support inheritance, including
multiple inheritance.

{% highlight capnp %}
interface Node {
  isDirectory @0 () -> (result :Bool);
}

interface Directory extends(Node) {
  list @0 () -> (list :List(Entry));
  struct Entry {
    name @0 :Text;
    node @1 :Node;
  }

  create @1 (name :Text) -> (file :File);
  mkdir @2 (name :Text) -> (directory :Directory);
  open @3 (name :Text) -> (node :Node);
  delete @4 (name :Text);
  link @5 (name :Text, node :Node);
}

interface File extends(Node) {
  size @0 () -> (size :UInt64);
  read @1 (startAt :UInt64 = 0, amount :UInt64 = 0xffffffffffffffff)
       -> (data :Data);
  # Default params = read entire file.

  write @2 (startAt :UInt64, data :Data);
  truncate @3 (size :UInt64);
}
{% endhighlight %}

Notice something interesting here: `Node`, `Directory`, and `File` are interfaces, but several
methods take these types as parameters or return them as results.  `Directory.Entry` is a struct,
but it contains a `Node`, which is an interface.  Structs (and primitive types) are passed over RPC
by value, but interfaces are passed by reference. So when `Directory.list` is called remotely, the
content of a `List(Entry)` (including the text of each `name`) is transmitted back, but for the
`node` field, only a reference to some remote `Node` object is sent.

When an address of an object is transmitted, the RPC system automatically manages making sure that
the recipient gets permission to call the addressed object -- because if the recipient wasn't
meant to have access, the sender shouldn't have sent the reference in the first place. This makes
it very easy to develop secure protocols with Cap'n Proto -- you almost don't need to think about
access control at all. This feature is what makes Cap'n Proto a "capability-based" RPC system -- a
reference to an object inherently represents a "capability" to access it.

### Generic Types

A struct or interface type may be parameterized, making it "generic". For example, this is useful
for defining type-safe containers:

{% highlight capnp %}
struct Map(Key, Value) {
  entries @0 :List(Entry);
  struct Entry {
    key @0 :Key;
    value @1 :Value;
  }
}

struct People {
  byName @0 :Map(Text, Person);
  # Maps names to Person instances.
}
{% endhighlight %}

Cap'n Proto generics work very similarly to Java generics or C++ templates. Some notes:

* Only pointer types (structs, lists, blobs, and interfaces) can be used as generic parameters,
  much like in Java. This is a pragmatic limitation: allowing parameters to have non-pointer types
  would mean that different parameterizations of a struct could have completely different layouts,
  which would excessively complicate the Cap'n Proto implementation.

* A type declaration nested inside a generic type may use the type parameters of the outer type,
  as you can see in the example above. This differs from Java, but matches C++. If you want to
  refer to a nested type from outside the outer type, you must specify the parameters on the outer
  type, not the inner. For example, `Map(Text, Person).Entry` is a valid type;
  `Map.Entry(Text, Person)` is NOT valid. (Of course, an inner type may declare additional generic
  parameters.)

* If you refer to a generic type but omit its parameters (e.g. declare a field of type `Map` rather
  than `Map(T, U)`), it is as if you specified `AnyPointer` for each parameter. Note that such
  a type is wire-compatible with any specific parameterization, so long as you interpret the
  `AnyPointer`s as the correct type at runtime.

* Relatedly, it is safe to cast a generic interface of a specific parameterization to a generic
  interface where all parameters are `AnyPointer` and vice versa, as long as the `AnyPointer`s are
  treated as the correct type at runtime. This means that e.g. you can implement a server in a
  generic way that is correct for all parameterizations but call it from clients using a specific
  parameterization.

* The encoding of a generic type is exactly the same as the encoding of a type produced by
  substituting the type parameters manually. For example, `Map(Text, Person)` is encoded exactly
  the same as:

  <figure class="highlight"><pre><code class="language-capnp" data-lang="capnp"><span></span><span class="k">struct</span> <span class="n">PersonMap</span> {
    <span class="c1"># Encoded the same as Map(Text, Person).</span>
    <span class="n">entries</span> <span class="nd">@0</span> <span class="nc">:List(Entry)</span>;
    <span class="k">struct</span> <span class="n">Entry</span> {
      <span class="n">key</span> <span class="nd">@0</span> <span class="nc">:Text</span>;
      <span class="n">value</span> <span class="nd">@1</span> <span class="nc">:Person</span>;
    }
  }</code></pre></figure>

  {% comment %}
  Highlighter manually invoked because of: https://github.com/jekyll/jekyll/issues/588
  Original code was:
    struct PersonMap {
      # Encoded the same as Map(Text, Person).
      entries @0 :List(Entry);
      struct Entry {
        key @0 :Text;
        value @1 :Person;
      }
    }
  {% endcomment %}

  Therefore, it is possible to upgrade non-generic types to generic types while retaining
  backwards-compatibility.

* Similarly, a generic interface's protocol is exactly the same as the interface obtained by
  manually substituting the generic parameters.

### Generic Methods

Interface methods may also have "implicit" generic parameters that apply to a particular method
call. This commonly applies to "factory" methods. For example:

{% highlight capnp %}
interface Assignable(T) {
  # A generic interface, with non-generic methods.
  get @0 () -> (value :T);
  set @1 (value :T) -> ();
}

interface AssignableFactory {
  newAssignable @0 [T] (initialValue :T)
      -> (assignable :Assignable(T));
  # A generic method.
}
{% endhighlight %}

Here, the method `newAssignable()` is generic. The return type of the method depends on the input
type.

Ideally, calls to a generic method should not have to explicitly specify the method's type
parameters, because they should be inferred from the types of the method's regular parameters.
However, this may not always be possible; it depends on the programming language and API details.

Note that if a method's generic parameter is used only in its returns, not its parameters, then
this implies that the returned value is appropriate for any parameterization. For example:

{% highlight capnp %}
newUnsetAssignable @1 [T] () -> (assignable :Assignable(T));
# Create a new assignable. `get()` on the returned object will
# throw an exception until `set()` has been called at least once.
{% endhighlight %}

Because of the way this method is designed, the returned `Assignable` is initially valid for any
`T`. Effectively, it doesn't take on a type until the first time `set()` is called, and then `T`
retroactively becomes the type of value passed to `set()`.

In contrast, if it's the case that the returned type is unknown, then you should NOT declare it
as generic. Instead, use `AnyPointer`, or omit a type's parameters (since they default to
`AnyPointer`). For example:

{% highlight capnp %}
getNamedAssignable @2 (name :Text) -> (assignable :Assignable);
# Get the `Assignable` with the given name. It is the
# responsibility of the caller to keep track of the type of each
# named `Assignable` and cast the returned object appropriately.
{% endhighlight %}

Here, we omitted the parameters to `Assignable` in the return type, because the returned object
has a specific type parameterization but it is not locally knowable.

### Constants

You can define constants in Cap'n Proto.  These don't affect what is sent on the wire, but they
will be included in the generated code, and can be [evaluated using the `capnp`
tool](capnp-tool.html#evaluating-constants).

{% highlight capnp %}
const pi :Float32 = 3.14159;
const bob :Person = (name = "Bob", email = "bob@example.com");
const secret :Data = 0x"9f98739c2b53835e 6720a00907abd42f";
{% endhighlight %}

Additionally, you may refer to a constant inside another value (e.g. another constant, or a default
value of a field).

{% highlight capnp %}
const foo :Int32 = 123;
const bar :Text = "Hello";
const baz :SomeStruct = (id = .foo, message = .bar);
{% endhighlight %}

Note that when substituting a constant into another value, the constant's name must be qualified
with its scope.  E.g. if a constant `qux` is declared nested in a type `Corge`, it would need to
be referenced as `Corge.qux` rather than just `qux`, even when used within the `Corge` scope.
Constants declared at the top-level scope are prefixed just with `.`.  This rule helps to make it
clear that the name refers to a user-defined constant, rather than a literal value (like `true` or
`inf`) or an enum value.

### Nesting, Scope, and Aliases

You can nest constant, alias, and type definitions inside structs and interfaces (but not enums).
This has no effect on any definition involved except to define the scope of its name. So in Java
terms, inner classes are always "static". To name a nested type from another scope, separate the
path with `.`s.

{% highlight capnp %}
struct Foo {
  struct Bar {
    #...
  }
  bar @0 :Bar;
}

struct Baz {
  bar @0 :Foo.Bar;
}
{% endhighlight %}

If typing long scopes becomes cumbersome, you can use `using` to declare an alias.

{% highlight capnp %}
struct Qux {
  using Foo.Bar;
  bar @0 :Bar;
}

struct Corge {
  using T = Foo.Bar;
  bar @0 :T;
}
{% endhighlight %}

### Imports

An `import` expression names the scope of some other file:

{% highlight capnp %}
struct Foo {
  baz @0 :import "bar.capnp".Baz;
  # Use type "Baz" defined in bar.capnp.
}
{% endhighlight %}

Of course, typically it's more readable to define an alias:

{% highlight capnp %}
using Bar = import "bar.capnp";

struct Foo {
  baz @0 :Bar.Baz;
  # Use type "Baz" defined in bar.capnp.
}
{% endhighlight %}

Or even:

{% highlight capnp %}
using import "bar.capnp".Baz;

struct Foo {
  baz @0 :Baz;
  # Use type "Baz" defined in bar.capnp.
}
{% endhighlight %}

The above imports specify relative paths.  If the path begins with a `/`, it is absolute -- in
this case, the `capnp` tool searches for the file in each of the search path directories specified
with `-I`, appending the path you specify to the path given to the `-I` flag. So, for example,
if you ran `capnp` with `-Ifoo/bar`, and the import statement is `import "/baz/qux.capnp"`, then
the compiler would open the file `foo/bar/baz/qux.capnp`.

### Annotations

Sometimes you want to attach extra information to parts of your protocol that isn't part of the
Cap'n Proto language.  This information might control details of a particular code generator, or
you might even read it at run time to assist in some kind of dynamic message processing.  For
example, you might create a field annotation which means "hide from the public", and when you send
a message to an external user, you might invoke some code first that iterates over your message and
removes all of these hidden fields.

You may declare annotations and use them like so:

{% highlight capnp %}
annotation foo(struct, enum) :Text;
# Declare an annotation 'foo' which applies to struct and enum types.

struct MyType $foo("bar") {
  # Apply 'foo' to to MyType.

  # ...
}
{% endhighlight %}

The possible targets for an annotation are: `file`, `struct`, `field`, `union`, `group`, `enum`,
`enumerant`, `interface`, `method`, `param`, `annotation`, `const`.
You may also specify `*` to cover them all.

{% highlight capnp %}
annotation baz(*) :Int32;
# 'baz' can annotate anything!

$baz(1);  # Annotate the file.

struct MyStruct $baz(2) {
  myField @0 :Text = "default" $baz(3);
  myUnion :union $baz(4) {
    # ...
  }
}

enum MyEnum $baz(5) {
  myEnumerant @0 $baz(6);
}

interface MyInterface $baz(7) {
  myMethod @0 (myParam :Text $baz(9)) -> () $baz(8);
}

annotation myAnnotation(struct) :Int32 $baz(10);
const myConst :Int32 = 123 $baz(11);
{% endhighlight %}

`Void` annotations can omit the value.  Struct-typed annotations are also allowed.  Tip:  If
you want an annotation to have a default value, declare it as a struct with a single field with
a default value.

{% highlight capnp %}
annotation qux(struct, field) :Void;

struct MyStruct $qux {
  string @0 :Text $qux;
  number @1 :Int32 $qux;
}

annotation corge(file) :MyStruct;

$corge(string = "hello", number = 123);

struct Grault {
  value @0 :Int32 = 123;
}

annotation grault(file) :Grault;

$grault();  # value defaults to 123
$grault(value = 456);
{% endhighlight %}

### Unique IDs

A Cap'n Proto file must have a unique 64-bit ID, and each type and annotation defined therein may
also have an ID.  Use `capnp id` to generate a new ID randomly.  ID specifications begin with `@`:

{% highlight capnp %}
@0xdbb9ad1f14bf0b36;
# file ID

struct Foo @0x8db435604d0d3723 {
  # ...
}

enum Bar @0xb400f69b5334aab3 {
  # ...
}

interface Baz @0xf7141baba3c12691 {
  # ...
}

annotation qux @0xf8a1bedf44c89f00 (field) :Text;
{% endhighlight %}

If you omit the ID for a type or annotation, one will be assigned automatically.  This default
ID is derived by taking the first 8 bytes of the MD5 hash of the parent scope's ID concatenated
with the declaration's name (where the "parent scope" is the file for top-level declarations, or
the outer type for nested declarations).  You can see the automatically-generated IDs by "compiling"
your file with the `-ocapnp` flag, which echos the schema back to the terminal annotated with
extra information, e.g. `capnp compile -ocapnp myschema.capnp`.  In general, you would only specify
an explicit ID for a declaration if that declaration has been renamed or moved and you want the ID
to stay the same for backwards-compatibility.

IDs exist to provide a relatively short yet unambiguous way to refer to a type or annotation from
another context.  They may be used for representing schemas, for tagging dynamically-typed fields,
etc.  Most languages prefer instead to define a symbolic global namespace e.g. full of "packages",
but this would have some important disadvantages in the context of Cap'n Proto:

* Programmers often feel the need to change symbolic names and organization in order to make their
  code cleaner, but the renamed code should still work with existing encoded data.
* It's easy for symbolic names to collide, and these collisions could be hard to detect in a large
  distributed system with many different binaries using different versions of protocols.
* Fully-qualified type names may be large and waste space when transmitted on the wire.

Note that IDs are 64-bit (actually, 63-bit, as the first bit is always 1).  Random collisions
are possible, but unlikely -- there would have to be on the order of a billion types before this
becomes a real concern.  Collisions from misuse (e.g. copying an example without changing the ID)
are much more likely.

## Evolving Your Protocol

A protocol can be changed in the following ways without breaking backwards-compatibility, and
without changing the [canonical](encoding.html#canonicalization) encoding of a message:

* New types, constants, and aliases can be added anywhere, since they obviously don't affect the
  encoding of any existing type.

* New fields, enumerants, and methods may be added to structs, enums, and interfaces, respectively,
  as long as each new member's number is larger than all previous members.  Similarly, new fields
  may be added to existing groups and unions.

* New parameters may be added to a method.  The new parameters must be added to the end of the
  parameter list and must have default values.

* Members can be re-arranged in the source code, so long as their numbers stay the same.

* Any symbolic name can be changed, as long as the type ID / ordinal numbers stay the same.  Note
  that type declarations have an implicit ID generated based on their name and parent's ID, but
  you can use `capnp compile -ocapnp myschema.capnp` to find out what that number is, and then
  declare it explicitly after your rename.

* Type definitions can be moved to different scopes, as long as the type ID is declared
  explicitly.

* A field can be moved into a group or a union, as long as the group/union and all other fields
  within it are new.  In other words, a field can be replaced with a group or union containing an
  equivalent field and some new fields.  Note that when creating a union this way, this particular
  change is not fully forwards-compatible: if you create a message where one of the union's new
  fields are set, and the message is read by an old program that dosen't know about the union, then
  it may expect the original field to be present, and if it tries to read that field, may see a
  garbage value or throw an exception. To avoid this problem, make sure to only use the new union
  members when talking to programs that know about the union. This caveat only applies when moving
  an existing field into a new union; adding new fields to an existing union does not create a
  problem, because existing programs should already know to check the union's tag (although they
  may or may not behave reasonably when the tag has a value they don't recognize).

* A non-generic type can be made [generic](#generic-types), and new generic parameters may be
  added to an existing generic type. Other types used inside the body of the newly-generic type can
  be replaced with the new generic parameter so long as all existing users of the type are updated
  to bind that generic parameter to the type it replaced. For example:

  <figure class="highlight"><pre><code class="language-capnp" data-lang="capnp"><span></span><span class="k">struct</span> <span class="n">Map</span> {
    <span class="n">entries</span> <span class="nd">@0</span> <span class="nc">:List(Entry)</span>;
    <span class="k">struct</span> <span class="n">Entry</span> {
      <span class="n">key</span> <span class="nd">@0</span> <span class="nc">:Text</span>;
      <span class="n">value</span> <span class="nd">@1</span> <span class="nc">:Text</span>;
    }
  }</code></pre></figure>

  {% comment %}
  Highlighter manually invoked because of: https://github.com/jekyll/jekyll/issues/588
  Original code was:
    struct Map {
      entries @0 :List(Entry);
      struct Entry {
        key @0 :Text;
        value @1 :Text;
      }
    }
  {% endcomment %}

  Can change to:

  <figure class="highlight"><pre><code class="language-capnp" data-lang="capnp"><span></span><span class="k">struct</span> <span class="n">Map</span>(<span class="n">Key</span>, <span class="n">Value</span>) {
    <span class="n">entries</span> <span class="nd">@0</span> <span class="nc">:List(Entry)</span>;
    <span class="k">struct</span> <span class="n">Entry</span> {
      <span class="n">key</span> <span class="nd">@0</span> <span class="nc">:Key</span>;
      <span class="n">value</span> <span class="nd">@1</span> <span class="nc">:Value</span>;
    }
  }</code></pre></figure>

  {% comment %}
  Highlighter manually invoked because of: https://github.com/jekyll/jekyll/issues/588
  Original code was:
    struct Map(Key, Value) {
      entries @0 :List(Entry);
      struct Entry {
        key @0 :Key;
        value @1 :Value;
      }
    }
  {% endcomment %}

  As long as all existing uses of `Map` are replaced with `Map(Text, Text)` (and any uses of
  `Map.Entry` are replaced with `Map(Text, Text).Entry`).

  (This rule applies analogously to generic methods.)

The following changes are backwards-compatible but may change the canonical encoding of a message.
Apps that rely on canonicalization (such as some cryptographic protocols) should avoid changes in
this list, but most apps can safely use them:

* A field of type `List(T)`, where `T` is a primitive type, blob, or list, may be changed to type
  `List(U)`, where `U` is a struct type whose `@0` field is of type `T`.  This rule is useful when
  you realize too late that you need to attach some extra data to each element of your list.
  Without this rule, you would be stuck defining parallel lists, which are ugly and error-prone.
  As a special exception to this rule, `List(Bool)` may **not** be upgraded to a list of structs,
  because implementing this for bit lists has proven unreasonably expensive.

Any change not listed above should be assumed NOT to be safe.  In particular:

* You cannot change a field, method, or enumerant's number.
* You cannot change a field or method parameter's type or default value.
* You cannot change a type's ID.
* You cannot change the name of a type that doesn't have an explicit ID, as the implicit ID is
  generated based in part on the type name.
* You cannot move a type to a different scope or file unless it has an explicit ID, as the implicit
  ID is based in part on the scope's ID.
* You cannot move an existing field into or out of an existing union, nor can you form a new union
  containing more than one existing field.

Also, these rules only apply to the Cap'n Proto native encoding.  It is sometimes useful to
transcode Cap'n Proto types to other formats, like JSON, which may have different rules (e.g.,
field names cannot change in JSON).



doc/otherlang.md
--------------------------------------
---
layout: page
title: Other Languages
---

# Other Languages

Cap'n Proto's reference implementation is in C++.  Implementations in other languages are
maintained by respective authors and have not been reviewed by me
([@kentonv](https://github.com/kentonv)). Below are the implementations I'm aware
of. Some of these projects are more "ready" than others; please consult each
project's documentation for details.

##### Serialization + RPC

* [C++](cxx.html) by [@kentonv](https://github.com/kentonv)
* [C#](https://github.com/c80k/capnproto-dotnetcore) by [@c80k](https://github.com/c80k)
* [Erlang](http://ecapnp.astekk.se/) by [@kaos](https://github.com/kaos)
* [Go](https://github.com/capnproto/go-capnp) currently maintained by [@zenhack](https://github.com/zenhack) and [@lthibault](https://github.com/lthibault)
* [Haskell](https://github.com/zenhack/haskell-capnp) by [@zenhack](https://github.com/zenhack)
* [JavaScript (Node.js only)](https://github.com/capnproto/node-capnp) by [@kentonv](https://github.com/kentonv)
* [OCaml](https://github.com/capnproto/capnp-ocaml) by [@pelzlpj](https://github.com/pelzlpj) with [RPC](https://github.com/mirage/capnp-rpc) by [@talex5](https://github.com/talex5)
* [Python](http://capnproto.github.io/pycapnp/) by [@jparyani](https://github.com/jparyani)
* [Rust](https://github.com/dwrensha/capnproto-rust) by [@dwrensha](https://github.com/dwrensha)

##### Serialization only

* [C](https://github.com/opensourcerouting/c-capnproto) by [OpenSourceRouting](https://www.opensourcerouting.org/) / [@eqvinox](https://github.com/eqvinox) (originally by [@jmckaskill](https://github.com/jmckaskill)) (no longer maintained)
    * [Forked and maintained](https://gitlab.com/dkml/ext/c-capnproto) by [@jonahbeckford](https://github.com/jonahbeckford)
* [D](https://github.com/capnproto/capnproto-dlang) by [@ThomasBrixLarsen](https://github.com/ThomasBrixLarsen)
* [Java](https://github.com/capnproto/capnproto-java/) by [@dwrensha](https://github.com/dwrensha)
* [JavaScript](https://github.com/capnp-js/plugin/) by [@popham](https://github.com/popham)
* [JavaScript](https://github.com/jscheid/capnproto-js) (older, abandoned) by [@jscheid](https://github.com/jscheid)
* [Lua](https://github.com/cloudflare/lua-capnproto) by [CloudFlare](http://www.cloudflare.com/) / [@calio](https://github.com/calio)
* [Nim](https://github.com/zielmicha/capnp.nim) by [@zielmicha](https://github.com/zielmicha)
* [Ruby](https://github.com/cstrahan/capnp-ruby) by [@cstrahan](https://github.com/cstrahan)
* [Scala](https://github.com/katis/capnp-scala) by [@katis](https://github.com/katis)

##### Tools

These are other misc projects related to Cap'n Proto that are not actually implementations in
new languages.

* [Common Test Framework](https://github.com/kaos/capnp_test) by [@kaos](https://github.com/kaos)
* [Sublime Syntax Highlighting](https://github.com/joshuawarner32/capnproto-sublime) by
  [@joshuawarner32](https://github.com/joshuawarner32)
* [Vim Syntax Highlighting](https://github.com/cstrahan/vim-capnp) by [@cstrahan](https://github.com/cstrahan)
* [Wireshark Dissector Plugin](https://github.com/kaos/wireshark-plugins) by [@kaos](https://github.com/kaos)
* [VS Code Syntax Highlighter](https://marketplace.visualstudio.com/items?itemName=xmonader.vscode-capnp) by [@xmonader](https://github.com/xmonader)
* [IntelliJ Syntax Highlighter](https://github.com/xmonader/sercapnp) by [@xmonader](https://github.com/xmonader)

## Contribute Your Own!

We'd like to support many more languages in the future!

If you'd like to own the implementation of Cap'n Proto in some particular language,
[let us know](https://groups.google.com/group/capnproto)!

**You should e-mail the list _before_ you start hacking.**  We don't bite, and we'll probably have
useful tips that will save you time.  :)

**Do not implement your own schema parser.**  The schema language is more complicated than it
looks, and the algorithm to determine offsets of fields is subtle.  If you reuse the official
parser, you won't risk getting these wrong, and you won't have to spend time keeping your parser
up-to-date.  In fact, you can still write your code generator in any language you want, using
compiler plugins!

### How to Write Compiler Plugins

The Cap'n Proto tool, `capnp`, does not actually know how to generate code.  It only parses schemas,
then hands the parse tree off to another binary -- known as a "plugin" -- which generates the code.
Plugins are independent executables (written in any language) which read a description of the
schema from standard input and then generate the necessary code.  The description is itself a
Cap'n Proto message, defined by
[schema.capnp](https://github.com/capnproto/capnproto/blob/master/c%2B%2B/src/capnp/schema.capnp).
Specifically, the plugin receives a `CodeGeneratorRequest`, using
[standard serialization](encoding.html#serialization-over-a-stream)
(not packed).  (Note that installing the C++ runtime causes schema.capnp to be placed in
`$PREFIX/include/capnp` -- `/usr/local/include/capnp` by default).

Of course, because the input to a plugin is itself in Cap'n Proto format, if you write your
plugin directly in the language you wish to support, you may have a bootstrapping problem:  you
somehow need to generate code for `schema.capnp` before you write your code generator.  Luckily,
because of the simplicity of the Cap'n Proto format, it is generally not too hard to do this by
hand.  Remember that you can use `capnp compile -ocapnp schema.capnp` to get a dump of the sizes
and offsets of all structs and fields defined in the file.

`capnp compile` normally looks for plugins in `$PATH` with the name `capnpc-[language]`, e.g.
`capnpc-c++` or `capnpc-capnp`.  However, if the language name given on the command line contains
a slash character, `capnp` assumes that it is an exact path to the plugin executable, and does not
search `$PATH`.  Examples:

    # Searches $PATH for executable "capnpc-mylang".
    capnp compile -o mylang addressbook.capnp

    # Uses plugin executable "myplugin" from the current directory.
    capnp compile -o ./myplugin addressbook.capnp

If the user specifies an output directory, the compiler will run the plugin with that directory
as the working directory, so you do not need to worry about this.

For examples of plugins, take a look at
[capnpc-capnp](https://github.com/capnproto/capnproto/blob/master/c%2B%2B/src/capnp/compiler/capnpc-capnp.c%2B%2B)
or [capnpc-c++](https://github.com/capnproto/capnproto/blob/master/c%2B%2B/src/capnp/compiler/capnpc-c%2B%2B.c%2B%2B).

### Supporting Dynamic Languages

Dynamic languages have no compile step.  This makes it difficult to work `capnp compile` into the
workflow for such languages.  Additionally, dynamic languages are often scripting languages that do
not support pointer arithmetic or any reasonably-performant alternative.

Fortunately, dynamic languages usually have facilities for calling native code.  The best way to
support Cap'n Proto in a dynamic language, then, is to wrap the C++ library, in particular the
[C++ dynamic API](cxx.html#dynamic-reflection).  This way you get reasonable performance while
still avoiding the need to generate any code specific to each schema.

To parse the schema files, use the `capnp::SchemaParser` class (defined in `capnp/schema-parser.h`).
This way, schemas are loaded at the same time as all the rest of the program's code -- at startup.
An advanced implementation might consider caching the compiled schemas in binary format, then
loading the cached version using `capnp::SchemaLoader`, similar to the way e.g. Python caches
compiled source files as `.pyc` bytecode, but that's up to you.

### Testing Your Implementation

The easiest way to test that you've implemented the spec correctly is to use the `capnp` tool
to [encode](capnp-tool.html#encoding-messages) test inputs and
[decode](capnp-tool.html#decoding-messages) outputs.



doc/roadmap.md
--------------------------------------
---
layout: page
title: Road Map
---

# Road Map

This is a list of big ideas we'd like to implement in Cap'n Proto. We don't know in what order
these will actually happen; as always, real work is driven by real-world needs.

## Language Features

* **Inline lists:**  Lets you define a field composed of a fixed number of elements of the same
  type, and have those elements be stored directly within the struct rather than as a separate
  object.  Useful mainly to avoid the need to validate list lengths when the length should always
  be the same.  Also saves a pointer's worth of space.
* **Type aliases:**  Ability to define a type which is just an alias of some other type, and
  have it show up as e.g. a `typedef` in languages that support that.  (The current `using`
  keyword is intended only for local use and does not affect code generation.)
* **Doc comments:**  Harvest doc comments from schema files and use them to generate doc comments
  on generated code.  Also make them available in the compiled schema so that a documentation
  generator could use them.
* **Encapsulated types:**  This will allow you to create a hand-written wrapper around a
  type which will be automatically injected into the generated code, so that you can provide a
  nicer interface which encapsulates the type's inner state.
* **Maps:**  Based on encapsulated and parameterized types.

## RPC Protocol Features

* **Dynamic schema transmission:**  Allow e.g. Python applications to obtain schemas directly from
  the RPC server so that they need not have a local copy.  Great for interactive debugging.
* **Three-way introductions (level 3 RPC):**  Allow RPC interactions between more than two parties,
  with new connections formed automatically as needed.
* **Bulk and Realtime**: Add features that make it easier to design Cap'n Proto APIs for bulk
  data transfers (with flow control) and realtime communications (where it's better to drop
  messages than to deliver them late).
* **UDP transport**: Cap'n Proto RPC could benefit from implementing a UDP transport, in order
  to achieve zero-round-trip three-party introductions and to implement "realtime" APIs (see
  "bulk and realtime", above).
* **Encrypted transport**: Cap'n Proto RPC should support an encrypted transport which uses
  capability-based authorization (not PKI), can accomplish zero-round-trip three-party
  introductions (via a pre-shared key from the introducer) and based on modern crypto. TLS is
  not designed for this, but we don't want to invent new crypto; we intend to build on
  [libsodium](https://github.com/jedisct1/libsodium) and the
  [Noise Protocol Framework](http://noiseprotocol.org/) as much as possible.

## C++ Cap'n Proto API Features

* **Plain Old C Structs:** The code generator should also generate a POCS type corresponding
  to each struct type. The POCS type would use traditional memory allocation, thus would not
  support zero-copy, but would support a more traditional and easy-to-use C++ API, including
  the ability to mutate the object over time without convoluted memory management. POCS types
  could be extracted from and inserted into messages with a single copy, allowing them to be
  used easily in non-performance-critical code.
* **Multi-threading:**  It should be made easy to assign different Cap'n Proto RPC objects
  to different threads and have them be able to safely call each other. Each thread would still
  have an anyschronous event loop serving the objects that belong to it.
* **Shared memory RPC:**  Zero-copy inter-process communication.
* **JSON codec customization:**  Extend the JSON library to support customizing the JSON
  representation using annotations. For example, a field could be given a different name in
  JSON than it is in Cap'n Proto. The goal of these features would be to allow any reasonable
  pre-existing JSON schema to be representable as a Cap'n Proto type definition, so that
  servers implementing JSON APIs can use Cap'n Proto exclusively on the server side.
* **LZ4 integration:**  Integrate LZ4 compression into the API to further reduce bandwidth needs
  with minimal CPU overhead.
* **Annotations API:**  For each annotation definition, generate code which assists in extracting
  that annotation from schema objects in a type-safe way.

## C++ KJ API Features

KJ is a framework library that is bundled with Cap'n Proto, but is broadly applicable to C++
applications even if they don't use Cap'n Proto serialization.

* **Fiber-based concurrency:**  The C++ runtime's event loop concurrency model will be augmented
  with support for fibers, which are like threads except that context switches happen only at
  well-defined points (thus avoiding the need for mutex locking).  Fibers essentially provide
  syntax sugar on top of the event loop model.
* **TLS bindings:** Write bindings for e.g. OpenSSL to make it easy to integrate with the KJ
  I/O framework, Cap'n Proto RPC, and the KJ HTTP library.
* **Modern crypto bindings:** A thin wrapper around
  [libsodium](https://github.com/jedisct1/libsodium) with a nice C++ API, e.g. representing
  keys using fixed-size, trivially-copyable classes.
* **Event loop integrations:** We should provide off-the-shelf integrations with popular event
  loop libraries, such as libuv, libev, libevent, boost::asio, and others, so that it's easier
  to use Cap'n Proto RPC in applications that already use another event framework.

## Storage

* **ORM interface:**  Define a standard interface for capabilities that represent remotely-stored
  objects, with get, put, publish, and subscribe methods.  Ideally, parameterize this interface
  on the stored type.
* **mmap-friendly mutable storage format:**  Define a standard storage format that is friendly
  to mmap-based use while allowing modification.  (With the current serialization format, mmap
  is only useful for read-only structures.)  Possibly based on the ORM interface, updates only
  possible at the granularity of a whole ORM entry.

## Tools

* **Schema compatibility checker:**  Add a `capnp` command which, given two schemas, verifies
  that the latter is a compatible upgrade from the former.  This could be used as a git hook
  to prevent submission of schema changes that would break wire compatibility.
* **RPC debugger:**  Add a `capnp` command which sends an RPC from the command line and prints
  the result.  Useful for debugging RPC servers.

## Quality Assurance

These things absolutely must happen before any 1.0 release.  Note that it's not yet decided when
a 1.0 release would happen nor how many 0.x releases might precede it.

* **Expand test coverage:**  There are lots of tests now, but some important scenarios, such as
  handling invalid of invalid input, need better testing.
* **Performance review:**  Performance is already very good compared to competitors, but at some
  point we need to break out the profiler and really hone down on the details.
* **Security review:**  We need a careful security review to make sure malicious input cannot
  crash an application or corrupt memory.

## Infrastructure

Note:  These are very large projects.

* **JSON-HTTP proxy:**  Develop a web server which can expose a Cap'n Proto RPC backend as a
  JSON-over-HTTP protocol.
* **Database:**  A fast storage database based on Cap'n Proto which implements the ORM interface
  on top of the mmap storage format.



doc/rpc.md
--------------------------------------
---
layout: page
title: RPC Protocol
---

# RPC Protocol

## Introduction

### Time Travel! _(Promise Pipelining)_

<img src='images/time-travel.png' style='max-width:639px'>

Cap'n Proto RPC employs TIME TRAVEL!  The results of an RPC call are returned to the client
instantly, before the server even receives the initial request!

There is, of course, a catch:  The results can only be used as part of a new request sent to the
same server.  If you want to use the results for anything else, you must wait.

This is useful, however:  Say that, as in the picture, you want to call `foo()`, then call `bar()`
on its result, i.e. `bar(foo())`.  Or -- as is very common in object-oriented programming -- you
want to call a method on the result of another call, i.e. `foo().bar()`.  With any traditional RPC
system, this will require two network round trips.  With Cap'n Proto, it takes only one.  In fact,
you can chain any number of such calls together -- with diamond dependencies and everything -- and
Cap'n Proto will collapse them all into one round trip.

By now you can probably imagine how it works:  if you execute `bar(foo())`, the client sends two
messages to the server, one saying "Please execute foo()", and a second saying "Please execute
bar() on the result of the first call".  These messages can be sent together -- there's no need
to wait for the first call to actually return.

To make programming to this model easy, in your code, each call returns a "promise".  Promises
work much like JavaScript promises or promises/futures in other languages:  the promise is returned
immediately, but you must later call `wait()` on it, or call `then()` to register an asynchronous
callback.

However, Cap'n Proto promises support an additional feature:
[pipelining](http://www.erights.org/elib/distrib/pipeline.html).  The promise
actually has methods corresponding to whatever methods the final result would have, except that
these methods may only be used for the purpose of calling back to the server.  Moreover, a
pipelined promise can be used in the parameters to another call without waiting.

**_But isn't that just syntax sugar?_**

OK, fair enough.  In a traditional RPC system, we might solve our problem by introducing a new
method `foobar()` which combines `foo()` and `bar()`.  Now we've eliminated the round trip, without
inventing a whole new RPC protocol.

The problem is, this kind of arbitrary combining of orthogonal features quickly turns elegant
object-oriented protocols into ad-hoc messes.

For example, consider the following interface:

{% highlight capnp %}
# A happy, object-oriented interface!

interface Node {}

interface Directory extends(Node) {
  list @0 () -> (list: List(Entry));
  struct Entry {
    name @0 :Text;
    file @1 :Node;
  }

  create @1 (name :Text) -> (node :Node);
  open @2 (name :Text) -> (node :Node);
  delete @3 (name :Text);
  link @4 (name :Text, node :Node);
}

interface File extends(Node) {
  size @0 () -> (size: UInt64);
  read @1 (startAt :UInt64, amount :UInt64) -> (data: Data);
  write @2 (startAt :UInt64, data :Data);
  truncate @3 (size :UInt64);
}
{% endhighlight %}

This is a very clean interface for interacting with a file system.  But say you are using this
interface over a satellite link with 1000ms latency.  Now you have a problem:  simply reading the
file `foo` in directory `bar` takes four round trips!

{% highlight python %}
# pseudocode
bar = root.open("bar");    # 1
foo = bar.open("foo");     # 2
size = foo.size();         # 3
data = foo.read(0, size);  # 4
# The above is four calls but takes only one network
# round trip with Cap'n Proto!
{% endhighlight %}

In such a high-latency scenario, making your interface elegant is simply not worth 4x the latency.
So now you're going to change it.  You'll probably do something like:

* Introduce a notion of path strings, so that you can specify "foo/bar" rather than make two
  separate calls.
* Merge the `File` and `Directory` interfaces into a single `Filesystem` interface, where every
  call takes a path as an argument.

{% highlight capnp %}
# A sad, singleton-ish interface.

interface Filesystem {
  list @0 (path :Text) -> (list :List(Text));
  create @1 (path :Text, data :Data);
  delete @2 (path :Text);
  link @3 (path :Text, target :Text);

  fileSize @4 (path :Text) -> (size: UInt64);
  read @5 (path :Text, startAt :UInt64, amount :UInt64)
       -> (data :Data);
  readAll @6 (path :Text) -> (data: Data);
  write @7 (path :Text, startAt :UInt64, data :Data);
  truncate @8 (path :Text, size :UInt64);
}
{% endhighlight %}

We've now solved our latency problem...  but at what cost?

* We now have to implement path string manipulation, which is always a headache.
* If someone wants to perform multiple operations on a file or directory, we now either have to
  re-allocate resources for every call or we have to implement some sort of cache, which tends to
  be complicated and error-prone.
* We can no longer give someone a specific `File` or a `Directory` -- we have to give them a
  `Filesystem` and a path.
  * But what if they are buggy and have hard-coded some path other than the one we specified?
  * Or what if we don't trust them, and we really want them to access only one particular `File` or
    `Directory` and not have permission to anything else.  Now we have to implement authentication
    and authorization systems!  Arrgghh!

Essentially, in our quest to avoid latency, we've resorted to using a singleton-ish design, and
[singletons are evil](http://www.object-oriented-security.org/lets-argue/singletons).

**Promise Pipelining solves all of this!**

With pipelining, our 4-step example can be automatically reduced to a single round trip with no
need to change our interface at all.  We keep our simple, elegant, singleton-free interface, we
don't have to implement path strings, caching, authentication, or authorization, and yet everything
performs as well as we can possibly hope for.

#### Example code

[The calculator example](https://github.com/capnproto/capnproto/blob/master/c++/samples/calculator-client.c++)
uses promise pipelining.  Take a look at the client side in particular.

### Distributed Objects

As you've noticed by now, Cap'n Proto RPC is a distributed object protocol.  Interface references --
or, as we more commonly call them, capabilities -- are a first-class type.  You can pass a
capability as a parameter to a method or embed it in a struct or list.  This is a huge difference
from many modern RPC-over-HTTP protocols that only let you address global URLs, or other RPC
systems like Protocol Buffers and Thrift that only let you address singleton objects exported at
startup.  The ability to dynamically introduce new objects and pass around references to them
allows you to use the same design patterns over the network that you use locally in object-oriented
programming languages.  Many kinds of interactions become vastly easier to express given the
richer vocabulary.

**_Didn't CORBA prove this doesn't work?_**

No!

CORBA failed for many reasons, with the usual problems of design-by-committee being a big one.

However, the biggest reason for CORBA's failure is that it tried to make remote calls look the
same as local calls. Cap'n Proto does NOT do this -- remote calls have a different kind of API
involving promises, and accounts for the presence of a network introducing latency and
unreliability.

As shown above, promise pipelining is absolutely critical to making object-oriented interfaces work
in the presence of latency. If remote calls look the same as local calls, there is no opportunity
to introduce promise pipelining, and latency is inevitable. Any distributed object protocol which
does not support promise pipelining cannot -- and should not -- succeed. Thus the failure of CORBA
(and DCOM, etc.) was inevitable, but Cap'n Proto is different.

### Handling disconnects

Networks are unreliable. Occasionally, connections will be lost. When this happens, all
capabilities (object references) served by the connection will become disconnected. Any further
calls addressed to these capabilities will throw "disconnected" exceptions. When this happens, the
client will need to create a new connection and try again. All Cap'n Proto applications with
long-running connections (and probably short-running ones too) should be prepared to catch
"disconnected" exceptions and respond appropriately.

On the server side, when all references to an object have been "dropped" (either because the
clients explicitly dropped them or because they became disconnected), the object will be closed
(in C++, the destructor is called; in GC'd languages, a `close()` method is called). This allows
servers to easily allocate per-client resources without having to clean up on a timeout or risk
leaking memory.

### Security

Cap'n Proto interface references are
[capabilities](http://en.wikipedia.org/wiki/Capability-based_security).  That is, they both
designate an object to call and confer permission to call it.  When a new object is created, only
the creator is initially able to call it.  When the object is passed over a network connection,
the receiver gains permission to make calls -- but no one else does.  In fact, it is impossible
for others to access the capability without consent of either the host or the receiver because
the host only assigns it an ID specific to the connection over which it was sent.

Capability-based design patterns -- which largely boil down to object-oriented design patterns --
work great with Cap'n Proto.  Such patterns tend to be much more adaptable than traditional
ACL-based security, making it easy to keep security tight and avoid confused-deputy attacks while
minimizing pain for legitimate users.  That said, you can of course implement ACLs or any other
pattern on top of capabilities.

For an extended discussion of what capabilities are and why they are often easier and more powerful
than ACLs, see Mark Miller's
["An Ode to the Granovetter Diagram"](http://www.erights.org/elib/capability/ode/index.html) and
[Capability Myths Demolished](http://zesty.ca/capmyths/usenix.pdf).

## Protocol Features

Cap'n Proto's RPC protocol has the following notable features.  Since the protocol is complicated,
the feature set has been divided into numbered "levels", so that implementations may declare which
features they have covered by advertising a level number.

* **Level 1:**  Object references and promise pipelining, as described above.
* **Level 2:**  Persistent capabilities.  You may request to "save" a capability, receiving a
  persistent token which can be used to "restore" it in the future (on a new connection).  Not
  all capabilities can be saved; the host app must implement support for it.  Building this into
  the protocol makes it possible for a Cap'n-Proto-based data store to transparently save
  structures containing capabilities without knowledge of the particular capability types or the
  application built on them, as well as potentially enabling more powerful analysis and
  visualization of stored data.
* **Level 3:**  Three-way interactions.  A network of Cap'n Proto vats (nodes) can pass object
  references to each other and automatically form direct connections as needed.  For instance, if
  Alice (on machine A) sends Bob (on machine B) a reference to Carol (on machine C), then machine B
  will form a new connection to machine C so that Bob can call Carol directly without proxying
  through machine A.
* **Level 4:**  Reference equality / joining.  If you receive a set of capabilities from different
  parties which should all point to the same underlying objects, you can verify securely that they
  in fact do.  This is subtle, but enables many security patterns that rely on one party being able
  to verify that two or more other parties agree on something (imagine a digital escrow agent).
  See [E's page on equality](http://erights.org/elib/equality/index.html).

## Encryption

At this time, Cap'n Proto does not specify an encryption scheme, but as it is a simple byte
stream protocol, it can easily be layered on top of SSL/TLS or other such protocols.

## Specification

The Cap'n Proto RPC protocol is defined in terms of Cap'n Proto serialization schemas.  The
documentation is inline.  See
[rpc.capnp](https://github.com/capnproto/capnproto/blob/master/c++/src/capnp/rpc.capnp).

Cap'n Proto's RPC protocol is based heavily on
[CapTP](http://www.erights.org/elib/distrib/captp/index.html), the distributed capability protocol
used by the [E programming language](http://www.erights.org/index.html).  Lots of useful material
for understanding capabilities can be found at those links.

The protocol is complex, but the functionality it supports is conceptually simple.  Just as TCP
is a complex protocol that implements the simple concept of a byte stream, Cap'n Proto RPC is a
complex protocol that implements the simple concept of objects with callable methods.



_posts/2013-04-01-announcing-capn-proto.md
--------------------------------------
---
layout: post
title: Announcing Cap'n Proto
author: kentonv
---

<img src='{{ site.baseurl }}images/infinity-times-faster.png' style='width:334px; height:306px; float: right;'>

So, uh...  I have a confession to make.

I may have rewritten Protocol Buffers.

Again.

[And it's infinity times faster.](https://capnproto.org)



_posts/2013-06-27-capn-proto-beta-release.md
--------------------------------------
---
layout: post
title: Cap'n Proto Beta Release
author: kentonv
---

It's been nearly three months since Cap'n Proto was originally announced, and by now you're
probably wondering what I've been up to.  The answer is basically
[non-stop coding](https://github.com/kentonv/capnproto/commits/master).  Features were implemented,
code was refactored, tests were written, and now Cap'n Proto is beginning to resemble something
like a real product.  But as is so often the case with me, I've been so engrossed in coding that I
forgot to post updates!

Well, that changes today, with the first official release of Cap'n Proto, v0.1.  While not yet
"done", this release should be usable for Real Work.  Feature-wise, for C++, the library is roughly
on par with [Google's Protocol Buffers](http://protobuf.googlecode.com) (which, as you know, I used
to maintain).  Features include:

* Types: numbers, bytes, text, enums, lists, structs, and unions.
* Code generation from schema definition files.
* Reading from and writing to file descriptors (or other streams).
* Text-format output (e.g. for debugging).
* Reflection, for writing generic code that dynamically walks over message contents.
* Dynamic schema loading (to manipulate types not known at compile time).
* Code generator plugins for extending the compiler to support new languages.
* Tested on Linux and Mac OSX with GCC and Clang.

Notably missing from this list is RPC (something Protocol Buffers never provided either).  The RPC
system is still in the design phase, but will be implemented over the coming weeks.

Also missing is support for languages other than C++.  However, I'm happy to report that a number
of awesome contributors have stepped up and are working on
[implementations in C, Go, Python]({{ site.baseurl }}otherlang.html), and a few others not yet
announced.  None of these are "ready" just yet, but watch this space.  (Would you like to work on
an implementation in your favorite language?
[Let us know!](https://groups.google.com/group/capnproto))

Going forward, Cap'n Proto releases will occur more frequently, perhaps every 2-4 weeks.
Consider [signing up for release announcements](https://groups.google.com/group/capnproto-announce).

In any case, go [download the release]({{ site.baseurl }}install.html) and
[tell us your thoughts](https://groups.google.com/group/capnproto).



_posts/2013-08-12-capnproto-0.2-no-more-haskell.md
--------------------------------------
---
layout: post
title: "Cap'n Proto v0.2: Compiler rewritten Haskell -> C++"
author: kentonv
---

Today I am releasing version 0.2 of Cap'n Proto.  The most notable change: the compiler / code
generator, which was previously written in Haskell, has been rewritten in C++11.  There are a few
other changes as well, but before I talk about those, let me try to calm the angry mob that is
not doubt reaching for their pitchforks as we speak.  There are a few reasons for this change,
some practical, some ideological.  I'll start with the practical.

**The practical:  Supporting dynamic languages**

Say you are trying to implement Cap'n Proto in an interpreted language like Python.  One of the big
draws of such a language is that you can edit your code and then run it without an intervening
compile step, allowing you to iterate faster.  But if the Python Cap'n Proto implementation worked
like the C++ one (or like Protobufs), you lose some of that: whenever you change your Cap'n Proto
schema files, you must run a command to regenerate the Python code from them.  That sucks.

What you really want to do is parse the schemas at start-up -- the same time that the Python code
itself is parsed.  But writing a proper schema parser is harder than it looks; you really should
reuse the existing implementation.  If it is written in Haskell, that's going to be problematic.
You either need to invoke the schema parser as a sub-process or you need to call Haskell code from
Python via an FFI.  Either approach is going to be a huge hack with lots of problems, not the least
of which is having a runtime dependency on an entire platform that your end users may not otherwise
want.

But with the schema parser written in C++, things become much simpler.  Python code calls into
C/C++ all the time.  Everyone already has the necessary libraries installed.  There's no need to
generate code, even; the parsed schema can be fed into the Cap'n Proto C++ runtime's dynamic API,
and Python bindings can trivially be implemented on top of that in just a few hundred lines of
code.  Everyone wins.

**The ideological:  I'm an object-oriented programmer**

I really wanted to like Haskell.  I used to be a strong proponent of functional programming, and
I actually once wrote a complete web server and CMS in a purely-functional toy language of my own
creation.  I love strong static typing, and I find a lot of the constructs in Haskell really
powerful and beautiful.  Even monads.  _Especially_ monads.

But when it comes down to it, I am an object-oriented programmer, and Haskell is not an
object-oriented language.  Yes, you can do object-oriented style if you want to, just like you
can do objects in C.  But it's just too painful.  I want to write `object.methodName`, not
`ModuleName.objectTypeMethodName object`.  I want to be able to write lots of small classes that
encapsulate complex functionality in simple interfaces -- _without_ having to place each one in
a whole separate module and ending up with thousands of source files.  I want to be able to build
a list of objects of varying types that implement the same interface without having to re-invent
virtual tables every time I do it (type classes don't quite solve the problem).

And as it turns out, even aside from the lack of object-orientation, I don't actually like
functional programming as much as I thought.  Yes, writing my parser was super-easy (my first
commit message was
"[Day 1: Learn Haskell, write a parser](https://github.com/kentonv/capnproto/commit/6bb49ca775501a9b2c7306992fd0de53c5ee4e95)").
But everything beyond that seemed to require increasing amounts of brain bending.  For instance, to
actually encode a Cap'n Proto message, I couldn't just allocate a buffer of zeros and then go
through each field and set its value.  Instead, I had to compute all the field values first, sort
them by position, then concatenate the results.

Of course, I'm sure it's the case that if I spent years writing Haskell code, I'd eventually become
as proficient with it as I am with C++.  Perhaps I could un-learn object-oriented style and learn
something else that works just as well or better.  Basically, though, I decided that this was
going to take a lot longer than it at first appeared, and that this wasn't a good use of my
limited resources.  So, I'm cutting my losses.

I still think Haskell is a very interesting language, and if works for you, by all means, use it.
I would love to see someone write at actual Cap'n Proto runtime implementation in Haskell.  But
the compiler is now C++.

**Parser Combinators in C++**

A side effect (so to speak) of the compiler rewrite is that Cap'n Proto's companion utility
library, KJ, now includes a parser combinator framework based on C++11 templates and lambdas.
Here's a sample:

{% highlight c++ %}
// Construct a parser that parses a number.
auto number = transform(
    sequence(
        oneOrMore(charRange('0', '9')),
        optional(sequence(
            exactChar<'.'>(),
            many(charRange('0', '9'))))),
    [](Array<char> whole, Maybe<Array<char>> maybeFraction)
        -> Number* {
      KJ_IF_MAYBE(fraction, maybeFraction) {
        return new RealNumber(whole, *fraction);
      } else {
        return new WholeNumber(whole);
      }
    });
{% endhighlight %}

An interesting fact about the above code is that constructing the parser itself does not allocate
anything on the heap.  The variable `number` in this case ends up being one 96-byte flat object,
most of which is composed of tables for character matching.  The whole thing could even be
declared `constexpr`...  if the C++ standard allowed empty-capture lambdas to be `constexpr`, which
unfortunately it doesn't (yet).

Unfortunately, KJ is largely undocumented at the moment, since people who just want to use
Cap'n Proto generally don't need to know about it.

**Other New Features**

There are a couple other notable changes in this release, aside from the compiler:

* Cygwin has been added as a supported platform, meaning you can now use Cap'n Proto on Windows.
  I am considering supporting MinGW as well.  Unfortunately, MSVC is unlikely to be supported any
  time soon as its C++11 support is
  [woefully lacking](http://blogs.msdn.com/b/somasegar/archive/2013/06/28/cpp-conformance-roadmap.aspx).

* The new compiler binary -- now called `capnp` rather than `capnpc` -- is more of a multi-tool.
  It includes the ability to decode binary messages to text as a debugging aid.  Type
  `capnp help decode` for more information.

* The new [Orphan]({{ site.baseurl }}/cxx.html#orphans) class lets you detach objects from a
  message tree and re-attach them elsewhere.

* Various contributors have declared their intentions to implement
  [Ruby](https://github.com/cstrahan/capnp-ruby),
  [Rust](https://github.com/dwrensha/capnproto-rust), C#, Java, Erlang, and Delphi bindings.  These
  are still works in progress, but exciting nonetheless!

**Backwards-compatibility Note**

Cap'n Proto v0.2 contains an obscure wire format incompatibility with v0.1.  If you are using
unions containing multiple primitive-type fields of varying sizes, it's possible that the new
compiler will position those fields differently.  A work-around to get back to the old layout
exists; if you believe you could be affected, please [send me](mailto:temporal@gmail.com) your
schema and I'll tell you what to do.  [Gory details.](https://groups.google.com/d/msg/capnproto/NIYbD0haP38/pH5LildInwIJ)

**Road Map**

v0.3 will come in a couple weeks and will include several new features and clean-ups that can now
be implemented more easily given the new compiler.  This will also hopefully be the first release
that officially supports a language other than C++.

The following release, v0.4, will hopefully be the first release implementing RPC.

_PS.  If you are wondering, compared to the Haskell version, the new compiler is about 50% more
lines of code and about 4x faster.  The speed increase should be taken with a grain of salt,
though, as my Haskell code did all kinds of horribly slow things.  The code size is, I think, not
bad, considering that Haskell specializes in concision -- but, again, I'm sure a Haskell expert
could have written shorter code._



_posts/2013-08-19-capnproto-0.2.1.md
--------------------------------------
---
layout: post
title: "Cap'n Proto v0.2.1: Minor bug fixes"
author: kentonv
---

Cap'n Proto was just bumped to v0.2.1.  This release contains a couple bug fixes, including
a work-around for [a GCC bug](http://gcc.gnu.org/bugzilla/show_bug.cgi?id=58192).  If you were
observing any odd memory corruption or crashes in 0.2.0 -- especially if you are compiling with
GCC with optimization enabled but without `-DNDEBUG` -- you should upgrade.



_posts/2013-09-04-capnproto-0.3-python-tools-features.md
--------------------------------------
---
layout: post
title: "Cap'n Proto v0.3: Python, tools, new features"
author: kentonv
---

The first release of Cap'n Proto came three months after the project was announced.  The second
release came six weeks after that.  And the third release is three weeks later.  If the pattern
holds, there will be an infinite number of releases before the end of this month.

Version 0.3 is not a paradigm-shifting release, but rather a slew of new features largely made
possible by building on the rewritten compiler from the last release.  Let's go through the
list...

### Python Support!

Thanks to the tireless efforts of contributor [Jason Paryani](https://github.com/jparyani), I can
now comfortably claim that Cap'n Proto supports multiple languages.  [His Python
implementation](http://jparyani.github.io/pycapnp/) wraps the C++ library and exposes
most of its features in a nice, easy-to-use way.

And I have to say, it's _way_ better than the old Python Protobuf implementation that I helped put
together at Google.  Here's why:

* Jason's implementation parses Cap'n Proto schema files at runtime.  There is no need to run a
  compiler to generate code every time you update your schema, as with protobufs.  So, you get
  to use Python the way Python was intended to be used.  In fact, he's hooked into the Python
  import mechanism, so you can basically import a `.capnp` schema file as if it were a `.py`
  module.  It's even convenient to load schema files and play with Cap'n Proto messages from the
  interactive interpreter prompt.
* It's _fast_.  Whereas the Python Protobuf implementation -- which we made the mistake of
  implementing in pure-Python -- is _slow_.  And while technically there is an experimental
  C-extension-based Python Protobuf implementation (which isn't enabled by default due to various
  obscure problems), Jason's Cap'n Proto implementation is faster than that, too.

Go [check it out](http://jparyani.github.io/pycapnp/)!

By the way, there is also a budding [Erlang implementation](http://ecapnp.astekk.se/)
(by Andreas Stenius), and work
continues on [Rust](https://github.com/dwrensha/capnproto-rust) (David Renshaw) and
[Ruby](https://github.com/cstrahan/capnp-ruby) (Charles Strahan) implementations.

### Tools: Cap'n Proto on the Command Line

The `capnp` command-line tool previously served mostly to generate code, via the `capnp compile`
command.  It now additionally supports converting encoded Cap'n Proto messages to a human-readable
text format via `capnp decode`, and converting that format back to binary with `capnp encode`.
These tools are, of course, critical for debugging.

You can also use the new `capnp eval` command to do something interesting: given a schema file and
the name of a constant defined therein, it will print out the value of that constant, or optionally
encode it to binary.  This is more interesting than it sounds because the schema language supports
variable substitution in the definitions of these constants.  This means you can build a large
structure by importing smaller bits from many different files.  This may make it convenient to
use Cap'n Proto schemas as a config format: define your service configuration as a constant in
a schema file, importing bits specific to each client from other files that those clients submit
to you.  Use `capnp eval` to "compile" the whole thing to binary for deployment.  (This has always
been a common use case for Protobuf text format, which doesn't even support variable substitution
or imports.)

Anyway, check out the [full documentation]({{ site.baseurl }}capnp-tool.html) for
more.

### New Features

The core product has been updated as well:

* Support for unnamed [unions]({{ site.baseurl }}language.html#unions) reduces the
  need for noise-words, improving code readability.  Additionally, the syntax for unions has been
  simplified by removing the unnecessary ordinal number.
* [Groups]({{ site.baseurl }}language.html#groups) pair nicely with unions.
* [Constants]({{ site.baseurl }}language.html#constants) are now
  [implemented in C++]({{ site.baseurl }}cxx.html#constants).  Additionally, they
  can now be defined in terms of other constants (variable substitution), as described earlier.
* The schema API and `schema.capnp` have been radically refactored, in particular to take advantage
  of the new union and group features, making the code more readable.
* More and better tests, bug fixes, etc.

### Users!

Some news originating outside of the project itself:

* [Debian Unstable (sid)](http://www.debian.org/releases/sid/) now features
  [a Cap'n Proto package](http://packages.debian.org/sid/capnproto), thanks to
  [Tom Lee](https://github.com/thomaslee).  Of course, since package updates take some time, this
  package is still v0.2.1 as of this writing, but it will be updated to v0.3 soon enough.
* Popular OSX-based text editor [TextMate](http://macromates.com/) now
  [uses Cap'n Proto internally](https://github.com/textmate/textmate/commit/5c02b4ff5cc0c7c319d3d4f127c8ee19b81f80b7),
  and the developer's feedback lead directly to several usability improvements included in this
  release.
* Many people using Cap'n Proto _haven't bothered to tell us about it_!  Please, if you use it,
  [let us know](https://groups.google.com/group/capnproto) about your experience, both what you like
  and especially what you don't like.  This is the critical time where the system is usable but
  can still be changed if it's not right, so your feedback is critical to our long-term success.
* I have revenue!  A whopping [$1.25 per week](https://www.gittip.com/kentonv/)!  >_>  It's
  totally worth it; I love this project.  (But thanks for the tips!)



_posts/2013-12-12-capnproto-0.4-time-travel.md
--------------------------------------
---
layout: post
title: "Cap'n Proto v0.4: Time Traveling RPC"
author: kentonv
---

Well, [Hofstadter](http://en.wikipedia.org/wiki/Hofstadter's_law) kicked in and this release took
way too long.  But, after three long months, I'm happy to announce:

### Time-Traveling RPC _(Promise Pipelining)_

<img src='{{ site.baseurl }}images/time-travel.png' style='max-width:639px'>

v0.4 finally introduces the long-promised [RPC system]({{ site.baseurl }}rpc.html).  Traditionally,
RPC is plagued by the fact that networks have latency, and pretending that latency doesn't exist by
hiding it behind what looks like a normal function call only makes the problem worse.
Cap'n Proto has a simple solution to this problem:  send call results _back in time_, so they
arrive at the client at the point in time when the call was originally made!

Curious how Cap'n Proto bypasses the laws of physics?
[Check out the docs!]({{ site.baseurl }}rpc.html)

_UPDATE:  There has been some confusion about what I'm claiming.  I am NOT saying that using
promises alone (i.e. being asynchronous) constitutes "time travel".  Cap'n Proto implements a
technique called Promise Pipelining which allows a new request to be formed based on the content
of a previous result (in part or in whole) before that previous result is returned.  Notice in the
diagram that the result of foo() is being passed to bar().  Please
[see the docs]({{ site.baseurl }}rpc.html) or
[check out the calculator example](https://github.com/kentonv/capnproto/blob/master/c++/samples)
for more._

### Promises in C++

_UPDATE:  More confusion.  This section is **not** about pipelining ("time travel").  This section
is just talking about implementing a promise API in C++.  Pipelining is another feature on top of
that.  Please [see the RPC page]({{ site.baseurl }}rpc.html) if you want to know more about
pipelining._

If you do a lot of serious JavaScript programming, you've probably heard of
[Promises/A+](http://promisesaplus.com/) and similar proposals.  Cap'n Proto RPC introduces a
similar construct in C++.  In fact, the API is nearly identical, and its semantics are nearly
identical.  Compare with
[Domenic Denicola's JavaScript example](http://domenic.me/2012/10/14/youre-missing-the-point-of-promises/):

{% highlight c++ %}
// C++ version of Domenic's JavaScript promises example.
getTweetsFor("domenic") // returns a promise
  .then([](vector<Tweet> tweets) {
    auto shortUrls = parseTweetsForUrls(tweets);
    auto mostRecentShortUrl = shortUrls[0];
    // expandUrlUsingTwitterApi returns a promise
    return expandUrlUsingTwitterApi(mostRecentShortUrl);
  })
  .then(httpGet) // promise-returning function
  .then(
    [](string responseBody) {
      cout << "Most recent link text:" << responseBody << endl;
    },
    [](kj::Exception&& error) {
      cerr << "Error with the twitterverse:" << error << endl;
    }
  );
{% endhighlight %}

This is C++, but it is no more lines -- nor otherwise more complex -- than the equivalent
JavaScript.  We're doing several I/O operations, we're doing them asynchronously, and we don't
have a huge unreadable mess of callback functions.  Promises are based on event loop concurrency,
which means you can perform concurrent operations with shared state without worrying about mutex
locking -- i.e., the JavaScript model.  (Of course, if you really want threads, you can run
multiple event loops in multiple threads and make inter-thread RPC calls between them.)

[More on C++ promises.]({{ site.baseurl }}cxxrpc.html#kj_concurrency_framework)

### Python too

[Jason](https://github.com/jparyani) has been diligently keeping his
[Python bindings](http://jparyani.github.io/pycapnp/) up to date, so you can already use RPC there
as well.  The Python interactive interpreter makes a great debugging tool for calling C++ servers.

### Up Next

Cap'n Proto is far from done, but working on it in a bubble will not produce ideal results.
Starting after the holidays, I will be refocusing some of my time into an adjacent project which
will be a heavy user of Cap'n Proto.  I hope this experience will help me discover first hand
the pain points in the current interface and keep development going in the right direction.

This does, however, mean that core Cap'n Proto development will slow somewhat (unless contributors
pick up the slack! ;) ).  I am extremely excited about this next project, though, and I think you
will be too.  Stay tuned!



_posts/2013-12-13-promise-pipelining-capnproto-vs-ice.md
--------------------------------------
---
layout: post
title: "Promise Pipelining and Dependent Calls: Cap'n Proto vs. Thrift vs. Ice"
author: kentonv
---

_UPDATED:  Added Thrift to the comparison._

So, I totally botched the 0.4 release announcement yesterday.  I was excited about promise
pipelining, but I wasn't sure how to describe it in headline form.  I decided to be a bit
silly and call it "time travel", tongue-in-cheek.  My hope was that people would then be
curious, read the docs, find out that this is actually a really cool feature, and start doing
stuff with it.

Unfortunately, [my post](2013-12-12-capnproto-0.4-time-travel.html) only contained a link to
the full explanation and then confusingly followed the "time travel" section with a separate section
describing the fact that I had implemented a promise API in C++.  Half the readers clicked through
to the documentation and understood.  The other half thought I was claiming that promises alone
constituted "time travel", and thought I was ridiculously over-hyping an already-well-known
technique.  My HN post was subsequently flagged into oblivion.

Let me be clear:

**Promises alone are _not_ what I meant by "time travel"!**

<img src='{{ site.baseurl }}images/capnp-vs-thrift-vs-ice.png' style='width:350px; height:275px; float: right;'>

So what did I mean?  Perhaps [this benchmark](https://github.com/kentonv/capnp-vs-ice) will
make things clearer.  Here, I've defined a server that exports a simple four-function calculator
interface, with `add()`, `sub()`, `mult()`, and `div()` calls, each taking two integers and\
returning a result.

You are probably already thinking:  That's a ridiculously bad way to define an RPC interface!
You want to have _one_ method `eval()` that takes an expression tree (or graph, even), otherwise
you will have ridiculous latency.  But this is exactly the point.  **With promise pipelining, simple,
composable methods work fine.**

To prove the point, I've implemented servers in Cap'n Proto, [Apache Thrift](http://thrift.apache.org/),
and [ZeroC Ice](http://www.zeroc.com/).  I then implemented clients against each one, where the
client attempts to evaluate the expression:

    ((5 * 2) + ((7 - 3) * 10)) / (6 - 4)

All three frameworks support asynchronous calls with a promise/future-like interface, and all of my
clients use these interfaces to parallelize calls.  However, notice that even with parallelization,
it takes four steps to compute the result:

    # Even with parallelization, this takes four steps!
    ((5 * 2) + ((7 - 3) * 10)) / (6 - 4)
      (10    + (   4    * 10)) /    2      # 1
      (10    +         40)     /    2      # 2
            50                 /    2      # 3
                              25           # 4

As such, the Thrift and Ice clients take four network round trips.  Cap'n Proto, however, takes
only one.

Cap'n Proto, you see, sends all six calls from the client to the server at one time.  For the
latter calls, it simply tells the server to substitute the former calls' results into the new
requests, once those dependency calls finish.  Typical RPC systems can only send three calls to
start, then must wait for some to finish before it can continue with the remaining calls.  Over
a high-latency connection, this means they take 4x longer than Cap'n Proto to do their work in
this test.

So, does this matter outside of a contrived example case?  Yes, it does, because it allows you to
write cleaner interfaces with simple, composable methods, rather than monster do-everything-at-once
methods.  The four-method calculator interface is much simpler than one involving sending an
expression graph to the server in one batch.  Moreover, pipelining allows you to define
object-oriented interfaces where you might otherwise be tempted to settle for singletons.  See
[my extended argument]({{ site.baseurl }}rpc.html#introduction) (this is what I was trying to get
people to click on yesterday :) ).

Hopefully now it is clearer what I was trying to illustrate with this diagram, and what I meant
by "time travel"!

<img src='{{ site.baseurl }}images/time-travel.png' style='max-width:639px'>



_posts/2014-03-11-capnproto-0.4.1-bugfixes.md
--------------------------------------
---
layout: post
title: "Cap'n Proto 0.4.1: Bugfix Release"
author: kentonv
---

Today I'm releasing [version 0.4.1 of Cap'n Proto](https://capnproto.org/capnproto-c++-0.4.1.tar.gz).
As hinted by the version number, this is a bugfix and tweak release, with no big new features.

You may be wondering:  If there are no big new features, what has been happening over the
last three months?  Most of my time lately has been spent laying the groundwork for an
interesting project built on Cap'n Proto which should launch by the end of this month.
Stay tuned!  And don't worry -- this new project is going to need many of the upcoming
features on [the roadmap]({{ site.baseurl }}roadmap.html), so work on version 0.5 will be
proceeding soon.

In the meantime, though, there have been some major updates from the community:

  * The folks at [CloudFlare](https://www.cloudflare.com/) have produced a
    [Lua port](https://github.com/cloudflare/lua-capnproto) which they are
    [using successfully in production](http://blog.cloudflare.com/introducing-lua-capnproto-better-serialization-in-lua)
    along with the existing [Go port](https://github.com/jmckaskill/go-capnproto).
  * [The Rust port of Cap'n Proto](https://github.com/dwrensha/capnproto-rust) now has
    preliminary RPC support, making it the third language to support Cap'n Proto RPC (after
    C++ and Python), and the second language to implement it from the ground up (Python just
    wraps the C++ implementation).  Check out author [David Renshaw](https://github.com/dwrensha)'s
    [talk at Mozilla](https://air.mozilla.org/rust-meetup-february-2014/).
  * A [JavaScript port](https://github.com/jscheid/capnproto-js) has appeared, but it needs help
    to keep going!




_posts/2014-06-17-capnproto-flatbuffers-sbe.md
--------------------------------------
---
layout: post
title: Cap'n Proto, FlatBuffers, and SBE
author: kentonv
---

**Update Jun 18, 2014:** I have made [some corrections](https://github.com/kentonv/capnproto/commit/e4e6c9076ae16804c07968cd3bdf6107155df7ee) since the original version of this post.

**Update Dec 15, 2014:** Updated to reflect that Cap'n Proto 0.5 now supports Visual Studio and that
Java is now well-supported.

Yesterday, some engineers at Google released [FlatBuffers](http://google-opensource.blogspot.com/2014/06/flatbuffers-memory-efficient.html), a new serialization protocol and library with similar design principles to Cap'n Proto. Also, a few months back, Real Logic released [Simple Binary Encoding](http://mechanical-sympathy.blogspot.com/2014/05/simple-binary-encoding.html), another protocol and library of this nature.

It seems we now have some friendly rivalry. :)

It's great to see that the concept of `mmap()`-able, zero-copy serialization formats are catching on, and it's wonderful that all are open source under liberal licenses. But as a user, you might be left wondering how all these systems compare. You have a vague idea that all these encodings are "fast", particularly compared to Protobufs or other more-traditional formats. But there is more to a serialization protocol than speed, and you may be wondering what else you should be considering.

The goal of this blog post is to highlight some of the main _qualitative_ differences between these libraries as I see them. Obviously, I am biased, and you should consider that as you read. Hopefully, though, this provides a good starting point for your own investigation of the alternatives.

### Feature Matrix

The following are a set of considerations I think are important. See something I missed? Please [let me know](mailto:kenton@sandstorm.io) and I'll add it. I'd like in particular to invite the SBE and FlatBuffers authors to suggest advantages of their libraries that I may have missed.

I will go into more detail on each item below.

Note: For features which are properties of the implementation rather than the protocol or project, unless otherwise stated, I am judging the C++ implementations.

<table class="pass-fail">
<tr><td>Feature</td><td>Protobuf</td><td>Cap'n Proto</td><td>SBE</td><td>FlatBuffers</td></tr>
<tr><td>Schema evolution</td><td class="pass">yes</td><td class="pass">yes</td><td class="warn">caveats</td><td class="pass">yes</td></tr>
<tr><td>Zero-copy</td><td class="fail">no</td><td class="pass">yes</td><td class="pass">yes</td><td class="pass">yes</td></tr>
<tr><td>Random-access reads</td><td class="fail">no</td><td class="pass">yes</td><td class="fail">no</td><td class="pass">yes</td></tr>
<tr><td>Safe against malicious input</td><td class="pass">yes</td><td class="pass">yes</td><td class="pass">yes</td><td class="warn">opt-in upfront</td></tr>
<tr><td>Reflection / generic algorithms</td><td class="pass">yes</td><td class="pass">yes</td><td class="pass">yes</td><td class="pass">yes</td></tr>
<tr><td>Initialization order</td><td class="pass">any</td><td class="pass">any</td><td class="fail">preorder</td><td class="warn">bottom-up</td></tr>
<tr><td>Unknown field retention</td><td class="warn">removed<br>in proto3</td><td class="pass">yes</td><td class="fail">no</td><td class="fail">no</td></tr>
<tr><td>Object-capability RPC system</td><td class="fail">no</td><td class="pass">yes</td><td class="fail">no</td><td class="fail">no</td></tr>
<tr><td>Schema language</td><td class="pass">custom</td><td class="pass">custom</td><td class="warn">XML</td><td class="pass">custom</td></tr>
<tr><td>Usable as mutable state</td><td class="pass">yes</td><td class="fail">no</td><td class="fail">no</td><td class="fail">no</td></tr>
<tr><td>Padding takes space on wire?</td><td class="pass">no</td><td class="warn">optional</td><td class="fail">yes</td><td class="fail">yes</td></tr>
<tr><td>Unset fields take space on wire?</td><td class="pass">no</td><td class="fail">yes</td><td class="fail">yes</td><td class="pass">no</td></tr>
<tr><td>Pointers take space on wire?</td><td class="pass">no</td><td class="fail">yes</td><td class="pass">no</td><td class="fail">yes</td></tr>
<tr><td>C++</td><td class="pass">yes</td><td class="pass">yes (C++11)*</td><td class="pass">yes</td><td class="pass">yes</td></tr>
<tr><td>Java</td><td class="pass">yes</td><td class="pass">yes*</td><td class="pass">yes</td><td class="pass">yes</td></tr>
<tr><td>C#</td><td class="pass">yes</td><td class="pass">yes*</td><td class="pass">yes</td><td class="pass">yes*</td></tr>
<tr><td>Go</td><td class="pass">yes</td><td class="pass">yes</td><td class="fail">no</td><td class="pass">yes*</td></tr>
<tr><td>Other languages</td><td class="pass">lots!</td><td class="warn">6+ others*</td><td class="fail">no</td><td class="fail">no</td></tr>
<tr><td>Authors' preferred use case</td><td>distributed<br>computing</td><td><a href="https://sandstorm.io">platforms /<br>sandboxing</a></td><td>financial<br>trading</td><td>games</td></tr>
</table>

\* Updated Dec 15, 2014 (Cap'n Proto 0.5.0).

**Schema Evolution**

All four protocols allow you to add new fields to a schema over time, without breaking backwards-compatibility. New fields will be ignored by old binaries, and new binaries will fill in a default value when reading old data.

SBE, however, as far as I can tell from reading the code, does not allow you to add new variable-width fields inside of a sub-object (group), as it is the application's responsibility to explicitly iterate over every variable-width field when reading. When an old app not knowing about the new nested field fails to cover it, its buffer pointer will get out-of-sync. Variable-width fields can be added to the topmost object since they'll end up at the end of the message, so there's no need for old code to traverse past them.

**Zero-copy**

The central thesis of all three competitors is that data should be structured the same way in-memory and on the wire, thus avoiding costly encode/decode steps.

Protobufs represents the old way of thinking.

**Random-access reads**

Can you traverse the message content in an arbitrary order? Relatedly, can you `mmap()` in a large (say, 2GB) file -- where the entire file is one enormous serialized message -- then traverse to and read one particular field without causing the entire file to be paged in from disk?

Protobufs does not allow this because the entire file must be parsed upfront before any of the content can be used. Even with a streaming Protobuf parser (which most libraries don't provide), you would at least need to parse all data appearing before the bit you want. The Protobuf documentation recommends splitting large files up into many small pieces and implementing some other framing format that allows seeking between them, but this is left entirely up to the app.

SBE does not allow random access because the message tree is written in preorder with no information that would allow one to skip over an entire sub-tree. While the primitive fields within a single object can be accessed in random order, sub-objects must be traversed strictly in preorder. SBE apparently chose to design around this restriction because sequential memory access is faster than random access, therefore this forces application code to be ordered to be as fast as possible. Similar to Protobufs, SBE recommends using some other framing format for large files.

Cap'n Proto permits random access via the use of pointers, exactly as in-memory data structures in C normally do. These pointers are not quite native pointers -- they are relative rather than absolute, to allow the message to be loaded at an arbitrary memory location.

FlatBuffers permits random access by having each record store a table of offsets to all of the field positions, and by using pointers between objects like Cap'n Proto does.

**Safe against malicious input**

Protobufs is carefully designed to be resiliant in the face of all kinds of malicious input, and has undergone a security review by Google's world-class security team. Not only is the Protobuf implementation secure, but the API is explicitly designed to discourage security mistakes in application code. It is considered a security flaw in Protobufs if the interface makes client apps likely to write insecure code.

Cap'n Proto inherits Protocol Buffers' security stance, and is believed to be similarly secure. However, it has not yet undergone security review.

SBE's C++ library does bounds checking as of the resolution of [this bug](https://github.com/real-logic/simple-binary-encoding/issues/130).

*Update July 12, 2014:* FlatBuffers [now supports](https://github.com/google/flatbuffers/commit/a0b6ffc25b9a3c726a21e52d6453779265186dbd) performing an optional upfront verification pass over a message to ensure that all pointers are in-bounds. You must explicitly call the verifier, otherwise no bounds checking is performed. The verifier performs a pass over the entire message; it should be very fast, but it is O(n), so you lose the "random access" advantage if you are mmap()ing in a very large file. FlatBuffers is primarily designed for use as a format for static, trusted data files, not network messages.

**Reflection / generic algorithms**

_Update: I originally failed to discover that SBE and FlatBuffers do in fact have reflection APIs. Sorry!_

Protobuf provides a "reflection" interface which allows dynamically iterating over all the fields of a message, getting their names and other metadata, and reading and modifying their values in a particular instance. Cap'n Proto also supports this, calling it the "Dynamic API". SBE provides the "OTF decoder" API with the usual SBE restriction that you can only iterate over the content in order. FlatBuffers has the `Parser` API in `idl.h`.

Having a reflection/dynamic API opens up a wide range of use cases. You can write reflection-based code which converts the message to/from another format such as JSON -- useful not just for interoperability, but for debugging, because it is human-readable. Another popular use of reflection is writing bindings for scripting languages. For example, Python's Cap'n Proto implementation is simply a wrapper around the C++ dynamic API. Note that you can do all these things with types that are not even known at compile time, by parsing the schemas at runtime.

The down side of reflection is that it is generally very slow (compared to generated code) and can lead to code bloat. Cap'n Proto is designed such that the reflection APIs need not be linked into your app if you do not use them, although this requires statically linking the library to get the benefit.

**Initialization order**

When building a message, depending on how your code is organized, it may be convenient to have flexibility in the order in which you fill in the data. If that flexibility is missing, you may find you have to do extra bookkeeping to store data off to the side until its time comes to be added to the message.

Protocol Buffers is naturally completely flexible in terms of initialization order because the message is being built on the heap. There is no reason to impose restrictions. (Although, the C++ Protobuf library heavily encourages top-down building.)

All the zero-copy systems, though, have to use some form of arena allocation to make sure that the message is built in a contiguous block of memory that can be written out all at once. So, things get more complicated.

SBE specifically requires the message tree to be written in preorder (though, as with reads, the primitive fields within a single object can be initialized in arbitrary order).

FlatBuffers requires that you completely finish one object before you can start building the next, because the size of an object depends on its content so the amount of space needed isn't known until it is finalized. This also implies that FlatBuffer messages must be built bottom-up, starting from the leaves.

Cap'n Proto imposes no ordering constraints. The size of an object is known when it is allocated, so more objects can be allocated immediately. Messages are normally built top-down, but bottom-up ordering is supported through the "orphans" API.

**Unknown field retention?**

Say you read in a message, then copy one sub-object of that message over to a sub-object of a new message, then write out the new message. Say that the copied object was created using a newer version of the schema than you have, and so contains fields you don't know about. Do those fields get copied over?

This question is extremely important for any kind of service that acts as a proxy or broker, forwarding messages on to others. It can be inconvenient if you have to update these middlemen every time a particular backend protocol changes, when the middlemen often don't care about the protocol details anyway.

When Protobufs sees an unknown field tag on the wire, it stores the value into the message's `UnknownFieldSet`, which can be copied and written back out later. (UPDATE: Apparently, version 3 of Protocol Buffers, aka "proto3", removes this feature. I honestly don't know what they're thinking. This feature has been absolutely essential in many of Google's internal systems.)

Cap'n Proto's wire format was very carefully designed to contain just enough information to make it possible to recursively copy its target from one message to another without knowing the object's schema. This is why Cap'n Proto pointers contain bits to indicate if they point to a struct or a list and how big it is -- seemingly redundant information.

SBE and FlatBuffers do not store any such type information on the wire, and thus it is not possible to copy an object without its schema. (Note that, however, if you are willing to require that the sender sends its full schema on the wire, you can always use reflection-based code to effectively make all fields known. This takes some work, though.)

**Object-capability RPC system**

Cap'n Proto features an object-capability RPC system. While this article is not intended to discuss RPC features, there is an important effect on the serialization format: in an object-capability RPC system, references to remote objects must be a first-class type. That is, a struct field's type can be "reference to remote object implementing RPC interface Foo".

Protobufs, SBC, and FlatBuffers do not support this type. Note that it is _not_ sufficient to simply store a string URL, or define some custom struct to represent a reference, because a proper capability-based RPC system must be aware of all references embedded in any message it sends. There are many reasons for this requirement, the most obvious of which is that the system must export the reference or change its permissions to make it available to the receiver.

**Schema language**

Protobufs, Cap'n Proto, and FlatBuffers have custom, concise schema languages.

SBE uses XML schemas, which are verbose.

**Usable as mutable state**

Protobuf generated classes have often been (ab)used as a convenient way to store an application's mutable internal state. There's mostly no problem with modifying a message gradually over time and then serializing it when needed.

This usage pattern does not work well with any zero-copy serialization format because these formats must use arena-style allocation to make sure the message is built in contiguous memory. Arena allocation has the property that you cannot free any object unless you free the entire arena. Therefore, when objects are discarded, the memory ends up leaked until the message as a whole is destroyed. A long-lived message that is modified many times will thus leak memory.

**Padding takes space on wire?**

Does the protocol tend to write a lot of zero-valued padding bytes to the wire?

This is a problem with zero-copy protocols: fixed-width integers tend to have a lot of zeros in the high-order bits, and padding sometimes needs to be inserted for alignment. This padding can easily double or triple the size of a message.

Protocol Buffers avoids padding by encoding integers using variable widths, which is only possible given a separate encoding/decoding step.

SBE and FlatBuffers leave the padding in to achieve zero-copy.

Cap'n Proto normally leaves the padding in, but comes with a built-in option to apply a very fast compression algorithm called "packing" which aims only to deflate zeros. This algorithm tends to achieve similar sizes to Protobufs while still being faster (and _much_ faster than general-purpose compression). In this mode, however, Cap'n Proto is no longer zero-copy.

Note that Cap'n Proto's packing algorithm would be appropriate for SBE and FlatBuffers as well. Feel free to steal it. :)

**Unset fields take space on wire?**

If a field has not been explicitly assigned a value, will it take any space on the wire?

Protobuf encodes tag-value pairs, so it simply skips pairs that have not been set.

Cap'n Proto and SBE position fields at fixed offsets from the start of the struct. The struct is always allocated large enough for all known fields according to the schema. So, unused fields waste space. (But Cap'n Proto's optional packing will tend to compress away this space.)

FlatBuffers uses a separate table of offsets (the vtable) to indicate the position of each field, with zero meaning the field isn't present. So, unset fields take no space on the wire -- although they do take space in the vtable. vtables can apparently be shared between instances where the offsets are all the same, amortizing this cost.

Of course, all this applies to primitive fields and pointer values, not the sub-objects to which those pointers point. All of these formats elide sub-objects that haven't been initialized.

**Pointers take space on wire?**

Do non-primitive fields require storing a pointer?

Protobufs uses tag-length-value for variable-width fields.

Cap'n Proto uses pointers for variable-width fields, so that the size of the parent object is independent of the size of any children. These pointers take some space on the wire.

SBE requires variable-width fields to be embedded in preorder, which means pointers aren't necessary.

FlatBuffers also uses pointers, even though most objects are variable-width, possibly because the vtables only store 16-bit offsets, limiting the size of any one object. However, note that FlatBuffers' "structs" (which are fixed-width and not extensible) are stored inline (what Cap'n Proto calls a "struct', FlatBuffer calls a "table").

**Platform Support**

As of Dec 15, 2014, Cap'n Proto supports a superset of the languages supported by FlatBuffers and
SBE, but is still far behind Protocol Buffers.

While Cap'n Proto C++ is well-supported on POSIX platforms using GCC or Clang as their compiler,
Cap'n Proto has only limited support for Visual C++: the basic serialization library works, but
reflection and RPC do not yet work. Support will be expanded once Visual Studio's C++ compiler
completes support for C++11.

In comparison, SBE and FlatBuffers have reflection interfaces that work in Visual C++, though
neither one has built-in RPC. Reflection is critical for certain use cases, but the majority of
users won't need it.

(This section has been updated. When originally written, Cap'n Proto did not support MSVC at all.)

### Benchmarks?

I do not provide benchmarks. I did not provide them when I launched Protobufs, nor when I launched Cap'n Proto, even though I had some with nice numbers (which you can find in git). And I don't see any reason to start now.

Why? Because they would tell you nothing. I could easily construct a benchmark to make any given library "win", by exploiting the relative tradeoffs each one makes. I can even construct one where Protobufs -- supposedly infinitely slower than the others -- wins.

The fact of the matter is that the relative performance of these libraries depends deeply on the use case. To know which one will be fastest for _your_ project, you really need to benchmark them in _your_ project, end-to-end. No contrived benchmark will give you the answer.

With that said, my intuition is that SBE will probably edge Cap'n Proto and FlatBuffers on performance in the average case, due to its decision to forgo support for random access. Between Cap'n Proto and FlatBuffers, it's harder to say. FlatBuffers' vtable approach seems like it would make access more expensive, though its simpler pointer format may be cheaper to follow. FlatBuffers also appears to do a lot of bookkeeping at encoding time which could get costly (such as de-duping vtables), but I don't know how costly.

For most people, the performance difference is probably small enough that qualitative (feature) differences in the libraries matter more.




_posts/2014-12-15-capnproto-0.5-generics-msvc-java-csharp.md
--------------------------------------
---
layout: post
title: "Cap'n Proto 0.5: Generics, Visual C++, Java, C#, Sandstorm.io"
author: kentonv
---

Today we're releasing Cap'n Proto 0.5. We've added lots of goodies!

### Finally: Visual Studio

Microsoft Visual Studio 2015 (currently in "preview") finally supports enough C++11 to get Cap'n
Proto working, and we've duly added official support for it!

Not all features are supported yet. The core serialization functionality sufficient for 90% of users
is available, but reflection and RPC APIs are not. We will turn on these APIs as soon as Visual C++
is ready (the main blocker is incomplete `constexpr` support).

As part of this, we now support CMake as a build system, and it can be used on Unix as well.

In related news, for Windows users not interested in C++ but who need the Cap'n Proto tools for
other languages, we now provide precompiled Windows binaries. See
[the installation page]({{site.baseurl}}install.html).

I'd like to thank [Bryan Boreham](https://github.com/bboreham),
[Joshua Warner](https://github.com/joshuawarner32), and [Phillip Quinn](https://github.com/pqu) for
their help in getting this working.

### C#, Java

While not strictly part of this release, our two biggest missing languages recently gained support
for Cap'n Proto:

* [Marc Gravell](https://github.com/mgravell) -- the man responsible for the most popular C#
  implementation of Protobufs -- has now implemented
  [Cap'n Proto in C#](https://github.com/mgravell/capnproto-net).
* [David Renshaw](https://github.com/dwrensha), author of our existing Rust implementation and
  [Sandstorm.io](https://sandstorm.io) core developer, has implemented
  [Cap'n Proto in Java](https://github.com/dwrensha/capnproto-java).

### Generics

Cap'n Proto now supports [generics]({{site.baseurl}}language.html#generic-types),
in the sense of Java generics or C++ templates. While working on
[Sandstorm.io](https://sandstorm.io) we frequently found that we wanted this, and it turned out
to be easy to support.

This is a feature which Protocol Buffers does not support and likely never will. Cap'n Proto has a
much easier time supporting exotic language features because the generated code is so simple. In
C++, nearly all Cap'n Proto generated code is inline accessor methods, which can easily become
templates. Protocol Buffers, in contrast, has generated parse and serialize functions and a host
of other auxiliary stuff, which is too complex to inline and thus would need to be adapted to
generics without using C++ templates. This would get ugly fast.

Generics are not yet supported by all Cap'n Proto language implementations, but where they are not
supported, things degrade gracefully: all type parameters simply become `AnyPointer`. You can still
use generics in your schemas as documentation. Meanwhile, at least our C++, Java, and Python
implementations have already been updated to support generics, and other implementations that
wrap the C++ reflection API are likely to work too.

### Canonicalization

0.5 introduces a (backwards-compatible) change in
[the way struct lists should be encoded]({{site.baseurl}}encoding.html#lists), in
order to support [canonicalization]({{site.baseurl}}encoding.html#canonicalization).
We believe this will make Cap'n Proto more appropriate for use in cryptographic protocols. If
you've implemented Cap'n Proto in another language, please update your code!

### Sandstorm and Capability Systems

[Sandstorm.io](https://sandstorm.io) is Cap'n Proto's parent project: a platform for personal
servers that is radically easier and more secure.

Cap'n Proto RPC is the underlying communications layer powering Sandstorm. Sandstorm is a
[capability system](http://www.erights.org/elib/capability/overview.html): applications can send
each other object references and address messages to those objects. Messages can themselves contain
new object references, and the recipient implicitly gains permission to use any object reference
they receive. Essentially, Sandstorm allows the interfaces between two apps, or between and app
and the platform, to be designed using the same vocabulary as interfaces between objects or
libraries in an object-oriented programming language (but
[without the mistakes of CORBA or DCOM]({{site.baseurl}}rpc.html#distributed-objects)).
Cap'n Proto RPC is at the core of this.

This has powerful implications: Consider the case of service discovery. On Sandstorm, all
applications start out isolated from each other in secure containers. However, applications can
(or, will be able to) publish Cap'n Proto object references to the system representing APIs they
support. Then, another app can make a request to the system, saying "I need an object that
implements interface Foo". At this point, the system can display a picker UI to the user,
presenting all objects the user owns that satisfy the requirement. However, the requesting app only
ever receives a reference to the object the user chooses; all others remain hidden. Thus, security
becomes "automatic". The user does not have to edit an ACL on the providing app, nor copy around
credentials, nor even answer any security question at all; it all derives automatically and
naturally from the user's choices. We call this interface "The Powerbox".

Moreover, because Sandstorm is fully aware of the object references held by every app, it will
be able to display a visualization of these connections, allowing a user to quickly see which of
their apps have access to each other and even revoke connections that are no longer desired with
a mouse click.

Cap'n Proto 0.5 introduces primitives to support "persistent" capabilities -- that is, the ability
to "save" an object reference to disk and then restore it later, on a different connection.
Obviously, the features described above totally depend on this feature.

The next release of Cap'n Proto is likely to include another feature essential for Sandstorm: the
ability to pass capabilities from machine to machine and have Cap'n Proto automatically form direct
connections when you do. This allows servers running on different machines to interact with each
other in a completely object-oriented way. Instead of passing around URLs (which necessitate a
global namespace, lifetime management, firewall traversal, and all sorts of other obstacles), you
can pass around capabilities and not worry about it. This will be central to Sandstorm's strategies
for federation and cluster management.

### Other notes

* The C++ RPC code now uses `epoll` on Linux.
* We now test Cap'n Proto on Android and MinGW, in addition to Linux, Mac OSX, Cygwin, and Visual
  Studio. (iOS and FreeBSD are also reported to work, though are not yet part of our testing
  process.)



_posts/2015-01-23-capnproto-0.5.1-bugfixes.md
--------------------------------------
---
layout: post
title: "Cap'n Proto 0.5.1: Bugfixes"
author: kentonv
---

Cap'n Proto 0.5.1 has just been released with some bug fixes:

* On Windows, the `capnp` tool would crash when it tried to generate an ID, e.g. when using `capnp id` or when compiling a file that was missing the file ID, because it tried to get random bytes from `/dev/urandom`, which of course doesn't exist on Windows. Oops. Now it uses `CryptGenRandom()`.
* Declaring a generic method (with method-specific type parameters) inside a generic interface generated code that didn't compile.
* `joinPromises()` didn't work on an array of `Promise<void>`.
* Unnecessary error messages were being printed to the console when RPC clients disconnected.

Sorry about the bugs.

In other news, as you can see, the Cap'n Proto web site now lives at `capnproto.org`. Additionally, the Github repo has been moved to the [Sandstorm.io organization](https://github.com/capnproto). Both moves have left behind redirects so that old links / repository references should continue to work.



_posts/2015-03-02-security-advisory-and-integer-overflow-protection.md
--------------------------------------
---
layout: post
title: "Security Advisory -- And how to catch integer overflows with template metaprogramming"
author: kentonv
---

As the installation page has always stated, I do not yet recommend using Cap'n Proto's C++ library for handling possibly-malicious input, and will not recommend it until it undergoes a formal security review. That said, security is obviously a high priority for the project. The security of Cap'n Proto is in fact essential to the security of [Sandstorm.io](https://sandstorm.io), Cap'n Proto's parent project, in which sandboxed apps communicate with each other and the platform via Cap'n Proto RPC.

A few days ago, the first major security bugs were found in Cap'n Proto C++ -- two by security guru [Ben Laurie](http://en.wikipedia.org/wiki/Ben_Laurie) and one by myself during subsequent review (see below). You can read details about each bug in our new [security advisories directory](https://github.com/capnproto/capnproto/tree/master/security-advisories):

* [Integer overflow in pointer validation.](https://github.com/capnproto/capnproto/tree/master/security-advisories/2015-03-02-0-c++-integer-overflow.md)
* [Integer underflow in pointer validation.](https://github.com/capnproto/capnproto/tree/master/security-advisories/2015-03-02-1-c++-integer-underflow.md)
* [CPU usage amplification attack.](https://github.com/capnproto/capnproto/tree/master/security-advisories/2015-03-02-2-all-cpu-amplification.md)

I have backported the fixes to the last two release branches -- 0.5 and 0.4:

- Release 0.5.1.1: [source](https://capnproto.org/capnproto-c++-0.5.1.1.tar.gz), [win32](https://capnproto.org/capnproto-c++-win32-0.5.1.1.zip)
- Release 0.4.1.1: [source](https://capnproto.org/capnproto-c++-0.4.1.1.tar.gz)

Note that we added a "nano" component to the version number (rather than use 0.5.2/0.4.2) to indicate that this release is ABI-compatible with the previous release. If you are linking Cap'n Proto as a shared library, you only need to update the library, not re-compile your app.

To be clear, the first two bugs affect only the C++ implementation of Cap'n Proto; implementations in other languages are likely safe. The third bug probably affects all languages, and as of this writing only the C++ implementation (and wrappers around it) is fixed. However, this third bug is not as serious as the other two.

### Preventative Measures

It is our policy that any time a security problem is found, we will not only fix the problem, but also implement new measures to prevent the class of problems from occurring again. To that end, here's what we're doing doing to avoid problems like these in the future:

1. A fuzz test of each pointer type has been added to the standard unit test
   suite.
2. We will additionally add fuzz testing with American Fuzzy Lop to our
   extended test suite.
3. In parallel, we will extend our use of template metaprogramming for
   compile-time unit analysis (kj::Quantity in kj/units.h) to also cover
   overflow detection (by tracking the maximum size of an integer value across
   arithmetic expressions and raising an error when it overflows). More on this
   below.
4. We will continue to require that all tests (including the new fuzz test) run
   cleanly under Valgrind before each release.
5. We will commission a professional security review before any 1.0 release.
   Until that time, we continue to recommend against using Cap'n Proto to
   interpret data from potentially-malicious sources.

I am pleased to report that measures 1, 2, and 3 all detected both integer overflow/underflow problems, and AFL additionally detected the CPU amplification problem.

### Integer Overflow is Hard

Integer overflow is a nasty problem.

In the past, C and C++ code has been plagued by buffer overrun bugs, but these days, systems engineers have mostly learned to avoid them by simply never using static-sized buffers for dynamically-sized content. If we don't see proof that a buffer is the size of the content we're putting in it, our "spidey sense" kicks in.

But developing a similar sense for integer overflow is hard. We do arithmetic in code all the time, and the vast majority of it isn't an issue. The few places where overflow can happen all too easily go unnoticed.

And by the way, integer overflow affects many memory-safe languages too! Java and C# don't protect against overflow. Python does, using slow arbitrary-precision integers. JavaScript doesn't use integers, and is instead succeptible to loss-of-precision bugs, which can have similar (but more subtle) consequences.

While writing Cap'n Proto, I made sure to think carefully about overflow and managed to correct for it most of the time. On learning that I missed a case, I immediately feared that I might have missed many more, and wondered how I might go about systematically finding them.

Fuzz testing -- e.g. using [American Fuzzy Lop](http://lcamtuf.coredump.cx/afl/) -- is one approach, and is indeed how Ben found the two bugs he reported. As mentioned above, we will make AFL part of our release process in the future. However, AFL cannot really _prove_ anything -- it can only try lots of possibilities. I want my compiler to refuse to compile arithmetic which might overflow.

### Proving Safety Through Template Metaprogramming

C++ Template Metaprogramming is powerful -- many would say _too_ powerful. As it turns out, it's powerful enough to do what we want.

I defined a new type:

{% highlight C++ %}
template <uint64_t maxN, typename T>
class Guarded {
  // Wraps T (a basic integer type) and statically guarantees
  // that the value can be no more than `maxN` and no less than
  // zero.

  static_assert(maxN <= T(kj::maxValue), "possible overflow detected");
  // If maxN is not representable in type T, we can no longer
  // guarantee no overflows.

public:
  // ...

  template <uint64_t otherMax, typename OtherT>
  inline constexpr Guarded(const Guarded<otherMax, OtherT>& other)
      : value(other.value) {
    // You cannot construct a Guarded from another Guarded
    // with a higher maximum.
    static_assert(otherMax <= maxN, "possible overflow detected");
  }

  // ...

  template <uint64_t otherMax, typename otherT>
  inline constexpr Guarded<guardedAdd<maxN, otherMax>(),
                           decltype(T() + otherT())>
      operator+(const Guarded<otherMax, otherT>& other) const {
    // Addition operator also computes the new maximum.
    // (`guardedAdd` is a constexpr template that adds two
    // constants while detecting overflow.)
    return Guarded<guardedAdd<maxN, otherMax>(),
                   decltype(T() + otherT())>(
        value + other.value, unsafe);
  }

  // ...

private:
  T value;
};
{% endhighlight %}

So, a `Guarded<10, int>` represents a `int` which is statically guaranteed to hold a non-negative value no greater than 10. If you add a `Guarded<10, int>` to `Guarded<15, int>`, the result is a `Guarded<25, int>`. If you try to initialize a `Guarded<10, int>` from a `Guarded<25, int>`, you'll trigger a `static_assert` -- the compiler will complain. You can, however, initialize a `Guarded<25, int>` from a `Guarded<10, int>` with no problem.

Moreover, because all of `Guarded`'s operators are inline and `constexpr`, a good optimizing compiler will be able to optimize `Guarded` down to the underlying primitive integer type. So, in theory, using `Guarded` has no runtime overhead. (I have not yet verified that real compilers get this right, but I suspect they do.)

Of course, the full implementation is considerably more complicated than this. The code has not been merged into the Cap'n Proto tree yet as we need to do more analysis to make sure it has no negative impact. For now, you can find it in the [overflow-safe](https://github.com/capnproto/capnproto/tree/overflow-safe) branch, specifically in the second half of [kj/units.h](https://github.com/capnproto/capnproto/blob/overflow-safe/c++/src/kj/units.h). (This header also contains metaprogramming for compile-time unit analysis, which Cap'n Proto has been using since its first release.)

### Results

I switched Cap'n Proto's core pointer validation code (`capnp/layout.c++`) over to `Guarded`. In the process, I found:

* Several overflows that could be triggered by the application calling methods with invalid parameters, but not by a remote attacker providing invalid message data. We will change the code to check these in the future, but they are not critical security problems.
* The overflow that Ben had already reported ([2015-03-02-0](https://github.com/capnproto/capnproto/tree/master/security-advisories/2015-03-02-0-c++-integer-overflow.md)). I had intentionally left this unfixed during my analysis to verify that `Guarded` would catch it.
* One otherwise-undiscovered integer underflow ([2015-03-02-1](https://github.com/capnproto/capnproto/tree/master/security-advisories/2015-03-02-1-c++-integer-underflow.md)).

Based on these results, I conclude that `Guarded` is in fact effective at finding overflow bugs, and that such bugs are thankfully _not_ endemic in Cap'n Proto's code.

With that said, it does not seem practical to change every integer throughout the Cap'n Proto codebase to use `Guarded` -- using it in the API would create too much confusion and cognitive overhead for users, and would force application code to be more verbose. Therefore, this approach unfortunately will not be able to find all integer overflows throughout the entire library, but fortunately the most sensitive parts are covered in `layout.c++`.

### Why don't programming languages do this?

Anything that can be implemented in C++ templates can obviously be implemented by the compiler directly. So, why have so many languages settled for either modular arithmetic or slow arbitrary-precision integers?

Languages could even do something which my templates cannot: allow me to declare relations between variables. For example, I would like to be able to declare an integer whose value is less than the size of some array. Then I know that the integer is a safe index for the array, without any run-time check.

Obviously, I'm not the first to think of this. "Dependent types" have been researched for decades, but we have yet to see a practical language supporting them. Apparently, something about them is complicated, even though the rules look like they should be simple enough from where I'm standing.

Some day, I would like to design a language that gets this right. But for the moment, I remain focused on [Sandstorm.io](https://sandstorm.io). Hopefully someone will beat me to it. Hint hint.



_posts/2015-03-05-another-cpu-amplification.md
--------------------------------------
---
layout: post
title: "Another security advisory -- Additional CPU amplification case"
author: kentonv
---

Unfortunately, it turns out that our fix for one of [the security advisories issued on Monday](2015-03-02-security-advisory-and-integer-overflow-protection.html) was not complete.

Fortunately, the incomplete fix is for the non-critical vulnerability. The worst case is that an attacker could consume excessive CPU time.

Nevertheless, we've issued [a new advisory](https://github.com/capnproto/capnproto/tree/master/security-advisories/2015-03-05-0-c++-addl-cpu-amplification.md) and pushed a new release:

- Release 0.5.1.2: [source](https://capnproto.org/capnproto-c++-0.5.1.2.tar.gz), [win32](https://capnproto.org/capnproto-c++-win32-0.5.1.2.zip)
- Release 0.4.1.2: [source](https://capnproto.org/capnproto-c++-0.4.1.2.tar.gz)

Sorry for the rapid repeated releases, but we don't like sitting on security bugs.



_posts/2017-05-01-capnproto-0.6-msvc-json-http-more.md
--------------------------------------
---
layout: post
title: "Cap'n Proto 0.6 Released: Two and a half years of improvements"
author: kentonv
---

<div style="float: right"><a class="block_link" style="color: #fff"
href="{{site.baseurl}}install.html">Get it now &raquo;</a></div>

Today we're releasing Cap'n Proto 0.6, the first major Cap'n Proto release in nearly 2.5 years.

Cap'n Proto has been under active development the entire time, as part of its parent project, [Sandstorm.io](https://sandstorm.io). The lack of releases did not indicate a lack of development, but rather a lack of keeping the code running on every platform it supports -- especially Windows. Without a working Windows build, we couldn't do a release. But as Sandstorm didn't need Windows, it was hard to prioritize -- that is, until contributors stepped up!

Note that this release encompasses the core tools and the C++ reference implementation. Implementations in other languages have their own release schedules, but it's likely that several will be updated soon to integrate new language features.

### Brought to you by Cloudflare

[As announced on the Sandstorm blog](https://sandstorm.io/news/2017-03-13-joining-cloudflare), most of the Sandstorm team (including myself) now work for [Cloudflare](https://cloudflare.com). Cloudflare is one of the largest users of Cap'n Proto, [as described in this talk by John-Graham Cumming](https://youtu.be/LA-gNoxSLCE?t=12m47s), and as such maintaining Cap'n Proto is part of my job at Cloudflare.

<div style="text-align: center"><a class="block_link" style="color: #fff"
href="https://www.meetup.com/Sandstorm-SF-Bay-Area/events/239341254/">Come to our release party May 18 at Cloudflare SF</a></div>

### What's New?

#### Full Windows / Visual Studio Support

With this release, all of Cap'n Proto's functionality now works on Windows with Visual Studio 2015 and 2017. That includes the serialization, dynamic API, schema parser, async I/O framework (using I/O completion ports), RPC, and tools. This is a huge step up from 0.5, in which Cap'n Proto could only be built in "lite mode", which supported only basic serialization.

Most of the work to make this happen was contributed by [**Harris Hancock**](https://github.com/harrishancock) (with some help from [Gordon McShane](https://github.com/gordonmcshane), [Mark Grimes](https://github.com/mark-grimes), myself, and others). It was no small feat: Visual Studio's C++ compiler is still quite buggy, so lots of work-arounds were needed. Meanwhile, the Cap'n Proto developers working on Linux were continuously introducing new issues with their changes. Harris sorted it all out and delivered a beautiful series of patches. He also helped get us set up with [continuous integration on AppVeyor](https://ci.appveyor.com/project/kentonv/capnproto), so that we can stay on top of these issues going forward.

#### Security Hardening

The 0.6 release includes a number of measures designed to harden Cap'n Proto's C++ implementation against possible security bugs. These include:

* The core pointer validation code has been refactored to detect possible integer overflows at compile time using C++ template metaprogramming, as [described in this old blog post](https://capnproto.org/news/2015-03-02-security-advisory-and-integer-overflow-protection.html).
* The core test suite -- which runs when you type `make check` -- now includes a targeted fuzz test of the pointer validation code.
* We additionally tested this release using [American Fuzzy Lop](http://lcamtuf.coredump.cx/afl/), running several different test cases for over three days each.

#### JSON converter

Cap'n Proto messages can now be converted to and from JSON using `libcapnp-json`. This makes it easy to integrate your JSON front-end API with your Cap'n Proto back-end.

See the <code><a href="https://github.com/capnproto/capnproto/blob/master/c++/src/capnp/compat/json.h">capnp/compat/json.h</a></code> header for API details.

This library was primarily built by [**Kamal Marhubi**](https://github.com/kamalmarhubi) and [**Branislav Katreniak**](https://github.com/katreniak), using Cap'n Proto's [dynamic API]({{site.baseurl}}cxx.html#dynamic-reflection).

#### HTTP library

KJ (the C++ framework library bundled with Cap'n Proto) now ships with a minimalist HTTP library, `libkj-http`. The library is based on the KJ asynchronous I/O framework and covers both client-side and server-side use cases. Although functional and used in production today, the library should be considered a work in progress -- expect improvements in future releases, such as client connection pooling and TLS support.

See the <code><a href="https://github.com/capnproto/capnproto/blob/master/c++/src/kj/compat/http.h">kj/compat/http.h</a></code> header for API details.

#### Smaller things

With two years of development, there are far too many changes to list, but here are some more things:

* KJ now offers its own unit test framework under `kj/test.h`, as well as a compatibility shim with Google Test under `kj/compat/gtest.h`. The KJ and Cap'n Proto tests no longer depend on Google Test.
* New API `capnp::TextCodec` in `capnp/serialize-text.h` provides direct access to parse text-format Cap'n Proto messages (requires `libcapnpc`, the schema parser library). (Contributed by: [**Philip Quinn**](https://github.com/pqu))
* It is possible to compare Cap'n Proto messages for equality (with correct handling of unknown fields, something Protocol Buffers struggled with) using APIs in `capnp/any.h`. (Contributed by: [**Joshua Warner**](https://github.com/joshuawarner32))
* A function `capnp::canonicalize()` has been added which returns the canonical serialization of a given struct. (Contributed by: [**Matthew Maurer**](https://github.com/maurer))
* `AnyPointer` fields can now be assigned in constant values, by referencing another named constant (which itself is defined with a specific type).
* In addition to `AnyPointer`, the types `AnyStruct`, `AnyList`, and `Capability` can now be used in schemas.
* New class `capnp::CapabilityServerSet` in `capnp/capability.h` allows an RPC server to detect when capabilities to its own local objects are passed back to it and allows it to "unwrap" them to get at the underlying native object.
* A membrane framework library was added (header `capnp/membrane.h`). This makes it easy to set up a MITM layer between RPC actors, e.g. to implement revocability, transformations, and many other useful capability patterns.
* Basic flow control can now be applied to an RPC connection, preventing new messages from being accepted if outstanding calls exceed a certain watermark, which helps prevent excessive buffering / malicious resource exhaustion. See `RpcSystem::setFlowLimit()`.
* KJ's networking API now includes datagram protocols (UDP).
* In `.capnp` syntax, all comma-delimited lists can now have a trailing comma. (Contributed by: [**Drew Fisher**](https://github.com/zarvox))
* Hundreds more small feature additions and bug fixes.

<div style="text-align: center"><a class="block_link" style="color: #fff; width: 45%"
href="{{site.baseurl}}install.html">Download &raquo;</a> <a class="block_link" style="color: #fff; width: 45%"
href="https://www.meetup.com/Sandstorm-SF-Bay-Area/events/239341254/">Release Party &raquo;</a></div>



_posts/2018-08-28-capnproto-0.7.md
--------------------------------------
---
layout: post
title: "Cap'n Proto 0.7 Released"
author: kentonv
---

<div style="float: right"><a class="block_link" style="color: #fff"
href="{{site.baseurl}}install.html">Get it now &raquo;</a></div>

Today we're releasing Cap'n Proto 0.7.

### As used in Cloudflare Workers

The biggest high-level development in Cap'n Proto since the last release is its use in the implementation of [Cloudflare Workers](https://blog.cloudflare.com/cloudflare-workers-unleashed/) (of which I am the tech lead).

Cloudflare operates a global network of 152 datacenters and growing, and Cloudflare Workers allows you to deploy "serveless" JavaScript to all of those locations in under 30 seconds. Your code is written against the W3C standard [Service Workers API](https://developer.mozilla.org/en-US/docs/Web/API/Service_Worker_API) and handles HTTP traffic for your web site.

The Cloudflare Workers runtime implementation is written in C++, leveraging the V8 JavaScript engine and libKJ, the C++ toolkit library distributed with Cap'n Proto.

Cloudflare Workers are all about handling HTTP traffic, and the runtime uses KJ's HTTP library to do it. This means the KJ HTTP library is now battle-tested in production. Every package downloaded from [npm](https://npmjs.org), for example, passes through KJ's HTTP client and server libraries on the way (since npm uses Workers).

The Workers runtime makes heavy use of KJ, but so far only makes light use of Cap'n Proto serialization. Cap'n Proto is used as a format for distributing configuration as well as (ironically) to handle JSON. We anticipate, however, making deeper use of Cap'n Proto in the future, including RPC.

### What else is new?

* The C++ library now requires C++14 or newer. It requires GCC 4.9+, Clang 3.6+, or Microsoft Visual Studio 2017. This change allows us to make faster progress and provide cleaner APIs by utilizing newer language features.
* The JSON parser now supports [annotations to customize conversion behavior](https://github.com/capnproto/capnproto/blob/master/c++/src/capnp/compat/json.capnp). These allow you to override field names (e.g. to use underscores instead of camelCase), flatten sub-objects, and express unions in various more-idiomatic ways.
* The KJ HTTP library supports WebSockets, and has generally become much higher-quality as it has been battle-tested in Cloudflare Workers.
* KJ now offers its own [hashtable- and b-tree-based container implementations](https://github.com/capnproto/capnproto/blob/master/c++/src/kj/map.h). `kj::HashMap` is significantly faster and more memory-efficient than `std::unordered_map`, with more optimizations coming. `kj::TreeMap` is somewhat slower than `std::map`, but uses less memory and has a smaller code footprint. Both are implemented on top of `kj::Table`, a building block that can also support multi-maps. Most importantly, all these interfaces are cleaner and more modern than their ancient STL counterparts.
* KJ now includes [TLS bindings](https://github.com/capnproto/capnproto/blob/master/c++/src/kj/compat/tls.h). `libkj-tls` wraps OpenSSL or BoringSSL and provides a simple, hard-to-mess-up API integrated with the KJ event loop.
* KJ now includes [gzip bindings](https://github.com/capnproto/capnproto/blob/master/c++/src/kj/compat/gzip.h), which wrap zlib in KJ stream interfaces (sync and async).
* KJ now includes [helpers for encoding/decoding Unicode (UTF-8/UTF-16/UTF-32), base64, hex, URI-encoding, and C-escaped text](https://github.com/capnproto/capnproto/blob/master/c++/src/kj/encoding.h).
* The [`kj::Url` helper class](https://github.com/capnproto/capnproto/blob/master/c++/src/kj/compat/url.h) is provided to parse and compose URLs.
* KJ now includes [a filesystem API](https://github.com/capnproto/capnproto/blob/master/c++/src/kj/filesystem.h) which is designed to be resistant to path injection attacks, is dependency-injection-friendly to ease unit testing, is cross-platform (Unix and Windows), makes atomic file replacement easy, makes mmap easy, and [other neat features](https://github.com/capnproto/capnproto/pull/384).
* The `capnp` tool now has a `convert` command which can be used to convert between all known message encodings, such as binary, packed, text, JSON, canonical, etc. This obsoletes the old `encode` and `decode` commands.
* Many smaller tweaks and bug fixes.

<div style="text-align: center"><a class="block_link" style="color: #fff; width: 45%"
href="{{site.baseurl}}install.html">Download &raquo;</a></div>



_posts/2020-04-23-capnproto-0.8.md
--------------------------------------
---
layout: post
title: "Cap'n Proto 0.8: Streaming flow control, HTTP-over-RPC, fibers, etc."
author: kentonv
---

<div style="float: right"><a class="block_link" style="color: #fff"
href="{{site.baseurl}}install.html">Get it now &raquo;</a></div>

Today I'm releasing Cap'n Proto 0.8.

### What's new?

* [Multi-stream Flow Control](#multi-stream-flow-control)
* [HTTP-over-Cap'n-Proto](#http-over-capn-proto)
* [KJ improvements](#kj-improvements)
* Lots and lots of minor tweaks and fixes.

#### Multi-stream Flow Control

It is commonly believed, wrongly, that Cap'n Proto doesn't support "streaming", in the way that gRPC does. In fact, Cap'n Proto's object-capability model and promise pipelining make it much more expressive than gRPC. In Cap'n Proto, "streaming" is just a pattern, not a built-in feature.

Streaming is accomplished by introducing a temporary RPC object as part of a call. Each streamed message becomes a call to the temporary object. Think of this like providing a callback function in an object-oriented language.

For instance, server -> client streaming ("returning multiple responses") can look like this:

{% highlight capnp %}
# NOT NEW: Server -> client streaming example.
interface MyInterface {
  streamingCall @0 (callback :Callback) -> ();

  interface Callback {
    sendChunk @0 (chunk :Data) -> ();
  }
}
{% endhighlight %}

Or for client -> server streaming, the server returns a callback:

{% highlight capnp %}
# NOT NEW: Client -> Server streaming example.
interface MyInterface {
  streamingCall @0 () -> (callback :Callback);

  interface Callback {
    sendChunk @0 (chunk :Data) -> ();
  }
}
{% endhighlight %}

Note that the client -> server example relies on [promise pipelining](https://capnproto.org/rpc.html#time-travel-promise-pipelining): When the client invokes `streamingCall()`, it does NOT have to wait for the server to respond before it starts making calls to the `callback`. Using promise pipelining (which has been a built-in feature of Cap'n Proto RPC since its first release in 2013), the client sends messages to the server that say: "Once my call to `streamingCall()` call is finished, take the returned callback and call this on it."

Obviously, you can also combine the two examples to create bidirectional streams. You can also introduce "callback" objects that have multiple methods, methods that themselves return values (maybe even further streaming callbacks!), etc. You can send and receive multiple new RPC objects in a single call. Etc.

But there has been one problem that arises in the context of streaming specifically: flow control. Historically, if an app wanted to stream faster than the underlying network connection would allow, then it could end up queuing messages in memory. Worse, if other RPC calls were happening on the same connection concurrently, they could end up blocked behind these queued streaming calls.

In order to avoid such problems, apps needed to implement some sort of flow control strategy. An easy strategy was to wait for each `sendChunk()` call to return before starting the next call, but this would incur an unnecessary network round trip for each chunk. A better strategy was for apps to allow multiple concurrent calls, but only up to some limit before waiting for in-flight calls to return. For example, an app could limit itself to four in-flight stream calls at a time, or to 64kB worth of chunks.

This sort of worked, but there were two problems. First, this logic could get pretty complicated, distracting from the app's business logic. Second, the "N-bytes-in-flight-at-a-time" strategy only works well if the value of N is close to the [bandwidth-delay product (BDP)](https://en.wikipedia.org/wiki/Bandwidth-delay_product) of the connection. If N was chosen too low, the connection would be under-utilized. If too high, it would increase queuing latency for all users of the connection.

Cap'n Proto 0.8 introduces a built-in feature to manage flow control. Now, you can declare your streaming calls like this:

{% highlight capnp %}
interface MyInterface {
  streamingCall @0 (callback :Callback) -> ();

  interface Callback {
    # NEW: This streaming call features flow control!
    sendChunk @0 (chunk :Data) -> stream;
    done @1 ();
  }
}
{% endhighlight %}

Methods declared with `-> stream` behave like methods with empty return types (`-> ()`), but with special behavior when the call is sent over a network connection. Instead of waiting for the remote site to respond to the call, the Cap'n Proto client library will act as if the call has "returned" as soon as it thinks the app should send the next call. So, now the app can use a simple loop that calls `sendChunk()`, waits for it to "complete", then sends the next chunk. Each call will appear to "return immediately" until such a time as Cap'n Proto thinks the connection is fully-utilized, and then each call will block until space frees up.

When using streaming, it is important that apps be aware that error handling works differently. Since the client side may indicate completion of the call before the call has actually executed on the server, any exceptions thrown on the server side obviously cannot propagate to the client. Instead, we introduce a new rule: If a streaming call ends up throwing an exception, then all later method invocations on the same object (streaming or not) will also throw the same exception. You'll notice that we added a `done()` method to the callback interface above. After completing all streaming calls, the caller _must_ call `done()` to check for errors. If any previous streaming call failed, then `done()` will fail too.

Under the hood, Cap'n Proto currently implements flow control using a simple hack: it queries the send buffer size of the underlying network socket, and sets that as the "window size" for each stream. The operating system will typically increase the socket buffer as needed to match the TCP congestion window, and Cap'n Proto's streaming window size will increase to match. This is not a very good implementation for a number of reasons. The biggest problem is that it doesn't account for proxying: with Cap'n Proto it is common to pass objects through multiple nodes, which automatically arranges for calls to the object to be proxied though the middlemen. But, the TCP socket buffer size only approximates the BDP of the first hop. A better solution would measure the end-to-end BDP using an algorithm like [BBR](https://queue.acm.org/detail.cfm?id=3022184). Expect future versions of Cap'n Proto to improve on this.

Note that this new feature does not come with any change to the underlying RPC protocol! The flow control behavior is implemented entirely on the client side. The `-> stream` declaration in the schema is merely a hint to the client that it should use this behavior. Methods declared with `-> stream` are wire-compatible with methods declared with `-> ()`. Currently, flow control is only implemented in the C++ library. RPC implementations in other languages will treat `-> stream` the same as `-> ()` until they add explicit support for it. Apps in those languages will need to continue doing their own flow control in the meantime, as they did before this feature was added.

#### HTTP-over-Cap'n-Proto

Cap'n Proto 0.8 defines [a protocol for tunnelling HTTP calls over Cap'n Proto RPC](https://github.com/capnproto/capnproto/blob/master/c++/src/capnp/compat/http-over-capnp.capnp), along with an [adapter library](https://github.com/capnproto/capnproto/blob/master/c++/src/capnp/compat/http-over-capnp.h) adapting it to the [KJ HTTP API](https://github.com/capnproto/capnproto/blob/master/c++/src/kj/compat/http.h). Thus, programs written to send or receive HTTP requests using KJ HTTP can easily be adapted to communicate over Cap'n Proto RPC instead. It's also easy to build a proxy that converts regular HTTP protocol into Cap'n Proto RPC and vice versa.

In principle, http-over-capnp can achieve similar advantages to HTTP/2: Multiple calls can multiplex over the same connection with arbitrary ordering. But, unlike HTTP/2, calls can be initiated in either direction, can be addressed to multiple virtual endpoints (without relying on URL-based routing), and of course can be multiplexed with non-HTTP Cap'n Proto traffic.

In practice, however, http-over-capnp is new, and should not be expected to perform as well as mature HTTP/2 implementations today. More work is needed.

We use http-over-capnp in [Cloudflare Workers](https://workers.cloudflare.com/) to communicate HTTP requests between components of the system, especially into and out of sandboxes. Using this protocol, instead of plain HTTP or HTTP/2, allows us to communicate routing and metadata out-of-band (rather than e.g. stuffing it into private headers). It also allows us to design component APIs using an [object-capability model](http://erights.org/elib/capability/ode/ode-capabilities.html), which turns out to be an excellent choice when code needs to be securely sandboxed.

Today, our use of this protocol is fairly experimental, but we plan to use it more heavily as the code matures.

#### KJ improvements

KJ is the C++ toolkit library developed together with Cap'n Proto's C++ implementation. Ironically, most of the development in the Cap'n Proto repo these days is actually improvements to KJ, in part because it is used heavily in the implementation of [Cloudflare Workers](https://workers.cloudflare.com/).

* The KJ Promise API now supports fibers. Fibers allow you to execute code in a synchronous style within a thread driven by an asynchronous event loop. The synchronous code runs on an alternate call stack. The code can synchronously wait on a promise, at which point the thread switches back to the main stack and runs the event loop. We generally recommend that new code be written in asynchronous style rather than using fibers, but fibers can be useful in cases where you want to call a synchronous library, and then perform asynchronous tasks in callbacks from said library. [See the pull request for more details.](https://github.com/capnproto/capnproto/pull/913)
* New API `kj::Executor` can be used to communicate directly between event loops on different threads. You can use it to execute an arbitrary lambda on a different thread's event loop. Previously, it was necessary to use some OS construct like a pipe, signal, or eventfd to wake up the receiving thread.
* KJ's mutex API now supports conditional waits, meaning you can unlock a mutex and sleep until such a time as a given lambda function, applied to the mutex's protected state, evaluates to true.
* The KJ HTTP library has continued to be developed actively for its use in [Cloudflare Workers](https://workers.cloudflare.com/). This library now handles millions of requests per second worldwide, both as a client and as a server (since most Workers are proxies), for a wide variety of web sites big and small.

### Towards 1.0

Cap'n Proto has now been around for seven years, with many huge production users (such as Cloudflare). But, we're still on an 0.x release? What gives?

Well, to be honest, there are still a lot of missing features that I feel like are critical to Cap'n Proto's vision, the most obvious one being three-party handoff. But, so far I just haven't had a real production need to implement those features. Clearly, I should stop waiting for perfection.

Still, there are a couple smaller things I want to do for an upcoming 1.0 release:

1. Properly document KJ, independent of Cap'n Proto. KJ has evolved into an extremely useful general-purpose C++ toolkit library.
2. Fix a mistake in the design of KJ's `AsyncOutputStream` interface. The interface currently does not have a method to write EOF; instead, EOF is implied by the destructor. This has proven to be the wrong design. Since fixing it will be a breaking API change for anyone using this interface, I want to do it before declaring 1.0.

I aim to get these done sometime this summer...



_posts/2021-08-14-capnproto-0.9.md
--------------------------------------
---
layout: post
title: "Cap'n Proto 0.9"
author: kentonv
---

<div style="float: right"><a class="block_link" style="color: #fff"
href="{{site.baseurl}}install.html">Get it now &raquo;</a></div>

Today I'm releasing Cap'n Proto 0.9.

There's no huge new features in this release, but there are many minor improvements and bug fixes. You can [read the PR history](https://github.com/capnproto/capnproto/pulls?q=is%3Apr+is%3Aclosed) to find out what has changed.

Cap'n Proto development has continued to be primarily driven by the [Cloudflare Workers](https://workers.cloudflare.com/) project (of which I'm the lead engineer). As of the previous release, Cloudflare Workers primarily used the [KJ C++ toolkit](https://github.com/capnproto/capnproto/blob/master/kjdoc/tour.md) that is developed with Cap'n Proto, but made only light use of Cap'n Proto serialization and RPC itself. That has now changed: the implementation of [Durable Objects](https://blog.cloudflare.com/introducing-workers-durable-objects/) makes heavy use of Cap'n Proto RPC for essentially all communication within the system.



_posts/2022-06-03-capnproto-0.10.md
--------------------------------------
---
layout: post
title: "Cap'n Proto 0.10"
author: kentonv
---

<div style="float: right"><a class="block_link" style="color: #fff"
href="{{site.baseurl}}install.html">Get it now &raquo;</a></div>

Today I'm releasing Cap'n Proto 0.10.

Like last time, there's no huge new features in this release, but there are many minor improvements and bug fixes. You can [read the PR history](https://github.com/capnproto/capnproto/pulls?q=is%3Apr+is%3Aclosed) to find out what has changed.



_posts/2022-11-30-CVE-2022-46149-security-advisory.md
--------------------------------------
---
layout: post
title: "CVE-2022-46149: Possible out-of-bounds read related to list-of-pointers"
author: kentonv
---

David Renshaw, the author of the Rust implementation of Cap'n Proto, discovered a security vulnerability affecting both the C++ and Rust implementations of Cap'n Proto. The vulnerability was discovered using fuzzing. In theory, the vulnerability could lead to out-of-bounds reads which could cause crashes or perhaps exfiltration of memory.

The vulnerability is exploitable only if an application performs a certain unusual set of actions. As of this writing, we are not aware of any applications that are actually affected. However, out of an abundance of caution, we are issuing a security advisory and advising everyone to patch.

[Our security advisory](https://github.com/capnproto/capnproto/blob/master/security-advisories/2022-11-30-0-pointer-list-bounds.md) explains the impact of the bug, what an app must do to be affected, and where to find the fix.

Check out [David's blog post](https://dwrensha.github.io/capnproto-rust/2022/11/30/out_of_bounds_memory_access_bug.html) for an in-depth explanation of the bug itself, including some of the inner workings of Cap'n Proto.



_posts/2023-07-28-capnproto-1.0.md
--------------------------------------
---
layout: post
title: "Cap'n Proto 1.0"
author: kentonv
---

<div style="float: right"><a class="block_link" style="color: #fff"
href="{{site.baseurl}}install.html">Get it now &raquo;</a></div>

It's been a little over ten years since the first release of Cap'n Proto, on April 1, 2013. Today I'm releasing version 1.0 of Cap'n Proto's C++ reference implementation.

Don't get too excited! There's not actually much new. Frankly, I should have declared 1.0 a long time ago – probably around version 0.6 (in 2017) or maybe even 0.5 (in 2014). I didn't mostly because there were a few advanced features (like three-party handoff, or shared-memory RPC) that I always felt like I wanted to finish before 1.0, but they just kept not reaching the top of my priority list. But the reality is that Cap'n Proto has been relied upon in production for a long time. In fact, you are using Cap'n Proto right now, to view this site, which is served by Cloudflare, which uses Cap'n Proto extensively (and is also my employer, although they used Cap'n Proto before they hired me). Cap'n Proto is used to encode millions (maybe billions) of messages and gigabits (maybe terabits) of data every single second of every day. As for those still-missing features, the real world has seemingly proven that they aren't actually that important. (I still do want to complete them though.)

Ironically, the thing that finally motivated the 1.0 release is so that we can start working on 2.0. But again here, don't get too excited! Cap'n Proto 2.0 is not slated to be a revolutionary change. Rather, there are a number of changes we (the Cloudflare Workers team) would like to make to Cap'n Proto's C++ API, and its companion, the KJ C++ toolkit library. Over the ten years these libraries have been available, I have kept their APIs pretty stable, despite being 0.x versioned. But for 2.0, we want to make some sweeping backwards-incompatible changes, in order to fix some footguns and improve developer experience for those on our team.

Some users probably won't want to keep up with these changes. Hence, I'm releasing 1.0 now as a sort of "long-term support" release. We'll backport bugfixes as appropriate to the 1.0 branch for the long term, so that people who aren't interested in changes can just stick with it.

## What's actually new in 1.0?

Again, not a whole lot has changed since the last version, 0.10. But there are a few things worth mentioning:

* A number of optimizations were made to improve performance of Cap'n Proto RPC. These include reducing the amount of memory allocation done by the RPC implementation and KJ I/O framework, adding the ability to elide certain messages from the RPC protocol to reduce traffic, and doing better buffering of small messages that are sent and received together to reduce syscalls. These are incremental improvements.

* **Breaking change:** Previously, servers could opt into allowing RPC cancellation by calling `context.allowCancellation()` after a call was delivered. In 1.0, opting into cancellation is instead accomplished using an annotation on the schema (the `allowCancellation` annotation defined in `c++.capnp`). We made this change after observing that in practice, we almost always wanted to allow cancellation, but we almost always forgot to do so. The schema-level annotation can be set on a whole file at a time, which is easier not to forget. Moreover, the dynamic opt-in required a lot of bookkeeping that had a noticeable performance impact in practice; switching to the annotation provided a performance boost. For users that never used `context.allowCancellation()` in the first place, there's no need to change anything when upgrading to 1.0 – cancellation is still disallowed by default. (If you are affected, you will see a compile error. If there's no compile error, you have nothing to worry about.)

* KJ now uses `kqueue()` to handle asynchronous I/O on systems that have it (MacOS and BSD derivatives). KJ has historically always used `epoll` on Linux, but until now had used a slower `poll()`-based approach on other Unix-like platforms.

* KJ's HTTP client and server implementations now support the `CONNECT` method.

* [A new class `capnp::RevocableServer` was introduced](https://github.com/capnproto/capnproto/pull/1700) to assist in exporting RPC wrappers around objects whose lifetimes are not controlled by the wrapper. Previously, avoiding use-after-free bugs in such scenarios was tricky.

* Many, many smaller bug fixes and improvements. [See the PR history](https://github.com/capnproto/capnproto/pulls?q=is%3Apr+is%3Aclosed) for details.

## What's planned for 2.0?

The changes we have in mind for version 2.0 of Cap'n Proto's C++ implementation are mostly NOT related to the protocol itself, but rather to the C++ API and especially to KJ, the C++ toolkit library that comes with Cap'n Proto. These changes are motivated by our experience building a large codebase on top of KJ: namely, the Cloudflare Workers runtime, [`workerd`](https://github.com/cloudflare/workerd).

KJ is a C++ toolkit library, arguably comparable to things like Boost, Google's Abseil, or Facebook's Folly. I started building KJ at the same time as Cap'n Proto in 2013, at a time when C++11 was very new and most libraries were not really designing around it yet. The intent was never to create a new standard library, but rather to address specific needs I had at the time. But over many years, I ended up building a lot of stuff. By the time I joined Cloudflare and started the Workers Runtime, KJ already featured a powerful async I/O framework, HTTP implementation, TLS bindings, and more.

Of course, KJ has nowhere near as much stuff as Boost or Abseil, and nowhere near as much engineering effort behind it. You might argue, therefore, that it would have been better to choose one of those libraries to build on. However, KJ had a huge advantage: that we own it, and can shape it to fit our specific needs, without having to fight with anyone to get those changes upstreamed.

One example among many: KJ's HTTP implementation features the ability to "suspend" the state of an HTTP connection, after receiving headers, and transfer it to a different thread or process to be resumed. This is an unusual thing to want, but is something we needed for resource management in the Workers Runtime. Implementing this required some deep surgery in KJ HTTP and definitely adds complexity. If we had been using someone else's HTTP library, would they have let us upstream such a change?

That said, even though we own KJ, we've still tried to avoid making any change that breaks third-party users, and this has held back some changes that would probably benefit Cloudflare Workers. We have therefore decided to "fork" it. Version 2.0 is that fork.

Development of version 2.0 will take place on Cap'n Proto's new `v2` branch. The `master` branch will become the 1.0 LTS branch, so that existing projects which track `master` are not disrupted by our changes.

We don't yet know all the changes we want to make as we've only just started thinking seriously about it. But, here's some ideas we've had so far:

* We will require a compiler with support for C++20, or maybe even C++23. Cap'n Proto 1.0 only requires C++14.

* In particular, we will require a compiler that supports C++20 coroutines, as lots of KJ async code will be refactored to rely on coroutines. This should both make the code clearer and improve performance by reducing memory allocations. However, coroutine support is still spotty – as of this writing, GCC seems to ICE on KJ's coroutine implementation.

* Cap'n Proto's RPC API, KJ's HTTP APIs, and others are likely to be revised to make them more coroutine-friendly.

* `kj::Maybe` will become more ergonomic. It will no longer overload `nullptr` to represent the absence of a value; we will introduce `kj::none` instead. `KJ_IF_MAYBE` will no longer produce a pointer, but instead a reference (a trick that becomes possible by utilizing C++17 features).

* We will drop support for compiling with exceptions disabled. KJ's coding style uses exceptions as a form of software fault isolation, or "catchable panics", such that errors can cause the "current task" to fail out without disrupting other tasks running concurrently. In practice, this ends up affecting every part of how KJ-style code is written. And yet, since the beginning, KJ and Cap'n Proto have been designed to accommodate environments where exceptions are turned off at compile time, using an elaborate system to fall back to callbacks and distinguish between fatal and non-fatal exceptions. In practice, maintaining this ability has been a drag on development – no-exceptions mode is constantly broken and must be tediously fixed before each release. Even when the tests are passing, it's likely that a lot of KJ's functionality realistically cannot be used in no-exceptions mode due to bugs and fragility. Today, I would strongly recommend against anyone using this mode except maybe for the most basic use of Cap'n Proto's serialization layer. Meanwhile, though, I'm honestly not sure if anyone uses this mode at all! In theory I would expect many people do, since many people choose to use C++ with exceptions disabled, but I've never actually received a single question or bug report related to it. It seems very likely that this was wasted effort all along. By removing support, we can simplify a lot of stuff and probably do releases more frequently going forward.

* Similarly, we'll drop support for no-RTTI mode and other exotic modes that are a maintenance burden.

* We may revise KJ's approach to reference counting, as the current design has proven to be unintuitive to many users.

* We will fix a longstanding design flaw in `kj::AsyncOutputStream`, where EOF is currently signaled by destroying the stream. Instead, we'll add an explicit `end()` method that returns a Promise. Destroying the stream without calling `end()` will signal an erroneous disconnect. (There are several other aesthetic improvements I'd like to make to the KJ stream APIs as well.)

* We may want to redesign several core I/O APIs to be a better fit for Linux's new-ish io_uring event notification paradigm.

* The RPC implementation may switch to allowing cancellation by default. As discussed above, this is opt-in today, but in practice I find it's almost always desirable, and disallowing it can lead to subtle problems.

* And so on.

It's worth noting that at present, there is no plan to make any backwards-incompatible changes to the serialization format or RPC protocol. The changes being discussed only affect the C++ API. Applications written in other languages are completely unaffected by all this.

It's likely that a formal 2.0 release will not happen for some time – probably a few years. I want to make sure we get through all the really big breaking changes we want to make, before we inflict update pain on most users. Of course, if you're willing to accept breakages, you can always track the `v2` branch. Cloudflare Workers releases from `v2` twice a week, so it should always be in good working order.



slides-2017.05.18/index.md
--------------------------------------
---
layout: slides
title: "Slides: What's Next for Cap'n Proto"
---

<!--===================================================================================-->

<section markdown="1" id="slides-cover">

What's Next for Cap'n Proto?

</section>

<!--===================================================================================-->

<section markdown="1" data-title="Streaming">

Cap'n Proto supports streaming!

{% highlight capnp %}
interface FileStore {
  get @0 (name :Text, stream :Stream);
  put @1 (name :Text) -> (stream :Stream);
}

interface Stream {
  write @0 (data :Data);
  end @1 ();
}
{% endhighlight %}

But flow control is up to the app.

</section>

<!--===================================================================================-->

<section markdown="1" data-title="Flow Control">

Let's build it in.

{% highlight capnp %}
interface Stream {
  write @0 (data :Data) -> bulk;
  end @1 ();
}
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="Realtime">

What about realtime streams?

{% highlight capnp %}
interface VideoCallStream {
  sendFrame @0 (frame :Frame) -> realtime;
}
{% endhighlight %}

<br>Best served on a UDP transport...

</section>

<!--===================================================================================-->

<section markdown="1" data-title="Three-Party Handoff">

<img class="ph3" src="3ph.png">

Forwarded request.

Where does response go?

</section>

<!--===================================================================================-->

<section markdown="1" data-title="Three-Party Handoff">

<img class="ph3" src="3ph-proxy.png">

Classic solution:

Proxy

</section>

<!--===================================================================================-->

<section markdown="1" data-title="Three-Party Handoff">

<img class="ph3" src="3ph-redirect.png">

Classic solution:

Redirect

</section>

<!--===================================================================================-->

<section markdown="1" data-title="Three-Party Handoff">

<img class="ph3" src="3ph-0rt.png">

Cap'n Proto:

3-Party Handoff

(aka 3PH)

</section>

<!--===================================================================================-->

<section markdown="1" data-title="Three-Party Handoff">

<img class="ph3" src="3ph-0rt.png">

Cap'n Proto:

3-Party Handoff

(aka 3PH)

... gonna need UDP

</section>

<!--===================================================================================-->

<section markdown="1" data-title="Three-Party Handoff">

<img class="ph3" src="3ph-0rt.png">

Cap'n Proto:

3-Party Handoff

(aka 3PH)

... gonna need UDP

... and 0-RT crypto

</section>

<!--===================================================================================-->

<section markdown="1" data-title="Three-Party Handoff">

API: "Tail call"

{% highlight c++ %}
kj::Promise<void> myRpc(MyRpcContext context) override {
  // Begin sub-request.
  auto subRequest = someCapability.someRpcRequest();
  subRequest.setSomeParam(someValue);

  // Send as a tail call.
  return context.tailCall(kj::mv(subRequest));
}
{% endhighlight %}

Today: Will proxy<br>Future: 3PH

</section>

<!--===================================================================================-->

<section markdown="1" data-title="KJ TLS Bindings">

KJ client networking, no TLS:

{% highlight c++ %}
void send() {
  auto io = kj::setupAsyncIo();
  auto& network = io.provider->getNetwork();
  auto addr = network.parseAddress("capnproto.org", 80)
      .wait(io.waitScope);
  auto connection = addr->connect().wait(io.waitScope);
  connection->write("GET /", 5).wait(io.waitScope);
}
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="KJ TLS Bindings">

KJ client networking with TLS:

{% highlight c++ %}
void send() {
  auto io = kj::setupAsyncIo();
  kj::TlsContext tls;
  auto network = tls.wrapNetwork(io.provider->getNetwork());
  auto addr = network->parseAddress("capnproto.org", 443)
      .wait(io.waitScope);
  auto connection = addr->connect().wait(io.waitScope);
  connection->write("GET /", 5).wait(io.waitScope);
}
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="KJ TLS Bindings">

Diff:

{% highlight c++ %}
void send() {

  kj::TlsContext tls;
                 tls.wrapNetwork(                         );




}
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="KJ TLS Bindings">

{% highlight c++ %}
void receive() {
  auto io = kj::setupAsyncIo();
  auto& network = io.provider->getNetwork();
  auto addr = network.parseAddress("*", 80)
      .wait(io.waitScope);
  auto listener = addr->listen();
  auto connection = listener->accept().wait(io.waitScope);
  connection->write("HTTP/1.1 404 Not Found\r\n\r\n", 26)
      .wait(io.waitScope);
}
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="KJ TLS Bindings">

{% highlight c++ %}
void receive() {
  auto io = kj::setupAsyncIo();
  kj::TlsKeypair keypair { KEY_PEM_TEXT, CERT_PEM_TEXT };
  kj::TlsContext::Options options;
  options.defaultKeypair = keypair;
  kj::TlsContext tls(options);
  auto& network = io.provider->getNetwork();
  auto addr = network.parseAddress("*", 443).wait(io.waitScope);
  auto listener = tls.wrapPort(addr->listen());
  auto connection = listener->accept().wait(io.waitScope);
  connection->write("HTTP/1.1 404 Not Found\r\n\r\n", 26)
      .wait(io.waitScope);
}
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="KJ TLS Bindings">

{% highlight c++ %}
void receive() {

  kj::TlsKeypair keypair { KEY_PEM_TEXT, CERT_PEM_TEXT };
  kj::TlsContext::Options options;
  options.defaultKeypair = keypair;
  kj::TlsContext tls(options);


                  tls.wrapPort(              );



}
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="KJ HTTP Library">

{% highlight c++ %}
auto io = kj::setupAsyncIo();
kj::HttpHeaderTable headerTable;
auto client = kj::newHttpClient(
    *headerTable, io.provider->getNetwork());

kj::HttpHeaders headers(*headerTable);
auto response = client->request(
    kj::HttpMethod::GET, "http://capnproto.org", headers)
    .response.wait(io.waitScope);

KJ_ASSERT(response.statusCode == 200);
KJ_LOG(INFO, response.body->readAllText().wait(io.waitScope));
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="KJ HTTP Library">

Headers identified by small numbers.

{% highlight c++ %}
kj::HttpHeaderTable::Builder builder;
kj::HttpHeaderId userAgent = builder.add("User-Agent");
auto headerTable = builder.build();

kj::HttpHeaders headers(*headerTable);
headers.set(kj::HttpHeaderId::HOST, "capnproto.org");
headers.set(userAgent, "kj-http/0.6");
{% endhighlight %}

Header parsing is zero-copy.

</section>

<!--===================================================================================-->

<section markdown="1" data-title="Designated Initializers">

Ugly imperative code:

{% highlight c++ %}
capnp::MallocMessageBuilder message;

auto root = message.initRoot<MyStruct>();
root.setFoo(123);
root.setBar("foo");
auto inner = root.initBaz();
inner.setQux(true);

capnp::writeMessageToFd(fd, message);
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="Designated Initializers">

Nice declarative code:

{% highlight c++ %}
using namespace capnp::init;

capnp::MallocMessageBuilder message;
message.initRoot<MyStruct>(
  $foo = 123,
  $bar = "foo",
  $baz(
    $qux = true
  )
);
capnp::writeMessageToFd(fd, message);
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="Designated Initializers">

Even better:

{% highlight c++ %}
using namespace capnp::init;

capnp::writeMessageToFd<MyStruct>(fd,
  $foo = 123,
  $bar = "foo",
  $baz(
    $qux = true
  )
);
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="Designated Initializers">

{% highlight c++ %}
struct {
  template <typename T>
  struct Setter {
    T value;
    template <typename U> void operator()(U& target) {
      target.setFoo(kj::fwd<T>(value));
    }
  };

  template <typename T>
  Setter<T> operator=(T&& value) {
    return { kj::fwd<T>(value) };
  }
} $foo;
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="POCS">

Not idiomatic:

{% highlight c++ %}
capnp::MallocMessageBuilder message;

MyStruct::Builder root = message.initRoot<MyStruct>();
root.setFoo(123);
root.setBar("foo");
InnerStruct::Builder inner = root.initBaz();
inner.setQux(true);

capnp::writeMessageToFd(fd, message);
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="POCS">

Plain Old C++ Structs?

{% highlight c++ %}
MyStruct root;
root.foo = 123;
root.bar = "foo";
InnerStruct inner;
inner.qux = true;
root.baz = kj::mv(inner);

capnp::writeMessageToFd(fd, message);
{% endhighlight %}

Caveat: No longer zero-copy.

</section>

<!--===================================================================================-->

<section markdown="1" data-title="POCS">

{% highlight c++ %}
capnp::MallocMessageBuilder message;
capnp::readMessageCopy(input, message);
auto root = message.getRoot<MyStruct>();
auto oldListOrphan = root.disownStructList();
auto oldList = oldListOrphan.getReader();
auto newList = root.initStructList(oldList.size() - 1);
for (auto i: kj::indices(newList)) {
  newList.setWithCaveats(i,
      oldList[i < indexToRemove ? i : i + 1]);
}
capnp::MallocMessageBuilder message2;
message2.setRoot(root.asReader());
capnp::writeMessage(output, message2);
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="POCS">

{% highlight c++ %}
auto root = capnp::readMessageCopy<MyStruct>(input);
root.structList.erase(indexToRemove);
capnp::writeMessageCopy(output, root);
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="JSON-HTTP Bridge">

{% highlight capnp %}
interface AddressBook {
  getPerson @0 (id :UInt32 $httpPath)
            -> (person :Person $httpBody(type = json))
      $http(method = get, route = "person");
  # GET /person/<id>
  # JSON response body

  updatePerson @1 (id :UInt32 $httpPath,
                   person :Person $httpBody(type = json));
      $http(method = put, route = "person");
  # PUT /person/<id>
  # JSON request body
}
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="JSON-HTTP Bridge">

{% highlight capnp %}
addPerson @2 (person :Person $httpBody(type = json))
          -> (id :UInt32 $httpBody(type = jsonField));
    $http(method = post, route = "person");
# POST /person
# JSON request body
# JSON response body (object containing field `id`)

getAll @3 (page :UInt32 = 0 $httpQuery)
       -> (people: List(Person) $httpBody(type = json));
    $http(method = get);
# GET /?page=<num>
# Query is optional.
# JSAN (JSON array) response body.
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" data-title="JSON-HTTP Bridge">

{% highlight capnp %}
interface AddressBookService {
  getAddressBook @0 (key :String $httpPath)
                 -> (result :AddressBook $httpPipeline);
      $http(route = "book");
  # GET /book/JrpmUduyHd8uW3x3TOXn2g/person/123
  # Becomes:
  #     service.getAddressBook("JrpmUduyHd8uW3x3TOXn2g").send()
  #            .getResult().getPerson(123).send()
  #
  # GET /book/JrpmUduyHd8uW3x3TOXn2g
  # Becomes:
  #     service.getAddressBook("JrpmUduyHd8uW3x3TOXn2g").send()
  #            .getResult().getAll().send()
}
{% endhighlight %}

</section>

<!--===================================================================================-->

<section markdown="1" id="slides-cover">

Questions?

</section>



emacs/README.md
--------------------------------------
Syntax Coloring for Emacs
=========================

How to use:

Add this to your .emacs file (altering the path to wherever your
capnproto directory lives):

```elisp
(add-to-list 'load-path "~/src/capnproto/highlighting/emacs")
(require 'capnp-mode)
```



kjdoc/index.md
--------------------------------------
# Introducing KJ

KJ is Modern C++'s missing base library.

## What's wrong with `std`?

The C++ language has advanced rapidly over the last decade. However, its standard library (`std`) remains a weak point. Most modern languages ship with libraries that have built-in support for common needs, such as making HTTP requests. `std`, meanwhile, not only lacks HTTP, but doesn't even support basic networking. Developers are forced either to depend on low-level, non-portable OS APIs, or pull in a bunch of third-party dependencies with inconsistent styles and quality.

Worse, `std` was largely designed before C++ best practices were established. Much of it predates C++11, which changed almost everything about how C++ is written. Some critical parts of `std` -- such as the `iostreams` component -- were designed before anyone really knew how to write quality object-oriented code, and are atrociously bad by modern standards.

Finally, `std` is designed by committee, which has advantages and disadvantages. On one hand, committees are less likely to make major errors in design. However, they also struggle to make bold decisions, and they move slowly. Committees can also lose touch with real-world concerns, over-engineering features that aren't needed while missing essential basics.

## How is KJ different?

KJ was designed and implemented primarily by one developer, Kenton Varda. Every feature was designed to solve a real-world need in a project Kenton was working on -- first [Cap'n Proto](https://capnproto.org), then [Sandstorm](https://sandstorm.io), and more recently, [Cloudflare Workers](https://workers.dev). KJ was designed from the beginning to target exclusively Modern C++ (C++11 and later).

Since its humble beginnings in 2013, KJ has developed a huge range of practical functionality, including:

* RAII utilities, especially for memory management
* Basic types and data structures: `Array`, `Maybe`, `OneOf`, `Tuple`, `Function`, `Quantity` (unit analysis), `String`, `Vector`, `HashMap`, `HashSet`, `TreeMap`, `TreeSet`, etc.
* Convenient stringification
* Exception/assertion framework with friggin' stack traces
* Event loop framework with `Promise` API inspired by E (which also inspired JavaScript's `Promise`).
* Threads, fibers, mutexes, lazy initialization
* I/O: Clocks, filesystem, networking
* Protocols: HTTP (client and server), TLS (via OpenSSL/BoringSSL), gzip (via libz)
* Parsers: URL, JSON (using Cap'n Proto), parser combinator framework
* Encodings: UTF-8/16/32, base64, hex, URL encoding, C escapes
* Command-line argument parsing
* Unit testing framework
* And more!

KJ is not always perfectly organized, and admittedly has some quirks. But, it has proven pragmatic and powerful in real-world applications.

# Getting KJ

KJ is bundled with Cap'n Proto -- see [installing Cap'n Proto](https://capnproto.org/install.html). KJ is built as a separate set of libraries, so that you can link against it without Cap'n Proto if desired.

KJ is officially tested on Linux (GCC and Clang), Windows (Visual Studio, MinGW, and Cygwin), MacOS, and Android. It should additionally be easy to get working on any POSIX platform targeted by GCC or Clang.

# FAQ

## What does KJ stand for?

Nothing.

The name "KJ" was chosen to be a relatively unusual combination of two letters that is easy to type (on both Qwerty and Dvorak layouts). This is important, because users of KJ will find themselves typing `kj::` very frequently.

## Why reinvent modern `std` features that are well-designed?

Some features of KJ appear to replace `std` features that were introduced recently with decent, modern designs. Examples include `kj::Own` vs `std::unique_ptr`, `kj::Maybe` vs `std::optional`, and `kj::Promise` vs `std::task`.

First, in many cases, the KJ feature actually predates the corresponding `std` feature. `kj::Maybe` was one of the first KJ types, introduced in 2013; `std::optional` arrived in C++17. `kj::Promise` was also introduced in 2013; `std::task` is coming in C++20 (with coroutines).

Second, consistency. KJ uses somewhat different idioms from `std`, resulting in some friction when trying to use KJ and `std` types together. The most obvious friction is aesthetic (e.g. naming conventions), but some deeper issues exist. For example, KJ tries to treat `const` as transitive, especially so that it can be used to help enforce thread-safety. This can lead to subtle problems (e.g. unexpected compiler errors) with `std` containers not designed with transitive constness in mind. KJ also uses a very different [philosophy around exceptions](../style-guide.md#exceptions) compared to `std`; KJ believes exception-free code is a myth, but `std` sometimes requires it.

Third, even some modern `std` APIs have design flaws. For example, `std::optional`s can be dereferenced without an explicit null check, resulting in a crash if the value is null -- exactly what this type should have existed to prevent! `kj::Maybe`, in contrast, forces you to write an if/else block or an explicit assertion. For another example, `kj::Own` uses dynamic dispatch for deleters, which allows for lots of useful patterns that `std::unique_ptr`'s static dispatch cannot do.

## Shouldn't modern software be moving away from memory-unsafe languages?

Probably!

Similarly, modern software should also move away from type-unsafe languages. Type-unsafety and memory-unsafety are both responsible for a huge number of security bugs. (Think SQL injection for an example of a security bug resulting from type-unsafety.)

Hence, all other things being equal, I would suggest Rust for new projects.

But it's rare that all other things are really equal, and you may have your reasons for using C++. KJ is here to help, not to judge.



kjdoc/style-guide.md
--------------------------------------
# KJ Style

This document describes how to write C++ code in KJ style. It may be compared to the [Google C++ Style Guide](http://google-styleguide.googlecode.com/svn/trunk/cppguide.html).

KJ style is used by KJ (obviously), [Cap'n Proto](https://capnproto.org), [Sandstorm.io](https://sandstorm.io), and possibly other projects. When submitting code to these projects, you should follow this guide.

**Table of Contents**

- [Rule #1: There are no rules](#rule-1-there-are-no-rules)
- [Design Philosophy](#design-philosophy)
  - [Value Types vs. Resource Types](#value-types-vs-resource-types)
  - [RAII (Resource Acquisition Is Initialization)](#raii-resource-acquisition-is-initialization)
  - [Ownership](#ownership)
  - [No Singletons](#no-singletons)
  - [Exceptions](#exceptions)
  - [Threads vs. Event Loops](#threads-vs-event-loops)
  - [Lazy input validation](#lazy-input-validation)
  - [Premature optimization fallacy](#premature-optimization-fallacy)
  - [Text is always UTF-8](#text-is-always-utf-8)
- [C++ usage](#c-usage)
  - [Use C++11 (or later)](#use-c11-or-later)
  - [Heap allocation](#heap-allocation)
  - [Pointers, references](#pointers-references)
  - [Constness](#constness)
  - [Inheritance](#inheritance)
  - [Exceptions Usage](#exceptions-usage)
  - [Template Metaprogramming](#template-metaprogramming)
  - [Global Constructors](#global-constructors)
  - [`dynamic_cast`](#dynamic_cast)
  - [Use of Standard libraries](#use-of-standard-libraries)
  - [Compiler warnings](#compiler-warnings)
  - [Tools](#tools)
- [Irrelevant formatting rules](#irrelevant-formatting-rules)
  - [Naming](#naming)
  - [Spacing and bracing](#spacing-and-bracing)
  - [Comments](#comments)
  - [File templates](#file-templates)

## Rule #1: There are no rules

This guide contains suggestions, not rules.

If you wish to submit code to a project following KJ style, you should follow the guide so long as there is no good reason not to. You should not break rules just because you feel like it -- consistency is important for future maintainability. But, if you have a good, pragmatic reason to break a rule, do it. Do not ask permission. Just do it.

## Design Philosophy

This section contains guidelines on software design that aren't necessarily C++-specific (though KJ's preferences here are obviously influenced by C++).

### Value Types vs. Resource Types

There are two kinds of types: values and resources. Value types are simple data structures; they serve no purpose except to represent pure data. Resource types represent live objects with state and behavior, and often represent resources external to the program.

* Value types make sense to copy (though they don't necessarily have copy constructors). Resource types are not copyable.
* Value types always have move constructors (and sometimes copy constructors). Resource types are not movable; if ownership transfer is needed, the resource must be allocated on the heap.
* Value types almost always have implicit destructors. Resource types may have an explicit destructor.
* Value types should only be compared by value, not identity. Resource types can only be compared by identity.
* Value types make sense to serialize. Resource types fundamentally cannot be serialized.
* Value types rarely use inheritance and never have virtual methods. Resource types commonly do.
* Value types generally use templates for polymorphism. Resource types generally use virtual methods / abstract interfaces.
* You might even say that value types are used in functional programming style while resource types are used in object-oriented style.

In Cap'n Proto there is a very clear distinction between values and resources: interfaces are resource types whereas everything else is a value.

### RAII (Resource Acquisition Is Initialization)

KJ code is RAII-strict. Whenever it is the case that "this block of code cannot exit cleanly without performing operation X", then X *must* be performed in a destructor, so that X will happen regardless of how the block is exited (including by exception).

Use the macros `KJ_DEFER`, `KJ_ON_SCOPE_SUCCESS`, and `KJ_ON_SCOPE_FAILURE` to easily specify some code that must be executed on exit from the current scope, without the need to define a whole class with a destructor.

Be careful when writing complicated destructors. If a destructor performs multiple cleanup actions, you generally need to make sure that the latter actions occur even if the former ones throw an exception. For this reason, a destructor should generally perform no more than one cleanup action. If you need to clean up multiple things, have your class contain multiple members representing the different things that need cleanup, each with its own destructor. This way, if one member's destructor throws, the others still run.

### Ownership

Every object has an "owner". The owner may be another object, or it may be a stack frame (which is in turn owned by its parent stack frame, and so on up to the top frame, which is owned by the thread, which itself is an object which is owned by something).

The owner decides when to destroy an object. If the owner itself is destroyed, everything it owns must be transitively destroyed. This should be accomplished through RAII style.

The owner specifies the lifetime of the object and how the object may be accessed. This specification may be through documented convention or actually enforced through the type system; the latter is preferred when possible.

An object can never own itself, including transitively.

When declaring a pointer to an object which is owned by the scope, always use `kj::Own<T>`. Regular C++ pointers and references always point to objects that are *not* owned.

When passing a regular C++ pointer or reference as a parameter or return value of a function, care must be taken to document assumptions about the lifetime of the object. In the absence of documentation, make the following assumptions:

* A pointer or reference passed as a constructor parameter must remain valid for the lifetime of the constructed object.
* A pointer or reference passed as a function or method parameter must remain valid until the function returns. In the case that the function returns a promise, then the object must remain live until the promise completes or is canceled.
* A pointer or reference returned by a method remains valid at least until the object whose method was called is destroyed.
* A pointer or reference returned by a stand-alone function likely refers to content of one of the function's parameters, and remains valid until that parameter is destroyed.

Note that ownership isn't just about memory management -- it matters even in languages that implement garbage collection! Unless an object is 100% immutable, you need to keep track of who is allowed to modify it, and that generally requires declaring an owner. Moreover, even with GC, resource types commonly need `close()` method that acts very much like a C++ destructor, leading to all the same considerations. It is therefore completely wrong to believe garbage collection absolves you of thinking about ownership -- and this misconception commonly leads to huge problems in large-scale systems written in GC languages.

#### Reference Counting

Reference counting is allowed, in which case an object will have multiple owners.

When using reference counting, care must be taken to ensure that there is a clear contract between all owners about how the object shall be accessed. In general, this should mean one of the following:

* Reference-counted value types should be immutable.
* Reference-counted resource types should have an interface which clearly specifies how multiple clients should coordinate.

Care must also be taken to avoid cyclic references (which would constitute self-ownership, and would cause a memory leak). Think carefully about what the object ownership graph looks like.

Avoid reference counting when it is not absolutely necessary.

Keep in mind that atomic (thread-safe) reference counting can be extremely slow. Consider non-atomic reference counting if it is feasible under your threading philosophy (under KJ's philosophy, non-atomic reference counting is OK).

### No Singletons

A "singleton" is any mutable object or value that is globally accessible. "Globally accessible" means that the object is declared as a global variable or static member variable, or that the object can be found by following pointers from such variables.

Never use singletons. Singletons cause invisible and unexpected dependencies between components of your software that appear unrelated. Worse, the assumption that "there should only be one of this object per process" is almost always wrong, but its wrongness only becomes apparent after so much code uses the singleton that it is infeasible to change. Singleton interfaces often turn into unusable monstrosities in an attempt to work around the fact that they should never have been a singleton in the first place.

See ["Singletons Considered Harmful"](http://www.object-oriented-security.org/lets-argue/singletons) for a complete discussion.

#### Global registries are singletons

An all-too-common-pattern in modular frameworks is to design a way to register named components via global-scope macros. For example:

    // BAD BAD BAD
    REGISTER_PLUGIN("foo", fooEntryPoint);

This global registry is a singleton, and has many of the same problems as singletons. Don't do this. Again, see ["Singletons Considered Harmful"](http://www.object-oriented-security.org/lets-argue/singletons) for discussion.

#### What to do instead

High-level code (such as your `main()` function) should explicitly initialize the components the program needs. If component Foo depends on component Bar, then Foo's constructor should take a pointer to Bar as a parameter; the high-level code can then point each component at its dependencies explicitly.

For example, instead of a global registry, have high-level code construct a registry object and explicitly call some `register()` method to register each component that should be available through it. This way, when you read your `main()` function it's easy to see what components your program is using.

#### Working around OS singletons

Unfortunately, operating system APIs are traditionally singleton-heavy. The most obvious example is, of course, the filesystem.

In order to use these APIs while avoiding the problems of singletons, try to encapsulate OS singletons inside non-singleton interfaces as early on as possible in your program. For example, you might define an abstract interface called `Directory` with an implementation `DiskDirectory` representing a directory on disk. In your `main()` function, create two `DiskDirectory`s representing the root directory and the current working directory. From then on, have all of your code operate in terms of `Directory`. Pass the original `DiskDirectory` pointers into the components that need it.

### Exceptions

An exception represents something that "should never happen", assuming everything is working as expected. Of course, things that "should never happen" in fact happen all the time. But, a program should never be written in such a way that it _expects_ an exception under normal circumstances.

Put another way, exceptions are a way to achieve _fault tolerance_. Throwing an exception is a less-disruptive alternative to aborting the process. Exceptions are a _logistical_ construct, as opposed to a semantic one: an exception should never be part of your "business logic".

For example, exceptions may indicate conditions like:

* Logistics of software development:
  * There is a bug in the code.
  * The requested method is not implemented.
* Logistics of software usage:
  * There is an error in the program's configuration.
  * The input is invalid.
* Logistics of distributed systems:
  * A network connection was reset.
  * An optimistic transaction was aborted due to concurrent modification.
* Logistics of physical computation:
  * The system's resources are exhausted (e.g. out of memory, out of disk space).
  * The system is overloaded and must reject some requests to avoid long queues.

#### Business logic should never catch

If you find that callers of your interface need to catch and handle certain kinds of exceptions in order to operate correctly, then you must change your interface (or overload it) such that those conditions can be handled without an exception ever being thrown. For example, if you have a method `Own<File> open(StringPtr name)` that opens a file, you may also want to offer `Maybe<Own<File>> openIfExists(StringPtr name)` that returns null rather than throwing an exception if the file is not found. (But you should probably keep `open()` as well, for the convenience of the common case where the caller will just throw an exception anyway.)

Note that with this exception philosophy, Java-style "checked exceptions" (exceptions which are explicitly declared to be thrown by an interface) make no sense.

#### How to handle an exception

In framework and logistical code, you may catch exceptions and try to handle them. Given the nature of exceptions, though, there are only a few things that are reasonable to do when receiving an exception:

* On network disconnect or transaction failures, back up and start over from the beginning (restore connections and state, redo operations).
* On resources exhausted / overloaded, retry again later, with exponential back-off.
* On unimplemented methods, retry with a different implementation strategy, if there is one.
* When no better option is available, report the problem to a human (the user and/or the developer).

#### Exceptions can happen anywhere (including destructors)

Any piece of code may contain a bug. Therefore, an exception can happen anywhere. This includes destructors. It doesn't matter how much you argue that destructors should not throw exceptions, because that is equivalent to arguing that code should not have bugs. We all wish our code never had bugs, but nevertheless it happens.

Unfortunately, C++ made the awful decision that an exception thrown from a destructor that itself is called during stack unwind due to some other exception should cause the process to abort. This is an error in the language specification. Apparently, the committee could not agree on any other behavior, so they chose the worst possible behavior.

If exceptions are merely a means to fault tolerance, then it is perfectly clear what should happen in the case that a second exception is thrown while unwinding due to a first: the second exception should merely be discarded, or perhaps attached to the first as a supplementary note. The catching code usually does not care about the exception details anyway; it's just going to report that something went wrong, then maybe try to continue executing other, unrelated parts of the program. In fact, in most cases discarding the secondary exception makes sense, because it is often simply a side-effect of the fact that the code didn't complete normally, and so provides no useful additional information.

Alas, C++ is what it is. So, in KJ, we work around the problem in a couple ways:

* `kj::UnwindDetector` may be used to detect when a destructor is called during unwind and squelch secondary exceptions.
* The `KJ_ASSERT` family of macros -- from which most exceptions are thrown in the first place -- implement a concept of "recoverable" exceptions, where it is safe to continue execution without throwing in cases where throwing would be bad. Assert macros in destructors must always be recoverable.

#### Allowing `-fno-exceptions`

KJ and Cap'n Proto are designed to function even when compiled with `-fno-exceptions`. In this case, throwing an exception behaves differently depending on whether the exception is "fatal" or "recoverable". Fatal exceptions abort the process. On a recoverable exception, on the other hand, execution continues normally, perhaps after replacing invalid data with some safe default. The exception itself is stored in a thread-local variable where code up the stack can check for it later on.

This compromise is made only so that C++ applications which eschew exceptions are still able to use Cap'n Proto. We do NOT recommend disabling exceptions if you have a choice. Moreover, code following this style guide (other than KJ and Cap'n Proto) is not required to be `-fno-exceptions`-safe, and in fact we recommend against it.

### Threads vs. Event Loops

Threads are hard, and synchronization between threads is slow. Even "lock-free" data structures usually require atomic operations, which are costly, and such algorithms are notoriously difficult to get right. Fine-grained synchronization will therefore be expensive at best and highly unstable at worst.

KJ instead prefers event loop concurrency. In this model, each event callback is effectively a transaction; it does not need to worry about concurrent modification within the body of the function.

Multiple threads may exist, but each one has its own event loop and is treated as sort of a lightweight process with shared memory. Every object in the process either belongs to a specific thread (who is allowed to read and modify it) or is transitively immutable (in which case all threads can safely read it concurrently). Threads communicate through asynchronous message-passing. In fact, the only big difference between KJ-style threads compared to using separate processes is that threads may transfer ownership of in-memory objects as part of a message send.

Note that with hardware transactional memory, it may become possible to execute a single event loop across multiple CPU cores while behaving equivalently to a single thread, by executing each event callback as a hardware transaction. If so, this will be implemented as part of KJ's event loop machinery, transparently to apps.

### Lazy input validation

As we all know, you should always validate your input.

But, when should you validate it? There are two plausible answers:

* Upfront, on receipt.
* Lazily, on use.

Upfront validation occasionally makes sense for the purpose of easier debugging of problems: if an error is reported earlier, it's easier to find where it came from.

However, upfront validation has some big problems.

* It is inefficient, as it requires a redundant pass over the data. Lazy validation, in contrast, occurs at a time when you have already loaded the data for the purpose of using it. Extra passes are often cache-unfriendly and/or entail redundant I/O operations.

* It encourages people to skip validation at time of use, on the assumption that it was already validated earlier. This is dangerous, as it entails a non-local assumption. E.g. are you really sure that there is no way to insert data into your database without having validated it? Are you really sure that the data hasn't been corrupted? Are you really sure that your code will never be called in a new situation where validation hasn't happened? Are you sure the data cannot have been modified between validation and use? In practice, you should be validating your input at time of use _even if_ you know it has already been checked previously.

* The biggest problem: Upfront validation tends not to match actual usage, because the validation site is far away from the usage site. Over time, as the usage code changes, the validator can easily get out-of-sync. Note that this could mean the code itself is out-of-sync, or it could be that running servers are out-of-sync, because they have different update schedules. Or, the validator may be written with incorrect assumptions in the first place. The consequences of this can be severe. Protocol Buffers' concept of "required fields" is essentially an upfront validation check that [has been responsible for outages of Google Search, GMail, and others](https://capnproto.org/faq.html#how-do-i-make-a-field-required-like-in-protocol-buffers).

We recommend, therefore, that validation occur at time of use. Code should be written to be tolerant of validation failures. For example, most code dealing with UTF-8 text should treat it as a blob of bytes, not worrying about invalid byte sequences. When you actually need to decode the code points -- such as to display them -- you should do something reasonable with invalid sequences -- such as display the Unicode replacement character.

With that said, when storing data in a database long-term, it can make sense to perform an additional validation check at time of storage, in order to more directly notify the caller that their input was invalid. This validation should be considered optional, since the data will be validated again when it is read from storage and used.

### Premature optimization fallacy

_"We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil."_ -- Donald Knuth

_"The improvement in speed from Example 2 to Example 2a is only about 12%, and many people would pronounce that insignificant. The conventional wisdom shared by many of today’s software engineers calls for ignoring efficiency in the small; but I believe this is simply an overreaction to the abuses they see being practiced by penny-wise- and-pound-foolish programmers, who can’t debug or maintain their “optimized” programs. In established engineering disciplines a 12% improvement, easily obtained, is never considered marginal; and I believe the same viewpoint should prevail in software engineering. Of course I wouldn’t bother making such optimizations on a one-shot job, but when it’s a question of preparing quality programs, I don’t want to restrict myself to tools that deny me such efficiencies."_ -- Donald Knuth, **in the same paper**.

(Credit: [Stop Misquoting Donald Knuth!](http://www.joshbarczak.com/blog/?p=580))

You should not obsess over optimization or write unmaintainable code for the sake of speed.

However, you _should_ be thinking about efficiency of all the code you write. When writing efficient code is not much harder and not much uglier than inefficient code, you should be writing efficient code. If the efficient approach to a problem would take _much_ longer than the inefficient way then go ahead and code the inefficient way first, but in many cases it's not that stark. Rewriting your code later is _much_ more expensive than writing it correctly the first time, because by then you'll have lost context.

You should be constantly aware of whether the code you are writing is low-level (called frequently) or high-level (called infrequently). You should consider optimizations relative to the code's level. In low-level code, optimizations like avoiding heap allocations may make sense. In high-level code you should not worry about heap, but you may still want to think about expensive operations like disk I/O or contacting remote servers (things that low-level code should never do in the first place, of course).

Programmers who ignore efficiency until they have no choice inevitably end up shipping slow, bloated software. Their code's speed is always pushing the boundary of bearability, because they only do anything about it when it becomes unbearable. But programs which are "bearable" can still make users intensely unhappy due to their slowness.

### Text is always UTF-8

Always encode text as UTF-8. Always assume text is encoded as UTF-8.

(Do not, however, assume text is _valid_ UTF-8; see the section on lazy validation.)

Do not write code that tries to distinguish characters. Unless you are writing code to render text to a display, you probably don't care about characters. Besides, Unicode itself contains code points which act as modifiers to previous characters; it's futile for you to handle these. Most code only really cares about bytes.

Note that even parsers for machine-readable text-based languages (config languages, programming languages, other DSLs) do not really care about "characters" in the Unicode sense because such languages are almost always pure-ASCII. They may allow arbitrary UTF-8 in, say, string literals, but only ASCII code points have any special meaning to the language. Therefore, they still only care about bytes (since ASCII characters are single-byte, and multi-byte UTF-8 codepoints never contain individual bytes in the ASCII range).

## C++ usage

This section contains guidelines for usage of C++ language features.

### Use C++11 (or later)

C++11 completely transformed the way the C++ language is used. New code should take heavy advantage of the new features, especially rvalue references (move semantics) and lambda expressions.

KJ requires C++11. Application code (not used as a library) may consider requiring C++14, or even requiring a specific compiler and tracking the latest language features implemented by it.

### Heap allocation

* Never write `new` or `delete` explicitly. Use `kj::heap` to allocate single objects or `kj::heapArray` for arrays; these return "owned" pointers (`kj::Own<T>` or `kj::Array<T>`, respectively) which enforce RAII/ownership semantics. You may transfer ownership of these pointers via move semantics, but otherwise the objects will be automatically deleted when they go out of scope. This makes memory leaks very rare in KJ code.
* Only allocate objects on the heap when you actually need to be able to move them. Otherwise, avoid a heap allocation by declaring the object directly on the stack or as a member of some other object.
* If a class's copy constructor would require memory allocation, consider providing a `clone()` method instead and deleting the copy constructor. Allocation in implicit copies is a common source of death-by-1000-cuts performance problems. `kj::String`, for example, is movable but not copyable.

### Pointers, references

* Pointers and references always point to things that are owned by someone else. Take care to think about the lifetime of that object compared to the lifetime of the pointer.
* Always use `kj::ArrayPtr<T>` rather than `T*` to point to an array.
* Always use `kj::StringPtr` rather than `const char*` to point to a NUL-terminated string.
* Always use `kj::Maybe<T&>` rather than `T*` when a pointer can be null. This forces the user to check for null-ness.
* In other cases, prefer references over pointers. Note, though, that members of an assignable type cannot be references, so you'll need to use pointers in that case (darn).

**Rationale:** There is an argument that says that references should always be const and pointers mutable, because then when you see `foo(&bar)` you know that the function modifies `bar`. This is a nice theory, but in practice real C++ code is rarely so consistent that you can use this as a real signal. We prefer references because they make it unambiguous that the value cannot be null.

### Constness

* Treat `const`-ness as transitive. So, if you have a const instance of a struct which in turn contains a pointer (or reference), treat that pointer as pointing to const even if it is not declared as such. To enforce this, copyable classes which contain pointer fields should declare their copy constructor as `T(T& other)` rather than `T(const T& other)` (and similarly for assignment operators) in order to prevent escalating a transitively-const pointer to non-const via copy. You may inherit `kj::DisallowConstCopy` to force the implicit copy constructor and assignment operator to be declared this way.
* Try to treat const/non-const pointers like shared/exclusive locks. So, when a new const pointer to an object is created, all other pointers should also be considered const at least until the new pointer is destroyed. When a new non-const pointer is created, all other pointers should be considered not dereferenceable until the non-const pointer is destroyed. In theory, these rules help keep different objects from interfering with each other by modifying some third object in incompatible ways. Note that these rules are (as I understand it) enforceable by the Rust type system.
* `const` methods are safe to call on the same object from multiple threads simultaneously. Conversely, it is unsafe to call a non-`const` method if any other thread might be calling methods on that object concurrently. Note that KJ defines synchronization primitives including `kj::Mutex` which integrate nicely with this rule.

### Inheritance

A class is either an interface or an implementation. Interfaces have no fields. Implementations have no non-final virtual methods. You should not mix these: a class with state should never have virtual methods, as this leads to fragile base class syndrome.

Interfaces should NOT declare a destructor, because:

* That destructor is never called anyway (because we don't use `delete`, and `kj::Own` has a different mechanism for dispatching the destructor).
* Declaring destructors for interfaces is tedious.
* If you declare a destructor but do not declare it `noexcept(false)`, C++11 will (regrettably) decide that it is `noexcept` and that all derived classes must also have a `noexcept` destructor, which is wrong. (See the exceptions philosophy section for discussion on exceptions in destructors.)

Multiple inheritance is allowed and encouraged, keeping in mind that you are usually inheriting interfaces.

You should think carefully about whether to use virtual inheritance; it's not often needed, and it is relatively inefficient, but in complex inheritance hierarchies it becomes critical.

Implementation inheritance (that is, inheriting an implementation class) is allowed as a way to compose classes without requiring extra allocations. For example, Cap'n Proto's `capnp::InputStreamMessageReader` implements the `capnp::MessageReader` interface by reading from a `kj::InputStream`, which is itself an interface. One implementation of `kj::InputStream` is `kj::FdInputStream`, which reads from a unix file descriptor. As a convenience, Cap'n Proto defines `capnp::StreamFdMessageReader` which multiply-inherits `capnp::InputStreamMessageReader` and `kj::FdInputStream` -- that is, it inherits two implementations, and even inherits the latter privately. Many style guides would consider this taboo. The benefit, though, is that people can declare this composed class on the stack as one unit, with no heap allocation, and end up with something that they can directly treat as a `capnp::MessageReader`; any other solution would lose one of these benefits.

### Exceptions Usage

KJ's exception philosophy is described earlier in this document. Here we describe only how to actually use exceptions in code.

Never use `throw` explicitly. Almost all exceptions should originate from the `KJ_ASSERT`, `KJ_REQUIRE`, and `KJ_SYSCALL` macros (see `kj/debug.h`). These macros allow you to easily attach useful debug information to the exception message without spending time on string formatting.

Never declare anything `noexcept`. As explained in the philosophy section, whether you like it or not, bugs can happen anywhere and therefore exceptions can happen anywhere. `noexcept` causes the process to abort on exceptions. Aborting is _never_ the right answer.

Explicit destructors must always be declared `noexcept(false)`, to work around C++11's regrettable decision that destructors should be `noexcept` by default. In destructors, always use `kj::UnwindDetector` or make all your asserts recoverable in order to ensure that an exception is not thrown during unwind.

Do not fret too much about recovering into a perfectly consistent state after every exception. That's not the point. The point is to be able to recover at all -- to _improve_ reliability, but not to make it perfect. So, write your code to do a reasonable thing in most cases.

For example, if you are implementing a data structure like a vector, do not worry about whether move constructors might throw. In practice, it is extraordinarily rare for move constructors to contain any code that could throw. So just assume they don't. Do NOT do what the C++ standard library does and require that all move constructors be explicitly `noexcept`, because people will not remember to mark their move constructors `noexcept`, and you'll just be creating a huge headache for everyone with _no practical benefit_.

### Template Metaprogramming

#### Reducing Verbosity

Before C++11, it was common practice to write "template functions" in the form of a templated struct which contained a single member representing the output of the function. For example, you might see `std::is_integral<int>::value` to check if `int` is integral. This pattern is excessively verbose, especially when composed into complex expressions.

In C++11, we can do better. Where before you would have declared a struct named `Foo<T>` with a single member as described above, in C++11 you should:

1. Define the struct as before, but with the name `Foo_<T>`.
2. Define a template `Foo<T>` which directly aliases the single member of `Foo_<T>`. If the output is a type, use a template `using`, whereas if the output is a value, use a `constexpr` function.

Example:

    template <typename T> struct IsConst_ { static constexpr bool value = false; };
    template <typename T> struct IsConst_<const T> { static constexpr bool value = true; };
    template <typename T> constexpr bool isConst() { return IsConst_<T>::value; }
    // Return true if T is const.

Or:

    template <typename T> struct UnConst_ { typedef T Type; };
    template <typename T> struct UnConst_<const T> { typedef T Type; };
    template <typename T> using UnConst = typename UnConst_<T>::Type;
    // If T is const, return the underlying non-const type.
    // Otherwise, just return T.

Now people can use your template metafunction without the pesky `::Type` or `::value` suffix.

#### Other hints

* To explicitly disable a template under certain circumstances, bind an unnamed template parameter to `kj::EnableIf`:

        template <typename T, typename = kj::EnableIf(!isConst<T>())>
        void mutate(T& ptr);
        // T must not be const.

* Say you're writing a template type with a constructor function like so:

        template <typename T>
        Wrapper<T> makeWrapper(T&& inner);
        // Wraps `inner` and returns the wrapper.
  
  Should `inner` be taken by reference or by value here? Both might be useful, depending on the use case. The right answer is actually to support both: if the input is an lvalue, take it by reference, but if it's an rvalue, take it by value (move). And as it turns out, if you write your declaration exactly as shown above, this is exactly what you get, because if the input is an lvalue, `T` will implicitly bind to a reference type, whereas if the input is an rvalue or rvalue reference, T will not be a reference.
  
  In general, you should assume KJ code in this pattern uses this rule, so if you are passing in an lvalue but don't actually want it wrapped by reference, wrap it in `kj::mv()`.

* Never use function or method pointers. Prefer templating across functors (like STL does), or for non-templates use `kj::Function` (which will handle this for you).

### Global Constructors

Do not declare global or static variables with dynamic constructors. Global constructors disproportionately hurt startup time because they force code to be paged in before it is really needed. They also are usually only needed by singletons, which you should not be using in general (see philosophy section).

You can have global constants of non-trivial class type as long as they are declared `constexpr`. If you want to declare complex data structures as constants, try to declare all the pieces as separate globals that reference each other, so that nothing has to be heap allocated and everything can be `constexpr`.

Use Clang's `-Wglobal-constructors` warning to catch mistakes.

### `dynamic_cast`

Do not use `dynamic_cast` as a way to implement polymorphism. That is, do not write long blocks of if/else statements each trying to cast an object to a different derived class to handle in a different way. Instead, extend the base class's interface to cover the functionality you need.

With that said, `dynamic_cast` is not always bad. It is fine to use `dynamic_cast` for "logistical" improvements, such as optimization. As a rule of thumb, imagine if `dynamic_cast` were replaced with a function that always returned null. Would your code still be correct (if, perhaps, slower, or with less detailed logging, etc.)? If so, then your use of `dynamic_cast` is fine.

#### `-fno-rtti`

The KJ and Cap'n Proto libraries are designed to function correctly when compiled with `-fno-rtti`. To that end, `kj::dynamicCastIfAvailable` is a version of `dynamic_cast` that, when compiled with `-fno-rtti`, always returns null, and KJ and Cap'n Proto code always uses this version.

We do NOT recommend disabling RTTI in your own code.

### Lambdas

Lamba capture lists must never use `=` to specify "capture all by value", because this makes it hard to review the capture list for possible lifetime issues.

Capture lists *may* use `&` ("capture all by reference") but *only* in cases where it is known that the lambda will not outlive the current stack frame. In fact, they generally *should* use `&` in this case, to make clear that there are no lifetime issues to think about.

### Use of Standard libraries

#### C++ Standard Library

The C++ standard library is old and full of a lot of cruft. Many APIs are designed in pre-C++11 styles that are no longer ideal. Mistakes like giving copy constructors to objects that own heap space (because in the absence of move semantics, it was needed for usability) and atomically-reference-counted strings (intended as an optimization to avoid so much heap copying, but actually a pessimization) are now baked into the library and cannot change. The `iostream` library was designed before anyone knew how to write good C++ code and is absolutely awful by today's standards. Some parts of the library, such as `<chrono>`, are over-engineered, designed by committees more interested in theoretical perfection than practicality. To add insult to injury, the library's naming style does not distinguish types from values.

For these reasons and others, KJ aims to be a replacement for the C++ standard libraries.

It is not there yet. As of this writing, the biggest missing piece is that KJ provides no implementation of maps or sets, nor a `sort()` function.

We recommend that KJ code use KJ APIs where available, falling back to C++ standard types when necessary. To avoid breaking clients later, avoid including C++ standard library headers from other headers; only include them from source files.

All users of the KJ library should familiarize themselves at least with the declarations in the following files, as you will use them all the time:

* `kj/common.h`
* `kj/memory.h`
* `kj/array.h`
* `kj/string.h`
* `kj/vector.h`
* `kj/debug.h`

#### C Library

As a general rule of thumb, C library functions documented in man section 3 should be treated with skepticism.

Do not use the C standard I/O functions -- your code should never contain `FILE*`. For formatting strings, `kj::str()` is much safer and easier than `sprintf()`. For debug logging, `KJ_DBG()` will produce more information with fewer keystrokes compared to `printf()`. For parsing, KJ's parser combinator library is cleaner and more powerful than `scanf()`. `fread()` and `fwrite()` imply buffering that you usually don't want; use `kj/io.h` instead, or raw file descriptors.

### Compiler warnings

Use the following warning settings with Clang or GCC:

* `-Wall -Wextra`: Enable most warnings.
* `-Wglobal-constructors`: (Clang-only) This catches global variables with constructors, which KJ style disallows (see above). You will, however, want to disable this warning in tests, since test frameworks use global constructors and are excepted from the style rule.
* `-Wno-sign-compare`: While comparison between signed and unsigned values could be a serious bug, we find that in practice this warning is almost always spurious.
* `-Wno-unused-parameter`: This warning is always spurious. I have never seen it find a real bug. Worse, it encourages people to delete parameter names which harms readability.

For development builds, `-Werror` should also be enabled. However, this should not be on by default in open source code as not everyone uses the same compiler or compiler version and different compiler versions often produce different warnings.

### Tools

We use:

* Clang for compiling.
* `KJ_DBG()` for simple debugging.
* Valgrind for complicated debugging.
* [Ekam](https://github.com/capnproto/ekam) for a build system.
* Git for version control.

## Irrelevant formatting rules

Many style guides dwell on formatting. We mention it only because it's vaguely nice to have some formatting consistency, but know that this section is the *least* relevant section of the document.

As a code reviewer, when you see a violation of formatting rules, think carefully about whether or not it really matters that you point it out. If you believe the author may be unfamiliar with the rules, it may be worth letting them know to read this document, if only so that they can try to be consistent in the future. However, it is NOT worth the time to comment on every misplaced whitespace. As long as the code is readable, move on.

### Naming

* Type names: `TitleCase`
* Variable, member, function, and method names: `camelCase`
* Constant and enumerant names: `CAPITAL_WITH_UNDERSCORES`
* Macro names: `CAPITAL_WITH_UNDERSCORES`, with an appropriate project-specific prefix like `KJ_` or `CAPNP_`.
* Namespaces: `oneword`. Namespaces should be kept short, because you'll have to type them a lot. The name of KJ itself was chosen for the sole purpose of making the namespace easy to type (while still being sufficiently unique). Use a nested namespace called `_` to contain package-private declarations.
* Files: `module-name.c++`, `module-name.h`, `module-name-test.c++`

**Rationale:** There has never been broad agreement on C++ naming style. The closest we have is the C++ standard library. Unfortunately, the C++ standard library made the awful decision of naming types and values in the same style, losing a highly useful visual cue that makes programming more pleasant, and preventing variables from being named after their type (which in many contexts is perfectly appropriate).

Meanwhile, the Java style, which KJ emulates, has been broadly adopted to varying degrees in other languages, from JavaScript to Haskell. Using a similar style in KJ code makes it less jarring to switch between C++ and those other languages. Being consistent with JavaScript is especially useful because it is the one language that everyone pretty much has to use, due to its use in the web platform.

There has also never been any agreement on C++ file extensions, for some reason. The extension `.c++`, though not widely used, is accepted by all reasonable tools and is clearly the most precise choice.

### Spacing and bracing

* Indents are two spaces.
* Never use tabs.
* Maximum line length is 100 characters.
* Indent continuation lines for braced init lists by two spaces.
* Indent all other continuation lines by four spaces.
* Alternatively, line up continuation lines with previous lines if it makes them easier to read.
* Place a space between a keyword and an open parenthesis, e.g.: `if (foo)`
* Do not place a space between a function name and an open parenthesis, e.g.: `foo(bar)`
* Place an opening brace at the end of the statement which initiates the block, not on its own line.
* Place a closing brace on a new line indented the same as the parent block. If there is post-brace code related to the block (e.g. `else` or `while`), place it on the same line as the closing brace.
* Always place braces around a block *unless* the block is so short that it can actually go on the same line as the introductory `if` or `while`, e.g.: `if (done) return;`.
* `case` statements are indented within the `switch`, and their following blocks are **further** indented (so the actual statements in a case are indented four spaces more than the `switch`).
* `public:`, `private:`, and `protected:` are reverse-indented by one stop.
* Statements inside a `namespace` are **not** indented unless the namespace is a short block that is just forward-declaring things at the top of a file.
* Set your editor to strip trailing whitespace on save, otherwise other people who use this setting will see spurious diffs when they edit a file after you.

<br>

    if (foo) {
      bar();
    } else if (baz) {
      qux(quux);
    } else {
      corge();
    }

    if (done) return;

    switch (grault) {
      case GARPLY:
        print("mew");
        break;
      case WALDO: {  // note: needs braces due to variable
        Location location = findWaldo();
        print(location);
        break;
      }
    }

<br>

    namespace external {
      class Forward;
      class Declarations;
      namespace nested {
        class More;
      }
    }

    namespace myproj {

    class Fred {
    public:
      Fred();
      ~Fred();
    private:
      int plugh;
    };

    }  // namespace myproj

**Rationale:** Code which is inconsistently or sloppily formatted gives the impression that the author is not observant or simply doesn't care about quality, and annoys other people trying to read your code.

Other than that, there is absolutely no good reason to space things one way or another.

### Comments

* Always use line comments (`//`). Never use block comments (`/**/`).

  **Rationale:** Block comments don't nest. Block comments tend to be harder to re-arrange, whereas a group of line comments can be moved easily. Also, typing `*` is just way harder than typing `/` so why would you want to?

* Write comments that add useful information that the reader might not already know. Do NOT write comments which say things that are already blatantly obvious from the code. For example, for a function `void frob(Bar bar)`, do not write a comment `// Frobs the Bar.`; that's already obvious. It's better to have no comment.

* Doc comments go **after** the declaration. If the declaration starts a block, the doc comment should go inside the block at the top. A group of related declarations can have a single group doc comment after the last one as long as there are no black lines between the declarations.

        int foo();
        // This is documentation for foo().

        class Bar {
          // This is documentation for Bar.
        public:
          Bar();

          inline int baz() { return 5; }
          inline int qux() { return 6; }
          // This is documentation for baz() and qux().
        };

  **Rationale:** When you start reading a doc comment, the first thing you want to know is *what the heck is being documented*. Having to scroll down through a long comment to see the declaration, then back up to read the docs, is bad. Sometimes, people actually repeat the declaration at the top of the comment just so that it's visible. This is silly. Let's just put the comment after the declaration.

* TODO comments are of the form `// TODO(type): description`, where `type` is one of:
  * `now`: Do before next `git push`.
  * `soon`: Do before next stable release.
  * `someday`: A feature that might be nice to have some day, but no urgency.
  * `perf`: Possible performance enhancement.
  * `security`: Possible security concern. (Used for low-priority issues. Obviously, code with serious security problems should never be written in the first place.)
  * `cleanup`: An improvement to maintainability with no user-visible effects.
  * `port`: Things to do when porting to a new platform.
  * `test`: Something that needs better testing.
  * `msvc`: Something to revisit when the next, hopefully less-broken version of Microsoft Visual Studio becomes available.
  * others: Additional TODO types may be defined for use in certain contexts.

  **Rationale:** Google's guide suggests that TODOs should bear the name of their author ("the person to ask for more information about the comment"), but in practice there's no particular reason why knowing the author is more useful for TODOs than for any other comment (or, indeed, code), and anyway that's what `git blame` is for. Meanwhile, having TODOs classified by type allows for useful searches, so that e.g. release scripts can error out if release-blocking TODOs are present.

### File templates

Generally, a "module" should consist of three files: `module.h`, `module.c++`, and `module-test.c++`. One or more of these can be omitted if it would otherwise be empty. Use the following templates when creating new files.

Headers:

    // Project Name - Project brief description
    // Copyright (c) 2015 Primary Author and contributors
    //
    // Licensed under the Whatever License blah blah no warranties.

    #pragma once
    // Documentation for file.

    #include <kj/common.h>

    namespace myproject {

    // declarations

    namespace _ {  // private

    // private declarations

    }  // namespace _ (private)

    }  // namespace myproject

Source code:

    // Project Name - Project brief description
    // Copyright (c) 2015 Primary Author and contributors
    //
    // Licensed under the Whatever License blah blah no warranties.

    #include "this-module.h"
    #include <other-module.h>

    namespace myproject {

    // definitions

    }  // namespace myproject

Test:

    // Project Name - Project brief description
    // Copyright (c) 2015 Primary Author and contributors
    //
    // Licensed under the Whatever License blah blah no warranties.

    #include "this-module.h"
    #include <other-module.h>

    namespace myproject {
    namespace {

    // KJ_TESTs

    }  // namespace
    }  // namespace myproject

Note that in both the source and test files, you should *always* include the corresponding header first, in order to ensure that it is self-contained (does not secretly require including some other header before it).



kjdoc/tour.md
--------------------------------------
---
title: A tour of KJ
---

This page is a tour through the functionality provided by KJ. It is intended for developers new to KJ who want to learn the ropes.

**This page is not an API reference.** KJ's reference documentation is provided by comments in the headers themselves. Keeping reference docs in the headers makes it easy to find them using your editor's "jump to declaration" hotkey. It also ensures that the documentation is never out-of-sync with the version of KJ you are using.

Core Programming
======================================================================

This section covers core KJ features used throughout nearly all KJ-based code.

Every KJ developer should familiarize themselves at least with this section.

## Core Utility Functions

### Argument-passing: move, copy, forward

`kj::mv` has exactly the same semantics as `std::move`, but takes fewer keystrokes to type. Since this is used extraordinarily often, saving a few keystrokes really makes a legitimate difference. If you aren't familiar with `std::move`, I recommend reading up on [C++11 move semantics](https://stackoverflow.com/questions/3106110/what-is-move-semantics).

`kj::cp` is invoked in a similar way to `kj::mv`, but explicitly invokes the copy constructor of its argument, returning said copy. This is occasionally useful when invoking a function that wants an rvalue reference as a parameter, which normally requires pass-by-move, but you really want to pass it a copy.

`kj::fwd`, is equivalent to `std::forward`. It is used to implement [perfect forwarding](https://en.cppreference.com/w/cpp/utility/forward), that is, forwarding arbitrary arguments from a template function into some other function without understanding their types.

### Deferring code to scope exit

This macro declares some code which must execute when exiting the current scope (whether normally or by exception). It is essentially a shortcut for declaring a class with a destructor containing said code, and instantiating that destructor. Example:

```c++
void processFile() {
  int fd = open("file.txt", O_RDONLY);
  KJ_ASSERT(fd >= 0);

  // Make sure file is closed on return.
  KJ_DEFER(close(fd));

  // ... do something with the file ...
}
```

You can also pass a multi-line block (in curly braces) as the argument to `KJ_DEFER`.

There is also a non-macro version, `kj::defer`, which takes a lambda as its argument, and returns an object that invokes that lambda on destruction. The returned object has move semantics. This is convenient when the scope of the deferral isn't necessarily exactly function scope, such as when capturing context in a callback. Example:

```c++
kj::Function<void(int arg)> processFile() {
  int fd = open("file.txt", O_RDONLY);
  KJ_ASSERT(fd >= 0);

  // Make sure file is closed when the returned function
  // is eventually destroyed.
  auto deferredClose = kj::defer([fd]() { close(fd); });

  return [fd, deferredClose = kj::mv(deferredClose)]
         (int arg) {
    // ... do something with fd and arg ...
  }
}
```

Sometimes, you want a deferred action to occur only when the scope exits normally via `return`, or only when it exits due to an exception. For those purposes, `KJ_ON_SCOPE_SUCCESS` and `KJ_ON_SCOPE_FAILURE` may be used, with the same syntax as `KJ_DEFER`.

### Size and range helpers

`kj::size()` accepts a built-in array or a container as an argument, and returns the number of elements. In the case of a container, the container must implement a `.size()` method. The idea is that you can use this to find out how many iterations a range-based `for` loop on that container would execute. That said, in practice `kj::size` is most commonly used with arrays, as a shortcut for something like `sizeof(array) / sizeof(array[0])`.

```c++
int arr[15];
KJ_ASSERT(kj::size(arr) == 15);
```

`kj::range(i, j)` returns an iterable that contains all integers from `i` to `j` (including `i`, but not including `j`). This is typically used in `for` loops:

```c++
for (auto i: kj::range(5, 10)) {
  KJ_ASSERT(i >= 5 && i < 10);
}
```

In the very-common case of iterating from zero, `kj::zeroTo(i)` should be used instead of `kj::range(0, i)`, in order to avoid ambiguity about what type of integer should be generated.

`kj::indices(container)` is equivalent to `kj::zeroTo(kj::size(container))`. This is extremely convenient when iterating over parallel arrays.

```c++
KJ_ASSERT(foo.size() == bar.size());
for (auto i: kj::indices(foo)) {
  foo[i] = bar[i];
}
```

`kj::repeat(value, n)` returns an iterable that acts like an array of size `n` where every element is `value`. This is not often used, but can be convenient for string formatting as well as generating test data.

### Casting helpers

`kj::implicitCast<T>(value)` is equivalent to `static_cast<T>(value)`, but will generate a compiler error if `value` cannot be implicitly cast to `T`. For example, `static_cast` can be used for both upcasts (derived type to base type) and downcasts (base type to derived type), but `implicitCast` can only be used for the former.

`kj::downcast<T>(value)` is equivalent to `static_cast<T>(value)`, except that when compiled in debug mode with RTTI available, a runtime check (`dynamic_cast`) will be performed to verify that `value` really has type `T`. Use this in cases where you are casting a base type to a derived type, and you are confident that the object is actually an instance of the derived type. The debug-mode check will help you catch bugs.

`kj::dynamicDowncastIfAvailable<T>(value)` is like `dynamic_cast<T*>(value)` with two differences. First, it returns `kj::Maybe<T&>` instead of `T*`. Second, if the program is compiled without RTTI enabled, the function always returns null. This function is intended to be used to implement optimizations, where the code can do something smarter if `value` happens to be of some specific type -- but if RTTI is not available, it is safe to skip the optimization. See [KJ idiomatic use of dynamic_cast](../style-guide.md#dynamic_cast) for more background.

### Min/max, numeric limits, and special floats

`kj::min()` and `kj::max()` return the minimum and maximum of the input arguments, automatically choosing the appropriate return type even if the inputs are of different types.

`kj::minValue` and `kj::maxValue` are special constants that, when cast to an integer type, become the minimum or maximum value of the respective type. For example:

```c++
int16_t i = kj::maxValue;
KJ_ASSERT(i == 32767);
```

`kj::inf()` evaluates to floating-point infinity, while `kj::nan()` evaluates to floating-point NaN. `kj::isNaN()` returns true if its argument is NaN.

### Explicit construction and destruction

`kj::ctor()` and `kj::dtor()` explicitly invoke a constructor or destructor in a way that is readable and convenient. The first argument is a reference to memory where the object should live.

These functions should almost never be used in high-level code. They are intended for use in custom memory management, or occasionally with unions that contain non-trivial types (but consider using `kj::OneOf` instead). You must understand C++ memory aliasing rules to use these correctly.

## Ownership and memory management

KJ style makes heavy use of [RAII](../style-guide.md#raii-resource-acquisition-is-initialization). KJ-based code should never use `new` and `delete` directly. Instead, use the utilities in this section to manage memory in a RAII way.

### Owned pointers, heap allocation, and disposers

`kj::Own<T>` is a pointer to a value of type `T` which is "owned" by the holder. When a `kj::Own` goes out-of-scope, the value it points to will (typically) be destroyed and freed.

`kj::Own` has move semantics. Thus, when used as a function parameter or return type, `kj::Own` indicates that ownership of the object is being transferred.

`kj::heap<T>(args...)` allocates an object of type `T` on the heap, passing `args...` to its constructor, and returns a `kj::Own<T>`. This is the most common way to create owned objects.

However, a `kj::Own` does not necessarily refer to a heap object. A `kj::Own` is actually implemented as a pair of a pointer to the object, and a pointer to a `kj::Disposer` object that knows how to destroy it; `kj::Own`'s destructor invokes the disposer. `kj::Disposer` is an abstract interface with many implementations. `kj::heap` uses an implementation that invokes the object's destructor then frees its underlying space from the heap (like `delete` does), but other implementations exist. Alternative disposers allow an application to control memory allocation more precisely when desired.

Some example uses of disposers include:

* `kj::fakeOwn(ref)` returns a `kj::Own` that points to `ref` but doesn't actually destroy it. This is useful when you know for sure that `ref` will outlive the scope of the `kj::Own`, and therefore heap allocation is unnecessary. This is common in cases where, for example, the `kj::Own` is being passed into an object which itself will be destroyed before `ref` becomes invalid. It also makes sense when `ref` is actually a static value or global that lives forever.
* `kj::refcounted<T>(args...)` allocates a `T` which uses reference counting. It returns a `kj::Own<T>` that represents one reference to the object. Additional references can be created by calling `kj::addRef(*ptr)`. The object is destroyed when no more `kj::Own`s exist pointing at it. Note that `T` must be a subclass of `kj::Refcounted`. If references may be shared across threads, then atomic refcounting must be used; use `kj::atomicRefcounted<T>(args...)` and inherit `kj::AtomicRefcounted`. Reference counting should be using sparingly; see [KJ idioms around reference counting](../style-guide.md#reference-counting) for a discussion of when it should be used and why it is designed the way it is.
* `kj::attachRef(ref, args...)` returns a `kj::Own` pointing to `ref` that actually owns `args...`, so that when the `kj::Own` goes out-of-scope, the other arguments are destroyed. Typically these arguments are themselves `kj::Own`s or other pass-by-move values that themselves own the object referenced by `ref`. `kj::attachVal(value, args...)` is similar, where `value` is a pass-by-move value rather than a reference; a copy of it will be allocated on the heap. Finally, `ownPtr.attach(args...)` returns a new `kj::Own` pointing to the same value that `ownPtr` pointed to, but such that `args...` are owned as well and will be destroyed together. Attachments are always destroyed after the thing they are attached to.
* `kj::SpaceFor<T>` contains enough space for a value of type `T`, but does not construct the value until its `construct(args...)` method is called. That method returns an `kj::Own<T>`, whose disposer destroys the value. `kj::SpaceFor` is thus a safer way to perform manual construction compared to invoking `kj::ctor()` and `kj::dtor()`.

These disposers cover most use cases, but you can also implement your own if desired. `kj::Own` features a constructor overload that lets you pass an arbitrary disposer.

### Arrays

`kj::Array<T>` is similar to `kj::Own<T>`, but points to (and owns) an array of `T`s.

A `kj::Array<T>` can be allocated with `kj::heapArray<T>(size)`, if `T` can be default-constructed. Otherwise, you will need to use a `kj::ArrayBuilder<T>` to build the array. First call `kj::heapArrayBuilder<T>(size)`, then invoke the builder's `add(value)` method to add each element, then finally call its `finish()` method to obtain the completed `kj::Array<T>`. `ArrayBuilder` requires that you know the final size before you start; if you don't, you may want to use `kj::Vector<T>` instead.

Passing a `kj::Array<T>` implies an ownership transfer. If you merely want to pass a pointer to an array, without transferring ownership, use `kj::ArrayPtr<T>`. This type essentially encapsulates a pointer to the beginning of the array, plus its size. Note that a `kj::ArrayPtr` points to _the underlying memory_ backing a `kj::Array`, not to the `kj::Array` itself; thus, moving a `kj::Array` does NOT invalidate any `kj::ArrayPtr`s already pointing at it. You can also construct a `kj::ArrayPtr` pointing to any C-style array (doesn't have to be a `kj::Array`) using `kj::arrayPtr(ptr, size)` or `kj::arrayPtr(beginPtr, endPtr)`.

Both `kj::Array` and `kj::ArrayPtr` contain a number of useful methods, like `slice()`. Be sure to check out the class definitions for more details.

## Strings

A `kj::String` is a segment of text. By convention, this text is expected to be Unicode encoded in UTF-8. But, `kj::String` itself is not Unicode-aware; it is merely an array of `char`s.

NUL characters (`'\0'`) are allowed to appear anywhere in a string and do not terminate the string. However, as a convenience, the buffer backing a `kj::String` always has an additional NUL character appended to the end (but not counted in the size). This allows the text in a `kj::String` to be passed to legacy C APIs that use NUL-terminated strings without an extra copy; use the `.cStr()` method to get a `const char*` for such cases. (Of course, keep in mind that if the string contains NUL characters other than at the end, legacy C APIs will interpret the string as truncated at that point.)

`kj::StringPtr` represents a pointer to a `kj::String`. Similar to `kj::ArrayPtr`, `kj::StringPtr` does not point at the `kj::String` object itself, but at its backing array. Thus, moving a `kj::String` does not invalidate any `kj::StringPtr`s. This is a major difference from `std::string`! Moving an `std::string` invalidates all pointers into its backing buffer (including `std::string_view`s), because `std::string` inlines small strings as an optimization. This optimization may seem clever, but means that `std::string` cannot safely be used as a way to hold and transfer ownership of a text buffer. Doing so can lead to subtle, data-dependent bugs; a program might work fine until someone gives it an unusually small input, at which point it segfaults. `kj::String` foregoes this optimization for simplicity.

Also similar to `kj::ArrayPtr`, a `kj::StringPtr` does not have to point at a `kj::String`. It can be initialized from a string literal or any C-style NUL-terminated `const char*` without making a copy. Also, KJ defines the special literal suffix `_kj` to write a string literal whose type is implicitly `kj::StringPtr`.

```c++
// It's OK to initialize a StringPtr from a classic literal.
// No copy is performed; the StringPtr points directly at
// constant memory.
kj::StringPtr foo = "foo";

// But if you add the _kj suffix, then you don't even need
// to declare the type. `bar` will implicitly have type
// kj::StringPtr. Also, this version can be declared
// `constexpr`.
constexpr auto bar = "bar"_kj;
```

### Stringification

To allocate and construct a `kj::String`, use `kj::str(args...)`. Each argument is stringified and the results are concatenated to form the final string. (You can also allocate an uninitialized string buffer with `kj::heapString(size)`.)

```c++
kj::String makeGreeting(kj::StringPtr name) {
  return kj::str("Hello, ", name, "!");
}
```

KJ knows how to stringify most primitive types as well as many KJ types automatically. Note that integers will be stringified in base 10; if you want hexadecimal, use `kj::hex(i)` as the parameter to `kj::str()`.

You can additionally extend `kj::str()` to work with your own types by declaring a stringification method using `KJ_STRINGIFY`, like so:

```c++
enum MyType { A, B, C };
kj::StringPtr KJ_STRINGIFY(MyType value) {
  switch (value) {
    case A: return "A"_kj;
    case B: return "B"_kj;
    case C: return "C"_kj;
  }
  KJ_UNREACHABLE;
}
```

The `KJ_STRINGIFY` declaration should appear either in the same namespace where the type is defined, or in the global scope. The function can return any random-access iterable sequence of `char`, such as a `kj::String`, `kj::StringPtr`, `kj::ArrayPtr<char>`, etc. As an alternative to `KJ_STRINGIFY`, you can also declare a `toString()` method on your type, with the same return type semantics.

When constructing very large, complex strings -- for example, when writing a code generator -- consider using `kj::StringTree`, which maintains a tree of strings and only concatenates them at the very end. For example, `kj::strTree(foo, kj::strTree(bar, baz)).flatten()` only performs one concatenation, whereas `kj::str(foo, kj::str(bar, baz))` would perform a redundant intermediate concatenation.

## Core Utility Types

### Maybes

`kj::Maybe<T>` is either `nullptr`, or contains a `T`. In KJ-based code, nullable values should always be expressed using `kj::Maybe`. Primitive pointers should never be null. Use `kj::Maybe<T&>` instead of `T*` to express that the pointer/reference can be null.

In order to dereference a `kj::Maybe`, you must use the `KJ_IF_MAYBE` macro, which behaves like an `if` statement.

```c++
kj::Maybe<int> maybeI = 123;
kj::Maybe<int> maybeJ = nullptr;

KJ_IF_MAYBE(i, maybeI) {
  // This block will execute, with `i` being a
  // pointer into `maybeI`'s value. In a better world,
  // `i` would be a reference rather than a pointer,
  // but we couldn't find a way to trick the compiler
  // into that.
  KJ_ASSERT(*i == 123);
} else {
  KJ_FAIL_ASSERT("can't get here");
}

KJ_IF_MAYBE(j, maybeJ) {
  KJ_FAIL_ASSERT("can't get here");
} else {
  // This block will execute.
}
```

Note that `KJ_IF_MAYBE` forces you to think about the null case. This differs from `std::optional`, which can be dereferenced using `*`, resulting in undefined behavior if the value is null.

Similarly, `map()` and `orDefault()` allow transforming and retrieving the stored value in a safe manner without complex control flows.

Performance nuts will be interested to know that `kj::Maybe<T&>` and `kj::Maybe<Own<T>>` are both optimized such that they take no more space than their underlying pointer type, using a literal null pointer to indicate nullness. For other types of `T`, `kj::Maybe<T>` must maintain an extra boolean and so is somewhat larger than `T`.

### Variant types

`kj::OneOf<T, U, V>` is a variant type that can be assigned to exactly one of the input types. To unpack the variant, use `KJ_SWITCH_ONEOF`:

```c++
void handle(kj::OneOf<int, kj::String> value) {
  KJ_SWITCH_ONEOF(value) {
    KJ_CASE_ONEOF(i, int) {
      // Note that `i` is an lvalue reference to the content
      // of the OneOf. This differs from `KJ_IF_MAYBE` where
      // the variable is a pointer.
      handleInt(i);
    }
    KJ_CASE_ONEOF(s, kj::String) {
      handleString(s);
    }
  }
}
```

Often, in real-world usage, the type of each variant in a `kj::OneOf` is not sufficient to understand its meaning; sometimes two different variants end up having the same type used for different purposes. In these cases, it would be useful to assign a name to each variant. A common way to do this is to define a custom `struct` type for each variant, and then declare the `kj::OneOf` using those:

```c++
struct NotStarted {
  kj::String filename;
};
struct Running {
  kj::Own<File> file;
};
struct Done {
  kj::String result;
};

typedef kj::OneOf<NotStarted, Running, Done> State;
```

### Functions

`kj::Function<ReturnType(ParamTypes...)>` represents a callable function with the given signature. A `kj::Function` can be initialized from any callable object, such as a lambda, function pointer, or anything with `operator()`. `kj::Function` is useful when you want to write an API that accepts a lambda callback, without defining the API itself as a template. `kj::Function` supports move semantics.

`kj::ConstFunction` is like `kj::Function`, but is used to indicate that the function should be safe to call from multiple threads. (See [KJ idioms around constness and thread-safety](../style-guide.md#constness).)

A special optimization type, `kj::FunctionParam`, is like `kj::Function` but designed to be used specifically as the type of a callback parameter to some other function where that callback is only called synchronously; i.e., the callback won't be called anymore after the outer function returns. Unlike `kj::Function`, a `kj::FunctionParam` can be constructed entirely on the stack, with no heap allocation.

### Vectors (growable arrays)

Like `std::vector`, `kj::Vector` is an array that supports appending an element in amortized O(1) time. When the underlying backing array is full, an array of twice the size is allocated and all elements moved.

### Hash/tree maps/sets... and tables

`kj::HashMap`, `kj::HashSet`, `kj::TreeMap`, and `kj::TreeSet` do what you'd expect, with modern lambda-oriented interfaces that are less awkward than the corresponding STL types.

All of these types are actually specific instances of the more-general `kj::Table`. A `kj::Table` can have any number of columns (whereas "sets" have exactly 1 and "maps" have exactly 2), and can maintain indexes on multiple columns at once. Each index can be hash-based, tree-based, or a custom index type that you provide.

Unlike STL's, KJ's hashtable-based containers iterate in a well-defined deterministic order based on the order of insertion and removals. Deterministic behavior is important for reproducibility, which is important not just for debugging, but also in distributed systems where multiple systems must independently reproduce the same state. KJ's hashtable containers are also faster than `libstdc++`'s in benchmarks.

KJ's tree-based containers use a b-tree design for better memory locality than the more traditional red-black trees. The implementation is tuned to avoid code bloat by keeping most logic out of templates, though this does make it slightly slower than `libstdc++`'s `map` and `set` in benchmarks.

`kj::hashCode(params...)` computes a hash across all the inputs, appropriate for use in a hash table. It is extensible in a similar fashion to `kj::str()`, by using `KJ_HASHCODE` or defining a `.hashCode()` method on your custom types. `kj::Table`'s hashtable-based index uses `kj::hashCode` to compute hashes.

## Debugging and Observability

KJ believes that there is no such thing as bug-free code. Instead, we must expect that our code will go wrong, and try to extract as much information as possible when it does. To that end, KJ provides powerful assertion macros designed for observability. (Be sure also to read about [KJ's exception philosophy](../style-guide.md#exceptions); this section describes the actual APIs involved.)

### Assertions

Let's start with the basic assert:

```c++
KJ_ASSERT(foo == bar.baz, "the baz is not foo", bar.name, i);
```

When `foo == bar.baz` evaluates false, this line will throw an exception with a description like this:

```
src/file.c++:52: failed: expected foo == bar.baz [123 == 321]; the baz is not foo; bar.name = "banana"; i = 5
stack: libqux.so@0x52134 libqux.so@0x16f582 bin/corge@0x12515 bin/corge@0x5552
```

Notice all the information this contains:

* The file and line number in the source code where the assertion macro was used.
* The condition which failed.
* The stringified values of the operands to the condition, i.e. `foo` and `bar.baz` (shown in `[]` brackets).
* The values of all other parameters passed to the assertion, i.e. `"the baz is not foo"`, `bar.name`, and `i`. For expressions that aren't just string literals, both the expression and the stringified result of evaluating it are shown.
* A numeric stack trace. If possible, the addresses will be given relative to their respective binary, so that ASLR doesn't make traces useless. The trace can be decoded with tools like `addr2line`. If possible, KJ will also shell out to `addr2line` itself to produce a human-readable trace.

Note that the work of producing an error description happens only in the case that it's needed. If the condition evaluates true, then that is all the work that is done.

`KJ_ASSERT` should be used in cases where you are checking conditions that, if they fail, represent a bug in the code where the assert appears. On the other hand, when checking for preconditions -- i.e., bugs in the _caller_ of the code -- use `KJ_REQUIRE` instead:

```c++
T& operator[](size_t i) {
  KJ_REQUIRE(i < size(), "index out-of-bounds");
  // ...
}
```

`KJ_REQUIRE` and `KJ_ASSERT` do exactly the same thing; using one or the other is only a matter of self-documentation.

`KJ_FAIL_ASSERT(...)` should be used instead of `KJ_ASSERT(false, ...)` when you want a branch always to fail.

Assertions operate exactly the same in debug and release builds. To express a debug-only assertion, you can use `KJ_DASSERT`. However, we highly recommend letting asserts run in production, as they are frequently an invaluable tool for tracking down bugs that weren't covered in testing.

### Logging

The `KJ_LOG` macro can be used to log messages meant for the developer or operator without interrupting control flow.

```c++
if (foo.isWrong()) {
  KJ_LOG(ERROR, "the foo is wrong", foo);
}
```

The first parameter is the log level, which can be `INFO`, `WARNING`, `ERROR`, or `FATAL`. By default, `INFO` logs are discarded, while other levels are displayed. For programs whose main function is based on `kj/main.h`, the `-v` flag can be used to enable `INFO` logging. A `FATAL` log should typically be followed by `abort()` or similar.

Parameters other than the first are stringified in the same manner as with `KJ_ASSERT`. These parameters will not be evaluated at all, though, if the specified log level is not enabled.

By default, logs go to standard error. However, you can implement a `kj::ExceptionCallback` (in `kj/exception.h`) to capture logs and customize how they are handled.

### Debug printing

Let's face it: "printf() debugging" is easy and effective. KJ embraces this with the `KJ_DBG()` macro.

```c++
KJ_DBG("hi", foo, bar, baz.qux)
```

`KJ_DBG(...)` is equivalent to `KJ_LOG(DEBUG, ...)` -- logging at the `DEBUG` level, which is always enabled. The dedicated macro exists for brevity when debugging. `KJ_DBG` is intended to be used strictly for temporary debugging code that should never be committed. We recommend setting up commit hooks to reject code that contains invocations of `KJ_DBG`.

### System call error checking

KJ includes special variants of its assertion macros that convert traditional C API error conventions into exceptions.

```c++
int fd;
KJ_SYSCALL(fd = open(filename, O_RDONLY), "couldn't open the document", filename);
```

This macro evaluates the first parameter, which is expected to be a system call. If it returns a negative value, indicating an error, then an exception is thrown. The exception description incorporates a description of the error code communicated by `errno`, as well as the other parameters passed to the macro (stringified in the same manner as other assertion/logging macros do).

Additionally, `KJ_SYSCALL()` will automatically retry calls that fail with `EINTR`. Because of this, it is important that the expression is idempotent.

Sometimes, you need to handle certain error codes without throwing. For those cases, use `KJ_SYSCALL_HANDLE_ERRORS`:

```c++
int fd;
KJ_SYSCALL_HANDLE_ERRORS(fd = open(filename, O_RDONLY)) {
  case ENOENT:
    // File didn't exist, return null.
    return nullptr;
  default:
    // Some other error. The error code (from errno) is in a local variable `error`.
    // `KJ_FAIL_SYSCALL` expects its second parameter to be this integer error code.
    KJ_FAIL_SYSCALL("open()", error, "couldn't open the document", filename);
}
```

On Windows, two similar macros are available based on Windows API calling conventions: `KJ_WIN32` works with API functions that return a `BOOLEAN`, `HANDLE`, or pointer type. `KJ_WINSOCK` works with Winsock APIs that return negative values to indicate errors. Some Win32 APIs follow neither of these conventions, in which case you will have to write your own code to check for an error and use `KJ_FAIL_WIN32` to turn it into an exception.

### Alternate exception types

As described in [KJ's exception philosophy](../style-guide.md#exceptions), KJ supports a small set of exception types. Regular assertions throw `FAILED` exceptions. `KJ_SYSCALL` usually throws `FAILED`, but identifies certain error codes as `DISCONNECTED` or `OVERLOADED`. For example, `ECONNRESET` is clearly a `DISCONNECTED` exception.

If you wish to manually construct and throw a different exception type, you may use `KJ_EXCEPTION`:

```c++
kj::Exception e = KJ_EXCEPTION(DISCONNECTED, "connection lost", addr);
```

### Throwing and catching exceptions

KJ code usually should not use `throw` or `catch` directly, but rather use KJ's wrappers:

```c++
// Throw an exception.
kj::Exception e = ...;
kj::throwFatalException(kj::mv(e));

// Run some code catching exceptions.
kj::Maybe<kj::Exception> maybeException = kj::runCatchingExceptions([&]() {
  doSomething();
});
KJ_IF_MAYBE(e, maybeException) {
  // handle exception
}
```

These wrappers perform some extra bookkeeping:
* `kj::runCatchingExceptions()` will catch any kind of exception, whether it derives from `kj::Exception` or not, and will do its best to convert it into a `kj::Exception`.
* `kj::throwFatalException()` and `kj::throwRecoverableException()` invoke the thread's current `kj::ExceptionCallback` to throw the exception, allowing apps to customize how exceptions are handled. The default `ExceptionCallback` makes sure to throw the exception in such a way that it can be understood and caught by code looking for `std::exception`, such as the C++ library's standard termination handler.
* These helpers also work, to some extent, even when compiled with `-fno-exceptions` -- see below. (Note that "fatal" vs. "recoverable" exceptions are only different in this case; when exceptions are enabled, they are handled the same.)

### Supporting `-fno-exceptions`

KJ strongly recommends using C++ exceptions. However, exceptions are controversial, and many C++ applications are compiled with exceptions disabled. Some KJ-based libraries (especially Cap'n Proto) would like to accommodate such users. To that end, KJ's exception and assertion infrastructure is designed to degrade gracefully when compiled without exception support. In this case, exceptions are split into two types:

* Fatal exceptions, when compiled with `-fno-exceptions`, will terminate the program when thrown.
* Recoverable exceptions, when compiled with `-fno-exceptions`, will be recorded on the side. Control flow then continues normally, possibly using a dummy value or skipping code which cannot execute. Later, the application can check if an exception has been raised and handle it.

`KJ_ASSERT`s (and `KJ_REQUIRE`s) are fatal by default. To make them recoverable, add a "recovery block" after the assert:

```c++
kj::StringPtr getItem(int i) {
  KJ_REQUIRE(i >= 0 && i < items.size()) {
    // This is the recovery block. Recover by simply returning an empty string.
    return "";
  }
  return items[i];
}
```

When the code above is compiled with exceptions enabled, an out-of-bounds index will result in an exception being thrown. But when compiled with `-fno-exceptions`, the function will store the exception off to the side (in KJ), and then return an empty string.

A recovery block can indicate that control flow should continue normally even in case of error by using a `break` statement.

```c++
void incrementBy(int i) {
  KJ_REQUIRE(i >= 0, "negative increments not allowed") {
    // Pretend the caller passed `0` and continue.
    i = 0;
    break;
  }

  value += i;
}
```

**WARNING:** The recovery block is executed even when exceptions are enabled. The exception is thrown upon exit from the block (even if a `return` or `break` statement is present). Therefore, be careful about side effects in the recovery block. Also, note that both GCC and Clang have a longstanding bug where a returned value's destructor is not called if the return is interrupted by an exception being thrown. Therefore, you must not return a value with a non-trivial destructor from a recovery block.

There are two ways to handle recoverable exceptions:

* Use `kj::runCatchingExceptions()`. When compiled with `-fno-exceptions`, this function will arrange for any recoverable exception to be stored off to the side. Upon completion of the given lambda, `kj::runCatchingExceptions()` will return the exception.
* Write a custom `kj::ExceptionCallback`, which can handle exceptions in any way you choose.

Note that while most features of KJ work with `-fno-exceptions`, some of them have not been carefully written for this case, and may trigger fatal exceptions too easily. People relying on this mode will have to tread carefully.

### Exceptions in Destructors

Bugs can occur anywhere -- including in destructors. KJ encourages applications to detect bugs using assertions, which throw exceptions. As a result, exceptions can be thrown in destructors. There is no way around this. You cannot simply declare that destructors shall not have bugs.

Because of this, KJ recommends that all destructors be declared with `noexcept(false)`, in order to negate C++11's unfortunate decision that destructors should be `noexcept` by default.

However, this does not solve C++'s Most Unfortunate Decision, namely that throwing an exception from a destructor that was called during an unwind from another exception always terminates the program. It is very common for exceptions to cause "secondary" exceptions during unwind. For example, the destructor of a buffered stream might check whether the buffer has been flushed, and raise an exception if it has not, reasoning that this is a serious bug that could lead to data loss. But if the program is already unwinding due to some other exception, then it is likely that the failure to flush the buffer is because of that other exception. The "secondary" exception might as well be ignored. Terminating the program is the worst possible response.

To work around the MUD, KJ offers two tools:

First, during unwind from one exception, KJ will handle all "recoverable" exceptions as if compiled with `-fno-exceptions`, described in the previous section. So, whenever writing assertions in destructors, it is a good idea to give them a recovery block like `{break;}` or `{return;}`.

```c++
BufferedStream::~BufferedStream() noexcept(false) {
  KJ_REQUIRE(buffer.size() == 0, "buffer was not flushed; possible data loss") {
    // Don't throw if we're unwinding!
    break;
  }
}
```

Second, `kj::UnwindDetector` can be used to squelch exceptions during unwind. This is especially helpful in cases where your destructor needs to call complex external code that wasn't written with destructors in mind. Use it like so:

```c++
class Transaction {
public:
  // ...

private:
  kj::UnwindDetector unwindDetector;
  // ...
};

Transaction::~Transaction() noexcept(false) {
  unwindDetector.catchExceptionsIfUnwinding([&]() {
    if (!committed) {
      rollback();
    }
  });
}
```

Core Systems
======================================================================

This section describes KJ APIs that control process execution and low-level interactions with the operating system. Most users of KJ will need to be familiar with most of this section.

## Threads and Synchronization

`kj::Thread` creates a thread in which the lambda passed to `kj::Thread`'s constructor will be executed. `kj::Thread`'s destructor waits for the thread to exit before continuing, and rethrows any exception that had been thrown from the thread's main function -- unless the thread's `.detach()` method has been called, in which case `kj::Thread`'s destructor does nothing.

`kj::MutexGuarded<T>` holds an instance of `T` that is protected by a mutex. In order to access the protected value, you must first create a lock. `.lockExclusive()` returns `kj::Locked<T>` which can be used to access the underlying value. `.lockShared()` returns `kj::Locked<const T>`, [using constness to enforce thread-safe read-only access](../style-guide.md#constness) so that multiple threads can take the lock concurrently. In this way, KJ mutexes make it difficult to forget to take a lock before accessing the protected object.

`kj::Locked<T>` has a method `.wait(cond)` which temporarily releases the lock and waits, taking the lock back as soon as `cond(value)` evaluates true. This provides a much cleaner and more readable interface than traditional conditional variables.

`kj::Lazy<T>` is an instance of `T` that is constructed on first access in a thread-safe way.

Macros `KJ_TRACK_LOCK_BLOCKING` and `KJ_SAVE_ACQUIRED_LOCK_INFO` can be used to enable support utilities to implement deadlock detection & analysis.
* `KJ_TRACK_LOCK_BLOCKING`: When the current thread is doing a blocking synchronous KJ operation, that operation is available via `kj::blockedReason()` (intention is for this to be invoked from the signal handler running on the thread that's doing the synchronous operation).
* `KJ_SAVE_ACQUIRED_LOCK_INFO`: When enabled, lock acquisition will save state about the location of the acquired lock. When combined with `KJ_TRACK_LOCK_BLOCKING` this can be particularly helpful because any watchdog can just forward the signal to the thread that's holding the lock.
## Asynchronous Event Loop

### Promises

KJ makes asynchronous programming manageable using an API modeled on E-style Promises. E-style Promises were also the inspiration for JavaScript Promises, so modern JavaScript programmers should find KJ Promises familiar, although there are some important differences.

A `kj::Promise<T>` represents an asynchronous background task that, upon completion, either "resolves" to a value of type `T`, or "rejects" with an exception.

In the simplest case, a `kj::Promise<T>` can be directly constructed from an instance of `T`:

```c++
int i = 123;
kj::Promise<int> promise = i;
```

In this case, the promise is immediately resolved to the given value.

A promise can also immediately reject with an exception:

```c++
kj::Exception e = KJ_EXCEPTION(FAILED, "problem");
kj::Promise<int> promise = kj::mv(e);
```

Of course, `Promise`s are much more interesting when they don't complete immediately.

When a function returns a `Promise`, it means that the function performs some asynchronous operation that will complete in the future. These functions are always non-blocking -- they immediately return a `Promise`. The task completes asynchronously on the event loop. The eventual results of the promise can be obtained using `.then()` to register a callback, or, in certain situations, `.wait()` to synchronously wait. These are described in more detail below.

### Basic event loop setup

In order to execute `Promise`-based code, the thread must be running an event loop. Typically, at the top level of the thread, you would do something like:

```c++
kj::AsyncIoContext io = kj::setupAsyncIo();

kj::AsyncIoProvider& ioProvider = *io.provider;
kj::LowLevelAsyncIoProvider& lowLevelProvider = *io.lowLevelProvider;
kj::WaitScope& waitScope = io.waitScope;
```

`kj::setupAsyncIo()` constructs and returns a bunch of objects:

* A `kj::AsyncIoProvider`, which provides access to a variety of I/O APIs, like timers, pipes, and networking.
* A `kj::LowLevelAsyncIoProvider`, which allows you to wrap existing low-level operating system handles (Unix file descriptors, or Windows `HANDLE`s) in KJ asynchronous interfaces.
* A `kj::WaitScope`, which allows you to perform synchronous waits (see next section).
* OS-specific interfaces for even lower-level access -- see the API definition for more details.

In order to implement all this, KJ will set up the appropriate OS-specific constructs to handle I/O events on the host platform. For example, on Linux, KJ will use `epoll`, whereas on Windows, it will set up an I/O Completion Port.

Sometimes, you may need KJ promises to cooperate with some existing event loop, rather than set up its own. For example, you might be using libuv, or Boost.Asio. Usually, a thread can only have one event loop, because it can only wait on one OS event queue (e.g. `epoll`) at a time. To accommodate this, it is possible (though not easy) to adapt KJ to run on top of some other event loop, by creating a custom implementation of `kj::EventPort`. The details of how to do this are beyond the scope of this document.

Sometimes, you may find that you don't really need to perform operating system I/O at all. For example, a unit test might only need to call some asynchronous functions using mock I/O interfaces, or a thread in a multi-threaded program may only need to exchange events with other threads and not the OS. In these cases, you can create a simple event loop instead:

```c++
kj::EventLoop eventLoop;
kj::WaitScope waitScope(eventLoop);
```

### Synchronous waits

In the top level of your program (or thread), the program is allowed to synchronously wait on a promise using the `kj::WaitScope` (see above).

```
kj::Timer& timer = io.provider->getTimer();
kj::Promise<void> promise = timer.afterDelay(5 * kj::SECONDS);
promise.wait(waitScope);  // returns after 5 seconds' delay
```

`promise.wait()` will run the thread's event loop until the promise completes. It will then return the `Promise`'s result (or throw the `Promise`'s exception). `.wait()` consumes the `Promise`, as if the `Promise` has been moved away.

Synchronous waits cannot be nested -- i.e. a `.then()` callback (see below) that is called by the event loop itself cannot execute another level of synchronous waits. Hence, synchronous waits generally can only be used at the top level of the thread. The API requires passing a `kj::WaitScope` to `.wait()` as a way to demonstrate statically that the caller is allowed to perform synchronous waits. Any function which wishes to perform synchronous waits must take a `kj::WaitScope&` as a parameter to indicate that it does this.

Synchronous waits often make sense to use in "client" programs that only have one task to complete before they exit. On the other end of the spectrum, server programs that handle many clients generally must do everything asynchronously. At the top level of a server program, you will typically instruct the event loop to run forever, like so:

```c++
// Run event loop forever, do everything asynchronously.
kj::NEVER_DONE.wait(waitScope);
```

Libraries should always be asynchronous, so that either kind of program can use them.

### Asynchronous callbacks

Similar to JavaScript promises, you may register a callback to call upon completion of a KJ promise using `.then()`:

```c++
kj::Promise<kj::String> textPromise = stream.readAllText();
kj::Promise<int> lineCountPromise = textPromise
    .then([](kj::String text) {
  int lineCount = 0;
  for (char c: text) {
    if (c == '\n') {
      ++lineCount;
    }
  }
  return lineCount;
});
```

`promise.then()` takes, as its argument, a lambda which transforms the result of the `Promise`. It returns a new `Promise` for the transformed result. We call this lambda a "continuation".

Calling `.then()`, like `.wait()`, consumes the original promise, as if it were "moved away". Ownership of the original promise is transferred into the new, derived promise. If you want to register multiple continuations on the same promise, you must fork it first (see below).

If the continuation itself returns another `Promise`, then the `Promise`s become chained. That is, the final type is reduced from `Promise<Promise<T>>` to just `Promise<T>`.

```c++
kj::Promise<kj::Own<kj::AsyncIoStream>> connectPromise =
    networkAddress.connect();
kj::Promise<kj::String> textPromise = connectPromise
    .then([](kj::Own<kj::AsyncIoStream> stream) {
  return stream->readAllText().attach(kj::mv(stream));
});
```

If a promise rejects (throws an exception), then the exception propagates through `.then()` to the new derived promise, without calling the continuation. If you'd like to actually handle the exception, you may pass a second lambda as the second argument to `.then()`.

```c++
kj::Promise<kj::String> promise = networkAddress.connect()
    .then([](kj::Own<kj::AsyncIoStream> stream) {
  return stream->readAllText().attach(kj::mv(stream));
}, [](kj::Exception&& exception) {
  return kj::str("connection error: ", exception);
});
```

You can also use `.catch_(errorHandler)`, which is a shortcut for `.then(identityFunction, errorHandler)`.

### `kj::evalNow()`, `kj::evalLater()`, and `kj::evalLast()`

These three functions take a lambda as the parameter, and return the result of evaluating the lambda. They differ in when, exactly, the execution happens.

```c++
kj::Promise<int> promise = kj::evalLater([]() {
  int i = doSomething();
  return i;
});
```

As with `.then()` continuations, the lambda passed to these functions may itself return a `Promise`.

`kj::evalNow()` executes the lambda immediately -- before `evalNow()` even returns. The purpose of `evalNow()` is to catch any exceptions thrown and turn them into a rejected promise. This is often a good idea when you don't want the caller to have to handle both synchronous and asynchronous exceptions -- wrapping your whole function in `kj::evalNow()` ensures that all exceptions are delivered asynchronously.

`kj::evalLater()` executes the lambda on a future turn of the event loop. This is equivalent to `kj::Promise<void>().then()`.

`kj::evalLast()` arranges for the lambda to be called only after all other work queued to the event loop has completed (but before querying the OS for new I/O events). This can often be useful e.g. for batching. For example, if a program tends to make many small write()s to a socket in rapid succession, you might want to add a layer that collects the writes into a batch, then sends the whole batch in a single write from an `evalLast()`. This way, none of the bytes are significantly delayed, but they can still be coalesced.

If multiple `evalLast()`s exist at the same time, they will execute in last-in-first-out order. If the first one out schedules more work on the event loop, that work will be completed before the next `evalLast()` executes, and so on.

### Attachments

Often, a task represented by a `Promise` will require that some object remains alive until the `Promise` completes. In particular, under KJ conventions, unless documented otherwise, any class method which returns a `Promise` inherently expects that the caller will ensure that the object it was called on will remain alive until the `Promise` completes (or is canceled). Put another way, member function implementations may assume their `this` pointer is valid as long as their returned `Promise` is alive.

You may use `promise.attach(kj::mv(object))` to give a `Promise` direct ownership of an object that must be kept alive until the promise completes. `.attach()`, like `.then()`, consumes the promise and returns a new one of the same type.

```c++
kj::Promise<kj::Own<kj::AsyncIoStream>> connectPromise =
    networkAddress.connect();
kj::Promise<kj::String> textPromise = connectPromise
    .then([](kj::Own<kj::AsyncIoStream> stream) {
  // We must attach the stream so that it remains alive until `readAllText()`
  // is done. The stream will then be discarded.
  return stream->readAllText().attach(kj::mv(stream));
});
```

Using `.attach()` is semantically equivalent to using `.then()`, passing an identity function as the continuation, while having that function capture ownership of the attached object, i.e.:

```c++
// This...
promise.attach(kj::mv(attachment));
// ...is equivalent to this...
promise.then([a = kj::mv(attachment)](auto x) { return kj::mv(x); });
```

Note that you can use `.attach()` together with `kj::defer()` to construct a "finally" block -- code which will execute after the promise completes (or is canceled).

```c++
promise = promise.attach(kj::defer([]() {
  // This code will execute when the promise completes or is canceled.
}));
```

### Background tasks

If you construct a `Promise` and then just leave it be without calling `.then()` or `.wait()` to consume it, the task it represents will nevertheless execute when the event loop runs, "in the background". You can call `.then()` or `.wait()` later on, when you're ready. This makes it possible to run multiple concurrent tasks at once.

Note that, when possible, KJ evaluates continuations lazily. Continuations which merely transform the result (without returning a new `Promise` that might require more waiting) are only evaluated when the final result is actually needed. This is an optimization which allows a long chain of `.then()`s to be executed all at once, rather than turning the event loop for each one. However, it can lead to some confusion when storing an unconsumed `Promise`. For example:

```c++
kj::Promise<void> promise = timer.afterDelay(5 * kj::SECONDS)
    .then([]() {
  // This log line will never be written, because nothing
  // is waiting on the final result of the promise.
  KJ_LOG(WARNING, "It has been 5 seconds!!!");
});
kj::NEVER_DONE.wait(waitScope);
```

To solve this, use `.eagerlyEvaluate()`:

```c++
kj::Promise<void> promise = timer.afterDelay(5 * kj::SECONDS)
    .then([]() {
  // This log will correctly be written after 5 seconds.
  KJ_LOG(WARNING, "It has been 5 seconds!!!");
}).eagerlyEvaluate([](kj::Exception&& exception) {
  KJ_LOG(ERROR, exception);
});
kj::NEVER_DONE.wait(waitScope);
```

`.eagerlyEvaluate()` takes an error handler callback as its parameter, with the same semantics as `.catch_()` or the second parameter to `.then()`. This is required because otherwise, it is very easy to forget to install an error handler on background tasks, resulting in errors being silently discarded. However, if you are certain that errors will be properly handled elsewhere, you may pass `nullptr` as the parameter to skip error checking -- this is equivalent to passing a callback that merely re-throws the exception.

If you have lots of background tasks, use `kj::TaskSet` to manage them. Any promise added to a `kj::TaskSet` will be run to completion (with eager evaluation), with any exceptions being reported to a provided error handler callback.

### Cancellation

If you destroy a `Promise` before it has completed, any incomplete work will be immediately canceled.

Upon cancellation, no further continuations are executed at all, not even error handlers. Only destructors are executed. Hence, when there is cleanup that must be performed after a task, it is not sufficient to use `.then()` to perform the cleanup in continuations. You must instead use `.attach()` to attach an object whose destructor performs the cleanup (or perhaps `.attach(kj::defer(...))`, as mentioned earlier).

Promise cancellation has proven to be an extremely useful feature of KJ promises which is missing in other async frameworks, such as JavaScript's. However, it places new responsibility on the developer. Just as developers who allow exceptions must design their code to be "exception safe", developers using KJ promises must design their code to be "cancellation safe".

It is especially important to note that once a promise has been canceled, then any references that were received along with the promise may no longer be valid. For example, consider this function:

```
kj::Promise<void> write(kj::ArrayPtr<kj::byte> data);
```

The function receives a pointer to some data owned elsewhere. By KJ convention, the caller must ensure this pointer remains valid until the promise completes _or is canceled_. If the caller decides it needs to free the data early, it may do so as long as it cancels the promise first. This property is important as otherwise it becomes impossible to reason about ownership in complex systems.

This means that the implementation of `write()` must immediately stop using `data` as soon as cancellation occurs. For example, if `data` has been placed in some sort of queue where some other concurrent task takes items from the queue to write them, then it must be ensured that `data` will be removed from that queue upon cancellation. This "queued writes" pattern has historically been a frequent source of bugs in KJ code, to the point where experienced KJ developers now become immediately suspicious of such queuing. The `kj::AsyncOutputStream` interface explicitly prohibits overlapping calls to `write()` specifically so that the implementation need not worry about maintaining queues.

### Promise-Fulfiller Pairs and Adapted Promises

Sometimes, it's difficult to express asynchronous control flow as a simple chain of continuations. For example, imagine a producer-consumer queue, where producers and consumers are executing concurrently on the same event loop. The consumer doesn't directly call the producer, nor vice versa, but the consumer would like to wait for the producer to produce an item for consumption.

For these situations, you may use a `Promise`-`Fulfiller` pair.

```c++
kj::PromiseFulfillerPair<int> paf = kj::newPromiseAndFulfiller<int>();

// Consumer waits for the promise.
paf.promise.then([](int i) { ... });

// Producer calls the fulfiller to fulfill the promise.
paf.fulfiller->fulfill(123);

// Producer can also reject the promise.
paf.fulfiller->reject(KJ_EXCEPTION(FAILED, "something went wrong"));
```

**WARNING! DANGER!** When using promise-fulfiller pairs, it is very easy to forget about both exception propagation and, more importantly, cancellation-safety.

* **Exception-safety:** If your code stops early due to an exception, it may forget to invoke the fulfiller. Upon destroying the fulfiller, the consumer end will receive a generic, unhelpful exception, merely saying that the fulfiller was destroyed unfulfilled. To aid in debugging, you should make sure to catch exceptions and call `fulfiller->reject()` to propagate them.
* **Cancellation-safety:** Either the producer or the consumer task could be canceled, and you must consider how this affects the other end.
    * **Canceled consumer:** If the consumer is canceled, the producer may waste time producing an item that no one is waiting for. Or, worse, if the consumer has provided references to the producer (for example, a buffer into which results should be written), those references may become invalid upon cancellation, but the producer will continue executing, possibly resulting in a use-after-free. To avoid these problems, the producer can call `fulfiller->isWaiting()` to check if the consumer is still waiting -- this method returns false if either the consumer has been canceled, or if the producer has already fulfilled or rejected the promise previously. However, `isWaiting()` requires polling, which is not ideal. For better control, consider using an adapted promise (see below).
    * **Canceled producer:** If the producer is canceled, by default it will probably destroy the fulfiller without fulfilling or reject it. As described previously, the consumer will receive a non-descript exception, which is likely unhelpful for debugging. To avoid this scenario, the producer could perhaps use `.attach(kj::defer(...))` with a lambda that checks `fulfiller->isWaiting()` and rejects it if not.

Because of the complexity of the above issues, it is generally recommended that you **avoid promise-fulfiller pairs** except in cases where these issues very clearly don't matter (such as unit tests).

Instead, when cancellation concerns matter, consider using "adapted promises", a more sophisticated alternative. `kj::newAdaptedPromise<T, Adapter>()` constructs an instance of the class `Adapter` (which you define) encapsulated in a returned `Promise<T>`. `Adapter`'s constructor receives a `kj::PromiseFulfiller<T>&` used to fulfill the promise. The constructor should then register the fulfiller with the desired producer. If the promise is canceled, `Adapter`'s destructor will be invoked, and should un-register the fulfiller. One common technique is for `Adapter` implementations to form a linked list with other `Adapter`s waiting for the same producer. Adapted promises make consumer cancellation much more explicit and easy to handle, at the expense of requiring more code.

### Loops

Promises, due to their construction, don't lend themselves easily to classic `for()`/`while()` loops. Instead, loops should be expressed recursively, as in a functional language. For example:

```c++
kj::Promise<void> boopEvery5Seconds(kj::Timer& timer) {
  return timer.afterDelay(5 * kj::SECONDS).then([&timer]() {
    boop();
    // Loop by recursing.
    return boopEvery5Seconds(timer);
  });
}
```

KJ promises include "tail call optimization" for loops like the one above, so that the promise chain length remains finite no matter how many times the loop iterates.

**WARNING!** It is very easy to accidentally break tail call optimization, creating a memory leak. Consider the following:

```c++
kj::Promise<void> boopEvery5Seconds(kj::Timer& timer) {
  // WARNING! MEMORY LEAK!
  return timer.afterDelay(5 * kj::SECONDS).then([&timer]() {
    boop();
    // Loop by recursing.
    return boopEvery5Seconds(timer);
  }).catch_([](kj::Exception&& exception) {
    // Oh no, an error! Log it and end the loop.
    KJ_LOG(ERROR, exception);
    kj::throwFatalException(kj::mv(exception));
  });
}
```

The problem in this example is that the recursive call is _not_ a tail call, due to the `.catch_()` appended to the end. Every time around the loop, a new `.catch_()` is added to the promise chain. If an exception were thrown, that exception would end up being logged many times -- once for each time the loop has repeated so far. Or if the loop iterated enough times, and the top promise was then canceled, the chain could be so long that the destructors overflow the stack.

In this case, the best fix is to pull the `.catch_()` out of the loop entirely:

```c++
kj::Promise<void> boopEvery5Seconds(kj::Timer& timer) {
  return boopEvery5SecondsLoop(timer)
      .catch_([](kj::Exception&& exception) {
    // Oh no, an error! Log it and end the loop.
    KJ_LOG(ERROR, exception);
    kj::throwFatalException(kj::mv(exception));
  })
}

kj::Promise<void> boopEvery5SecondsLoop(kj::Timer& timer) {
  // No memory leaks now!
  return timer.afterDelay(5 * kj::SECONDS).then([&timer]() {
    boop();
    // Loop by recursing.
    return boopEvery5SecondsLoop(timer);
  });
}
```

Another possible fix would be to make sure the recursive continuation and the error handler are passed to the same `.then()` invocation:

```c++
kj::Promise<void> boopEvery5Seconds(kj::Timer& timer) {
  // No more memory leaks, but hard to reason about.
  return timer.afterDelay(5 * kj::SECONDS).then([&timer]() {
    boop();
  }).then([&timer]() {
    // Loop by recursing.
    return boopEvery5Seconds(timer);
  }, [](kj::Exception&& exception) {
    // Oh no, an error! Log it and end the loop.
    KJ_LOG(ERROR, exception);
    kj::throwFatalException(kj::mv(exception));
  });
}
```

Notice that in this second case, the error handler is scoped so that it does _not_ catch exceptions thrown by the recursive call; it only catches exceptions from `boop()`. This solves the problem, but it's a bit trickier to understand and to ensure that exceptions can't accidentally slip past the error handler.

### Forking and splitting promises

As mentioned above, `.then()` and similar functions consume the promise on which they are called, so they can only be called once. But what if you want to start multiple tasks using the result of a promise? You could solve this in a convoluted way using adapted promises, but KJ has a built-in solution: `.fork()`

```c++
kj::Promise<int> promise = ...;
kj::ForkedPromise<int> forked = promise.fork();
kj::Promise<int> branch1 = promise.addBranch();
kj::Promise<int> branch2 = promise.addBranch();
kj::Promise<int> branch3 = promise.addBranch();
```

A forked promise can have any number of "branches" which represent different consumers waiting for the same result.

Forked promises use reference counting. The `ForkedPromise` itself, and each branch created from it, each represent a reference to the original promise. The original promise will only be canceled if all branches are canceled and the `ForkedPromise` itself is destroyed.

Forked promises require that the result type has a copy constructor, so that it can be copied to each branch. (Regular promises only require the result type to be movable, not copyable.) Or, alternatively, if the result type is `kj::Own<T>` -- which is never copyable -- then `T` must have a method `kj::Own<T> T::addRef()`; this method will be invoked to create each branch. Typically, `addRef()` would be implemented using reference counting.

Sometimes, the copyable requirement of `.fork()` can be burdensome and unnecessary. If the result type has multiple components, and each branch really only needs one of the components, then being able to copy (or refcount) is unnecessary. In these cases, you can use `.split()` instead. `.split()` converts a promise for a `kj::Tuple` into a `kj::Tuple` of promises. That is:

```c++
kj::Promise<kj::Tuple<kj::Own<Foo>, kj::String>> promise = ...;
kj::Tuple<kj::Promise<kj::Own<Foo>>, kj::Promise<kj::String>> promises = promise.split();
```

### Joining promises

The opposite of forking promises is joining promises. There are two types of joins:
* **Exclusive** joins wait for any one input promise to complete, then cancel the rest, returning the result of the promise that completed.
* **Inclusive** joins wait for all input promises to complete, and render all of the results.

For an exclusive join, use `promise.exclusiveJoin(kj::mv(otherPromise))`. The two promises must return the same type. The result is a promise that returns whichever result is produced first, and cancels the other promise at that time. (To exclusively join more than two promises, call `.exclusiveJoin()` multiple times in a chain.)

To perform an inclusive join, use `kj::joinPromises()` or `kj::joinPromisesFailFast()`. These turn a `kj::Array<kj::Promise<T>>` into a `kj::Promise<kj::Array<T>>`. However, note that `kj::joinPromises()` has a couple common gotchas:
* Trailing continuations on the promises passed to `kj::joinPromises()` are evaluated lazily after all the promises become ready. Use `.eagerlyEvaluate()` on each one to force trailing continuations to happen eagerly. (See earlier discussion under "Background Tasks".)
* If any promise in the array rejects, the exception will be held until all other promises have completed (or rejected), and only then will the exception propagate. In practice we've found that most uses of `kj::joinPromises()` would prefer "exclusive" or "fail-fast" behavior in the case of an exception.

`kj::joinPromisesFailFast()` addresses the gotchas described above: promise continuations are evaluated eagerly, and if any promise results in an exception, the join promise is immediately rejected with that exception.

### Threads

The KJ async framework is designed around single-threaded event loops. However, you can have multiple threads, with each running its own loop.

All KJ async objects, unless specifically documented otherwise, are intrinsically tied to the thread and event loop on which they were created. These objects must not be accessed from any other thread.

To communicate between threads, you may use `kj::Executor`. Each thread (that has an event loop) may call `kj::getCurrentThreadExecutor()` to get a reference to its own `Executor`. That reference may then be shared with other threads. The other threads can use the methods of `Executor` to queue functions to execute on the owning thread's event loop.

The threads which call an `Executor` do not have to have KJ event loops themselves. Thus, you can use an `Executor` to signal a KJ event loop thread from a non-KJ thread.

### Fibers

Fibers allow code to be written in a synchronous / blocking style while running inside the KJ event loop, by executing the code on an alternate call stack. The code running on this alternate stack is given a special `kj::WaitScope&`, which it can pass to `promise.wait()` to perform synchronous waits. When such a `.wait()` is invoked, the thread switches back to the main call stack and continues running the event loop there. When the waited promise resolves, execution switches back to the alternate call stack and `.wait()` returns (or throws).

```c++
constexpr size_t STACK_SIZE = 65536;
kj::Promise<int> promise =
    kj::startFiber(STACK_SIZE, [](kj::WaitScope& waitScope) {
  int i = someAsyncFunc().wait(waitScope);
  i += anotherAsyncFunc().wait(waitScope);
  return i;
});
```

**CAUTION:** Fibers produce attractive-looking code, but have serious drawbacks. Every fiber must allocate a new call stack, which is typically rather large. The above example allocates a 64kb stack, which is the _minimum_ supported size. Some programs and libraries expect to be able to allocate megabytes of data on the stack. On modern Linux systems, a default stack size of 8MB is typical. Stack space is allocated lazily on page faults, but just setting up the memory mapping is much more expensive than a typical `malloc()`. If you create lots of fibers, you should use `kj::FiberPool` to reduce allocation costs -- but while this reduces allocation overhead, it will increase memory usage.

Because of this, fibers should not be used just to make code look nice (C++20's `co_await`, described below, is a better way to do that). Instead, the main use case for fibers is to be able to call into existing libraries that are not designed to operate in an asynchronous way. For example, say you find a library that performs stream I/O, and lets you provide your own `read()`/`write()` implementations, but expects those implementations to operate in a blocking fashion. With fibers, you can use such a library within the asynchronous KJ event loop.

### Coroutines

C++20 brings us coroutines, which, like fibers, allow code to be written in a synchronous / blocking style while running inside the KJ event loop. Coroutines accomplish this with a different strategy than fibers: instead of running code on an alternate stack and switching stacks on suspension, coroutines save local variables and temporary objects in a heap-allocated "coroutine frame" and always unwind the stack on suspension.

A C++ function is a KJ coroutine if it follows these two rules:
- The function returns a `kj::Promise<T>`.
- The function uses a `co_await` or `co_return` keyword in its implementation.

```c++
kj::Promise<int> aCoroutine() {
  int i = co_await someAsyncFunc();
  i += co_await anotherAsyncFunc();
  co_return i;
});

// Call like any regular promise-returning function.
auto promise = aCoroutine();
```

The promise returned by a coroutine owns the coroutine frame. If you destroy the promise, any objects alive in the frame will be destroyed, and the frame freed, thus cancellation works exactly as you'd expect.

There are some caveats one should be aware of while writing coroutines:
- Lambda captures **do not** live inside of the coroutine frame, meaning lambda objects must outlive any coroutine Promises they return, or else the coroutine will encounter dangling references to captured objects. This is a defect in the C++ standard: https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#Rcoro-capture. To safely use a capturing lambda as a coroutine, first wrap it using `kj::coCapture([captures]() { ... })`, then invoke that object.
- Holding a mutex lock across a `co_await` is almost always a bad idea, with essentially the same problems as holding a lock while calling `promise.wait(waitScope)`. This would cause the coroutine to hold the lock for however many turns of the event loop is required to drive the coroutine to release the lock; if I/O is involved, this could cause significant problems. Additionally, a reentrant call to the coroutine on the same thread would deadlock. Instead, if a coroutine must temporarily hold a lock, always keep the lock in a new lexical scope without any `co_await`.
- Attempting to define (and use) a variable-length array will cause a compile error, because the size of coroutine frames must be knowable at compile-time. The error message that clang emits for this, "Coroutines cannot handle non static allocas yet", suggests this may be relaxed in the future.

As of this writing, KJ supports C++20 coroutines and Coroutines TS coroutines, the latter being an experimental precursor to C++20 coroutines. They are functionally the same thing, but enabled with different compiler/linker flags:

- Enable C++20 coroutines by requesting that language standard from your compiler.
- Enable Coroutines TS coroutines with `-fcoroutines-ts` in C++17 clang, and `/await` in MSVC.

KJ prefers C++20 coroutines when both implementations are available.

### Unit testing tips

When unit-testing promise APIs, two tricky challenges frequently arise:

* Testing that a promise has completed when it is supposed to. You can use `promise.wait()`, but if the promise has not completed as expected, then the test may simply hang. This can be frustrating to debug.
* Testing that a promise has not completed prematurely. You obviously can't use `promise.wait()`, because you _expect_ the promise has not completed, and therefore this would hang. You might try using `.then()` with a continuation that sets a flag, but if the flag is not set, it's hard to tell whether this is because the promise really has not completed, or merely because the event loop hasn't yet called the `.then()` continuation.

To solve these problems, you can use `promise.poll(waitScope)`. This function runs the event loop until either the promise completes, or there is nothing left to do except to wait. This includes running any continuations in the queue as well as checking for I/O events from the operating system, repeatedly, until nothing is left. The only thing `.poll()` will not do is block. `.poll()` returns true if the promise has completed, false if it hasn't.

```c++
// In a unit test...
kj::Promise<void> promise = waitForBoop();

// The promise should not be done yet because we haven't booped yet.
KJ_ASSERT(!promise.poll(waitScope));

boop();

// Assert the promise is done, to make sure wait() won't hang!
KJ_ASSERT(promise.poll(waitScope));

promise.wait(waitScope);
```

Sometimes, you may need to ensure that some promise has completed that you don't have a reference to, so you can observe that some side effect has occurred. You can use `waitScope.poll()` to flush the event loop without waiting for a specific promise to complete.

## System I/O

### Async I/O

On top of KJ's async framework (described earlier), KJ provides asynchronous APIs for byte streams, networking, and timers.

As mentioned previously, `kj::setupAsyncIo()` allocates an appropriate OS-specific event queue (such as `epoll` on Linux), returning implementations of `kj::AsyncIoProvider` and `kj::LowLevelAsyncIoProvider` implemented in terms of that queue. `kj::AsyncIoProvider` provides an OS-independent API for byte streams, networking, and timers. `kj::LowLevelAsyncIoProvider` allows native OS handles (file descriptors on Unix, `HANDLE`s on Windows) to be wrapped in KJ byte stream APIs, like `kj::AsyncIoStream`.

Please refer to the API reference (the header files) for details on these APIs.

### Synchronous I/O

Although most complex KJ applications use async I/O, sometimes you want something a little simpler.

`kj/io.h` provides some more basic, synchronous streaming interfaces, like `kj::InputStream` and `kj::OutputStream`. Implementations are provided on top of file descriptors and Windows `HANDLE`s.

Additionally, the important utility class `kj::AutoCloseFd` (and `kj::AutoCloseHandle` for Windows) can be found here. This is an RAII wrapper around a file descriptor (or `HANDLE`), which you will likely want to use any time you are manipulating raw file descriptors (or `HANDLE`s) in KJ code.

### Filesystem

KJ provides an advanced, cross-platform filesystem API in `kj/filesystem.h`. Features include:

* Paths represented using `kj::Path`. In addition to providing common-sense path parsing and manipulation functions, this class is designed to defend against path injection attacks.
* All interfaces are abstract, allowing multiple implementations.
* An in-memory implementation is provided, useful in particular for mocking the filesystem in unit tests.
* On Unix, disk `kj::Directory` objects are backed by open file descriptors and use the `openat()` family of system calls.
* Makes it easy to use atomic replacement when writing new files -- and even whole directories.
* Symlinks, hard links, listing directories, recursive delete, recursive create parents, recursive copy directory, memory mapping, and unnamed temporary files are all exposed and easy to use.
* Sparse files ("hole punching"), copy-on-write file cloning (`FICLONE`, `FICLONERANGE`), `sendfile()`-based copying, `renameat2()` atomic replacements, and more will automatically be used when available.

See the API reference (header file) for details.

### Clocks and time

KJ provides a time library in `kj/time.h` which uses the type system to enforce unit safety.

`kj::Duration` represents a length of time, such as a number of seconds. Multiply an integer by `kj::SECONDS`, `kj::MINUTES`, `kj::NANOSECONDS`, etc. to get a `kj::Duration` value. Divide by the appropriate constant to get an integer.

`kj::Date` represents a point in time in the real world. `kj::UNIX_EPOCH` represents January 1st, 1970, 00:00 UTC. Other dates can be constructed by adding a `kj::Duration` to `kj::UNIX_EPOCH`. Taking the difference between to `kj::Date`s produces a `kj::Duration`.

`kj::TimePoint` represents a time point measured against an unspecified origin time. This is typically used with monotonic clocks that don't necessarily reflect calendar time. Unlike `kj::Date`, there is no implicit guarantee that two `kj::TimePoint`s are measured against the same origin and are therefore comparable; it is up to the application to track which clock any particular `kj::TimePoint` came from.

`kj::Clock` is a simple interface whose `now()` method returns the current `kj::Date`. `kj::MonotonicClock` is a similar interface returning a `kj::TimePoint`, but with the guarantee that times returned always increase (whereas a `kj::Clock` might go "back in time" if the user manually modifies their system clock).

`kj::systemCoarseCalendarClock()`, `kj::systemPreciseCalendarClock()`, `kj::systemCoarseMonotonicClock()`, `kj::systemPreciseMonotonicClock()` are global functions that return implementations of `kj::Clock` or `kJ::MonotonicClock` based on system time.

`kj::Timer` provides an async (promise-based) interface to wait for a specified time to pass. A `kj::Timer` is provided via `kj::AsyncIoProvider`, constructed using `kj::setupAsyncIo()` (see earlier discussion on async I/O).

## Program Harness

TODO: kj::Main, unit test framework

Libraries
======================================================================

TODO: parser combinator framework, HTTP, TLS, URL, encoding, JSON



security-advisories/2015-03-02-0-c++-integer-overflow.md
--------------------------------------
Problem
=======

Integer overflow in pointer validation.

Discovered by
=============

Ben Laurie &lt;ben@links.org> using [American Fuzzy Lop](http://lcamtuf.coredump.cx/afl/)

Announced
=========

2015-03-02

CVE
===

CVE-2015-2310

Impact
======

- Remotely segfault a peer by sending it a malicious message.
- Possible exfiltration of memory, depending on application behavior.

Fixed in
========

- git commit [f343f0dbd0a2e87f17cd74f14186ed73e3fbdbfa][0]
- release 0.5.1.1:
  - Unix: https://capnproto.org/capnproto-c++-0.5.1.1.tar.gz
  - Windows: https://capnproto.org/capnproto-c++-win32-0.5.1.1.zip
- release 0.4.1.1:
  - Unix: https://capnproto.org/capnproto-c++-0.4.1.1.tar.gz
- release 0.6 (future)

[0]: https://github.com/capnproto/capnproto/commit/f343f0dbd0a2e87f17cd74f14186ed73e3fbdbfa

Details
=======

*The following text contains speculation about the exploitability of this
bug. This is provided for informational purposes, but as such speculation is
often shown to be wrong, you should not rely on the accuracy of this
section for the safety of your service. Please update your library.*

A specially-crafted pointer could escape bounds checking by triggering an
integer overflow in the check. This causes the message to appear as if it
contains an extremely long list (over 2^32 bytes), stretching far beyond the
memory actually allocated to the message. If the application reads that list,
it will likely segfault, but if it manages to avoid a segfault (e.g. because
it has mapped a very large contiguous block of memory following the message,
or because it only reads some parts of the list and not others), it could end
up treating arbitrary parts of memory as input. If the application happens to
pass that data back to the user in some way, this problem could lead to
exfiltration of secrets.

The pointer is transitively read-only, therefore it is believed that this
vulnerability on its own CANNOT lead to memory corruption nor code execution.

This vulnerability is NOT a Sandstorm sandbox breakout. A Sandstorm app's
Cap'n Proto communications pass through a supervisor process which performs a
deep copy of the structure. As the supervisor has a very small heap, this
will always lead to a segfault, which has the effect of killing the app, but
does not affect any other app or the system at large. If somehow the copy
succeeds, the copied message will no longer contain an invalid pointer and
so will not harm its eventual destination, and the supervisor itself has no
secrets to steal. These mitigations are by design.

Preventative measures
=====================

In order to gain confidence that this is a one-off bug rather than endemic,
and to help prevent new bugs from being added, we have taken / will take the
following preventative measures going forward:

1. A fuzz test of each pointer type has been added to the standard unit test
   suite. This test was confirmed to find the vulnerability in question.
2. We will additionally add fuzz testing with American Fuzzy Lop to our
   extended test suite. AFL was used to find the original vulnerability. Our
   current tests with AFL show only one other (less-critical) vulnerability
   which will be reported separately ([2015-03-02-2][2]).
3. In parallel, we will extend our use of template metaprogramming for
   compile-time unit analysis (kj::Quantity in kj/units.h) to also cover
   overflow detection (by tracking the maximum size of an integer value across
   arithmetic expressions and raising an error when it overflows). Preliminary
   work with this approach successfully detected the vulnerability reported
   here as well as one other vulnerability ([2015-03-02-1][3]).
   [See the blog post][4] for more details.
4. We will continue to require that all tests (including the new fuzz test) run
   cleanly under Valgrind before each release.
5. We will commission a professional security review before any 1.0 release.
   Until that time, we continue to recommend against using Cap'n Proto to
   interpret data from potentially-malicious sources.

I am pleased that measures 1, 2, and 3 all detected this bug, suggesting that
they have a high probability of catching any similar bugs.

[1]: https://github.com/capnproto/capnproto/tree/master/security-advisories/2015-03-02-0-all-cpu-amplification.md
[2]: https://github.com/capnproto/capnproto/tree/master/security-advisories/2015-03-02-1-c++-integer-underflow.md
[3]: https://capnproto.org/news/2015-03-02-security-advisory-and-integer-overflow-protection.html



security-advisories/2015-03-02-1-c++-integer-underflow.md
--------------------------------------
Problem
=======

Integer underflow in pointer validation.

Discovered by
=============

Kenton Varda &lt;kenton@sandstorm.io>

Announced
=========

2015-03-02

CVE
===

CVE-2015-2311

Impact
======

- Remotely segfault a peer by sending it a malicious message.
- Possible exfiltration of memory, depending on application behavior.
- If the application performs a sequence of operations that "probably" no
  application does (see below), possible memory corruption / code execution.

Fixed in
========

- git commit [26bcceda72372211063d62aab7e45665faa83633][0]
- release 0.5.1.1:
  - Unix: https://capnproto.org/capnproto-c++-0.5.1.1.tar.gz
  - Windows: https://capnproto.org/capnproto-c++-win32-0.5.1.1.zip
- release 0.4.1.1:
  - Unix: https://capnproto.org/capnproto-c++-0.4.1.1.tar.gz
- release 0.6 (future)

[0]: https://github.com/capnproto/capnproto/commit/26bcceda72372211063d62aab7e45665faa83633

Details
=======

*The following text contains speculation about the exploitability of this
bug. This is provided for informational purposes, but as such speculation is
often shown to be wrong, you should not rely on the accuracy of this
section for the safety of your service. Please update your library.*

A `Text` pointer, when non-null, must point to a NUL-terminated string, meaning
it must have a size of at least 1. Under most circumstances, Cap'n Proto will
reject zero-size text objects. However, if an application performs the
following sequence, they may hit a code path that was missing a check:

1. Receive a message containing a `Text` value, but do not actually look at
   that value.
2. Copy the message into a `MessageBuilder`.
3. Call the `get()` method for the `Text` value within the `MessageBuilder`,
   obtaining a `Text::Builder` for the *copy*.

In this case, the `Text::Builder` will appear to point at a string with size
2^32-1, starting at a location within the Cap'n Proto message.

The `Text::Builder` is writable. If the application decided to overwrite the
text in-place, it could overwrite arbitrary memory in the next 4GB of virtual
address space. However, there are several reasons to believe this is unusual:

- Usually, when an application `get()`s a text field, it only intends to
  read it. Overwriting the text in-place is unusual.
- Calling `set()` on the field -- the usual way to overwrite text -- will
  create an all-new text object and harmlessly discard the old, invalid
  pointer.

Note that even if an application does overwrite the text, it would still be
hard for the attacker to exploit this for code execution unless the attacker
also controls the data that the application writes into the field.

This vulnerability is somewhat more likely to allow exfiltration of memory.
However, this requires the app to additionally echo the text back to the
attacker. To do this without segfaulting, the app would either need to attempt
to read only a subset of the text, or would need to have 2^32 contiguous bytes
of virtual memory mapped into its address space.

A related problem, also fixed in this change, occurs when a `Text` value
has non-zero size but lacks a NUL terminator. Again, if an application
performs the series of operations described above, the NUL terminator check
may be bypassed. If the app then passes the string to an API that assumes
NUL-terminated strings, the contents of memory after the text blob may be
interpreted as being part of the string, up to the next zero-valued byte.
This again could lead to exfiltration of data, this time without the high
chance of segfault, although only up to the next zero-valued byte, which
are typically quite common.

Preventative measures
=====================

This problem was discovered through preventative measures implemented after
the security problem discussed in the [previous advisory][1]. Specifically, this
problem was found by using template metaprogramming to implement integer
bounds analysis in order to effectively "prove" that there are no integer
overflows in the core pointer validation code (capnp/layout.c++).
Tentatively, I believe that this analysis exhaustively covers this file.
The instrumentation has not been merged into master yet as it requires some
cleanup, but [check the Cap'n Proto blog for an in-depth discussion][2].

This problem is also caught by capnp/fuzz-test.c++, which *has* been
merged into master but likely doesn't have as broad coverage.

[1]: https://github.com/capnproto/capnproto/tree/master/security-advisories/2015-03-02-0-c++-integer-overflow.md
[2]: https://capnproto.org/news/2015-03-02-security-advisory-and-integer-overflow-protection.html



security-advisories/2015-03-02-2-all-cpu-amplification.md
--------------------------------------
Problem
=======

CPU usage amplification attack.

Discovered by
=============

Ben Laurie &lt;ben@links.org> using [American Fuzzy Lop](http://lcamtuf.coredump.cx/afl/)

Announced
=========

2015-03-02

CVE
===

CVE-2015-2312

Impact
======

- Remotely cause a peer to use excessive CPU time and other resources to
  process a very small message, possibly enabling a DoS attack.

Fixed in
========

- git commit [104870608fde3c698483fdef6b97f093fc15685d][0]
- release 0.5.1.1:
  - Unix: https://capnproto.org/capnproto-c++-0.5.1.1.tar.gz
  - Windows: https://capnproto.org/capnproto-c++-win32-0.5.1.1.zip
- release 0.4.1.1:
  - Unix: https://capnproto.org/capnproto-c++-0.4.1.1.tar.gz
- release 0.6 (future)

[0]: https://github.com/capnproto/capnproto/commit/104870608fde3c698483fdef6b97f093fc15685d

Details
=======

The Cap'n Proto list pointer format allows encoding a list whose elements are
claimed each to have a size of zero. Such a list could claim to have up to
2^29-1 elements while only taking 8 or 16 bytes on the wire. The receiving
application may expect, say, a list of structs. A zero-size struct is a
perfectly legal (and, in fact, canonical) encoding for a struct whose fields
are all set to their default values. Therefore, the application may notice
nothing wrong and proceed to iterate through and handle each element in the
list, potentially taking a lot of time and resources to do so.

Note that this kind of vulnerability is very common in other systems. Any
system which accepts compressed input can allow an attacker to deliver an
arbitrarily large uncompressed message using very little compressed bandwidth.
Applications should do their own validation to ensure that lists and blobs
inside a message have reasonable size. However, Cap'n Proto takes the
philosophy that any security mistake that is likely to be common in
naively-written application code is in fact a bug in Cap'n Proto -- we should
provide defenses so that the application developer doesn't have to.

To fix the problem, this change institutes the policy that, for the purpose of
the "message traversal limit", a list of zero-sized elements will be counted as
if each element were instead one word wide. The message traversal limit is an
existing anti-amplification measure implemented by Cap'n Proto; see:

https://capnproto.org/encoding.html#amplification-attack

Preventative measures
=====================

This problem was discovered through fuzz testing using American Fuzzy Lop,
which identified the problem as a "hang", although in fact the test case just
took a very long time to complete. We are incorporating testing with AFL into
our release process going forward.



security-advisories/2015-03-05-0-c++-addl-cpu-amplification.md
--------------------------------------
Problem
=======

CPU usage amplification attack, similar to previous vulnerability
[2015-03-02-2][1].

Discovered by
=============

David Renshaw &lt;david@sandstorm.io>

Announced
=========

2015-03-05

CVE
===

CVE-2015-2313

Impact
======

- Remotely cause a peer to execute a tight `for` loop counting from 0 to
  2^29, possibly repeatedly, by sending it a small message. This could enable
  a DoS attack by consuming CPU resources.

Fixed in
========

- git commit [80149744bdafa3ad4eedc83f8ab675e27baee868][0]
- release 0.5.1.2:
  - Unix: https://capnproto.org/capnproto-c++-0.5.1.2.tar.gz
  - Windows: https://capnproto.org/capnproto-c++-win32-0.5.1.2.zip
- release 0.4.1.1:
  - Unix: https://capnproto.org/capnproto-c++-0.4.1.2.tar.gz
- release 0.6 (future)

[0]: https://github.com/capnproto/capnproto/commit/80149744bdafa3ad4eedc83f8ab675e27baee868

Details
=======

Advisory [2015-03-02-2][1] described a bug allowing a remote attacker to
consume excessive CPU time or other resources using a specially-crafted message.
The present advisory is simply another case of the same bug which was initially
missed.

The new case occurs only if the application invokes the `totalSize()` method
on an object reader.

The new case is somewhat less severe, in that it only spins in a tight `for`
loop that doesn't call any application code. Only CPU time is possibly
consumed, not RAM or other resources. However, it is still possible to create
significant delays for the receiver with a specially-crafted message.

[1]: https://github.com/capnproto/capnproto/blob/master/security-advisories/2015-03-02-2-all-cpu-amplification.md

Preventative measures
=====================

Our fuzz test actually covered this case, but we didn't notice the problem
because the loop actually completes in less than a second. We've added a new
test case which is more demanding, and will make sure that when we do extended
testing with American Fuzzy Lop, we treat unexpectedly long run times as
failures.



security-advisories/2017-04-17-0-apple-clang-elides-bounds-check.md
--------------------------------------
Problem
=======

Some bounds checks are elided by Apple's compiler and possibly others, leading
to a possible attack especially in 32-bit builds.

Although triggered by a compiler optimization, this is a bug in Cap'n Proto,
not the compiler.

Discovered by
=============

Kenton Varda &lt;kenton@cloudflare.com> &lt;kenton@sandstorm.io>

Announced
=========

2017-04-17

CVE
===

CVE-2017-7892

Impact
======

- Remotely segfault a 32-bit application by sending it a malicious message.
- Exfiltration of memory from such applications **might** be possible, but our
  current analysis indicates that other checks would cause any such attempt to
  fail (see below).
- At present I've only reproduced the problem on Mac OS using Apple's
  compiler. Other compilers and platforms do not seem to apply the problematic
  optimization.

Fixed in
========

- git commit [52bc956459a5e83d7c31be95763ff6399e064ae4][0]
- release 0.5.3.1:
  - Unix: https://capnproto.org/capnproto-c++-0.5.3.1.tar.gz
  - Windows: https://capnproto.org/capnproto-c++-win32-0.5.3.1.zip
- release 0.6 (future)

[0]: https://github.com/capnproto/capnproto/commit/52bc956459a5e83d7c31be95763ff6399e064ae4

Details
=======

*The following text contains speculation about the exploitability of this
bug. This is provided for informational purposes, but as such speculation is
often shown to be wrong, you should not rely on the accuracy of this
section for the safety of your service. Please update your library.*

During regular testing in preparation for a release, it was discovered that
when Cap'n Proto is built using the current version of Apple's Clang compiler
in 32-bit mode with optimizations enabled, it is vulnerable to attack via
specially-crafted malicious input, causing the application to read from an
invalid memory location and crash.

Specifically, a malicious message could contain a [far pointer][1] pointing
outside of the message. Cap'n Proto messages are broken into segments of
continuous memory. A far pointer points from one segment into another,
indicating both the segment number and an offset within that segment. If a
malicious far pointer contained an offset large enough to overflow the pointer
arithmetic (wrapping around to the beginning of memory), then it would escape
bounds checking, in part because the compiler would elide half of the bounds
check as an optimization.

That is, the code looked like (simplification):

    word* target = segmentStart + farPointer.offset;
    if (target < segmentStart || target >= segmentEnd) {
      throwBoundsError();
    }
    doSomething(*target);

To most observers, this code would appear to be correct. However, as it turns
out, pointer arithmetic that overflows is undefined behavior under the C
standard. As a result, the compiler is allowed to assume that the addition on
the first line never overflows. Since `farPointer.offset` is an unsigned
number, the compiler is able to conclude that `target < segmentStart` always
evaluates false. Thus, the compiler removes this part of the check.
Unfortunately, in the case of overflow, this is exactly the part of the check
that we need.

Based on both fuzz testing and logical analysis, I believe that the far pointer
bounds check is the only check affected by this optimization. If true, then it
is not possible to exfiltrate memory through this vulnerability: the target of
a far pointer is always expected to be another pointer, which in turn points to
the final object. That second pointer will go through its own bounds checking,
which will fail (unless it somehow happens to point back into the message
bounds, in which case no harm is done).

I believe this bug does not affect any common 64-bit platform because the
maximum offset expressible by a far pointer is 2^32 bytes. In order to trigger
the bug in a 64-bit address space, the message location would have to begin
with 0xFFFFFFFF (allocated in the uppermost 4GB of address space) and the
target would have to begin with 0x00000000 (allocated in the lowermost 4GB of
address space). Typically, on 64-bit architectures, the upper half of the
address space is reserved for the OS kernel, thus a message could not possibly
be located there.

I was not able to reproduce this bug on other platforms, perhaps because the
compiler optimization is not applied by other compilers. On Linux, I tested GCC
4.9, 5.4, and 6.3, as well as Clang 3.6, 3.8, and 4.0. None were affected.
Nevertheless, the compiler behavior is technically allowed, thus it should be
assumed that it can happen on other platforms as well.

The specific compiler version which was observed to be affected is:

    $ clang++ --version
    Apple LLVM version 8.1.0 (clang-802.0.41)
    Target: x86_64-apple-darwin16.5.0
    Thread model: posix
    InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

(Note: Despite being Clang-based, Apple's compiler version numbers have no
apparent relationship to Clang version numbers.)

[1]: https://capnproto.org/encoding.html#inter-segment-pointers

Preventative measures
=====================

The problem was caught by running Cap'n Proto's standard fuzz tests in this
configuration. These tests are part of the Cap'n Proto test suite which runs
when you invoke `make check`, which Cap'n Proto's installation instructions
suggest to all users.

However, these fuzz tests were introduced after the 0.5.x release branch,
hence are not currently present in release versions of Cap'n Proto, only in
git. A 0.6 release will come soon, fixing this.

The bugfix commit forces the compiler to perform all checks by casting the
relevant pointers to `uintptr_t`. According to the standard, unsigned integers
implement modulo arithmetic, rather than leaving overflow undefined, thus the
compiler cannot make the assumptions that lead to eliding the check. This
change has been shown to fix the problem in practice. However, this quick fix
does not technically avoid undefined behavior, as the code still computes
pointers that point to invalid locations before they are checked. A
technically-correct solution has been implemented in the next commit,
[2ca8e41140ebc618b8fb314b393b0a507568cf21][2]. However, as this required more
extensive refactoring, it is not appropriate for cherry-picking, and will
only land in versions 0.6 and up.

[2]: https://github.com/capnproto/capnproto/commit/2ca8e41140ebc618b8fb314b393b0a507568cf21



security-advisories/2022-11-30-0-pointer-list-bounds.md
--------------------------------------
Problem
=======

Out-of-bounds read due to logic error handling list-of-list.

Discovered by
=============

David Renshaw &lt;dwrenshaw@gmail.com>, the maintainer of Cap'n Proto's Rust
implementation, which is affected by the same bug. David discovered this bug
while running his own fuzzer.

Announced
=========

2022-11-30

CVE
===

CVE-2022-46149

Impact
======

- Remotely segfault a peer by sending it a malicious message, if the victim
  performs certain actions on a list-of-pointer type.
- Possible exfiltration of memory, if the victim performs additional certain
  actions on a list-of-pointer type.
- To be vulnerable, an application must perform a specific sequence of actions,
  described below. At present, **we are not aware of any vulnerable
  application**, but we advise updating regardless.

Fixed in
========

Unfortunately, the bug is present in inlined code, therefore the fix will
require rebuilding dependent applications.

C++ fix:

- git commit [25d34c67863fd960af34fc4f82a7ca3362ee74b9][0]
- release 0.11 (future)
- release 0.10.3:
  - Unix: https://capnproto.org/capnproto-c++-0.10.3.tar.gz
  - Windows: https://capnproto.org/capnproto-c++-win32-0.10.3.zip
- release 0.9.2:
  - Unix: https://capnproto.org/capnproto-c++-0.9.2.tar.gz
  - Windows: https://capnproto.org/capnproto-c++-win32-0.9.2.zip
- release 0.8.1:
  - Unix: https://capnproto.org/capnproto-c++-0.8.1.tar.gz
  - Windows: https://capnproto.org/capnproto-c++-win32-0.8.1.zip
- release 0.7.1:
  - Unix: https://capnproto.org/capnproto-c++-0.7.1.tar.gz
  - Windows: https://capnproto.org/capnproto-c++-win32-0.7.1.zip

Rust fix:

- `capnp` crate version `0.15.2`, `0.14.11`, or `0.13.7`.

[0]: https://github.com/capnproto/capnproto/commit/25d34c67863fd960af34fc4f82a7ca3362ee74b9

Details
=======

A specially-crafted pointer could escape bounds checking by exploiting
inconsistent handling of pointers when a list-of-structs is downgraded to a
list-of-pointers.

For an in-depth explanation of how this bug works, see [David Renshaw's
blog post][1]. This details below focus only on determining whether an
application is vulnerable.

In order to be vulnerable, an application must have certain properties.

First, the application must accept messages with a schema in which a field has
list-of-pointer type. This includes `List(Text)`, `List(Data)`,
`List(List(T))`, or `List(C)` where `C` is an interface type. In the following
discussion, we will assume this field is named `foo`.

Second, the application must accept a message of this schema from a malicious
source, where the attacker can maliciously encode the pointer representing the
field `foo`.

Third, the application must call `getFoo()` to obtain a `List<T>::Reader` for
the field, and then use it in one of the following two ways:

1. Pass it as the parameter to another message's `setFoo()`, thus copying the
   field into a new message. Note that copying the parent struct as a whole
   will *not* trigger the bug; the bug only occurs if the specific field `foo`
   is get/set on its own.

2. Convert it into `AnyList::Reader`, and then attempt to access it through
   that. This is much less likely; very few apps use the `AnyList` API.

The dynamic API equivalents of these actions (`capnp/dynamic.h`) are also
affected.

If the application does these steps, the attacker may be able to cause the
Cap'n Proto implementation to read beyond the end of the message. This could
induce a segmentation fault. Or, worse, data that happened to be in memory
immediately after the message might be returned as if it were part of the
message. In the latter case, if the application then forwards that data back
to the attacker or sends it to another third party, this could result in
exfiltration of secrets.

Any exfiltration of data would have the following limitations:

* The attacker could exfiltrate no more than 512 KiB of memory immediately
  following the message buffer.
  * The attacker chooses in advance how far past the end of the message to
    read.
  * The attacker's message itself must be larger than the exfiltrated data.
    Note that a sufficiently large message buffer will likely be allocated
    using mmap() in which case the attack will likely segfault.
* The attack can only work if the 8 bytes immediately following the
  exfiltrated data contains a valid in-bounds Cap'n Proto pointer. The
  easiest way to achieve this is if the pointer is null, i.e. 8 bytes of zero.
  * The attacker must specify exactly how much data to exfiltrate, so must
    guess exactly where such a valid pointer will exist.
  * If the exfiltrated data is not followed by a valid pointer, the attack
    will throw an exception. If an application has chosen to ignore exceptions
    (e.g. by compiling with `-fno-exceptions` and not registering an
    alternative exception callback) then the attack may be able to proceed
    anyway.

[1]: https://dwrensha.github.io/capnproto-rust/2022/11/30/out_of_bounds_memory_access_bug.html



security-advisories/README.md
--------------------------------------
# Security Advisories

This directory contains security advisories issued for Cap'n Proto.

Each advisory explains not just the bug that was fixed, but measures we are taking to avoid the class of bugs in the future.

## Reporting Bugs

Please report security bugs to [security@sandstorm.io](mailto:security@sandstorm.io).



libargparse/LICENSE.md
--------------------------------------
The MIT License (MIT)

Copyright 2017 K. Murray

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.



libargparse/README.md
--------------------------------------
libargparse
===========
This is (yet another) simple command-line parser for C++ applications, inspired by Python's agparse module.

It requires only a C++11 compiler, and has no external dependencies.

One of the advantages of libargparse is that all conversions from command-line strings to program types (bool, int etc.) are performed when the command line is parsed (and not when the options are accessed).
This avoids command-line related errors from showing up deep in the program execution, which can be problematic for long-running programs.

Basic Usage
===========

```cpp
#include "argparse.hpp"

struct Args {
    argparse::ArgValue<bool> do_foo;
    argparse::ArgValue<bool> enable_bar;
    argparse::ArgValue<std::string> filename;
    argparse::ArgValue<size_t> verbosity;
};

int main(int argc, const char** argv) {
    Args args;
    auto parser = argparse::ArgumentParser(argv[0], "My application description");

    parser.add_argument(args.filename, "filename")
        .help("File to process");

    parser.add_argument(args.do_foo, "--foo")
        .help("Causes foo")
        .default_value("false")
        .action(argparse::Action::STORE_TRUE);

    parser.add_argument(args.enable_bar, "--bar")
        .help("Control bar")
        .default_value("false");

    parser.add_argument(args.verbosity, "--verbosity", "-v")
        .help("Sets the verbosity")
        .default_value("1")
        .choices({"0", "1", "2"});

    parser.parse_args(argc, argv);

    //Show the arguments
    std::cout << "args.filename: " << args.filename << "\n";
    std::cout << "args.do_foo: " << args.do_foo << "\n";
    std::cout << "args.verbosity: " << args.verbosity << "\n";
    std::cout << "\n";

    //Do work
    if (args.do_foo) {
        if (args.verbosity > 0) {
            std::cout << "Doing foo with " << args.filename << "\n";
        }
        if (args.verbosity > 1) {
            std::cout << "Doing foo step 1" << "\n";
            std::cout << "Doing foo step 2" << "\n";
            std::cout << "Doing foo step 3" << "\n";
        }
    }

    if (args.enable_bar) {
        if (args.verbosity > 0) {
            std::cout << "Bar is enabled" << "\n";
        }
    } else {
        if (args.verbosity > 0) {
            std::cout << "Bar is disabled" << "\n";
        }
    }

    return 0;
}
```

and the resulting help:

```
$ ./argparse_example -h
usage: argparse_example filename [--foo] [--bar {true, false}] 
       [-v {0, 1, 2}] [-h]

My application description

arguments:
  filename          File to process
  --foo             Causes foo (Default: false)
  --bar {true, false}
                    Control whether bar is enabled (Default: false)
  -v {0, 1, 2}, --verbosity {0, 1, 2}
                    Sets the verbosity (Default: 1)
  -h, --help        Shows this help message
```
By default the usage and help messages are line-wrapped to 80 characters.

Custom Conversions
==================
By default libargparse performs string to program type conversions using ``<sstream>``, meaning any type supporting ``operator<<()`` and ``operator>>()`` should be automatically supported.

However this does not always provide sufficient flexibility.
As a result libargparse also supports custom conversions, allowing user-defined mappings between command-line strings to program types.

If we wanted to modify the above example so the '--bar' argument accepted the strings 'on' and 'off' (instead of the default 'true' and 'false') we would define a custom class as follows:
```cpp
struct OnOff {
    ConvertedValue<bool> from_str(std::string str) {
        ConvertedValue<bool> converted_value;

        if      (str == "on")  converted_value.set_value(true);
        else if (str == "off") converted_value.set_value(false);
        else                   converted_value.set_error("Invalid argument value");
        return converted_value;
    }

    ConvertedValue<std::string> to_str(bool val) {
        ConvertedValue<std::string> converted_value;
        if (val) converted_value.set_value("on");
        else     converted_value.set_value("off");
        return converted_value;
    }

    std::vector<std::string> default_choices() {
        return {"on", "off"};
    }
};
```

Where the `from_str()` and `to_str()` define the conversions to and from a string, and `default_choices()` returns the set of valid choices. Note that default_choices() can return an empty vector to indicate there is no specified default set of choices.

We then modify the ``add_argument()`` call to use our conversion object:
```cpp
    parser.add_argument<bool,OnOff>(args.enable_bar, "--bar")
        .help("Control whether bar is enabled")
        .default_value("off");
```

with the resulting help:
```
usage: argparse_example filename [--foo] [--bar {on, off}] [-v {0, 1, 2}] 
       [-h]

My application description

arguments:
  filename          File to process
  --foo             Causes foo (Default: false)
  --bar {on, off}   Control whether bar is enabled (Default: off)
  -v {0, 1, 2}, --verbosity {0, 1, 2}
                    Sets the verbosity (Default: 1)
  -h, --help        Shows this help message
```

Advanced Usage
==============
For more advanced usage such as argument groups see [argparse_test.cpp](argparse_test.cpp) and [argparse.hpp](src/argparse.hpp).

Future Work
===========
libargparse is missing a variety of more advanced features found in Python's argparse, including (but not limited to):
* action: append, count
* subcommands
* mutually exclusive options
* parsing only known args
* concatenated short options (e.g. `-xvf`, for options `-x`, `-v`, `-f`)
* equal concatenated option values (e.g. `--foo=VALUE`)

Acknowledgements
================
Python's [argparse module](https://docs.python.org/2.7/library/argparse.html)



libblifparse/README.md
--------------------------------------
libblifparse
----------------------------------

This library provides a parser for a Berkely Logic Interchange Format (BLIF) files.
See comments at the top of 'src/blifparse.hpp' for more detailed information and usage.




libcatch2/CODE_OF_CONDUCT.md
--------------------------------------
# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at github@philnash.me. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4, available at [http://contributor-covenant.org/version/1/4][version]

[homepage]: http://contributor-covenant.org
[version]: http://contributor-covenant.org/version/1/4/



libcatch2/README.md
--------------------------------------
<a id="top"></a>
![Catch2 logo](data/artwork/catch2-logo-small-with-background.png)

[![Github Releases](https://img.shields.io/github/release/catchorg/catch2.svg)](https://github.com/catchorg/catch2/releases)
[![Linux build status](https://github.com/catchorg/Catch2/actions/workflows/linux-simple-builds.yml/badge.svg)](https://github.com/catchorg/Catch2/actions/workflows/linux-simple-builds.yml)
[![Linux build status](https://github.com/catchorg/Catch2/actions/workflows/linux-other-builds.yml/badge.svg)](https://github.com/catchorg/Catch2/actions/workflows/linux-other-builds.yml)
[![MacOS build status](https://github.com/catchorg/Catch2/actions/workflows/mac-builds.yml/badge.svg)](https://github.com/catchorg/Catch2/actions/workflows/mac-builds.yml)
[![Build Status](https://ci.appveyor.com/api/projects/status/github/catchorg/Catch2?svg=true&branch=devel)](https://ci.appveyor.com/project/catchorg/catch2)
[![Code Coverage](https://codecov.io/gh/catchorg/Catch2/branch/devel/graph/badge.svg)](https://codecov.io/gh/catchorg/Catch2)
[![Try online](https://img.shields.io/badge/try-online-blue.svg)](https://godbolt.org/z/EdoY15q9G)
[![Join the chat in Discord: https://discord.gg/4CWS9zD](https://img.shields.io/badge/Discord-Chat!-brightgreen.svg)](https://discord.gg/4CWS9zD)


## What is Catch2?

Catch2 is mainly a unit testing framework for C++, but it also
provides basic micro-benchmarking features, and simple BDD macros.

Catch2's main advantage is that using it is both simple and natural.
Test names do not have to be valid identifiers, assertions look like
normal C++ boolean expressions, and sections provide a nice and local way
to share set-up and tear-down code in tests.

**Example unit test**
```cpp
#include <catch2/catch_test_macros.hpp>

#include <cstdint>

uint32_t factorial( uint32_t number ) {
    return number <= 1 ? number : factorial(number-1) * number;
}

TEST_CASE( "Factorials are computed", "[factorial]" ) {
    REQUIRE( factorial( 1) == 1 );
    REQUIRE( factorial( 2) == 2 );
    REQUIRE( factorial( 3) == 6 );
    REQUIRE( factorial(10) == 3'628'800 );
}
```

**Example microbenchmark**
```cpp
#include <catch2/catch_test_macros.hpp>
#include <catch2/benchmark/catch_benchmark.hpp>

#include <cstdint>

uint64_t fibonacci(uint64_t number) {
    return number < 2 ? number : fibonacci(number - 1) + fibonacci(number - 2);
}

TEST_CASE("Benchmark Fibonacci", "[!benchmark]") {
    REQUIRE(fibonacci(5) == 5);

    REQUIRE(fibonacci(20) == 6'765);
    BENCHMARK("fibonacci 20") {
        return fibonacci(20);
    };

    REQUIRE(fibonacci(25) == 75'025);
    BENCHMARK("fibonacci 25") {
        return fibonacci(25);
    };
}
```

_Note that benchmarks are not run by default, so you need to run it explicitly
with the `[!benchmark]` tag._


## Catch2 v3 has been released!

You are on the `devel` branch, where the v3 version is being developed.
v3 brings a bunch of significant changes, the big one being that Catch2
is no longer a single-header library. Catch2 now behaves as a normal
library, with multiple headers and separately compiled implementation.

The documentation is slowly being updated to take these changes into
account, but this work is currently still ongoing.

For migrating from the v2 releases to v3, you should look at [our
documentation](docs/migrate-v2-to-v3.md#top). It provides a simple
guidelines on getting started, and collects most common migration
problems.

For the previous major version of Catch2 [look into the `v2.x` branch
here on GitHub](https://github.com/catchorg/Catch2/tree/v2.x).


## How to use it
This documentation comprises these three parts:

* [Why do we need yet another C++ Test Framework?](docs/why-catch.md#top)
* [Tutorial](docs/tutorial.md#top) - getting started
* [Reference section](docs/Readme.md#top) - all the details


## More
* Issues and bugs can be raised on the [Issue tracker on GitHub](https://github.com/catchorg/Catch2/issues)
* For discussion or questions please use [our Discord](https://discord.gg/4CWS9zD)
* See who else is using Catch2 in [Open Source Software](docs/opensource-users.md#top)
or [commercially](docs/commercial-users.md#top).



libcatch2/SECURITY.md
--------------------------------------
# Security Policy

## Supported Versions

* Versions 1.x (branch Catch1.x) are no longer supported.
* Versions 2.x (branch v2.x) are currently supported.
* `devel` branch serves for stable-ish development and is supported,
  but branches `devel-*` are considered short lived and are not supported separately.


## Reporting a Vulnerability

Due to its nature as a _unit_ test framework, Catch2 shouldn't interact
with untrusted inputs and there shouldn't be many security vulnerabilities
in it.

However, if you find one you send email to martin <dot> horenovsky <at>
gmail <dot> com. If you want to encrypt the email, my pgp key is
`E29C 46F3 B8A7 5028 6079 3B7D ECC9 C20E 314B 2360`.



.github/pull_request_template.md
--------------------------------------
<!--
Please do not submit pull requests changing the `version.hpp`
or the single-include `catch.hpp` file, these are changed
only when a new release is made.

Before submitting a PR you should probably read the contributor documentation
at docs/contributing.md. It will tell you how to properly test your changes.
-->


## Description
<!--
Describe the what and the why of your pull request. Remember that these two
are usually a bit different. As an example, if you have made various changes
to decrease the number of new strings allocated, that's what. The why probably
was that you have a large set of tests and found that this speeds them up.
-->

## GitHub Issues
<!-- 
If this PR was motivated by some existing issues, reference them here.

If it is a simple bug-fix, please also add a line like 'Closes #123'
to your commit message, so that it is automatically closed.
If it is not, don't, as it might take several iterations for a feature
to be done properly. If in doubt, leave it open and reference it in the
PR itself, so that maintainers can decide.
-->



ISSUE_TEMPLATE/bug_report.md
--------------------------------------
---
name: Bug report
about: Create an issue that documents a bug
title: ''
labels: ''
assignees: ''

---

**Describe the bug**
A clear and concise description of what the bug is.

**Expected behavior**
A clear and concise description of what you expected to happen.

**Reproduction steps**
Steps to reproduce the bug.
<!-- Usually this means a small and self-contained piece of code that uses Catch and specifying compiler flags if relevant. -->


**Platform information:**
<!-- Fill in any extra information that might be important for your issue. -->
 - OS: **Windows NT**
 - Compiler+version: **GCC v2.9.5**
 - Catch version: **v1.2.3**


**Additional context**
Add any other context about the problem here.



ISSUE_TEMPLATE/feature_request.md
--------------------------------------
---
name: Feature request
about: Create an issue that requests a feature or other improvement
title: ''
labels: ''
assignees: ''

---

**Description**
Describe the feature/change you request and why do you want it.

**Additional context**
Add any other context or screenshots about the feature request here.



docs/Readme.md
--------------------------------------
<a id="top"></a>
# Reference

To get the most out of Catch2, start with the [tutorial](tutorial.md#top).
Once you're up and running consider the following reference material.

**Writing tests:**
* [Assertion macros](assertions.md#top)
* [Matchers (asserting complex properties)](matchers.md#top)
* [Comparing floating point numbers](comparing-floating-point-numbers.md#top)
* [Logging macros](logging.md#top)
* [Test cases and sections](test-cases-and-sections.md#top)
* [Test fixtures](test-fixtures.md#top)
* [Explicitly skipping, passing, and failing tests at runtime](skipping-passing-failing.md#top)
* [Reporters (output customization)](reporters.md#top)
* [Event Listeners](event-listeners.md#top)
* [Data Generators (value parameterized tests)](generators.md#top)
* [Other macros](other-macros.md#top)
* [Micro benchmarking](benchmarks.md#top)

**Fine tuning:**
* [Supplying your own main()](own-main.md#top)
* [Compile-time configuration](configuration.md#top)
* [String Conversions](tostring.md#top)

**Running:**
* [Command line](command-line.md#top)

**Odds and ends:**
* [Frequently Asked Questions (FAQ)](faq.md#top)
* [Best practices and other tips](usage-tips.md#top)
* [CMake integration](cmake-integration.md#top)
* [Tooling integration (CI, test runners, other)](ci-and-misc.md#top)
* [Known limitations](limitations.md#top)

**Other:**
* [Why Catch2?](why-catch.md#top)
* [Migrating from v2 to v3](migrate-v2-to-v3.md#top)
* [Open Source Projects using Catch2](opensource-users.md#top)
* [Commercial Projects using Catch2](commercial-users.md#top)
* [Contributing](contributing.md#top)
* [Release Notes](release-notes.md#top)
* [Deprecations and incoming changes](deprecations.md#top)



docs/assertions.md
--------------------------------------
<a id="top"></a>
# Assertion Macros

**Contents**<br>
[Natural Expressions](#natural-expressions)<br>
[Floating point comparisons](#floating-point-comparisons)<br>
[Exceptions](#exceptions)<br>
[Matcher expressions](#matcher-expressions)<br>
[Thread Safety](#thread-safety)<br>
[Expressions with commas](#expressions-with-commas)<br>

Most test frameworks have a large collection of assertion macros to capture all possible conditional forms (```_EQUALS```, ```_NOTEQUALS```, ```_GREATER_THAN``` etc).

Catch is different. Because it decomposes natural C-style conditional expressions most of these forms are reduced to one or two that you will use all the time. That said there is a rich set of auxiliary macros as well. We'll describe all of these here.

Most of these macros come in two forms:

## Natural Expressions

The ```REQUIRE``` family of macros tests an expression and aborts the test case if it fails.
The ```CHECK``` family are equivalent but execution continues in the same test case even if the assertion fails. This is useful if you have a series of essentially orthogonal assertions and it is useful to see all the results rather than stopping at the first failure.

* **REQUIRE(** _expression_ **)** and
* **CHECK(** _expression_ **)**

Evaluates the expression and records the result. If an exception is thrown, it is caught, reported, and counted as a failure. These are the macros you will use most of the time.

Examples:
```
CHECK( str == "string value" );
CHECK( thisReturnsTrue() );
REQUIRE( i == 42 );
```

Expressions prefixed with `!` cannot be decomposed. If you have a type
that is convertible to bool and you want to assert that it evaluates to
false, use the two forms below:


* **REQUIRE_FALSE(** _expression_ **)** and
* **CHECK_FALSE(** _expression_ **)**

Note that there is no reason to use these forms for plain bool variables,
because there is no added value in decomposing them.

Example:
```cpp
Status ret = someFunction();
REQUIRE_FALSE(ret); // ret must evaluate to false, and Catch2 will print
                    // out the value of ret if possibly
```


### Other limitations

Note that expressions containing either of the binary logical operators,
`&&` or `||`, cannot be decomposed and will not compile. The reason behind
this is that it is impossible to overload `&&` and `||` in a way that
keeps their short-circuiting semantics, and expression decomposition
relies on overloaded operators to work.

Simple example of an issue with overloading binary logical operators
is a common pointer idiom, `p && p->foo == 2`. Using the built-in `&&`
operator, `p` is only dereferenced if it is not null. With overloaded
`&&`, `p` is always dereferenced, thus causing a segfault if
`p == nullptr`.

If you want to test expression that contains `&&` or `||`, you have two
options.

1) Enclose it in parentheses. Parentheses force evaluation of the expression
   before the expression decomposition can touch it, and thus it cannot
   be used.

2) Rewrite the expression. `REQUIRE(a == 1 && b == 2)` can always be split
   into `REQUIRE(a == 1); REQUIRE(b == 2);`. Alternatively, if this is a
   common pattern in your tests, think about using [Matchers](#matcher-expressions).
   instead. There is no simple rewrite rule for `||`, but I generally
   believe that if you have `||` in your test expression, you should rethink
   your tests.


## Floating point comparisons

Comparing floating point numbers is complex, and [so it has its own
documentation page](comparing-floating-point-numbers.md#top).


## Exceptions

* **REQUIRE_NOTHROW(** _expression_ **)** and
* **CHECK_NOTHROW(** _expression_ **)**

Expects that no exception is thrown during evaluation of the expression.

* **REQUIRE_THROWS(** _expression_ **)** and
* **CHECK_THROWS(** _expression_ **)**

Expects that an exception (of any type) is be thrown during evaluation of the expression.

* **REQUIRE_THROWS_AS(** _expression_, _exception type_ **)** and
* **CHECK_THROWS_AS(** _expression_, _exception type_ **)**

Expects that an exception of the _specified type_ is thrown during evaluation of the expression. Note that the _exception type_ is extended with `const&` and you should not include it yourself.

* **REQUIRE_THROWS_WITH(** _expression_, _string or string matcher_ **)** and
* **CHECK_THROWS_WITH(** _expression_, _string or string matcher_ **)**

Expects that an exception is thrown that, when converted to a string, matches the _string_ or _string matcher_ provided (see next section for Matchers).

e.g.
```cpp
REQUIRE_THROWS_WITH( openThePodBayDoors(), ContainsSubstring( "afraid" ) && ContainsSubstring( "can't do that" ) );
REQUIRE_THROWS_WITH( dismantleHal(), "My mind is going" );
```

* **REQUIRE_THROWS_MATCHES(** _expression_, _exception type_, _matcher for given exception type_ **)** and
* **CHECK_THROWS_MATCHES(** _expression_, _exception type_, _matcher for given exception type_ **)**

Expects that exception of _exception type_ is thrown and it matches provided matcher (see the [documentation for Matchers](matchers.md#top)).


_Please note that the `THROW` family of assertions expects to be passed a single expression, not a statement or series of statements. If you want to check a more complicated sequence of operations, you can use a C++11 lambda function._

```cpp
REQUIRE_NOTHROW([&](){
    int i = 1;
    int j = 2;
    auto k = i + j;
    if (k == 3) {
        throw 1;
    }
}());
```



## Matcher expressions

To support Matchers a slightly different form is used. Matchers have [their own documentation](matchers.md#top).

* **REQUIRE_THAT(** _lhs_, _matcher expression_ **)** and
* **CHECK_THAT(** _lhs_, _matcher expression_ **)**

Matchers can be composed using `&&`, `||` and `!` operators.

## Thread Safety

Currently assertions in Catch are not thread safe.
For more details, along with workarounds, see the section on [the limitations page](limitations.md#thread-safe-assertions).

## Expressions with commas

Because the preprocessor parses code using different rules than the
compiler, multiple-argument assertions (e.g. `REQUIRE_THROWS_AS`) have
problems with commas inside the provided expressions. As an example
`REQUIRE_THROWS_AS(std::pair<int, int>(1, 2), std::invalid_argument);`
will fail to compile, because the preprocessor sees 3 arguments provided,
but the macro accepts only 2. There are two possible workarounds.

1) Use typedef:
```cpp
using int_pair = std::pair<int, int>;
REQUIRE_THROWS_AS(int_pair(1, 2), std::invalid_argument);
```

This solution is always applicable, but makes the meaning of the code
less clear.

2) Parenthesize the expression:
```cpp
TEST_CASE_METHOD((Fixture<int, int>), "foo", "[bar]") {
    SUCCEED();
}
```

This solution is not always applicable, because it might require extra
changes on the Catch's side to work.

---

[Home](Readme.md#top)



docs/benchmarks.md
--------------------------------------
<a id="top"></a>
# Authoring benchmarks

> [Introduced](https://github.com/catchorg/Catch2/issues/1616) in Catch2 2.9.0.

Writing benchmarks is not easy. Catch simplifies certain aspects but you'll
always need to take care about various aspects. Understanding a few things about
the way Catch runs your code will be very helpful when writing your benchmarks.

First off, let's go over some terminology that will be used throughout this
guide.

- *User code*: user code is the code that the user provides to be measured.
- *Run*: one run is one execution of the user code. Sometimes also referred
  to as an _iteration_.
- *Sample*: one sample is one data point obtained by measuring the time it takes
  to perform a certain number of runs. One sample can consist of more than one
  run if the clock available does not have enough resolution to accurately
  measure a single run. All samples for a given benchmark execution are obtained
  with the same number of runs.

## Execution procedure

Now I can explain how a benchmark is executed in Catch. There are three main
steps, though the first does not need to be repeated for every benchmark.

1. *Environmental probe*: before any benchmarks can be executed, the clock's
resolution is estimated. A few other environmental artifacts are also estimated
at this point, like the cost of calling the clock function, but they almost
never have any impact in the results.

2. *Estimation*: the user code is executed a few times to obtain an estimate of
the amount of runs that should be in each sample. This also has the potential
effect of bringing relevant code and data into the caches before the actual
measurement starts.

3. *Measurement*: all the samples are collected sequentially by performing the
number of runs estimated in the previous step for each sample.

This already gives us one important rule for writing benchmarks for Catch: the
benchmarks must be repeatable. The user code will be executed several times, and
the number of times it will be executed during the estimation step cannot be
known beforehand since it depends on the time it takes to execute the code.
User code that cannot be executed repeatedly will lead to bogus results or
crashes.

## Benchmark specification

Benchmarks can be specified anywhere inside a Catch test case.
There is a simple and a slightly more advanced version of the `BENCHMARK` macro.

Let's have a look how a naive Fibonacci implementation could be benchmarked:
```c++
std::uint64_t Fibonacci(std::uint64_t number) {
    return number < 2 ? 1 : Fibonacci(number - 1) + Fibonacci(number - 2);
}
```
Now the most straight forward way to benchmark this function, is just adding a `BENCHMARK` macro to our test case:
```c++
TEST_CASE("Fibonacci") {
    CHECK(Fibonacci(0) == 1);
    // some more asserts..
    CHECK(Fibonacci(5) == 8);
    // some more asserts..

    // now let's benchmark:
    BENCHMARK("Fibonacci 20") {
        return Fibonacci(20);
    };

    BENCHMARK("Fibonacci 25") {
        return Fibonacci(25);
    };

    BENCHMARK("Fibonacci 30") {
        return Fibonacci(30);
    };

    BENCHMARK("Fibonacci 35") {
        return Fibonacci(35);
    };
}
```
There's a few things to note:
- As `BENCHMARK` expands to a lambda expression it is necessary to add a semicolon after
 the closing brace (as opposed to the first experimental version).
- The `return` is a handy way to avoid the compiler optimizing away the benchmark code.

Running this already runs the benchmarks and outputs something similar to:
```
-------------------------------------------------------------------------------
Fibonacci
-------------------------------------------------------------------------------
C:\path\to\Catch2\Benchmark.tests.cpp(10)
...............................................................................
benchmark name                                  samples       iterations    est run time
                                                mean          low mean      high mean
                                                std dev       low std dev   high std dev
-------------------------------------------------------------------------------
Fibonacci 20                                            100       416439   83.2878 ms
                                                       2 ns         2 ns         2 ns
                                                       0 ns         0 ns         0 ns

Fibonacci 25                                            100       400776   80.1552 ms
                                                       3 ns         3 ns         3 ns
                                                       0 ns         0 ns         0 ns

Fibonacci 30                                            100       396873   79.3746 ms
                                                      17 ns        17 ns        17 ns
                                                       0 ns         0 ns         0 ns

Fibonacci 35                                            100       145169   87.1014 ms
                                                     468 ns       464 ns       473 ns
                                                      21 ns        15 ns        34 ns
```

### Advanced benchmarking
The simplest use case shown above, takes no arguments and just runs the user code that needs to be measured.
However, if using the `BENCHMARK_ADVANCED` macro and adding a `Catch::Benchmark::Chronometer` argument after
the macro, some advanced features are available. The contents of the simple benchmarks are invoked once per run,
while the blocks of the advanced benchmarks are invoked exactly twice:
once during the estimation phase, and another time during the execution phase.

```c++
BENCHMARK("simple"){ return long_computation(); };

BENCHMARK_ADVANCED("advanced")(Catch::Benchmark::Chronometer meter) {
    set_up();
    meter.measure([] { return long_computation(); });
};
```

These advanced benchmarks no longer consist entirely of user code to be measured.
In these cases, the code to be measured is provided via the
`Catch::Benchmark::Chronometer::measure` member function. This allows you to set up any
kind of state that might be required for the benchmark but is not to be included
in the measurements, like making a vector of random integers to feed to a
sorting algorithm.

A single call to `Catch::Benchmark::Chronometer::measure` performs the actual measurements
by invoking the callable object passed in as many times as necessary. Anything
that needs to be done outside the measurement can be done outside the call to
`measure`.

The callable object passed in to `measure` can optionally accept an `int`
parameter.

```c++
meter.measure([](int i) { return long_computation(i); });
```

If it accepts an `int` parameter, the sequence number of each run will be passed
in, starting with 0. This is useful if you want to measure some mutating code,
for example. The number of runs can be known beforehand by calling
`Catch::Benchmark::Chronometer::runs`; with this one can set up a different instance to be
mutated by each run.

```c++
std::vector<std::string> v(meter.runs());
std::fill(v.begin(), v.end(), test_string());
meter.measure([&v](int i) { in_place_escape(v[i]); });
```

Note that it is not possible to simply use the same instance for different runs
and resetting it between each run since that would pollute the measurements with
the resetting code.

It is also possible to just provide an argument name to the simple `BENCHMARK` macro to get
the same semantics as providing a callable to `meter.measure` with `int` argument:

```c++
BENCHMARK("indexed", i){ return long_computation(i); };
```

### Constructors and destructors

All of these tools give you a lot mileage, but there are two things that still
need special handling: constructors and destructors. The problem is that if you
use automatic objects they get destroyed by the end of the scope, so you end up
measuring the time for construction and destruction together. And if you use
dynamic allocation instead, you end up including the time to allocate memory in
the measurements.

To solve this conundrum, Catch provides class templates that let you manually
construct and destroy objects without dynamic allocation and in a way that lets
you measure construction and destruction separately.

```c++
BENCHMARK_ADVANCED("construct")(Catch::Benchmark::Chronometer meter) {
    std::vector<Catch::Benchmark::storage_for<std::string>> storage(meter.runs());
    meter.measure([&](int i) { storage[i].construct("thing"); });
};

BENCHMARK_ADVANCED("destroy")(Catch::Benchmark::Chronometer meter) {
    std::vector<Catch::Benchmark::destructable_object<std::string>> storage(meter.runs());
    for(auto&& o : storage)
        o.construct("thing");
    meter.measure([&](int i) { storage[i].destruct(); });
};
```

`Catch::Benchmark::storage_for<T>` objects are just pieces of raw storage suitable for `T`
objects. You can use the `Catch::Benchmark::storage_for::construct` member function to call a constructor and
create an object in that storage. So if you want to measure the time it takes
for a certain constructor to run, you can just measure the time it takes to run
this function.

When the lifetime of a `Catch::Benchmark::storage_for<T>` object ends, if an actual object was
constructed there it will be automatically destroyed, so nothing leaks.

If you want to measure a destructor, though, we need to use
`Catch::Benchmark::destructable_object<T>`. These objects are similar to
`Catch::Benchmark::storage_for<T>` in that construction of the `T` object is manual, but
it does not destroy anything automatically. Instead, you are required to call
the `Catch::Benchmark::destructable_object::destruct` member function, which is what you
can use to measure the destruction time.

### The optimizer

Sometimes the optimizer will optimize away the very code that you want to
measure. There are several ways to use results that will prevent the optimiser
from removing them. You can use the `volatile` keyword, or you can output the
value to standard output or to a file, both of which force the program to
actually generate the value somehow.

Catch adds a third option. The values returned by any function provided as user
code are guaranteed to be evaluated and not optimised out. This means that if
your user code consists of computing a certain value, you don't need to bother
with using `volatile` or forcing output. Just `return` it from the function.
That helps with keeping the code in a natural fashion.

Here's an example:

```c++
// may measure nothing at all by skipping the long calculation since its
// result is not used
BENCHMARK("no return"){ long_calculation(); };

// the result of long_calculation() is guaranteed to be computed somehow
BENCHMARK("with return"){ return long_calculation(); };
```

However, there's no other form of control over the optimizer whatsoever. It is
up to you to write a benchmark that actually measures what you want and doesn't
just measure the time to do a whole bunch of nothing.

To sum up, there are two simple rules: whatever you would do in handwritten code
to control optimization still works in Catch; and Catch makes return values
from user code into observable effects that can't be optimized away.

<i>Adapted from nonius' documentation.</i>



docs/ci-and-misc.md
--------------------------------------
<a id="top"></a>
# Tooling integration (CI, test runners and so on)

**Contents**<br>
[Continuous Integration systems](#continuous-integration-systems)<br>
[Bazel test runner integration](#bazel-test-runner-integration)<br>
[Low-level tools](#low-level-tools)<br>
[CMake](#cmake)<br>

This page talks about Catch2's integration with other related tooling,
like Continuous Integration and 3rd party test runners.


## Continuous Integration systems

Probably the most important aspect to using Catch with a build server is the use of different reporters. Catch comes bundled with three reporters that should cover the majority of build servers out there - although adding more for better integration with some is always a possibility (currently we also offer TeamCity, TAP, Automake and SonarQube reporters).

Two of these reporters are built in (XML and JUnit) and the third (TeamCity) is included as a separate header. It's possible that the other two may be split out in the future too - as that would make the core of Catch smaller for those that don't need them.

### XML Reporter
```-r xml```

The XML Reporter writes in an XML format that is specific to Catch.

The advantage of this format is that it corresponds well to the way Catch works (especially the more unusual features, such as nested sections) and is a fully streaming format - that is it writes output as it goes, without having to store up all its results before it can start writing.

The disadvantage is that, being specific to Catch, no existing build servers understand the format natively. It can be used as input to an XSLT transformation that could convert it to, say, HTML - although this loses the streaming advantage, of course.

### JUnit Reporter
```-r junit```

The JUnit Reporter writes in an XML format that mimics the JUnit ANT schema.

The advantage of this format is that the JUnit Ant schema is widely understood by most build servers and so can usually be consumed with no additional work.

The disadvantage is that this schema was designed to correspond to how JUnit works - and there is a significant mismatch with how Catch works. Additionally the format is not streamable (because opening elements hold counts of failed and passing tests as attributes) - so the whole test run must complete before it can be written.


### TeamCity Reporter
```-r teamcity```

The TeamCity Reporter writes TeamCity service messages to stdout. In order to be able to use this reporter an additional header must also be included.

Being specific to TeamCity this is the best reporter to use with it - but it is completely unsuitable for any other purpose. It is a streaming format (it writes as it goes) - although test results don't appear in the TeamCity interface until the completion of a suite (usually the whole test run).

### Automake Reporter
```-r automake```

The Automake Reporter writes out the [meta tags](https://www.gnu.org/software/automake/manual/html_node/Log-files-generation-and-test-results-recording.html#Log-files-generation-and-test-results-recording) expected by automake via `make check`.

### TAP (Test Anything Protocol) Reporter
```-r tap```

Because of the incremental nature of Catch's test suites and ability to run specific tests, our implementation of TAP reporter writes out the number of tests in a suite last.

### SonarQube Reporter
```-r sonarqube```
[SonarQube Generic Test Data](https://docs.sonarqube.org/latest/analysis/generic-test/) XML format for tests metrics.


## Bazel test runner integration

Catch2 understands some of the environment variables Bazel uses to control
test execution. Specifically it understands

 * JUnit output path via `XML_OUTPUT_FILE`
 * Test filtering via `TESTBRIDGE_TEST_ONLY`
 * Test sharding via `TEST_SHARD_INDEX`, `TEST_TOTAL_SHARDS`, and `TEST_SHARD_STATUS_FILE`

> Support for `XML_OUTPUT_FILE` was [introduced](https://github.com/catchorg/Catch2/pull/2399) in Catch2 3.0.1

> Support for `TESTBRIDGE_TEST_ONLY` and sharding was introduced in Catch2 3.2.0

This integration is enabled via either a [compile time configuration
option](configuration.md#bazel-support), or via `BAZEL_TEST` environment
variable set to "1".

> Support for `BAZEL_TEST` was [introduced](https://github.com/catchorg/Catch2/pull/2459) in Catch2 3.1.0


## Low-level tools

### CodeCoverage module (GCOV, LCOV...)

If you are using GCOV tool to get testing coverage of your code, and are not sure how to integrate it with CMake and Catch, there should be an external example over at https://github.com/claremacrae/catch_cmake_coverage


### pkg-config

Catch2 provides a rudimentary pkg-config integration, by registering itself
under the name `catch2`. This means that after Catch2 is installed, you
can use `pkg-config` to get its include path: `pkg-config --cflags catch2`.

### gdb and lldb scripts

Catch2's `extras` folder also contains two simple debugger scripts,
`gdbinit` for `gdb` and `lldbinit` for `lldb`. If loaded into their
respective debugger, these will tell it to step over Catch2's internals
when stepping through code.


## CMake

[As it has been getting kinda long, the documentation of Catch2's
integration with CMake has been moved to its own page.](cmake-integration.md#top)


---

[Home](Readme.md#top)



docs/cmake-integration.md
--------------------------------------
<a id="top"></a>
# CMake integration

**Contents**<br>
[CMake targets](#cmake-targets)<br>
[Automatic test registration](#automatic-test-registration)<br>
[CMake project options](#cmake-project-options)<br>
[`CATCH_CONFIG_*` customization options in CMake](#catch_config_-customization-options-in-cmake)<br>
[Installing Catch2 from git repository](#installing-catch2-from-git-repository)<br>
[Installing Catch2 from vcpkg](#installing-catch2-from-vcpkg)<br>
[Installing Catch2 from Bazel](#installing-catch2-from-bazel)<br>

Because we use CMake to build Catch2, we also provide a couple of
integration points for our users.

1) Catch2 exports a (namespaced) CMake target
2) Catch2's repository contains CMake scripts for automatic registration
of `TEST_CASE`s in CTest

## CMake targets

Catch2's CMake build exports two targets, `Catch2::Catch2`, and
`Catch2::Catch2WithMain`. If you do not need custom `main` function,
you should be using the latter (and only the latter). Linking against
it will add the proper include paths and link your target together with
2 static libraries that implement Catch2 and its main respectively.
If you need custom `main`, you should link only against `Catch2::Catch2`.

This means that if Catch2 has been installed on the system, it should
be enough to do
```cmake
find_package(Catch2 3 REQUIRED)
# These tests can use the Catch2-provided main
add_executable(tests test.cpp)
target_link_libraries(tests PRIVATE Catch2::Catch2WithMain)

# These tests need their own main
add_executable(custom-main-tests test.cpp test-main.cpp)
target_link_libraries(custom-main-tests PRIVATE Catch2::Catch2)
```

These targets are also provided when Catch2 is used as a subdirectory.
Assuming Catch2 has been cloned to `lib/Catch2`, you only need to replace
the `find_package` call with `add_subdirectory(lib/Catch2)` and the snippet
above still works.


Another possibility is to use [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html):
```cmake
Include(FetchContent)

FetchContent_Declare(
  Catch2
  GIT_REPOSITORY https://github.com/catchorg/Catch2.git
  GIT_TAG        v3.4.0 # or a later release
)

FetchContent_MakeAvailable(Catch2)

add_executable(tests test.cpp)
target_link_libraries(tests PRIVATE Catch2::Catch2WithMain)
```


## Automatic test registration

Catch2's repository also contains three CMake scripts that help users
with automatically registering their `TEST_CASE`s with CTest. They
can be found in the `extras` folder, and are

1) `Catch.cmake` (and its dependency `CatchAddTests.cmake`)
2) `ParseAndAddCatchTests.cmake` (deprecated)
3) `CatchShardTests.cmake` (and its dependency `CatchShardTestsImpl.cmake`)

If Catch2 has been installed in system, both of these can be used after
doing `find_package(Catch2 REQUIRED)`. Otherwise you need to add them
to your CMake module path.

<a id="catch_discover_tests"></a>
### `Catch.cmake` and `CatchAddTests.cmake`

`Catch.cmake` provides function `catch_discover_tests` to get tests from
a target. This function works by running the resulting executable with
`--list-test-names-only` flag, and then parsing the output to find all
existing tests.

#### Usage
```cmake
cmake_minimum_required(VERSION 3.5)

project(baz LANGUAGES CXX VERSION 0.0.1)

find_package(Catch2 REQUIRED)
add_executable(tests test.cpp)
target_link_libraries(tests PRIVATE Catch2::Catch2)

include(CTest)
include(Catch)
catch_discover_tests(tests)
```

When using `FetchContent`, `include(Catch)` will fail unless
`CMAKE_MODULE_PATH` is explicitly updated to include the extras
directory.

```cmake
# ... FetchContent ...
#
list(APPEND CMAKE_MODULE_PATH ${catch2_SOURCE_DIR}/extras)
include(CTest)
include(Catch)
catch_discover_tests(tests)
```

#### Customization
`catch_discover_tests` can be given several extra arguments:
```cmake
catch_discover_tests(target
                     [TEST_SPEC arg1...]
                     [EXTRA_ARGS arg1...]
                     [WORKING_DIRECTORY dir]
                     [TEST_PREFIX prefix]
                     [TEST_SUFFIX suffix]
                     [PROPERTIES name1 value1...]
                     [TEST_LIST var]
                     [REPORTER reporter]
                     [OUTPUT_DIR dir]
                     [OUTPUT_PREFIX prefix]
                     [OUTPUT_SUFFIX suffix]
                     [DISCOVERY_MODE <POST_BUILD|PRE_TEST>]
)
```

* `TEST_SPEC arg1...`

Specifies test cases, wildcarded test cases, tags and tag expressions to
pass to the Catch executable alongside the `--list-test-names-only` flag.


* `EXTRA_ARGS arg1...`

Any extra arguments to pass on the command line to each test case.


* `WORKING_DIRECTORY dir`

Specifies the directory in which to run the discovered test cases.  If this
option is not provided, the current binary directory is used.


* `TEST_PREFIX prefix`

Specifies a _prefix_ to be added to the name of each discovered test case.
This can be useful when the same test executable is being used in multiple
calls to `catch_discover_tests()`, with different `TEST_SPEC` or `EXTRA_ARGS`.


* `TEST_SUFFIX suffix`

Same as `TEST_PREFIX`, except it specific the _suffix_ for the test names.
Both `TEST_PREFIX` and `TEST_SUFFIX` can be specified at the same time.


* `PROPERTIES name1 value1...`

Specifies additional properties to be set on all tests discovered by this
invocation of `catch_discover_tests`.


* `TEST_LIST var`

Make the list of tests available in the variable `var`, rather than the
default `<target>_TESTS`.  This can be useful when the same test
executable is being used in multiple calls to `catch_discover_tests()`.
Note that this variable is only available in CTest.

* `REPORTER reporter`

Use the specified reporter when running the test case. The reporter will
be passed to the test runner as `--reporter reporter`.

* `OUTPUT_DIR dir`

If specified, the parameter is passed along as
`--out dir/<test_name>` to test executable. The actual file name is the
same as the test name. This should be used instead of
`EXTRA_ARGS --out foo` to avoid race conditions writing the result output
when using parallel test execution.

* `OUTPUT_PREFIX prefix`

May be used in conjunction with `OUTPUT_DIR`.
If specified, `prefix` is added to each output file name, like so
`--out dir/prefix<test_name>`.

* `OUTPUT_SUFFIX suffix`

May be used in conjunction with `OUTPUT_DIR`.
If specified, `suffix` is added to each output file name, like so
`--out dir/<test_name>suffix`. This can be used to add a file extension to
the output file name e.g. ".xml".

* `DISCOVERY_MODE mode`

If specified allows control over when test discovery is performed.
For a value of `POST_BUILD` (default) test discovery is performed at build time.
For a value of `PRE_TEST` test discovery is delayed until just prior to test
execution (useful e.g. in cross-compilation environments).
``DISCOVERY_MODE`` defaults to the value of the
``CMAKE_CATCH_DISCOVER_TESTS_DISCOVERY_MODE`` variable if it is not passed when
calling ``catch_discover_tests``. This provides a mechanism for globally
selecting a preferred test discovery behavior.

### `ParseAndAddCatchTests.cmake`

⚠ This script is [deprecated](https://github.com/catchorg/Catch2/pull/2120)
in Catch2 2.13.4 and superseded by the above approach using `catch_discover_tests`.
See [#2092](https://github.com/catchorg/Catch2/issues/2092) for details.

`ParseAndAddCatchTests` works by parsing all implementation files
associated with the provided target, and registering them via CTest's
`add_test`. This approach has some limitations, such as the fact that
commented-out tests will be registered anyway. More serious, only a
subset of the assertion macros currently available in Catch can be
detected by this script and tests with any macros that cannot be
parsed are *silently ignored*.


#### Usage

```cmake
cmake_minimum_required(VERSION 3.5)

project(baz LANGUAGES CXX VERSION 0.0.1)

find_package(Catch2 REQUIRED)
add_executable(tests test.cpp)
target_link_libraries(tests PRIVATE Catch2::Catch2)

include(CTest)
include(ParseAndAddCatchTests)
ParseAndAddCatchTests(tests)
```


#### Customization

`ParseAndAddCatchTests` provides some customization points:
* `PARSE_CATCH_TESTS_VERBOSE` -- When `ON`, the script prints debug
messages. Defaults to `OFF`.
* `PARSE_CATCH_TESTS_NO_HIDDEN_TESTS` -- When `ON`, hidden tests (tests
tagged with either of `[.]` or `[.foo]`) will not be registered.
Defaults to `OFF`.
* `PARSE_CATCH_TESTS_ADD_FIXTURE_IN_TEST_NAME` -- When `ON`, adds fixture
class name to the test name in CTest. Defaults to `ON`.
* `PARSE_CATCH_TESTS_ADD_TARGET_IN_TEST_NAME` -- When `ON`, adds target
name to the test name in CTest. Defaults to `ON`.
* `PARSE_CATCH_TESTS_ADD_TO_CONFIGURE_DEPENDS` -- When `ON`, adds test
file to `CMAKE_CONFIGURE_DEPENDS`. This means that the CMake configuration
step will be re-ran when the test files change, letting new tests be
automatically discovered. Defaults to `OFF`.


Optionally, one can specify a launching command to run tests by setting the
variable `OptionalCatchTestLauncher` before calling `ParseAndAddCatchTests`. For
instance to run some tests using `MPI` and other sequentially, one can write
```cmake
set(OptionalCatchTestLauncher ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} ${NUMPROC})
ParseAndAddCatchTests(mpi_foo)
unset(OptionalCatchTestLauncher)
ParseAndAddCatchTests(bar)
```


### `CatchShardTests.cmake`

> `CatchShardTests.cmake` was introduced in Catch2 3.1.0.

`CatchShardTests.cmake` provides a function
`catch_add_sharded_tests(TEST_BINARY)` that splits tests from `TEST_BINARY`
into multiple shards. The tests in each shard and their order is randomized,
and the seed changes every invocation of CTest.

Currently there are 3 customization points for this script:

 * SHARD_COUNT - number of shards to split target's tests into
 * REPORTER    - reporter spec to use for tests
 * TEST_SPEC   - test spec used for filtering tests

Example usage:

```
include(CatchShardTests)

catch_add_sharded_tests(foo-tests
  SHARD_COUNT 4
  REPORTER "xml::out=-"
  TEST_SPEC "A"
)

catch_add_sharded_tests(tests
  SHARD_COUNT 8
  REPORTER "xml::out=-"
  TEST_SPEC "B"
)
```

This registers total of 12 CTest tests (4 + 8 shards) to run shards
from `foo-tests` test binary, filtered by a test spec.

_Note that this script is currently a proof-of-concept for reseeding
shards per CTest run, and thus does not support (nor does it currently
aim to support) all customization points from
[`catch_discover_tests`](#catch_discover_tests)._


## CMake project options

Catch2's CMake project also provides some options for other projects
that consume it. These are:

* `BUILD_TESTING` -- When `ON` and the project is not used as a subproject,
Catch2's test binary will be built. Defaults to `ON`.
* `CATCH_INSTALL_DOCS` -- When `ON`, Catch2's documentation will be
included in the installation. Defaults to `ON`.
* `CATCH_INSTALL_EXTRAS` -- When `ON`, Catch2's extras folder (the CMake
scripts mentioned above, debugger helpers) will be included in the
installation. Defaults to `ON`.
* `CATCH_DEVELOPMENT_BUILD` -- When `ON`, configures the build for development
of Catch2. This means enabling test projects, warnings and so on.
Defaults to `OFF`.


Enabling `CATCH_DEVELOPMENT_BUILD` also enables further configuration
customization options:

* `CATCH_BUILD_TESTING` -- When `ON`, Catch2's SelfTest project will be
built. Defaults to `ON`. Note that Catch2 also obeys `BUILD_TESTING` CMake
variable, so _both_ of them need to be `ON` for the SelfTest to be built,
and either of them can be set to `OFF` to disable building SelfTest.
* `CATCH_BUILD_EXAMPLES` -- When `ON`, Catch2's usage examples will be
built. Defaults to `OFF`.
* `CATCH_BUILD_EXTRA_TESTS` -- When `ON`, Catch2's extra tests will be
built. Defaults to `OFF`.
* `CATCH_BUILD_FUZZERS` -- When `ON`, Catch2 fuzzing entry points will
be built. Defaults to `OFF`.
* `CATCH_ENABLE_WERROR` -- When `ON`, adds `-Werror` or equivalent flag
to the compilation. Defaults to `ON`.
* `CATCH_BUILD_SURROGATES` -- When `ON`, each header in Catch2 will be
compiled separately to ensure that they are self-sufficient.
Defaults to `OFF`.


## `CATCH_CONFIG_*` customization options in CMake

> CMake support for `CATCH_CONFIG_*` options was introduced in Catch2 3.0.1

Due to the new separate compilation model, all the options from the
[Compile-time configuration docs](configuration.md#top) can also be set
through Catch2's CMake. To set them, define the option you want as `ON`,
e.g. `-DCATCH_CONFIG_NOSTDOUT=ON`.

Note that setting the option to `OFF` doesn't disable it. To force disable
an option, you need to set the `_NO_` form of it to `ON`, e.g.
`-DCATCH_CONFIG_NO_COLOUR_WIN32=ON`.


To summarize the configuration option behaviour with an example:

| `-DCATCH_CONFIG_COLOUR_WIN32` | `-DCATCH_CONFIG_NO_COLOUR_WIN32` |      Result |
|-------------------------------|----------------------------------|-------------|
|                          `ON` |                             `ON` |       error |
|                          `ON` |                            `OFF` |    force-on |
|                         `OFF` |                             `ON` |   force-off |
|                         `OFF` |                            `OFF` | auto-detect |



## Installing Catch2 from git repository

If you cannot install Catch2 from a package manager (e.g. Ubuntu 16.04
provides catch only in version 1.2.0) you might want to install it from
the repository instead. Assuming you have enough rights, you can just
install it to the default location, like so:
```
$ git clone https://github.com/catchorg/Catch2.git
$ cd Catch2
$ cmake -B build -S . -DBUILD_TESTING=OFF
$ sudo cmake --build build/ --target install
```

If you do not have superuser rights, you will also need to specify
[CMAKE_INSTALL_PREFIX](https://cmake.org/cmake/help/latest/variable/CMAKE_INSTALL_PREFIX.html)
when configuring the build, and then modify your calls to
[find_package](https://cmake.org/cmake/help/latest/command/find_package.html)
accordingly.

## Installing Catch2 from vcpkg

Alternatively, you can build and install Catch2 using [vcpkg](https://github.com/microsoft/vcpkg/) dependency manager:
```
git clone https://github.com/Microsoft/vcpkg.git
cd vcpkg
./bootstrap-vcpkg.sh
./vcpkg integrate install
./vcpkg install catch2
```

The catch2 port in vcpkg is kept up to date by microsoft team members and community contributors.
If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.

## Installing Catch2 from Bazel

Catch2 is now a supported module in the Bazel Central Registry. You only need to add one line to your MODULE.bazel file;
please see https://registry.bazel.build/modules/catch2 for the latest supported version.

You can then add `catch2_main` to each of your C++ test build rules as follows:

```
cc_test(
    name = "example_test",
    srcs = ["example_test.cpp"],
    deps = [
        ":example",
        "@catch2//:catch2_main",
    ],
)
```

---

[Home](Readme.md#top)



docs/command-line.md
--------------------------------------
<a id="top"></a>
# Command line

**Contents**<br>
[Specifying which tests to run](#specifying-which-tests-to-run)<br>
[Choosing a reporter to use](#choosing-a-reporter-to-use)<br>
[Breaking into the debugger](#breaking-into-the-debugger)<br>
[Showing results for successful tests](#showing-results-for-successful-tests)<br>
[Aborting after a certain number of failures](#aborting-after-a-certain-number-of-failures)<br>
[Listing available tests, tags or reporters](#listing-available-tests-tags-or-reporters)<br>
[Sending output to a file](#sending-output-to-a-file)<br>
[Naming a test run](#naming-a-test-run)<br>
[Eliding assertions expected to throw](#eliding-assertions-expected-to-throw)<br>
[Make whitespace visible](#make-whitespace-visible)<br>
[Warnings](#warnings)<br>
[Reporting timings](#reporting-timings)<br>
[Load test names to run from a file](#load-test-names-to-run-from-a-file)<br>
[Specify the order test cases are run](#specify-the-order-test-cases-are-run)<br>
[Specify a seed for the Random Number Generator](#specify-a-seed-for-the-random-number-generator)<br>
[Identify framework and version according to the libIdentify standard](#identify-framework-and-version-according-to-the-libidentify-standard)<br>
[Wait for key before continuing](#wait-for-key-before-continuing)<br>
[Skip all benchmarks](#skip-all-benchmarks)<br>
[Specify the number of benchmark samples to collect](#specify-the-number-of-benchmark-samples-to-collect)<br>
[Specify the number of resamples for bootstrapping](#specify-the-number-of-resamples-for-bootstrapping)<br>
[Specify the confidence-interval for bootstrapping](#specify-the-confidence-interval-for-bootstrapping)<br>
[Disable statistical analysis of collected benchmark samples](#disable-statistical-analysis-of-collected-benchmark-samples)<br>
[Specify the amount of time in milliseconds spent on warming up each test](#specify-the-amount-of-time-in-milliseconds-spent-on-warming-up-each-test)<br>
[Usage](#usage)<br>
[Specify the section to run](#specify-the-section-to-run)<br>
[Filenames as tags](#filenames-as-tags)<br>
[Override output colouring](#override-output-colouring)<br>
[Test Sharding](#test-sharding)<br>
[Allow running the binary without tests](#allow-running-the-binary-without-tests)<br>
[Output verbosity](#output-verbosity)<br>

Catch works quite nicely without any command line options at all - but for those times when you want greater control the following options are available.
Click one of the following links to take you straight to that option - or scroll on to browse the available options.

<a href="#specifying-which-tests-to-run">               `    <test-spec> ...`</a><br />
<a href="#usage">                                       `    -h, -?, --help`</a><br />
<a href="#showing-results-for-successful-tests">        `    -s, --success`</a><br />
<a href="#breaking-into-the-debugger">                  `    -b, --break`</a><br />
<a href="#eliding-assertions-expected-to-throw">        `    -e, --nothrow`</a><br />
<a href="#invisibles">                                  `    -i, --invisibles`</a><br />
<a href="#sending-output-to-a-file">                    `    -o, --out`</a><br />
<a href="#choosing-a-reporter-to-use">                  `    -r, --reporter`</a><br />
<a href="#naming-a-test-run">                           `    -n, --name`</a><br />
<a href="#aborting-after-a-certain-number-of-failures"> `    -a, --abort`</a><br />
<a href="#aborting-after-a-certain-number-of-failures"> `    -x, --abortx`</a><br />
<a href="#warnings">                                    `    -w, --warn`</a><br />
<a href="#reporting-timings">                           `    -d, --durations`</a><br />
<a href="#input-file">                                  `    -f, --input-file`</a><br />
<a href="#run-section">                                 `    -c, --section`</a><br />
<a href="#filenames-as-tags">                           `    -#, --filenames-as-tags`</a><br />


</br>

<a href="#listing-available-tests-tags-or-reporters">   `    --list-tests`</a><br />
<a href="#listing-available-tests-tags-or-reporters">   `    --list-tags`</a><br />
<a href="#listing-available-tests-tags-or-reporters">   `    --list-reporters`</a><br />
<a href="#listing-available-tests-tags-or-reporters">   `    --list-listeners`</a><br />
<a href="#order">                                       `    --order`</a><br />
<a href="#rng-seed">                                    `    --rng-seed`</a><br />
<a href="#libidentify">                                 `    --libidentify`</a><br />
<a href="#wait-for-keypress">                           `    --wait-for-keypress`</a><br />
<a href="#skip-benchmarks">                             `    --skip-benchmarks`</a><br />
<a href="#benchmark-samples">                           `    --benchmark-samples`</a><br />
<a href="#benchmark-resamples">                         `    --benchmark-resamples`</a><br />
<a href="#benchmark-confidence-interval">               `    --benchmark-confidence-interval`</a><br />
<a href="#benchmark-no-analysis">                       `    --benchmark-no-analysis`</a><br />
<a href="#benchmark-warmup-time">                       `    --benchmark-warmup-time`</a><br />
<a href="#colour-mode">                                 `    --colour-mode`</a><br />
<a href="#test-sharding">                               `    --shard-count`</a><br />
<a href="#test-sharding">                               `    --shard-index`</a><br />
<a href=#no-tests-override>                             `    --allow-running-no-tests`</a><br />
<a href=#output-verbosity>                              `    --verbosity`</a><br />

</br>



<a id="specifying-which-tests-to-run"></a>
## Specifying which tests to run

<pre>&lt;test-spec> ...</pre>

By providing a test spec, you filter which tests will be run. If you call
Catch2 without any test spec, then it will run all non-hidden test
cases. A test case is hidden if it has the `[!benchmark]` tag, any tag
with a dot at the start, e.g. `[.]` or `[.foo]`.

There are three basic test specs that can then be combined into more
complex specs:

  * Full test name, e.g. `"Test 1"`.

    This allows only test cases whose name is "Test 1".

  * Wildcarded test name, e.g. `"*Test"`, or `"Test*"`, or `"*Test*"`.

    This allows any test case whose name ends with, starts with, or contains
    in the middle the string "Test". Note that the wildcard can only be at
    the start or end.

  * Tag name, e.g. `[some-tag]`.

    This allows any test case tagged with "[some-tag]". Remember that some
    tags are special, e.g. those that start with "." or with "!".


You can also combine the basic test specs to create more complex test
specs. You can:

  * Concatenate specs to apply all of them, e.g. `[some-tag][other-tag]`.

    This allows test cases that are tagged with **both** "[some-tag]" **and**
    "[other-tag]". A test case with just "[some-tag]" will not pass the filter,
    nor will test case with just "[other-tag]".

  * Comma-join specs to apply any of them, e.g. `[some-tag],[other-tag]`.

    This allows test cases that are tagged with **either** "[some-tag]" **or**
    "[other-tag]". A test case with both will obviously also pass the filter.

    Note that commas take precendence over simple concatenation. This means
    that `[a][b],[c]` accepts tests that are tagged with either both "[a]" and
    "[b]", or tests that are tagged with just "[c]".

  * Negate the spec by prepending it with `~`, e.g. `~[some-tag]`.

    This rejects any test case that is tagged with "[some-tag]". Note that
    rejection takes precedence over other filters.

    Note that negations always binds to the following _basic_ test spec.
    This means that `~[foo][bar]` negates only the "[foo]" tag and not the
    "[bar]" tag.

Note that when Catch2 is deciding whether to include a test, first it
checks whether the test matches any negative filters. If it does,
the test is rejected. After that, the behaviour depends on whether there
are positive filters as well. If there are no positive filters, all
remaining non-hidden tests are included. If there are positive filters,
only tests that match the positive filters are included.

You can also match test names with special characters by escaping them
with a backslash (`"\"`), e.g. a test named `"Do A, then B"` is matched
by `"Do A\, then B"` test spec. Backslash also escapes itself.


### Examples

Given these TEST_CASEs,
```
TEST_CASE("Test 1") {}

TEST_CASE("Test 2", "[.foo]") {}

TEST_CASE("Test 3", "[.bar]") {}

TEST_CASE("Test 4", "[.][foo][bar]") {}
```

this is the result of these filters
```
./tests                      # Selects only the first test, others are hidden
./tests "Test 1"             # Selects only the first test, other do not match
./tests ~"Test 1"            # Selects no tests. Test 1 is rejected, other tests are hidden
./tests "Test *"             # Selects all tests.
./tests [bar]                # Selects tests 3 and 4. Other tests are not tagged [bar]
./tests ~[foo]               # Selects test 1, because it is the only non-hidden test without [foo] tag
./tests [foo][bar]           # Selects test 4.
./tests [foo],[bar]          # Selects tests 2, 3, 4.
./tests ~[foo][bar]          # Selects test 3. 2 and 4 are rejected due to having [foo] tag
./tests ~"Test 2"[foo]       # Selects test 4, because test 2 is explicitly rejected
./tests [foo][bar],"Test 1"  # Selects tests 1 and 4.
./tests "Test 1*"            # Selects test 1, wildcard can match zero characters
```

_Note: Using plain asterisk on a command line can cause issues with shell
expansion. Make sure that the asterisk is passed to Catch2 and is not
interpreted by the shell._


<a id="choosing-a-reporter-to-use"></a>
## Choosing a reporter to use

<pre>-r, --reporter &lt;reporter[::key=value]*&gt;</pre>

Reporters are how the output from Catch2 (results of assertions, tests,
benchmarks and so on) is formatted and written out. The default reporter
is called the "Console" reporter and is intended to provide relatively
verbose and human-friendly output.

Reporters are also individually configurable. To pass configuration options
to the reporter, you append `::key=value` to the reporter specification
as many times as you want, e.g. `--reporter xml::out=someFile.xml` or
`--reporter custom::colour-mode=ansi::Xoption=2`.

The keys must either be prefixed by "X", in which case they are not parsed
by Catch2 and are only passed down to the reporter, or one of options
hardcoded into Catch2. Currently there are only 2,
["out"](#sending-output-to-a-file), and ["colour-mode"](#colour-mode).

_Note that the reporter might still check the X-prefixed options for
validity, and throw an error if they are wrong._

> Support for passing arguments to reporters through the `-r`, `--reporter` flag was introduced in Catch2 3.0.1

There are multiple built-in reporters, you can see what they do by using the
[`--list-reporters`](command-line.md#listing-available-tests-tags-or-reporters)
flag. If you need a reporter providing custom format outside of the already
provided ones, look at the ["write your own reporter" part of the reporter
documentation](reporters.md#writing-your-own-reporter).

This option may be passed multiple times to use multiple (different)
reporters  at the same time. See the [reporter documentation](reporters.md#multiple-reporters)
for details on what the resulting behaviour is. Also note that at most one
reporter can be provided without the output-file part of reporter spec.
This reporter will use the "default" output destination, based on
the [`-o`, `--out`](#sending-output-to-a-file) option.

> Support for using multiple different reporters at the same time was [introduced](https://github.com/catchorg/Catch2/pull/2183) in Catch2 3.0.1


_Note: There is currently no way to escape `::` in the reporter spec,
and thus the reporter names, or configuration keys and values, cannot
contain `::`. As `::` in paths is relatively obscure (unlike ':'), we do
not consider this an issue._


<a id="breaking-into-the-debugger"></a>
## Breaking into the debugger
<pre>-b, --break</pre>

Under most debuggers Catch2 is capable of automatically breaking on a test
failure. This allows the user to see the current state of the test during
failure.

<a id="showing-results-for-successful-tests"></a>
## Showing results for successful tests
<pre>-s, --success</pre>

Usually you only want to see reporting for failed tests. Sometimes it's useful to see *all* the output (especially when you don't trust that that test you just added worked first time!).
To see successful, as well as failing, test results just pass this option. Note that each reporter may treat this option differently. The Junit reporter, for example, logs all results regardless.

<a id="aborting-after-a-certain-number-of-failures"></a>
## Aborting after a certain number of failures
<pre>-a, --abort
-x, --abortx [&lt;failure threshold>]
</pre>

If a ```REQUIRE``` assertion fails the test case aborts, but subsequent test cases are still run.
If a ```CHECK``` assertion fails even the current test case is not aborted.

Sometimes this results in a flood of failure messages and you'd rather just see the first few. Specifying ```-a``` or ```--abort``` on its own will abort the whole test run on the first failed assertion of any kind. Use ```-x``` or ```--abortx``` followed by a number to abort after that number of assertion failures.

<a id="listing-available-tests-tags-or-reporters"></a>
## Listing available tests, tags or reporters
```
--list-tests
--list-tags
--list-reporters
--list-listeners
```

> The `--list*` options became customizable through reporters in Catch2 3.0.1

> The `--list-listeners` option was added in Catch2 3.0.1

`--list-tests` lists all registered tests matching specified test spec.
Usually this listing also includes tags, and potentially also other
information, like source location, based on verbosity and reporter's design.

`--list-tags` lists all tags from registered tests matching specified test
spec. Usually this also includes number of tests cases they match and
similar information.

`--list-reporters` lists all available reporters and their descriptions.

`--list-listeners` lists all registered listeners and their descriptions.

The [`--verbosity` argument](#output-verbosity) modifies the level of detail provided by the default `--list*` options
as follows:

| Option             | `normal` (default)              | `quiet`             | `high`                                  |
|--------------------|---------------------------------|---------------------|-----------------------------------------|
| `--list-tests`     | Test names and tags             | Test names only     | Same as `normal`, plus source code line |
| `--list-tags`      | Tags and counts                 | Same as `normal`    | Same as `normal`                        |
| `--list-reporters` | Reporter names and descriptions | Reporter names only | Same as `normal`                        |
| `--list-listeners` | Listener names and descriptions | Same as `normal`    | Same as `normal`                        |

<a id="sending-output-to-a-file"></a>
## Sending output to a file
<pre>-o, --out &lt;filename&gt;
</pre>

Use this option to send all output to a file, instead of stdout. You can
use `-` as the filename to explicitly send the output to stdout (this is
useful e.g. when using multiple reporters).

> Support for `-` as the filename was introduced in Catch2 3.0.1

Filenames starting with "%" (percent symbol) are reserved by Catch2 for
meta purposes, e.g. using `%debug` as the filename opens stream that
writes to platform specific debugging/logging mechanism.

Catch2 currently recognizes 3 meta streams:

* `%debug` - writes to platform specific debugging/logging output
* `%stdout` - writes to stdout
* `%stderr` - writes to stderr

> Support for `%stdout` and `%stderr` was introduced in Catch2 3.0.1


<a id="naming-a-test-run"></a>
## Naming a test run
<pre>-n, --name &lt;name for test run></pre>

If a name is supplied it will be used by the reporter to provide an overall name for the test run. This can be useful if you are sending to a file, for example, and need to distinguish different test runs - either from different Catch executables or runs of the same executable with different options. If not supplied the name is defaulted to the name of the executable.

<a id="eliding-assertions-expected-to-throw"></a>
## Eliding assertions expected to throw
<pre>-e, --nothrow</pre>

Skips all assertions that test that an exception is thrown, e.g. ```REQUIRE_THROWS```.

These can be a nuisance in certain debugging environments that may break when exceptions are thrown (while this is usually optional for handled exceptions, it can be useful to have enabled if you are trying to track down something unexpected).

Sometimes exceptions are expected outside of one of the assertions that tests for them (perhaps thrown and caught within the code-under-test). The whole test case can be skipped when using ```-e``` by marking it with the ```[!throws]``` tag.

When running with this option any throw checking assertions are skipped so as not to contribute additional noise. Be careful if this affects the behaviour of subsequent tests.

<a id="invisibles"></a>
## Make whitespace visible
<pre>-i, --invisibles</pre>

If a string comparison fails due to differences in whitespace - especially leading or trailing whitespace - it can be hard to see what's going on.
This option transforms tabs and newline characters into ```\t``` and ```\n``` respectively when printing.

<a id="warnings"></a>
## Warnings
<pre>-w, --warn &lt;warning name></pre>

You can think of Catch2's warnings as the equivalent of `-Werror` (`/WX`)
flag for C++ compilers. It turns some suspicious occurrences, like a section
without assertions, into errors. Because these might be intended, warnings
are not enabled by default, but user can opt in.

You can enable multiple warnings at the same time.

There are currently two warnings implemented:

```
    NoAssertions        // Fail test case / leaf section if no assertions
                        // (e.g. `REQUIRE`) is encountered.
    UnmatchedTestSpec   // Fail test run if any of the CLI test specs did
                        // not match any tests.
```

> `UnmatchedTestSpec` was introduced in Catch2 3.0.1.


<a id="reporting-timings"></a>
## Reporting timings
<pre>-d, --durations &lt;yes/no></pre>

When set to ```yes``` Catch will report the duration of each test case, in seconds with millisecond precision. Note that it does this regardless of whether a test case passes or fails. Note, also, the certain reporters (e.g. Junit) always report test case durations regardless of this option being set or not.

<pre>-D, --min-duration &lt;value></pre>

> `--min-duration` was [introduced](https://github.com/catchorg/Catch2/pull/1910) in Catch2 2.13.0

When set, Catch will report the duration of each test case that took more
than &lt;value> seconds, in seconds with millisecond precision. This option is overridden by both
`-d yes` and `-d no`, so that either all durations are reported, or none
are.


<a id="input-file"></a>
## Load test names to run from a file
<pre>-f, --input-file &lt;filename></pre>

Provide the name of a file that contains a list of test case names,
one per line. Blank lines are skipped.

A useful way to generate an initial instance of this file is to combine
the [`--list-tests`](#listing-available-tests-tags-or-reporters) flag with
the [`--verbosity quiet`](#output-verbosity) option. You can also
use test specs to filter this list down to what you want first.


<a id="order"></a>
## Specify the order test cases are run
<pre>--order &lt;decl|lex|rand&gt;</pre>

Test cases are ordered one of three ways:

### decl
Declaration order (this is the default order if no --order argument is provided).
Tests in the same translation unit are sorted using their declaration orders,
different TUs are sorted in an implementation (linking) dependent order.


### lex
Lexicographic order. Tests are sorted by their name, their tags are ignored.


### rand

Randomly ordered. The order is dependent on Catch2's random seed (see
[`--rng-seed`](#rng-seed)), and is subset invariant. What this means
is that as long as the random seed is fixed, running only some tests
(e.g. via tag) does not change their relative order.

> The subset stability was introduced in Catch2 v2.12.0

Since the random order was made subset stable, we promise that given
the same random seed, the order of test cases will be the same across
different platforms, as long as the tests were compiled against identical
version of Catch2. We reserve the right to change the relative order
of tests cases between Catch2 versions, but it is unlikely to happen often.


<a id="rng-seed"></a>
## Specify a seed for the Random Number Generator
<pre>--rng-seed &lt;'time'|'random-device'|number&gt;</pre>

Sets the seed for random number generators used by Catch2. These are used
e.g. to shuffle tests when user asks for tests to be in random order.

Using `time` as the argument asks Catch2 generate the seed through call
to `std::time(nullptr)`. This provides very weak randomness and multiple
runs of the binary can generate the same seed if they are started close
to each other.

Using `random-device` asks for `std::random_device` to be used instead.
If your implementation provides working `std::random_device`, it should
be preferred to using `time`. Catch2 uses `std::random_device` by default.


<a id="libidentify"></a>
## Identify framework and version according to the libIdentify standard
<pre>--libidentify</pre>

See [The LibIdentify repo for more information and examples](https://github.com/janwilmans/LibIdentify).

<a id="wait-for-keypress"></a>
## Wait for key before continuing
<pre>--wait-for-keypress &lt;never|start|exit|both&gt;</pre>

Will cause the executable to print a message and wait until the return/ enter key is pressed before continuing -
either before running any tests, after running all tests - or both, depending on the argument.

<a id="skip-benchmarks"></a>
## Skip all benchmarks
<pre>--skip-benchmarks</pre>

> [Introduced](https://github.com/catchorg/Catch2/issues/2408) in Catch2 3.0.1.

This flag tells Catch2 to skip running all benchmarks. Benchmarks in this
case mean code blocks in `BENCHMARK` and `BENCHMARK_ADVANCED` macros, not
test cases with the `[!benchmark]` tag.

<a id="benchmark-samples"></a>
## Specify the number of benchmark samples to collect
<pre>--benchmark-samples &lt;# of samples&gt;</pre>

> [Introduced](https://github.com/catchorg/Catch2/issues/1616) in Catch2 2.9.0.

When running benchmarks a number of "samples" is collected. This is the base data for later statistical analysis.
Per sample a clock resolution dependent number of iterations of the user code is run, which is independent of the number of samples. Defaults to 100.

<a id="benchmark-resamples"></a>
## Specify the number of resamples for bootstrapping
<pre>--benchmark-resamples &lt;# of resamples&gt;</pre>

> [Introduced](https://github.com/catchorg/Catch2/issues/1616) in Catch2 2.9.0.

After the measurements are performed, statistical [bootstrapping] is performed
on the samples. The number of resamples for that bootstrapping is configurable
but defaults to 100000. Due to the bootstrapping it is possible to give
estimates for the mean and standard deviation. The estimates come with a lower
bound and an upper bound, and the confidence interval (which is configurable but
defaults to 95%).

 [bootstrapping]: http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29

<a id="benchmark-confidence-interval"></a>
## Specify the confidence-interval for bootstrapping
<pre>--benchmark-confidence-interval &lt;confidence-interval&gt;</pre>

> [Introduced](https://github.com/catchorg/Catch2/issues/1616) in Catch2 2.9.0.

The confidence-interval is used for statistical bootstrapping on the samples to
calculate the upper and lower bounds of mean and standard deviation.
Must be between 0 and 1 and defaults to 0.95.

<a id="benchmark-no-analysis"></a>
## Disable statistical analysis of collected benchmark samples
<pre>--benchmark-no-analysis</pre>

> [Introduced](https://github.com/catchorg/Catch2/issues/1616) in Catch2 2.9.0.

When this flag is specified no bootstrapping or any other statistical analysis is performed.
Instead the user code is only measured and the plain mean from the samples is reported.

<a id="benchmark-warmup-time"></a>
## Specify the amount of time in milliseconds spent on warming up each test
<pre>--benchmark-warmup-time</pre>

> [Introduced](https://github.com/catchorg/Catch2/pull/1844) in Catch2 2.11.2.

Configure the amount of time spent warming up each test.

<a id="usage"></a>
## Usage
<pre>-h, -?, --help</pre>

Prints the command line arguments to stdout


<a id="run-section"></a>
## Specify the section to run
<pre>-c, --section &lt;section name&gt;</pre>

To limit execution to a specific section within a test case, use this option one or more times.
To narrow to sub-sections use multiple instances, where each subsequent instance specifies a deeper nesting level.

E.g. if you have:

<pre>
TEST_CASE( "Test" ) {
  SECTION( "sa" ) {
    SECTION( "sb" ) {
      /*...*/
    }
    SECTION( "sc" ) {
      /*...*/
    }
  }
  SECTION( "sd" ) {
    /*...*/
  }
}
</pre>

Then you can run `sb` with:
<pre>./MyExe Test -c sa -c sb</pre>

Or run just `sd` with:
<pre>./MyExe Test -c sd</pre>

To run all of `sa`, including `sb` and `sc` use:
<pre>./MyExe Test -c sa</pre>

There are some limitations of this feature to be aware of:
- Code outside of sections being skipped will still be executed - e.g. any set-up code in the TEST_CASE before the
start of the first section.</br>
- At time of writing, wildcards are not supported in section names.
- If you specify a section without narrowing to a test case first then all test cases will be executed
(but only matching sections within them).


<a id="filenames-as-tags"></a>
## Filenames as tags
<pre>-#, --filenames-as-tags</pre>

This option adds an extra tag to all test cases. The tag is `#` followed
by the unqualified filename the test case is defined in, with the _last_
extension stripped out.

For example, tests within the file `tests\SelfTest\UsageTests\BDD.tests.cpp`
will be given the `[#BDD.tests]` tag.


<a id="colour-mode"></a>
## Override output colouring
<pre>--colour-mode &lt;ansi|win32|none|default&gt;</pre>

> The `--colour-mode` option replaced the old `--colour` option in Catch2 3.0.1


Catch2 support two different ways of colouring terminal output, and by
default it attempts to make a good guess on which implementation to use
(and whether to even use it, e.g. Catch2 tries to avoid writing colour
codes when writing the results into a file).

`--colour-mode` allows the user to explicitly select what happens.

* `--colour-mode ansi` tells Catch2 to always use ANSI colour codes, even
when writing to a file
* `--colour-mode win32` tells Catch2 to use colour implementation based
  on Win32 terminal API
* `--colour-mode none` tells Catch2 to disable colours completely
* `--colour-mode default` lets Catch2 decide

`--colour-mode default` is the default setting.


<a id="test-sharding"></a>
## Test Sharding
<pre>--shard-count <#number of shards>, --shard-index <#shard index to run></pre>

> [Introduced](https://github.com/catchorg/Catch2/pull/2257) in Catch2 3.0.1.

When `--shard-count <#number of shards>` is used, the tests to execute
will be split evenly in to the given number of sets, identified by indices
starting at 0. The tests in the set given by
`--shard-index <#shard index to run>` will be executed. The default shard
count is `1`, and the default index to run is `0`.

_Shard index must be less than number of shards. As the name suggests,
it is treated as an index of the shard to run._

Sharding is useful when you want to split test execution across multiple
processes, as is done with the [Bazel test sharding](https://docs.bazel.build/versions/main/test-encyclopedia.html#test-sharding).


<a id="no-tests-override"></a>
## Allow running the binary without tests
<pre>--allow-running-no-tests</pre>

> Introduced in Catch2 3.0.1.

By default, Catch2 test binaries return non-0 exit code if no tests were run,
e.g. if the binary was compiled with no tests, the provided test spec matched no
tests, or all tests [were skipped at runtime](skipping-passing-failing.md#top). This flag
overrides that, so a test run with no tests still returns 0.

## Output verbosity
```
-v, --verbosity <quiet|normal|high>
```

Changing verbosity might change how many details Catch2's reporters output.
However, you should consider changing the verbosity level as a _suggestion_.
Not all reporters support all verbosity levels, e.g. because the reporter's
format cannot meaningfully change. In that case, the verbosity level is
ignored.

Verbosity defaults to _normal_.


---

[Home](Readme.md#top)



docs/commercial-users.md
--------------------------------------
<a id="top"></a>
# Commercial users of Catch2

Catch2 is also widely used in proprietary code bases. This page contains
some of them that are willing to share this information.

If you want to add your organisation, please check that there is no issue
with you sharing this fact.

 - Bloomberg
 - [Bloomlife](https://bloomlife.com)
 - [Inscopix Inc.](https://www.inscopix.com/)
 - Locksley.CZ
 - [Makimo](https://makimo.pl/)
 - NASA
 - [Nexus Software Systems](https://nexwebsites.com)
 - [UX3D](https://ux3d.io)
 - [King](https://king.com)


---

[Home](Readme.md#top)



docs/comparing-floating-point-numbers.md
--------------------------------------
<a id="top"></a>
# Comparing floating point numbers with Catch2

If you are not deeply familiar with them, floating point numbers can be
unintuitive. This also applies to comparing floating point numbers for
(in)equality.

This page assumes that you have some understanding of both FP, and the
meaning of different kinds of comparisons, and only goes over what
functionality Catch2 provides to help you with comparing floating point
numbers. If you do not have this understanding, we recommend that you first
study up on floating point numbers and their comparisons, e.g. by [reading
this blog post](https://codingnest.com/the-little-things-comparing-floating-point-numbers/).


## Floating point matchers

```
#include <catch2/matchers/catch_matchers_floating_point.hpp>
```

[Matchers](matchers.md#top) are the preferred way of comparing floating
point numbers in Catch2. We provide 3 of them:

* `WithinAbs(double target, double margin)`,
* `WithinRel(FloatingPoint target, FloatingPoint eps)`, and
* `WithinULP(FloatingPoint target, uint64_t maxUlpDiff)`.

> `WithinRel` matcher was introduced in Catch2 2.10.0

As with all matchers, you can combine multiple floating point matchers
in a single assertion. For example, to check that some computation matches
a known good value within 0.1% or is close enough (no different to 5
decimal places) to zero, we would write this assertion:

```cpp
    REQUIRE_THAT( computation(input),
        Catch::Matchers::WithinRel(expected, 0.001)
     || Catch::Matchers::WithinAbs(0, 0.000001) );
```


### WithinAbs

`WithinAbs` creates a matcher that accepts floating point numbers whose
difference with `target` is less-or-equal to the `margin`. Since `float`
can be converted to `double` without losing precision, only `double`
overload exists.

```cpp
REQUIRE_THAT(1.0, WithinAbs(1.2, 0.2));
REQUIRE_THAT(0.f, !WithinAbs(1.0, 0.5));
// Notice that infinity == infinity for WithinAbs
REQUIRE_THAT(INFINITY, WithinAbs(INFINITY, 0));
```


### WithinRel

`WithinRel` creates a matcher that accepts floating point numbers that
are _approximately equal_ to the `target` with a tolerance of `eps.`
Specifically, it matches if
`|arg - target| <= eps * max(|arg|, |target|)` holds. If you do not
specify `eps`, `std::numeric_limits<FloatingPoint>::epsilon * 100`
is used as the default.

```cpp
// Notice that WithinRel comparison is symmetric, unlike Approx's.
REQUIRE_THAT(1.0, WithinRel(1.1, 0.1));
REQUIRE_THAT(1.1, WithinRel(1.0, 0.1));
// Notice that inifnity == infinity for WithinRel
REQUIRE_THAT(INFINITY, WithinRel(INFINITY));
```


### WithinULP

`WithinULP` creates a matcher that accepts floating point numbers that
are no more than `maxUlpDiff`
[ULPs](https://en.wikipedia.org/wiki/Unit_in_the_last_place)
away from the `target` value. The short version of what this means
is that there is no more than `maxUlpDiff - 1` representable floating
point numbers between the argument for matching and the `target` value.

When using the ULP matcher in Catch2, it is important to keep in mind
that Catch2 interprets ULP distance slightly differently than
e.g. `std::nextafter` does.

Catch2's ULP calculation obeys these relations:
  * `ulpDistance(-x, x) == 2 * ulpDistance(x, 0)`
  * `ulpDistance(-0, 0) == 0` (due to the above)
  * `ulpDistance(DBL_MAX, INFINITY) == 1`
  * `ulpDistancE(NaN, x) == infinity`


**Important**: The WithinULP matcher requires the platform to use the
[IEEE-754](https://en.wikipedia.org/wiki/IEEE_754) representation for
floating point numbers.

```cpp
REQUIRE_THAT( -0.f, WithinULP( 0.f, 0 ) );
```


## `Approx`

```
#include <catch2/catch_approx.hpp>
```

**We strongly recommend against using `Approx` when writing new code.**
You should be using floating point matchers instead.

Catch2 provides one more way to handle floating point comparisons. It is
`Approx`, a special type with overloaded comparison operators, that can
be used in standard assertions, e.g.

```cpp
REQUIRE(0.99999 == Catch::Approx(1));
```

`Approx` supports four comparison operators, `==`, `!=`, `<=`, `>=`, and can
also be used with strong typedefs over `double`s. It can be used for both
relative and margin comparisons by using its three customization points.
Note that the semantics of this is always that of an _or_, so if either
the relative or absolute margin comparison passes, then the whole comparison
passes.

The downside to `Approx` is that it has a couple of issues that we cannot
fix without breaking backwards compatibility. Because Catch2 also provides
complete set of matchers that implement different floating point comparison
methods, `Approx` is left as-is, is considered deprecated, and should
not be used in new code.

The issues are
  * All internal computation is done in `double`s, leading to slightly
    different results if the inputs were floats.
  * `Approx`'s relative margin comparison is not symmetric. This means
    that `Approx( 10 ).epsilon(0.1) != 11.1` but `Approx( 11.1 ).epsilon(0.1) == 10`.
  * By default, `Approx` only uses relative margin comparison. This means
    that `Approx(0) == X` only passes for `X == 0`.


### Approx details

If you still want/need to know more about `Approx`, read on.

Catch2 provides a UDL for `Approx`; `_a`. It resides in the `Catch::literals`
namespace, and can be used like this:

```cpp
using namespace Catch::literals;
REQUIRE( performComputation() == 2.1_a );
```

`Approx` has three customization points for the comparison:

* **epsilon** - epsilon sets the coefficient by which a result
can differ from `Approx`'s value before it is rejected.
_Defaults to `std::numeric_limits<float>::epsilon()*100`._

```cpp
Approx target = Approx(100).epsilon(0.01);
100.0 == target; // Obviously true
200.0 == target; // Obviously still false
100.5 == target; // True, because we set target to allow up to 1% difference
```


* **margin** - margin sets the absolute value by which
a result can differ from `Approx`'s value before it is rejected.
_Defaults to `0.0`._

```cpp
Approx target = Approx(100).margin(5);
100.0 == target; // Obviously true
200.0 == target; // Obviously still false
104.0 == target; // True, because we set target to allow absolute difference of at most 5
```

* **scale** - scale is used to change the magnitude of `Approx` for the relative check.
_By default, set to `0.0`._

Scale could be useful if the computation leading to the result worked
on a different scale than is used by the results. Approx's scale is added
to Approx's value when computing the allowed relative margin from the
Approx's value.


---

[Home](Readme.md#top)



docs/configuration.md
--------------------------------------
<a id="top"></a>
# Compile-time configuration

**Contents**<br>
[Prefixing Catch macros](#prefixing-catch-macros)<br>
[Terminal colour](#terminal-colour)<br>
[Console width](#console-width)<br>
[stdout](#stdout)<br>
[Fallback stringifier](#fallback-stringifier)<br>
[Default reporter](#default-reporter)<br>
[Bazel support](#bazel-support)<br>
[C++11 toggles](#c11-toggles)<br>
[C++17 toggles](#c17-toggles)<br>
[Other toggles](#other-toggles)<br>
[Enabling stringification](#enabling-stringification)<br>
[Disabling exceptions](#disabling-exceptions)<br>
[Overriding Catch's debug break (`-b`)](#overriding-catchs-debug-break--b)<br>
[Static analysis support](#static-analysis-support)<br>

Catch2 is designed to "just work" as much as possible, and most of the
configuration options below are changed automatically during compilation,
according to the detected environment. However, this detection can also
be overridden by users, using macros documented below, and/or CMake options
with the same name.


## Prefixing Catch macros

    CATCH_CONFIG_PREFIX_ALL       // Prefix all macros with CATCH_
    CATCH_CONFIG_PREFIX_MESSAGES  // Prefix only INFO, UNSCOPED_INFO, WARN and CAPTURE

To keep test code clean and uncluttered Catch uses short macro names (e.g. ```TEST_CASE``` and ```REQUIRE```). Occasionally these may conflict with identifiers from platform headers or the system under test. In this case the above identifier can be defined. This will cause all the Catch user macros to be prefixed with ```CATCH_``` (e.g. ```CATCH_TEST_CASE``` and ```CATCH_REQUIRE```).


## Terminal colour

    CATCH_CONFIG_COLOUR_WIN32     // Force enables compiling colouring impl based on Win32 console API
    CATCH_CONFIG_NO_COLOUR_WIN32  // Force disables ...

Yes, Catch2 uses the british spelling of colour.

Catch2 attempts to autodetect whether the Win32 console colouring API,
`SetConsoleTextAttribute`, is available, and if it is available it compiles
in a console colouring implementation that uses it.

This option can be used to override Catch2's autodetection and force the
compilation either ON or OFF.


## Console width

    CATCH_CONFIG_CONSOLE_WIDTH = x // where x is a number

Catch formats output intended for the console to fit within a fixed number of characters. This is especially important as indentation is used extensively and uncontrolled line wraps break this.
By default a console width of 80 is assumed but this can be controlled by defining the above identifier to be a different value.

## stdout

    CATCH_CONFIG_NOSTDOUT

To support platforms that do not provide `std::cout`, `std::cerr` and
`std::clog`, Catch does not use them directly, but rather calls
`Catch::cout`, `Catch::cerr` and `Catch::clog`. You can replace their
implementation by defining `CATCH_CONFIG_NOSTDOUT` and implementing
them yourself, their signatures are:

    std::ostream& cout();
    std::ostream& cerr();
    std::ostream& clog();

[You can see an example of replacing these functions here.](
../examples/231-Cfg-OutputStreams.cpp)


## Fallback stringifier

By default, when Catch's stringification machinery has to stringify
a type that does not specialize `StringMaker`, does not overload `operator<<`,
is not an enumeration and is not a range, it uses `"{?}"`. This can be
overridden by defining `CATCH_CONFIG_FALLBACK_STRINGIFIER` to name of a
function that should perform the stringification instead.

All types that do not provide `StringMaker` specialization or `operator<<`
overload will be sent to this function (this includes enums and ranges).
The provided function must return `std::string` and must accept any type,
e.g. via overloading.

_Note that if the provided function does not handle a type and this type
requires to be stringified, the compilation will fail._


## Default reporter

Catch's default reporter can be changed by defining macro
`CATCH_CONFIG_DEFAULT_REPORTER` to string literal naming the desired
default reporter.

This means that defining `CATCH_CONFIG_DEFAULT_REPORTER` to `"console"`
is equivalent with the out-of-the-box experience.


## Bazel support

Compiling Catch2 with `CATCH_CONFIG_BAZEL_SUPPORT` force-enables Catch2's
support for Bazel's environment variables (normally Catch2 looks for
`BAZEL_TEST=1` env var first).

This can be useful if you are using older versions of Bazel, that do not
yet have `BAZEL_TEST` env var support.

> `CATCH_CONFIG_BAZEL_SUPPORT` was [introduced](https://github.com/catchorg/Catch2/pull/2399) in Catch2 3.0.1.

> `CATCH_CONFIG_BAZEL_SUPPORT` was [deprecated](https://github.com/catchorg/Catch2/pull/2459) in Catch2 3.1.0.


## C++11 toggles

    CATCH_CONFIG_CPP11_TO_STRING // Use `std::to_string`

Because we support platforms whose standard library does not contain
`std::to_string`, it is possible to force Catch to use a workaround
based on `std::stringstream`. On platforms other than Android,
the default is to use `std::to_string`. On Android, the default is to
use the `stringstream` workaround. As always, it is possible to override
Catch's selection, by defining either `CATCH_CONFIG_CPP11_TO_STRING` or
`CATCH_CONFIG_NO_CPP11_TO_STRING`.


## C++17 toggles

    CATCH_CONFIG_CPP17_UNCAUGHT_EXCEPTIONS  // Override std::uncaught_exceptions (instead of std::uncaught_exception) support detection
    CATCH_CONFIG_CPP17_STRING_VIEW          // Override std::string_view support detection (Catch provides a StringMaker specialization by default)
    CATCH_CONFIG_CPP17_VARIANT              // Override std::variant support detection (checked by CATCH_CONFIG_ENABLE_VARIANT_STRINGMAKER)
    CATCH_CONFIG_CPP17_OPTIONAL             // Override std::optional support detection (checked by CATCH_CONFIG_ENABLE_OPTIONAL_STRINGMAKER)
    CATCH_CONFIG_CPP17_BYTE                 // Override std::byte support detection (Catch provides a StringMaker specialization by default)

> `CATCH_CONFIG_CPP17_STRING_VIEW` was [introduced](https://github.com/catchorg/Catch2/issues/1376) in Catch2 2.4.1.

Catch contains basic compiler/standard detection and attempts to use
some C++17 features whenever appropriate. This automatic detection
can be manually overridden in both directions, that is, a feature
can be enabled by defining the macro in the table above, and disabled
by using `_NO_` in the macro, e.g. `CATCH_CONFIG_NO_CPP17_UNCAUGHT_EXCEPTIONS`.


## Other toggles

    CATCH_CONFIG_COUNTER                    // Use __COUNTER__ to generate unique names for test cases
    CATCH_CONFIG_WINDOWS_SEH                // Enable SEH handling on Windows
    CATCH_CONFIG_FAST_COMPILE               // Sacrifices some (rather minor) features for compilation speed
    CATCH_CONFIG_POSIX_SIGNALS              // Enable handling POSIX signals
    CATCH_CONFIG_WINDOWS_CRTDBG             // Enable leak checking using Windows's CRT Debug Heap
    CATCH_CONFIG_DISABLE_STRINGIFICATION    // Disable stringifying the original expression
    CATCH_CONFIG_DISABLE                    // Disables assertions and test case registration
    CATCH_CONFIG_WCHAR                      // Enables use of wchart_t
    CATCH_CONFIG_EXPERIMENTAL_REDIRECT      // Enables the new (experimental) way of capturing stdout/stderr
    CATCH_CONFIG_USE_ASYNC                  // Force parallel statistical processing of samples during benchmarking
    CATCH_CONFIG_ANDROID_LOGWRITE           // Use android's logging system for debug output
    CATCH_CONFIG_GLOBAL_NEXTAFTER           // Use nextafter{,f,l} instead of std::nextafter
    CATCH_CONFIG_GETENV                     // System has a working `getenv`
    CATCH_CONFIG_USE_BUILTIN_CONSTANT_P     // Use __builtin_constant_p to trigger warnings

> [`CATCH_CONFIG_ANDROID_LOGWRITE`](https://github.com/catchorg/Catch2/issues/1743) and [`CATCH_CONFIG_GLOBAL_NEXTAFTER`](https://github.com/catchorg/Catch2/pull/1739) were introduced in Catch2 2.10.0

> `CATCH_CONFIG_GETENV` was [introduced](https://github.com/catchorg/Catch2/pull/2562) in Catch2 3.2.0

> `CATCH_CONFIG_USE_BUILTIN_CONSTANT_P` was introduced in Catch2 vX.Y.Z

Currently Catch enables `CATCH_CONFIG_WINDOWS_SEH` only when compiled with MSVC, because some versions of MinGW do not have the necessary Win32 API support.

`CATCH_CONFIG_POSIX_SIGNALS` is on by default, except when Catch is compiled under `Cygwin`, where it is disabled by default (but can be force-enabled by defining `CATCH_CONFIG_POSIX_SIGNALS`).

`CATCH_CONFIG_GETENV` is on by default, except when Catch2 is compiled for
platforms that lacks working `std::getenv` (currently Windows UWP and
Playstation).

`CATCH_CONFIG_WINDOWS_CRTDBG` is off by default. If enabled, Windows's
CRT is used to check for memory leaks, and displays them after the tests
finish running. This option only works when linking against the default
main, and must be defined for the whole library build.

`CATCH_CONFIG_WCHAR` is on by default, but can be disabled. Currently
it is only used in support for DJGPP cross-compiler.

With the exception of `CATCH_CONFIG_EXPERIMENTAL_REDIRECT`,
these toggles can be disabled by using `_NO_` form of the toggle,
e.g. `CATCH_CONFIG_NO_WINDOWS_SEH`.

`CATCH_CONFIG_USE_BUILTIN_CONSTANT_P` is ON by default for Clang and GCC
(but as far as possible, not for other compilers masquerading for these
two). However, it can cause bugs where the enclosed code is evaluated, even
though it should not be, e.g. in [#2925](https://github.com/catchorg/Catch2/issues/2925).


### `CATCH_CONFIG_FAST_COMPILE`
This compile-time flag speeds up compilation of assertion macros by ~20%,
by disabling the generation of assertion-local try-catch blocks for
non-exception family of assertion macros ({`REQUIRE`,`CHECK`}{``,`_FALSE`, `_THAT`}).
This disables translation of exceptions thrown under these assertions, but
should not lead to false negatives.

`CATCH_CONFIG_FAST_COMPILE` has to be either defined, or not defined,
in all translation units that are linked into single test binary.

### `CATCH_CONFIG_DISABLE_STRINGIFICATION`
This toggle enables a workaround for VS 2017 bug. For details see [known limitations](limitations.md#visual-studio-2017----raw-string-literal-in-assert-fails-to-compile).

### `CATCH_CONFIG_DISABLE`
This toggle removes most of Catch from given file. This means that `TEST_CASE`s are not registered and assertions are turned into no-ops. Useful for keeping tests within implementation files (ie for functions with internal linkage), instead of in external files.

This feature is considered experimental and might change at any point.

_Inspired by Doctest's `DOCTEST_CONFIG_DISABLE`_


## Enabling stringification

By default, Catch does not stringify some types from the standard library. This is done to avoid dragging in various standard library headers by default. However, Catch does contain these and can be configured to provide them, using these macros:

    CATCH_CONFIG_ENABLE_PAIR_STRINGMAKER     // Provide StringMaker specialization for std::pair
    CATCH_CONFIG_ENABLE_TUPLE_STRINGMAKER    // Provide StringMaker specialization for std::tuple
    CATCH_CONFIG_ENABLE_VARIANT_STRINGMAKER  // Provide StringMaker specialization for std::variant, std::monostate (on C++17)
    CATCH_CONFIG_ENABLE_OPTIONAL_STRINGMAKER // Provide StringMaker specialization for std::optional (on C++17)
    CATCH_CONFIG_ENABLE_ALL_STRINGMAKERS     // Defines all of the above

> `CATCH_CONFIG_ENABLE_VARIANT_STRINGMAKER` was [introduced](https://github.com/catchorg/Catch2/issues/1380) in Catch2 2.4.1.

> `CATCH_CONFIG_ENABLE_OPTIONAL_STRINGMAKER` was [introduced](https://github.com/catchorg/Catch2/issues/1510) in Catch2 2.6.0.

## Disabling exceptions

> Introduced in Catch2 2.4.0.

By default, Catch2 uses exceptions to signal errors and to abort tests
when an assertion from the `REQUIRE` family of assertions fails. We also
provide an experimental support for disabling exceptions. Catch2 should
automatically detect when it is compiled with exceptions disabled, but
it can be forced to compile without exceptions by defining

    CATCH_CONFIG_DISABLE_EXCEPTIONS

Note that when using Catch2 without exceptions, there are 2 major
limitations:

1) If there is an error that would normally be signalled by an exception,
the exception's message will instead be written to `Catch::cerr` and
`std::terminate` will be called.
2) If an assertion from the `REQUIRE` family of macros fails,
`std::terminate` will be called after the active reporter returns.


There is also a customization point for the exact behaviour of what
happens instead of exception being thrown. To use it, define

    CATCH_CONFIG_DISABLE_EXCEPTIONS_CUSTOM_HANDLER

and provide a definition for this function:

```cpp
namespace Catch {
    [[noreturn]]
    void throw_exception(std::exception const&);
}
```

## Overriding Catch's debug break (`-b`)

> [Introduced](https://github.com/catchorg/Catch2/pull/1846) in Catch2 2.11.2.

You can override Catch2's break-into-debugger code by defining the
`CATCH_BREAK_INTO_DEBUGGER()` macro. This can be used if e.g. Catch2 does
not know your platform, or your platform is misdetected.

The macro will be used as is, that is, `CATCH_BREAK_INTO_DEBUGGER();`
must compile and must break into debugger.


## Static analysis support

> Introduced in Catch2 3.4.0.

Some parts of Catch2, e.g. `SECTION`s, can be hard for static analysis
tools to reason about. Catch2 can change its internals to help static
analysis tools reason about the tests.

Catch2 automatically detects some static analysis tools (initial
implementation checks for clang-tidy and Coverity), but you can override
its detection (in either direction) via

```
CATCH_CONFIG_EXPERIMENTAL_STATIC_ANALYSIS_SUPPORT     // force enables static analysis help
CATCH_CONFIG_NO_EXPERIMENTAL_STATIC_ANALYSIS_SUPPORT  // force disables static analysis help
```

_As the name suggests, this is currently experimental, and thus we provide
no backwards compatibility guarantees._

**DO NOT ENABLE THIS FOR BUILDS YOU INTEND TO RUN.** The changed internals
are not meant to be runnable, only "scannable".



---

[Home](Readme.md#top)



docs/contributing.md
--------------------------------------
<a id="top"></a>
# Contributing to Catch2

**Contents**<br>
[Using Git(Hub)](#using-github)<br>
[Testing your changes](#testing-your-changes)<br>
[Writing documentation](#writing-documentation)<br>
[Writing code](#writing-code)<br>
[CoC](#coc)<br>

So you want to contribute something to Catch2? That's great! Whether it's
a bug fix, a new feature, support for additional compilers - or just
a fix to the documentation - all contributions are very welcome and very
much appreciated. Of course so are bug reports, other comments, and
questions, but generally it is a better idea to ask questions in our
[Discord](https://discord.gg/4CWS9zD), than in the issue tracker.


This page covers some guidelines and helpful tips for contributing
to the codebase itself.

## Using Git(Hub)

Ongoing development happens in the `devel` branch for Catch2 v3, and in
`v2.x` for maintenance updates to the v2 versions.

Commits should be small and atomic. A commit is atomic when, after it is
applied, the codebase, tests and all, still works as expected. Small
commits are also preferred, as they make later operations with git history,
whether it is bisecting, reverting, or something else, easier.

_When submitting a pull request please do not include changes to the
amalgamated distribution files. This means do not include them in your
git commits!_

When addressing review comments in a MR, please do not rebase/squash the
commits immediately. Doing so makes it harder to review the new changes,
slowing down the process of merging a MR. Instead, when addressing review
comments, you should append new commits to the branch and only squash
them into other commits when the MR is ready to be merged. We recommend
creating new commits with `git commit --fixup` (or `--squash`) and then
later squashing them with `git rebase --autosquash` to make things easier.



## Testing your changes

_Note: Running Catch2's tests requires Python3_


Catch2 has multiple layers of tests that are then run as part of our CI.
The most obvious one are the unit tests compiled into the `SelfTest`
binary. These are then used in "Approval tests", which run (almost) all
tests from `SelfTest` through a specific reporter and then compare the
generated output with a known good output ("Baseline"). By default, new
tests should be placed here.

To configure a Catch2 build with just the basic tests, use the `basic-tests`
preset, like so:

```
# Assuming you are in Catch2's root folder

cmake -B basic-test-build -S . -DCMAKE_BUILD_TYPE=Debug --preset basic-tests
```

However, not all tests can be written as plain unit tests. For example,
checking that Catch2 orders tests randomly when asked to, and that this
random ordering is subset-invariant, is better done as an integration
test using an external check script. Catch2 integration tests are written
using CTest, either as a direct command invocation + pass/fail regex,
or by delegating the check to a Python script.

Catch2 is slowly gaining more and more types of tests, currently Catch2
project also has buildable examples, "ExtraTests", and CMake config tests.
Examples present a small and self-contained snippets of code that
use Catch2's facilities for specific purpose. Currently they are assumed
passing if they compile.

ExtraTests then are expensive tests, that we do not want to run all the
time. This can be either because they take a long time to run, or because
they take a long time to compile, e.g. because they test compile time
configuration and require separate compilation.

Finally, CMake config tests test that you set Catch2's compile-time
configuration options through CMake, using CMake options of the same name.

These test categories can be enabled one by one, by passing
`-DCATCH_BUILD_EXAMPLES=ON`, `-DCATCH_BUILD_EXTRA_TESTS=ON`, and
`-DCATCH_ENABLE_CONFIGURE_TESTS=ON` when configuring the build.

Catch2 also provides a preset that promises to enable _all_ test types,
`all-tests`.

The snippet below will build & run all tests, in `Debug` compilation mode.

<!-- snippet: catch2-build-and-test -->
<a id='snippet-catch2-build-and-test'></a>
```sh
# 1. Regenerate the amalgamated distribution (some tests are built against it)
./tools/scripts/generateAmalgamatedFiles.py

# 2. Configure the full test build
cmake -B debug-build -S . -DCMAKE_BUILD_TYPE=Debug --preset all-tests

# 3. Run the actual build
cmake --build debug-build

# 4. Run the tests using CTest
ctest -j 4 --output-on-failure -C Debug --test-dir debug-build
```
<sup><a href='/tools/scripts/buildAndTest.sh#L6-L19' title='File snippet `catch2-build-and-test` was extracted from'>snippet source</a> | <a href='#snippet-catch2-build-and-test' title='Navigate to start of snippet `catch2-build-and-test`'>anchor</a></sup>
<!-- endSnippet -->

For convenience, the above commands are in the script `tools/scripts/buildAndTest.sh`, and can be run like this:

```bash
cd Catch2
./tools/scripts/buildAndTest.sh
```

A Windows version of the script is available at `tools\scripts\buildAndTest.cmd`.

If you added new tests, you will likely see `ApprovalTests` failure.
After you check that the output difference is expected, you should
run `tools/scripts/approve.py` to confirm them, and include these changes
in your commit.


## Writing documentation

If you have added new feature to Catch2, it needs documentation, so that
other people can use it as well. This section collects some technical
information that you will need for updating Catch2's documentation, and
possibly some generic advise as well.


### Technicalities

First, the technicalities:

* If you have introduced a new document, there is a simple template you
should use. It provides you with the top anchor mentioned to link to
(more below), and also with a backlink to the top of the documentation:
```markdown
<a id="top"></a>
# Cool feature

> [Introduced](https://github.com/catchorg/Catch2/pull/123456) in Catch2 X.Y.Z

Text that explains how to use the cool feature.


---

[Home](Readme.md#top)
```

* Crosslinks to different pages should target the `top` anchor, like this
`[link to contributing](contributing.md#top)`.

* We introduced version tags to the documentation, which show users in
which version a specific feature was introduced. This means that newly
written documentation should be tagged with a placeholder, that will
be replaced with the actual version upon release. There are 2 styles
of placeholders used through the documentation, you should pick one that
fits your text better (if in doubt, take a look at the existing version
tags for other features).
  * `> [Introduced](link-to-issue-or-PR) in Catch2 X.Y.Z` - this
  placeholder is usually used after a section heading
  * `> X (Y and Z) was [introduced](link-to-issue-or-PR) in Catch2 X.Y.Z`
  - this placeholder is used when you need to tag a subpart of something,
  e.g. a list

* For pages with more than 4 subheadings, we provide a table of contents
(ToC) at the top of the page. Because GitHub markdown does not support
automatic generation of ToC, it has to be handled semi-manually. Thus,
if you've added a new subheading to some page, you should add it to the
ToC. This can be done either manually, or by running the
`updateDocumentToC.py` script in the `scripts/` folder.

### Contents

Now, for some content tips:

* Usage examples are good. However, having large code snippets inline
can make the documentation less readable, and so the inline snippets
should be kept reasonably short. To provide more complex compilable
examples, consider adding new .cpp file to `examples/`.

* Don't be afraid to introduce new pages. The current documentation
tends towards long pages, but a lot of that is caused by legacy, and
we know that some of the pages are overly big and unfocused.

* When adding information to an existing page, please try to keep your
formatting, style and changes consistent with the rest of the page.

* Any documentation has multiple different audiences, that desire
different information from the text. The 3 basic user-types to try and
cover are:
  * A beginner to Catch2, who requires closer guidance for the usage of Catch2.
  * Advanced user of Catch2, who want to customize their usage.
  * Experts, looking for full reference of Catch2's capabilities.


## Writing code

If want to contribute code, this section contains some simple rules
and tips on things like code formatting, code constructions to avoid,
and so on.

### C++ standard version

Catch2 currently targets C++14 as the minimum supported C++ version.
Features from higher language versions should be used only sparingly,
when the benefits from using them outweigh the maintenance overhead.

Example of good use of polyfilling features is our use of `conjunction`,
where if available we use `std::conjunction` and otherwise provide our
own implementation. The reason it is good is that the surface area for
maintenance is quite small, and `std::conjunction` can directly use
compiler built-ins, thus providing significant compilation benefits.

Example of bad use of polyfilling features would be to keep around two
sets of metaprogramming in the stringification implementation, once
using C++14 compliant TMP and once using C++17's `if constexpr`. While
the C++17 would provide significant compilation speedups, the maintenance
cost would be too high.


### Formatting

To make code formatting simpler for the contributors, Catch2 provides
its own config for `clang-format`. However, because it is currently
impossible to replicate existing Catch2's formatting in clang-format,
using it to reformat a whole file would cause massive diffs. To keep
the size of your diffs reasonable, you should only use clang-format
on the newly changed code.


### Code constructs to watch out for

This section is a (sadly incomplete) listing of various constructs that
are problematic and are not always caught by our CI infrastructure.


#### Naked exceptions and exceptions-related function

If you are throwing an exception, it should be done via `CATCH_ERROR`
or `CATCH_RUNTIME_ERROR` in `internal/catch_enforce.hpp`. These macros will handle
the differences between compilation with or without exceptions for you.
However, some platforms (IAR) also have problems with exceptions-related
functions, such as `std::current_exceptions`. We do not have IAR in our
CI, but luckily there should not be too many reasons to use these.
However, if you do, they should be kept behind a
`CATCH_CONFIG_DISABLE_EXCEPTIONS` macro.


#### Avoid `std::move` and `std::forward`

`std::move` and `std::forward` provide nice semantic name for a specific
`static_cast`. However, being function templates they have surprisingly
high cost during compilation, and can also have a negative performance
impact for low-optimization builds.

You should be using `CATCH_MOVE` and `CATCH_FORWARD` macros from
`internal/catch_move_and_forward.hpp` instead. They expand into the proper
`static_cast`, and avoid the overhead of `std::move` and `std::forward`.


#### Unqualified usage of functions from C's stdlib

If you are using a function from C's stdlib, please include the header
as `<cfoo>` and call the function qualified. The common knowledge that
there is no difference is wrong, QNX and VxWorks won't compile if you
include the header as `<cfoo>` and call the function unqualified.


#### User-Defined Literals (UDL) for Catch2' types

Due to messy standardese and ... not great ... implementation of
`-Wreserved-identifier` in Clang, avoid declaring UDLs as
```cpp
Approx operator "" _a(long double);
```
and instead declare them as
```cpp
Approx operator ""_a(long double);
```

Notice that the second version does not have a space between the `""` and
the literal suffix.



### New source file template

If you are adding new source file, there is a template you should use.
Specifically, every source file should start with the licence header:
```cpp

    //              Copyright Catch2 Authors
    // Distributed under the Boost Software License, Version 1.0.
    //   (See accompanying file LICENSE.txt or copy at
    //        https://www.boost.org/LICENSE_1_0.txt)

    // SPDX-License-Identifier: BSL-1.0
```

The include guards for header files should follow the pattern `{FILENAME}_INCLUDED`.
This means that for file `catch_matchers_foo.hpp`, the include guard should
be `CATCH_MATCHERS_FOO_HPP_INCLUDED`, for `catch_generators_bar.hpp`, the include
guard should be `CATCH_GENERATORS_BAR_HPP_INCLUDED`, and so on.


### Adding new `CATCH_CONFIG` option

When adding new `CATCH_CONFIG` option, there are multiple places to edit:
  * `CMake/CatchConfigOptions.cmake` - this is used to generate the
    configuration options in CMake, so that CMake frontends know about them.
  * `docs/configuration.md` - this is where the options are documented
  * `src/catch2/catch_user_config.hpp.in` - this is template for generating
    `catch_user_config.hpp` which contains the materialized configuration
  * `BUILD.bazel` - Bazel does not have configuration support like CMake,
    and all expansions need to be done manually
  * other files as needed, e.g. `catch2/internal/catch_config_foo.hpp`
    for the logic that guards the configuration


## CoC

This project has a [CoC](../CODE_OF_CONDUCT.md). Please adhere to it
while contributing to Catch2.

-----------

_This documentation will always be in-progress as new information comes
up, but we are trying to keep it as up to date as possible._

---

[Home](Readme.md#top)



docs/deprecations.md
--------------------------------------
<a id="top"></a>
# Deprecations and incoming changes

This page documents current deprecations and upcoming planned changes
inside Catch2. The difference between these is that a deprecated feature
will be removed, while a planned change to a feature means that the
feature will behave differently, but will still be present. Obviously,
either of these is a breaking change, and thus will not happen until
at least the next major release.


### `ParseAndAddCatchTests.cmake`

The CMake/CTest integration using `ParseAndAddCatchTests.cmake` is deprecated,
as it can be replaced by `Catch.cmake` that provides the function
`catch_discover_tests` to get tests directly from a CMake target via the
command line interface instead of parsing C++ code with regular expressions.


### `CATCH_CONFIG_BAZEL_SUPPORT`

Catch2 supports writing the Bazel JUnit XML output file when it is aware
that is within a bazel testing environment. Originally there was no way
to accurately probe the environment for this information so the flag
`CATCH_CONFIG_BAZEL_SUPPORT` was added. This now deprecated. Bazel has now had a change
where it will export `BAZEL_TEST=1` for purposes like the above. Catch2
will now instead inspect the environment instead of relying on build configuration.

### `IEventLister::skipTest( TestCaseInfo const& testInfo )`

This event (including implementations in derived classes such as `ReporterBase`)
is deprecated and will be removed in the next major release. It is currently
invoked for all test cases that are not going to be executed due to the test run
being aborted (when using `--abort` or `--abortx`). It is however
**NOT** invoked for test cases that are [explicitly skipped using the `SKIP`
macro](skipping-passing-failing.md#top).


### Non-const function for `TEST_CASE_METHOD`

> Deprecated in Catch2 vX.Y.Z

Currently, the member function generated for `TEST_CASE_METHOD` is
not `const` qualified. In the future, the generated member function will
be `const` qualified, just as `TEST_CASE_PERSISTENT_FIXTURE` does.

If you are mutating the fixture instance from within the test case, and
want to keep doing so in the future, mark the mutated members as `mutable`.


---

[Home](Readme.md#top)



docs/event-listeners.md
--------------------------------------
<a id="top"></a>
# Event Listeners

An event listener is a bit like a reporter, in that it responds to various
reporter events in Catch2, but it is not expected to write any output.
Instead, an event listener performs actions within the test process, such
as performing global initialization (e.g. of a C library), or cleaning out
in-memory logs if they are not needed (the test case passed).

Unlike reporters, each registered event listener is always active. Event
listeners are always notified before reporter(s).

To write your own event listener, you should derive from `Catch::TestEventListenerBase`,
as it provides empty stubs for all reporter events, allowing you to
only override events you care for. Afterwards you have to register it
with Catch2 using `CATCH_REGISTER_LISTENER` macro, so that Catch2 knows
about it and instantiates it before running tests.

Example event listener:
```cpp
#include <catch2/reporters/catch_reporter_event_listener.hpp>
#include <catch2/reporters/catch_reporter_registrars.hpp>

class testRunListener : public Catch::EventListenerBase {
public:
    using Catch::EventListenerBase::EventListenerBase;

    void testRunStarting(Catch::TestRunInfo const&) override {
        lib_foo_init();
    }
};

CATCH_REGISTER_LISTENER(testRunListener)
```

_Note that you should not use any assertion macros within a Listener!_

[You can find the list of events that the listeners can react to on its
own page](reporter-events.md#top).


---

[Home](Readme.md#top)



docs/faq.md
--------------------------------------
<a id="top"></a>
# Frequently Asked Questions (FAQ)

**Contents**<br>
[How do I run global setup/teardown only if tests will be run?](#how-do-i-run-global-setupteardown-only-if-tests-will-be-run)<br>
[How do I clean up global state between running different tests?](#how-do-i-clean-up-global-state-between-running-different-tests)<br>
[Why cannot I derive from the built-in reporters?](#why-cannot-i-derive-from-the-built-in-reporters)<br>
[What is Catch2's ABI stability policy?](#what-is-catch2s-abi-stability-policy)<br>
[What is Catch2's API stability policy?](#what-is-catch2s-api-stability-policy)<br>
[Does Catch2 support running tests in parallel?](#does-catch2-support-running-tests-in-parallel)<br>
[Can I compile Catch2 into a dynamic library?](#can-i-compile-catch2-into-a-dynamic-library)<br>
[What repeatability guarantees does Catch2 provide?](#what-repeatability-guarantees-does-catch2-provide)<br>
[My build cannot find `catch2/catch_user_config.hpp`, how can I fix it?](#my-build-cannot-find-catch2catch_user_confighpp-how-can-i-fix-it)<br>


## How do I run global setup/teardown only if tests will be run?

Write a custom [event listener](event-listeners.md#top) and place the
global setup/teardown code into the `testRun*` events.


## How do I clean up global state between running different tests?

Write a custom [event listener](event-listeners.md#top) and place the
cleanup code into either `testCase*` or `testCasePartial*` events,
depending on how often the cleanup needs to happen.


## Why cannot I derive from the built-in reporters?

They are not made to be overridden, in that we do not attempt to maintain
a consistent internal state if a member function is overridden, and by
forbidding users from using them as a base class, we can refactor them
as needed later.


## What is Catch2's ABI stability policy?

Catch2 provides no ABI stability guarantees whatsoever. Catch2 provides
rich C++ interface, and trying to freeze its ABI would take a lot of
pointless work.

Catch2 is not designed to be distributed as dynamic library, and you
should really be able to compile everything with the same compiler binary.


## What is Catch2's API stability policy?

Catch2 follows [semver](https://semver.org/) to the best of our ability.
This means that we will not knowingly make backwards-incompatible changes
without incrementing the major version number.


## Does Catch2 support running tests in parallel?

Not natively, no. We see running tests in parallel as the job of an
external test runner, that can also run them in separate processes,
support test execution timeouts and so on.

However, Catch2 provides some tools that make the job of external test
runners easier. [See the relevant section in our page on best
practices](usage-tips.md#parallel-tests).


## Can I compile Catch2 into a dynamic library?

Yes, Catch2 supports the [standard CMake `BUILD_SHARED_LIBS`
option](https://cmake.org/cmake/help/latest/variable/BUILD_SHARED_LIBS.html).
However, the dynamic library support is provided as-is. Catch2 does not
provide API export annotations, and so you can only use it as a dynamic
library on platforms that default to public visibility, or with tooling
support to force export Catch2's API.


## What repeatability guarantees does Catch2 provide?

There are two places where it is meaningful to talk about Catch2's
repeatability guarantees without taking into account user-provided
code. First one is in the test case shuffling, and the second one is
the output from random generators.

Test case shuffling is repeatable across different platforms since v2.12.0,
and it is also generally repeatable across versions, but we might break
it from time to time. E.g. we broke repeatability with previous versions
in v2.13.4 so that test cases with similar names are shuffled better.

Since Catch2 3.5.0 the random generators use custom distributions,
that should be repeatable across different platforms, with few caveats.
For details see the section on random generators in the [Generator
documentation](generators.md#random-number-generators-details).

Before this version, random generators relied on distributions from
platform's stdlib. We thus can provide no extra guarantee on top of the
ones given by your platform. **Important: `<random>`'s distributions
are not specified to be repeatable across different platforms.**


## My build cannot find `catch2/catch_user_config.hpp`, how can I fix it?

`catch2/catch_user_config.hpp` is a generated header that contains user
compile time configuration. It is generated by CMake/Meson/Bazel during
build. If you are not using either of these, your three options are to

1) Build Catch2 separately using build tool that will generate the header
2) Use the amalgamated files to build Catch2
3) Use CMake to configure a build. This will generate the header and you
   can copy it into your own checkout of Catch2.



---

[Home](Readme.md#top)



docs/generators.md
--------------------------------------
<a id="top"></a>
# Data Generators

> Introduced in Catch2 2.6.0.

Data generators (also known as _data driven/parametrized test cases_)
let you reuse the same set of assertions across different input values.
In Catch2, this means that they respect the ordering and nesting
of the `TEST_CASE` and `SECTION` macros, and their nested sections
are run once per each value in a generator.

This is best explained with an example:
```cpp
TEST_CASE("Generators") {
    auto i = GENERATE(1, 3, 5);
    REQUIRE(is_odd(i));
}
```

The "Generators" `TEST_CASE` will be entered 3 times, and the value of
`i` will be 1, 3, and 5 in turn. `GENERATE`s can also be used multiple
times at the same scope, in which case the result will be a cartesian
product of all elements in the generators. This means that in the snippet
below, the test case will be run 6 (2\*3) times.

```cpp
TEST_CASE("Generators") {
    auto i = GENERATE(1, 2);
    auto j = GENERATE(3, 4, 5);
}
```

There are 2 parts to generators in Catch2, the `GENERATE` macro together
with the already provided generators, and the `IGenerator<T>` interface
that allows users to implement their own generators.


## Combining `GENERATE` and `SECTION`.

`GENERATE` can be seen as an implicit `SECTION`, that goes from the place
`GENERATE` is used, to the end of the scope. This can be used for various
effects. The simplest usage is shown below, where the `SECTION` "one"
runs 4 (2\*2) times, and `SECTION` "two" is run 6 times (2\*3).

```cpp
TEST_CASE("Generators") {
    auto i = GENERATE(1, 2);
    SECTION("one") {
        auto j = GENERATE(-3, -2);
        REQUIRE(j < i);
    }
    SECTION("two") {
        auto k = GENERATE(4, 5, 6);
        REQUIRE(i != k);
    }
}
```

The specific order of the `SECTION`s will be "one", "one", "two", "two",
"two", "one"...


The fact that `GENERATE` introduces a virtual `SECTION` can also be used
to make a generator replay only some `SECTION`s, without having to
explicitly add a `SECTION`. As an example, the code below reports 3
assertions, because the "first" section is run once, but the "second"
section is run twice.

```cpp
TEST_CASE("GENERATE between SECTIONs") {
    SECTION("first") { REQUIRE(true); }
    auto _ = GENERATE(1, 2);
    SECTION("second") { REQUIRE(true); }
}
```

This can lead to surprisingly complex test flows. As an example, the test
below will report 14 assertions:

```cpp
TEST_CASE("Complex mix of sections and generates") {
    auto i = GENERATE(1, 2);
    SECTION("A") {
        SUCCEED("A");
    }
    auto j = GENERATE(3, 4);
    SECTION("B") {
        SUCCEED("B");
    }
    auto k = GENERATE(5, 6);
    SUCCEED();
}
```

> The ability to place `GENERATE` between two `SECTION`s was [introduced](https://github.com/catchorg/Catch2/issues/1938) in Catch2 2.13.0.

## Provided generators

Catch2's provided generator functionality consists of three parts,

* `GENERATE` macro,  that serves to integrate generator expression with
a test case,
* 2 fundamental generators
  * `SingleValueGenerator<T>` -- contains only single element
  * `FixedValuesGenerator<T>` -- contains multiple elements
* 5 generic generators that modify other generators
  * `FilterGenerator<T, Predicate>` -- filters out elements from a generator
  for which the predicate returns "false"
  * `TakeGenerator<T>` -- takes first `n` elements from a generator
  * `RepeatGenerator<T>` -- repeats output from a generator `n` times
  * `MapGenerator<T, U, Func>` -- returns the result of applying `Func`
  on elements from a different generator
  * `ChunkGenerator<T>` -- returns chunks (inside `std::vector`) of n elements from a generator
* 4 specific purpose generators
  * `RandomIntegerGenerator<Integral>` -- generates random Integrals from range
  * `RandomFloatGenerator<Float>` -- generates random Floats from range
  * `RangeGenerator<T>(first, last)` -- generates all values inside a `[first, last)` arithmetic range
  * `IteratorGenerator<T>` -- copies and returns values from an iterator range

> `ChunkGenerator<T>`, `RandomIntegerGenerator<Integral>`, `RandomFloatGenerator<Float>` and `RangeGenerator<T>` were introduced in Catch2 2.7.0.

> `IteratorGenerator<T>` was introduced in Catch2 2.10.0.

The generators also have associated helper functions that infer their
type, making their usage much nicer. These are

* `value(T&&)` for `SingleValueGenerator<T>`
* `values(std::initializer_list<T>)` for `FixedValuesGenerator<T>`
* `table<Ts...>(std::initializer_list<std::tuple<Ts...>>)` for `FixedValuesGenerator<std::tuple<Ts...>>`
* `filter(predicate, GeneratorWrapper<T>&&)` for `FilterGenerator<T, Predicate>`
* `take(count, GeneratorWrapper<T>&&)` for `TakeGenerator<T>`
* `repeat(repeats, GeneratorWrapper<T>&&)` for `RepeatGenerator<T>`
* `map(func, GeneratorWrapper<T>&&)` for `MapGenerator<T, U, Func>` (map `U` to `T`, deduced from `Func`)
* `map<T>(func, GeneratorWrapper<U>&&)` for `MapGenerator<T, U, Func>` (map `U` to `T`)
* `chunk(chunk-size, GeneratorWrapper<T>&&)` for `ChunkGenerator<T>`
* `random(IntegerOrFloat a, IntegerOrFloat b)` for `RandomIntegerGenerator` or `RandomFloatGenerator`
* `range(Arithmetic start, Arithmetic end)` for `RangeGenerator<Arithmetic>` with a step size of `1`
* `range(Arithmetic start, Arithmetic end, Arithmetic step)` for `RangeGenerator<Arithmetic>` with a custom step size
* `from_range(InputIterator from, InputIterator to)` for `IteratorGenerator<T>`
* `from_range(Container const&)` for `IteratorGenerator<T>`

> `chunk()`, `random()` and both `range()` functions were introduced in Catch2 2.7.0.

> `from_range` has been introduced in Catch2 2.10.0

> `range()` for floating point numbers has been introduced in Catch2 2.11.0

And can be used as shown in the example below to create a generator
that returns 100 odd random number:

```cpp
TEST_CASE("Generating random ints", "[example][generator]") {
    SECTION("Deducing functions") {
        auto i = GENERATE(take(100, filter([](int i) { return i % 2 == 1; }, random(-100, 100))));
        REQUIRE(i > -100);
        REQUIRE(i < 100);
        REQUIRE(i % 2 == 1);
    }
}
```


Apart from registering generators with Catch2, the `GENERATE` macro has
one more purpose, and that is to provide simple way of generating trivial
generators, as seen in the first example on this page, where we used it
as `auto i = GENERATE(1, 2, 3);`. This usage converted each of the three
literals into a single `SingleValueGenerator<int>` and then placed them all in
a special generator that concatenates other generators. It can also be
used with other generators as arguments, such as `auto i = GENERATE(0, 2,
take(100, random(300, 3000)));`. This is useful e.g. if you know that
specific inputs are problematic and want to test them separately/first.

**For safety reasons, you cannot use variables inside the `GENERATE` macro.
This is done because the generator expression _will_ outlive the outside
scope and thus capturing references is dangerous. If you need to use
variables inside the generator expression, make sure you thought through
the lifetime implications and use `GENERATE_COPY` or `GENERATE_REF`.**

> `GENERATE_COPY` and `GENERATE_REF` were introduced in Catch2 2.7.1.

You can also override the inferred type by using `as<type>` as the first
argument to the macro. This can be useful when dealing with string literals,
if you want them to come out as `std::string`:

```cpp
TEST_CASE("type conversion", "[generators]") {
    auto str = GENERATE(as<std::string>{}, "a", "bb", "ccc");
    REQUIRE(str.size() > 0);
}
```


### Random number generators: details

> This section applies from Catch2 3.5.0. Before that, random generators
> were a thin wrapper around distributions from `<random>`.

All of the `random(a, b)` generators in Catch2 currently generate uniformly
distributed number in closed interval \[a; b\]. This  is different from
`std::uniform_real_distribution`, which should return numbers in interval
\[a; b) (but due to rounding can end up returning b anyway), but the
difference is intentional, so that `random(a, a)` makes sense. If there is
enough interest from users, we can provide API to pick any of CC, CO, OC,
or OO ranges.

Unlike `std::uniform_int_distribution`, Catch2's generators also support
various single-byte integral types, such as `char` or `bool`.


#### Reproducibility

Given the same seed, the output from the integral generators is fully
reproducible across different platforms.

For floating point generators, the situation is much more complex.
Generally Catch2 only promises reproducibility (or even just correctness!)
on platforms that obey the IEEE-754 standard. Furthermore, reproducibility
only applies between binaries that perform floating point math in the
same way, e.g. if you compile a binary targetting the x87 FPU and another
one targetting SSE2 for floating point math, their results will vary.
Similarly, binaries compiled with compiler flags that relax the IEEE-754
adherence, e.g. `-ffast-math`, might provide different results than those
compiled for strict IEEE-754 adherence.

Finally, we provide zero guarantees on the reproducibility of generating
`long double`s, as the internals of `long double` varies across different
platforms.



## Generator interface

You can also implement your own generators, by deriving from the
`IGenerator<T>` interface:

```cpp
template<typename T>
struct IGenerator : GeneratorUntypedBase {
    // via GeneratorUntypedBase:
    // Attempts to move the generator to the next element.
    // Returns true if successful (and thus has another element that can be read)
    virtual bool next() = 0;

    // Precondition:
    // The generator is either freshly constructed or the last call to next() returned true
    virtual T const& get() const = 0;

    // Returns user-friendly string showing the current generator element
    // Does not have to be overridden, IGenerator provides default implementation
    virtual std::string stringifyImpl() const;
};
```

However, to be able to use your custom generator inside `GENERATE`, it
will need to be wrapped inside a `GeneratorWrapper<T>`.
`GeneratorWrapper<T>` is a value wrapper around a
`Catch::Detail::unique_ptr<IGenerator<T>>`.

For full example of implementing your own generator, look into Catch2's
examples, specifically
[Generators: Create your own generator](../examples/300-Gen-OwnGenerator.cpp).


### Handling empty generators

The generator interface assumes that a generator always has at least one
element. This is not always true, e.g. if the generator depends on an external
datafile, the file might be missing.

There are two ways to handle this, depending on whether you want this
to be an error or not.

 * If empty generator **is** an error, throw an exception in constructor.
 * If empty generator **is not** an error, use the [`SKIP`](skipping-passing-failing.md#skipping-test-cases-at-runtime) in constructor.



---

[Home](Readme.md#top)



docs/limitations.md
--------------------------------------
<a id="top"></a>
# Known limitations

Over time, some limitations of Catch2 emerged. Some of these are due
to implementation details that cannot be easily changed, some of these
are due to lack of development resources on our part, and some of these
are due to plain old 3rd party bugs.


## Implementation limits
### Sections nested in loops

If you are using `SECTION`s inside loops, you have to create them with
different name per loop's iteration. The recommended way to do so is to
incorporate the loop's counter into section's name, like so:

```cpp
TEST_CASE( "Looped section" ) {
    for (char i = '0'; i < '5'; ++i) {
        SECTION(std::string("Looped section ") + i) {
            SUCCEED( "Everything is OK" );
        }
    }
}
```

or with a `DYNAMIC_SECTION` macro (that was made for exactly this purpose):

```cpp
TEST_CASE( "Looped section" ) {
    for (char i = '0'; i < '5'; ++i) {
        DYNAMIC_SECTION( "Looped section " << i) {
            SUCCEED( "Everything is OK" );
        }
    }
}
```

### Tests might be run again if last section fails

If the last section in a test fails, it might be run again. This is because
Catch2 discovers `SECTION`s dynamically, as they are about to run, and
if the last section in test case is aborted during execution (e.g. via
the `REQUIRE` family of macros), Catch2 does not know that there are no
more sections in that test case and must run the test case again.


### MinGW/CygWin compilation (linking) is extremely slow

Compiling Catch2 with MinGW can be exceedingly slow, especially during
the linking step. As far as we can tell, this is caused by deficiencies
in its default linker. If you can tell MinGW to instead use lld, via
`-fuse-ld=lld`, the link time should drop down to reasonable length
again.


## Features
This section outlines some missing features, what is their status and their possible workarounds.

### Thread safe assertions
Catch2's assertion macros are not thread safe. This does not mean that
you cannot use threads inside Catch's test, but that only single thread
can interact with Catch's assertions and other macros.

This means that this is ok
```cpp
    std::vector<std::thread> threads;
    std::atomic<int> cnt{ 0 };
    for (int i = 0; i < 4; ++i) {
        threads.emplace_back([&]() {
            ++cnt; ++cnt; ++cnt; ++cnt;
        });
    }
    for (auto& t : threads) { t.join(); }
    REQUIRE(cnt == 16);
```
because only one thread passes the `REQUIRE` macro and this is not
```cpp
    std::vector<std::thread> threads;
    std::atomic<int> cnt{ 0 };
    for (int i = 0; i < 4; ++i) {
        threads.emplace_back([&]() {
            ++cnt; ++cnt; ++cnt; ++cnt;
            CHECK(cnt == 16);
        });
    }
    for (auto& t : threads) { t.join(); }
    REQUIRE(cnt == 16);
```

We currently do not plan to support thread-safe assertions.


### Process isolation in a test
Catch does not support running tests in isolated (forked) processes. While this might in the future, the fact that Windows does not support forking and only allows full-on process creation and the desire to keep code as similar as possible across platforms, mean that this is likely to take significant development time, that is not currently available.


### Running multiple tests in parallel

Catch2 keeps test execution in one process strictly serial, and there
are no plans to change this. If you find yourself with a test suite
that takes too long to run and you want to make it parallel, you have
to run multiple processes side by side.

There are 2 basic ways to do that,
* you can split your tests into multiple binaries, and run those binaries
  in parallel
* you can run the same test binary multiple times, but run a different
  subset of the tests in each process

There are multiple ways to achieve the latter, the easiest way is to use
[test sharding](command-line.md#test-sharding).


## 3rd party bugs

This section outlines known bugs in 3rd party components (this means compilers, standard libraries, standard runtimes).


### Visual Studio 2017 -- raw string literal in assert fails to compile

There is a known bug in Visual Studio 2017 (VC 15), that causes compilation
error when preprocessor attempts to stringize a raw string literal
(`#` preprocessor directive is applied to it). This snippet is sufficient
to trigger the compilation error:

```cpp
#include <catch2/catch_test_macros.hpp>

TEST_CASE("test") {
    CHECK(std::string(R"("\)") == "\"\\");
}
```

Catch2 provides a workaround, by letting the user disable stringification
of the original expression by defining `CATCH_CONFIG_DISABLE_STRINGIFICATION`,
like so:
```cpp
#define CATCH_CONFIG_DISABLE_STRINGIFICATION
#include <catch2/catch_test_macros.hpp>

TEST_CASE("test") {
    CHECK(std::string(R"("\)") == "\"\\");
}
```

_Do note that this changes the output:_
```
catchwork\test1.cpp(6):
PASSED:
  CHECK( Disabled by CATCH_CONFIG_DISABLE_STRINGIFICATION )
with expansion:
  ""\" == ""\"
```


### Clang/G++ -- skipping leaf sections after an exception
Some versions of `libc++` and `libstdc++` (or their runtimes) have a bug with `std::uncaught_exception()` getting stuck returning `true` after rethrow, even if there are no active exceptions. One such case is this snippet, which skipped the sections "a" and "b", when compiled against `libcxxrt` from the master branch
```cpp
#include <catch2/catch_test_macros.hpp>

TEST_CASE("a") {
    CHECK_THROWS(throw 3);
}

TEST_CASE("b") {
    int i = 0;
    SECTION("a") { i = 1; }
    SECTION("b") { i = 2; }
    CHECK(i > 0);
}
```

If you are seeing a problem like this, i.e. weird test paths that trigger only under Clang with `libc++`, or only under very specific version of `libstdc++`, it is very likely you are seeing this. The only known workaround is to use a fixed version of your standard library.


### Visual Studio 2022 -- can't compile assertion with the spaceship operator

[The C++ standard requires that `std::foo_ordering` is only comparable with
a literal 0](https://eel.is/c++draft/cmp#categories.pre-3). There are
multiple strategies a stdlib implementation can take to achieve this, and
MSVC's STL has changed the strategy they use between two releases of VS 2022.

With the new strategy, `REQUIRE((a <=> b) == 0)` no longer compiles under
MSVC. Note that Catch2 can compile code using MSVC STL's new strategy,
but only when compiled with a C++20 conforming compiler. MSVC is currently
not conformant enough, but `clang-cl` will compile the assertion above
using MSVC STL without problem.

This change got in with MSVC v19.37](https://godbolt.org/z/KG9obzdvE).




docs/list-of-examples.md
--------------------------------------
<a id="top"></a>
# List of examples

## Already available

- Test Case: [Single-file](../examples/010-TestCase.cpp)
- Test Case: [Multiple-file 1](../examples/020-TestCase-1.cpp), [2](../examples/020-TestCase-2.cpp)
- Assertion: [REQUIRE, CHECK](../examples/030-Asn-Require-Check.cpp)
- Fixture: [Sections](../examples/100-Fix-Section.cpp)
- Fixture: [Class-based fixtures](../examples/110-Fix-ClassFixture.cpp)
- Fixture: [Persistent fixtures](../examples/111-Fix-PersistentFixture.cpp)
- BDD: [SCENARIO, GIVEN, WHEN, THEN](../examples/120-Bdd-ScenarioGivenWhenThen.cpp)
- Listener: [Listeners](../examples/210-Evt-EventListeners.cpp)
- Configuration: [Provide your own output streams](../examples/231-Cfg-OutputStreams.cpp)
- Generators: [Create your own generator](../examples/300-Gen-OwnGenerator.cpp)
- Generators: [Use map to convert types in GENERATE expression](../examples/301-Gen-MapTypeConversion.cpp)
- Generators: [Run test with a table of input values](../examples/302-Gen-Table.cpp)
- Generators: [Use variables in generator expressions](../examples/310-Gen-VariablesInGenerators.cpp)
- Generators: [Use custom variable capture in generator expressions](../examples/311-Gen-CustomCapture.cpp)


## Planned

- Assertion: [REQUIRE_THAT and Matchers](../examples/040-Asn-RequireThat.cpp)
- Assertion: [REQUIRE_NO_THROW](../examples/050-Asn-RequireNoThrow.cpp)
- Assertion: [REQUIRE_THROWS](../examples/050-Asn-RequireThrows.cpp)
- Assertion: [REQUIRE_THROWS_AS](../examples/070-Asn-RequireThrowsAs.cpp)
- Assertion: [REQUIRE_THROWS_WITH](../examples/080-Asn-RequireThrowsWith.cpp)
- Assertion: [REQUIRE_THROWS_MATCHES](../examples/090-Asn-RequireThrowsMatches.cpp)
- Floating point: [Approx - Comparisons](../examples/130-Fpt-Approx.cpp)
- Logging: [CAPTURE - Capture expression](../examples/140-Log-Capture.cpp)
- Logging: [INFO - Provide information with failure](../examples/150-Log-Info.cpp)
- Logging: [WARN - Issue warning](../examples/160-Log-Warn.cpp)
- Logging: [FAIL, FAIL_CHECK - Issue message and force failure/continue](../examples/170-Log-Fail.cpp)
- Logging: [SUCCEED - Issue message and continue](../examples/180-Log-Succeed.cpp)
- Report: [User-defined type](../examples/190-Rpt-ReportUserDefinedType.cpp)
- Report: [User-defined reporter](../examples/202-Rpt-UserDefinedReporter.cpp)
- Report: [Automake reporter](../examples/205-Rpt-AutomakeReporter.cpp)
- Report: [TAP reporter](../examples/206-Rpt-TapReporter.cpp)
- Report: [Multiple reporter](../examples/208-Rpt-MultipleReporters.cpp)
- Configuration: [Provide your own main()](../examples/220-Cfg-OwnMain.cpp)
- Configuration: [Compile-time configuration](../examples/230-Cfg-CompileTimeConfiguration.cpp)
- Configuration: [Run-time configuration](../examples/240-Cfg-RunTimeConfiguration.cpp)

---

[Home](Readme.md#top)



docs/logging.md
--------------------------------------
<a id="top"></a>
# Logging macros

Additional messages can be logged during a test case. Note that the messages logged with `INFO` are scoped and thus will not be reported if failure occurs in scope preceding the message declaration. An example:

```cpp
TEST_CASE("Foo") {
    INFO("Test case start");
    for (int i = 0; i < 2; ++i) {
        INFO("The number is " << i);
        CHECK(i == 0);
    }
}

TEST_CASE("Bar") {
    INFO("Test case start");
    for (int i = 0; i < 2; ++i) {
        INFO("The number is " << i);
        CHECK(i == i);
    }
    CHECK(false);
}
```
When the `CHECK` fails in the "Foo" test case, then two messages will be printed.
```
Test case start
The number is 1
```
When the last `CHECK` fails in the "Bar" test case, then only one message will be printed: `Test case start`.

## Logging without local scope

> [Introduced](https://github.com/catchorg/Catch2/issues/1522) in Catch2 2.7.0.

`UNSCOPED_INFO` is similar to `INFO` with two key differences:

- Lifetime of an unscoped message is not tied to its own scope.
- An unscoped message can be reported by the first following assertion only, regardless of the result of that assertion.

In other words, lifetime of `UNSCOPED_INFO` is limited by the following assertion (or by the end of test case/section, whichever comes first) whereas lifetime of `INFO` is limited by its own scope.

These differences make this macro useful for reporting information from helper functions or inner scopes. An example:

```cpp
void print_some_info() {
    UNSCOPED_INFO("Info from helper");
}

TEST_CASE("Baz") {
    print_some_info();
    for (int i = 0; i < 2; ++i) {
        UNSCOPED_INFO("The number is " << i);
    }
    CHECK(false);
}

TEST_CASE("Qux") {
    INFO("First info");
    UNSCOPED_INFO("First unscoped info");
    CHECK(false);

    INFO("Second info");
    UNSCOPED_INFO("Second unscoped info");
    CHECK(false);
}
```

"Baz" test case prints:
```
Info from helper
The number is 0
The number is 1
```

With "Qux" test case, two messages will be printed when the first `CHECK` fails:
```
First info
First unscoped info
```

"First unscoped info" message will be cleared after the first `CHECK`, while "First info" message will persist until the end of the test case. Therefore, when the second `CHECK` fails, three messages will be printed:
```
First info
Second info
Second unscoped info
```

## Streaming macros

All these macros allow heterogeneous sequences of values to be streaming using the insertion operator (```<<```) in the same way that std::ostream, std::cout, etc support it.

E.g.:
```c++
INFO( "The number is " << i );
```

(Note that there is no initial ```<<``` - instead the insertion sequence is placed in parentheses.)
These macros come in three forms:

**INFO(** _message expression_ **)**

The message is logged to a buffer, but only reported with next assertions that are logged. This allows you to log contextual information in case of failures which is not shown during a successful test run (for the console reporter, without -s). Messages are removed from the buffer at the end of their scope, so may be used, for example, in loops.

_Note that in Catch2 2.x.x `INFO` can be used without a trailing semicolon as there is a trailing semicolon inside macro.
This semicolon will be removed with next major version. It is highly advised to use a trailing semicolon after `INFO` macro._

**UNSCOPED_INFO(** _message expression_ **)**

> [Introduced](https://github.com/catchorg/Catch2/issues/1522) in Catch2 2.7.0.

Similar to `INFO`, but messages are not limited to their own scope: They are removed from the buffer after each assertion, section or test case, whichever comes first.

**WARN(** _message expression_ **)**

The message is always reported but does not fail the test.

**SUCCEED(** _message expression_ **)**

The message is reported and the test case succeeds.

**FAIL(** _message expression_ **)**

The message is reported and the test case fails.

**FAIL_CHECK(** _message expression_ **)**

AS `FAIL`, but does not abort the test

## Quickly capture value of variables or expressions

**CAPTURE(** _expression1_, _expression2_, ... **)**

Sometimes you just want to log a value of variable, or expression. For
convenience, we provide the `CAPTURE` macro, that can take a variable,
or an expression, and prints out that variable/expression and its value
at the time of capture.

e.g. `CAPTURE( theAnswer );` will log message "theAnswer := 42", while
```cpp
int a = 1, b = 2, c = 3;
CAPTURE( a, b, c, a + b, c > b, a == 1);
```
will log a total of 6 messages:
```
a := 1
b := 2
c := 3
a + b := 3
c > b := true
a == 1 := true
```

You can also capture expressions that use commas inside parentheses
(e.g. function calls), brackets, or braces (e.g. initializers). To
properly capture expression that contains template parameters list
(in other words, it contains commas between angle brackets), you need
to enclose the expression inside parentheses:
`CAPTURE( (std::pair<int, int>{1, 2}) );`


---

[Home](Readme.md#top)



docs/matchers.md
--------------------------------------
<a id="top"></a>
# Matchers

**Contents**<br>
[Using Matchers](#using-matchers)<br>
[Built-in matchers](#built-in-matchers)<br>
[Writing custom matchers (old style)](#writing-custom-matchers-old-style)<br>
[Writing custom matchers (new style)](#writing-custom-matchers-new-style)<br>

Matchers, as popularized by the [Hamcrest](https://en.wikipedia.org/wiki/Hamcrest)
framework are an alternative way to write assertions, useful for tests
where you work with complex types or need to assert more complex
properties. Matchers are easily composable and users can write their
own and combine them with the Catch2-provided matchers seamlessly.


## Using Matchers

Matchers are most commonly used in tandem with the `REQUIRE_THAT` or
`CHECK_THAT` macros. The `REQUIRE_THAT` macro takes two arguments,
the first one is the input (object/value) to test, the second argument
is the matcher itself.

For example, to assert that a string ends with the "as a service"
substring, you can write the following assertion

```cpp
using Catch::Matchers::EndsWith;

REQUIRE_THAT( getSomeString(), EndsWith("as a service") );
```

Individual matchers can also be combined using the C++ logical
operators, that is `&&`, `||`, and `!`, like so:

```cpp
using Catch::Matchers::EndsWith;
using Catch::Matchers::ContainsSubstring;

REQUIRE_THAT( getSomeString(),
              EndsWith("as a service") && ContainsSubstring("web scale"));
```

The example above asserts that the string returned from `getSomeString`
_both_ ends with the suffix "as a service" _and_ contains the string
"web scale" somewhere.


Both of the string matchers used in the examples above live in the
`catch_matchers_string.hpp` header, so to compile the code above also
requires `#include <catch2/matchers/catch_matchers_string.hpp>`.

### Combining operators and lifetimes

**IMPORTANT**: The combining operators do not take ownership of the
matcher objects being combined.

This means that if you store combined matcher object, you have to ensure
that the individual matchers being combined outlive the combined matcher.
Note that the negation matcher from `!` also counts as combining matcher
for this.

Explained on an example, this is fine
```cpp
CHECK_THAT(value, WithinAbs(0, 2e-2) && !WithinULP(0., 1));
```

and so is this
```cpp
auto is_close_to_zero = WithinAbs(0, 2e-2);
auto is_zero          = WithinULP(0., 1);

CHECK_THAT(value, is_close_to_zero && !is_zero);
```

but this is not
```cpp
auto is_close_to_zero = WithinAbs(0, 2e-2);
auto is_zero          = WithinULP(0., 1);
auto is_close_to_but_not_zero = is_close_to_zero && !is_zero;

CHECK_THAT(a_value, is_close_to_but_not_zero); // UAF
```

because `!is_zero` creates a temporary instance of Negation matcher,
which the `is_close_to_but_not_zero` refers to. After the line ends,
the temporary is destroyed and the combined `is_close_to_but_not_zero`
matcher now refers to non-existent object, so using it causes use-after-free.


## Built-in matchers

Every matcher provided by Catch2 is split into 2 parts, a factory
function that lives in the `Catch::Matchers` namespace, and the actual
matcher type that is in some deeper namespace and should not be used by
the user. In the examples above, we used `Catch::Matchers::Contains`.
This is the factory function for the
`Catch::Matchers::StdString::ContainsMatcher` type that does the actual
matching.

Out of the box, Catch2 provides the following matchers:


### `std::string` matchers

Catch2 provides 5 different matchers that work with `std::string`,
* `StartsWith(std::string str, CaseSensitive)`,
* `EndsWith(std::string str, CaseSensitive)`,
* `ContainsSubstring(std::string str, CaseSensitive)`,
* `Equals(std::string str, CaseSensitive)`, and
* `Matches(std::string str, CaseSensitive)`.

The first three should be fairly self-explanatory, they succeed if
the argument starts with `str`, ends with `str`, or contains `str`
somewhere inside it.

The `Equals` matcher matches a string if (and only if) the argument
string is equal to `str`.

Finally, the `Matches` matcher performs an ECMAScript regex match using
`str` against the argument string. It is important to know that
the match is performed against the string as a whole, meaning that
the regex `"abc"` will not match input string `"abcd"`. To match
`"abcd"`, you need to use e.g. `"abc.*"` as your regex.

The second argument sets whether the matching should be case-sensitive
or not. By default, it is case-sensitive.

> `std::string` matchers live in `catch2/matchers/catch_matchers_string.hpp`


### Vector matchers

_Vector matchers have been deprecated in favour of the generic
range matchers with the same functionality._

Catch2 provides 5 built-in matchers that work on `std::vector`.

These are

 * `Contains` which checks whether a specified vector is present in the result
 * `VectorContains` which checks whether a specified element is present in the result
 * `Equals` which checks whether the result is exactly equal (order matters) to a specific vector
 * `UnorderedEquals` which checks whether the result is equal to a specific vector under a permutation
 * `Approx` which checks whether the result is "approx-equal" (order matters, but comparison is done via `Approx`) to a specific vector
> Approx matcher was [introduced](https://github.com/catchorg/Catch2/issues/1499) in Catch2 2.7.2.

An example usage:
```cpp
    std::vector<int> some_vec{ 1, 2, 3 };
    REQUIRE_THAT(some_vec, Catch::Matchers::UnorderedEquals(std::vector<int>{ 3, 2, 1 }));
```

This assertions will pass, because the elements given to the matchers
are a permutation of the ones in `some_vec`.

> vector matchers live in `catch2/matchers/catch_matchers_vector.hpp`


### Floating point matchers

Catch2 provides 4 matchers that target floating point numbers. These
are:

* `WithinAbs(double target, double margin)`,
* `WithinULP(FloatingPoint target, uint64_t maxUlpDiff)`, and
* `WithinRel(FloatingPoint target, FloatingPoint eps)`.
* `IsNaN()`

> `WithinRel` matcher was introduced in Catch2 2.10.0

> `IsNaN` matcher was introduced in Catch2 3.3.2.

The first three serve to compare two floating pointe numbers. For more
details about how they work, read [the docs on comparing floating point
numbers](comparing-floating-point-numbers.md#floating-point-matchers).

`IsNaN` then does exactly what it says on the tin. It matches the input
if it is a NaN (Not a Number). The advantage of using it over just plain
`REQUIRE(std::isnan(x))`, is that if the check fails, with `REQUIRE` you
won't see the value of `x`, but with `REQUIRE_THAT(x, IsNaN())`, you will.


### Miscellaneous matchers

Catch2 also provides some matchers and matcher utilities that do not
quite fit into other categories.

The first one of them is the `Predicate(Callable pred, std::string description)`
matcher. It creates a matcher object that calls `pred` for the provided
argument. The `description` argument allows users to set what the
resulting matcher should self-describe as if required.

Do note that you will need to explicitly specify the type of the
argument, like in this example:

```cpp
REQUIRE_THAT("Hello olleH",
             Predicate<std::string>(
                 [] (std::string const& str) -> bool { return str.front() == str.back(); },
                 "First and last character should be equal")
);
```

> the predicate matcher lives in `catch2/matchers/catch_matchers_predicate.hpp`


The other miscellaneous matcher utility is exception matching.


#### Matching exceptions

Because exceptions are a bit special, Catch2 has a separate macro for them.


The basic form is

```
REQUIRE_THROWS_MATCHES(expr, ExceptionType, Matcher)
```

and it checks that the `expr` throws an exception, that exception is derived
from the `ExceptionType` type, and then `Matcher::match` is called on
the caught exception.

> `REQUIRE_THROWS_MATCHES` macro lives in `catch2/matchers/catch_matchers.hpp`

For one-off checks you can use the `Predicate` matcher above, e.g.

```cpp
REQUIRE_THROWS_MATCHES(parse(...),
                       parse_error,
                       Predicate<parse_error>([] (parse_error const& err) -> bool { return err.line() == 1; })
);
```

but if you intend to thoroughly test your error reporting, I recommend
defining a specialized matcher.


Catch2 also provides 2 built-in matchers for checking the error message
inside an exception (it must be derived from `std::exception`):
* `Message(std::string message)`.
* `MessageMatches(Matcher matcher)`.

> `MessageMatches` was [introduced](https://github.com/catchorg/Catch2/pull/2570) in Catch2 3.3.0

`Message` checks that the exception's
message, as returned from `what` is exactly equal to `message`.

`MessageMatches` applies the provided matcher on the exception's
message, as returned from `what`. This is useful in conjunctions with the `std::string` matchers (e.g. `StartsWith`)

Example use:
```cpp
REQUIRE_THROWS_MATCHES(throwsDerivedException(),  DerivedException,  Message("DerivedException::what"));
REQUIRE_THROWS_MATCHES(throwsDerivedException(),  DerivedException,  MessageMatches(StartsWith("DerivedException")));
```

> the exception message matchers live in `catch2/matchers/catch_matchers_exception.hpp`


### Generic range Matchers

> Generic range matchers were introduced in Catch2 3.0.1

Catch2 also provides some matchers that use the new style matchers
definitions to handle generic range-like types. These are:

* `IsEmpty()`
* `SizeIs(size_t target_size)`
* `SizeIs(Matcher size_matcher)`
* `Contains(T&& target_element, Comparator = std::equal_to<>{})`
* `Contains(Matcher element_matcher)`
* `AllMatch(Matcher element_matcher)`
* `AnyMatch(Matcher element_matcher)`
* `NoneMatch(Matcher element_matcher)`
* `AllTrue()`, `AnyTrue()`, `NoneTrue()`
* `RangeEquals(TargetRangeLike&&, Comparator = std::equal_to<>{})`
* `UnorderedRangeEquals(TargetRangeLike&&, Comparator = std::equal_to<>{})`

> `IsEmpty`, `SizeIs`, `Contains` were introduced in Catch2 3.0.1

> `All/Any/NoneMatch` were introduced in Catch2 3.0.1

> `All/Any/NoneTrue` were introduced in Catch2 3.1.0

> `RangeEquals` and `UnorderedRangeEquals` matchers were [introduced](https://github.com/catchorg/Catch2/pull/2377) in Catch2 3.3.0

`IsEmpty` should be self-explanatory. It successfully matches objects
that are empty according to either `std::empty`, or ADL-found `empty`
free function.

`SizeIs` checks range's size. If constructed with `size_t` arg, the
matchers accepts ranges whose size is exactly equal to the arg. If
constructed from another matcher, then the resulting matcher accepts
ranges whose size is accepted by the provided matcher.

`Contains` accepts ranges that contain specific element. There are
again two variants, one that accepts the desired element directly,
in which case a range is accepted if any of its elements is equal to
the target element. The other variant is constructed from a matcher,
in which case a range is accepted if any of its elements is accepted
by the provided matcher.

`AllMatch`, `NoneMatch`, and `AnyMatch` match ranges for which either
all, none, or any of the contained elements matches the given matcher,
respectively.

`AllTrue`, `NoneTrue`, and `AnyTrue` match ranges for which either
all, none, or any of the contained elements are `true`, respectively.
It works for ranges of `bool`s and ranges of elements (explicitly)
convertible to `bool`.

`RangeEquals` compares the range that the matcher is constructed with
(the "target range") against the range to be tested, element-wise. The
match succeeds if all elements from the two ranges compare equal (using
`operator==` by default). The ranges do not need to be the same type,
and the element types do not need to be the same, as long as they are
comparable. (e.g. you may compare `std::vector<int>` to `std::array<char>`).

`UnorderedRangeEquals` is similar to `RangeEquals`, but the order
does not matter. For example "1, 2, 3" would match "3, 2, 1", but not
"1, 1, 2, 3" As with `RangeEquals`, `UnorderedRangeEquals` compares
the individual elements using `operator==` by default.

Both `RangeEquals` and `UnorderedRangeEquals` optionally accept a
predicate which can be used to compare the containers element-wise.

To check a container elementwise against a given matcher, use
`AllMatch`.


## Writing custom matchers (old style)

The old style of writing matchers has been introduced back in Catch
Classic. To create an old-style matcher, you have to create your own
type that derives from `Catch::Matchers::MatcherBase<ArgT>`, where
`ArgT` is the type your matcher works for. Your type has to override
two methods, `bool match(ArgT const&) const`,
and `std::string describe() const`.

As the name suggests, `match` decides whether the provided argument
is matched (accepted) by the matcher. `describe` then provides a
human-oriented description of what the matcher does.

We also recommend that you create factory function, just like Catch2
does, but that is mostly useful for template argument deduction for
templated matchers (assuming you do not have CTAD available).

To combine these into an example, let's say that you want to write
a matcher that decides whether the provided argument is a number
within certain range. We will call it `IsBetweenMatcher<T>`:

```c++
#include <catch2/catch_test_macros.hpp>
#include <catch2/matchers/catch_matchers.hpp>
// ...


template <typename T>
class IsBetweenMatcher : public Catch::Matchers::MatcherBase<T> {
    T m_begin, m_end;
public:
    IsBetweenMatcher(T begin, T end) : m_begin(begin), m_end(end) {}

    bool match(T const& in) const override {
        return in >= m_begin && in <= m_end;
    }

    std::string describe() const override {
        std::ostringstream ss;
        ss << "is between " << m_begin << " and " << m_end;
        return ss.str();
    }
};

template <typename T>
IsBetweenMatcher<T> IsBetween(T begin, T end) {
    return { begin, end };
}

// ...

TEST_CASE("Numbers are within range") {
    // infers `double` for the argument type of the matcher
    CHECK_THAT(3., IsBetween(1., 10.));
    // infers `int` for the argument type of the matcher
    CHECK_THAT(100, IsBetween(1, 10));
}
```

Obviously, the code above can be improved somewhat, for example you
might want to `static_assert` over the fact that `T` is an arithmetic
type... or generalize the matcher to cover any type for which the user
can provide a comparison function object.

Note that while any matcher written using the old style can also be
written using the new style, combining old style matchers should
generally compile faster. Also note that you can combine old and new
style matchers arbitrarily.

> `MatcherBase` lives in `catch2/matchers/catch_matchers.hpp`


## Writing custom matchers (new style)

> New style matchers were introduced in Catch2 3.0.1

To create a new-style matcher, you have to create your own type that
derives from `Catch::Matchers::MatcherGenericBase`. Your type has to
also provide two methods, `bool match( ... ) const` and overridden
`std::string describe() const`.

Unlike with old-style matchers, there are no requirements on how
the `match` member function takes its argument. This means that the
argument can be taken by value or by mutating reference, but also that
the matcher's `match` member function can be templated.

This allows you to write more complex matcher, such as a matcher that
can compare one range-like (something that responds to `begin` and
`end`) object to another, like in the following example:

```cpp
#include <catch2/catch_test_macros.hpp>
#include <catch2/matchers/catch_matchers_templated.hpp>
// ...

template<typename Range>
struct EqualsRangeMatcher : Catch::Matchers::MatcherGenericBase {
    EqualsRangeMatcher(Range const& range):
        range{ range }
    {}

    template<typename OtherRange>
    bool match(OtherRange const& other) const {
        using std::begin; using std::end;

        return std::equal(begin(range), end(range), begin(other), end(other));
    }

    std::string describe() const override {
        return "Equals: " + Catch::rangeToString(range);
    }

private:
    Range const& range;
};

template<typename Range>
auto EqualsRange(const Range& range) -> EqualsRangeMatcher<Range> {
    return EqualsRangeMatcher<Range>{range};
}

TEST_CASE("Combining templated matchers", "[matchers][templated]") {
    std::array<int, 3> container{{ 1,2,3 }};

    std::array<int, 3> a{{ 1,2,3 }};
    std::vector<int> b{ 0,1,2 };
    std::list<int> c{ 4,5,6 };

    REQUIRE_THAT(container, EqualsRange(a) || EqualsRange(b) || EqualsRange(c));
}
```

Do note that while you can rewrite any matcher from the old style to
a new style matcher, combining new style matchers is more expensive
in terms of compilation time. Also note that you can combine old style
and new style matchers arbitrarily.

> `MatcherGenericBase` lives in `catch2/matchers/catch_matchers_templated.hpp`


---

[Home](Readme.md#top)



docs/migrate-v2-to-v3.md
--------------------------------------
<a id="top"></a>
# Migrating from v2 to v3

v3 is the next major version of Catch2 and brings three significant changes:
 * Catch2 is now split into multiple headers
 * Catch2 is now compiled as a static library
 * C++14 is the minimum required C++ version

There are many reasons why we decided to go from the old single-header
distribution model to a more standard library distribution model. The
big one is compile-time performance, but moving over to a split header
distribution model also improves the future maintainability and
extendability of the codebase. For example v3 adds a new kind of matchers
without impacting the compilation times of users that do not use matchers
in their tests. The new model is also more friendly towards package
managers, such as vcpkg and Conan.

The result of this move is a significant improvement in compilation
times, e.g. the inclusion overhead of Catch2 in the common case has been
reduced by roughly 80%. The improved ease of maintenance also led to
various runtime performance improvements and the introduction of new features.
For details, look at [the release notes of 3.0.1](release-notes.md#301).

_Note that we still provide one header + one translation unit (TU)
distribution but do not consider it the primarily supported option. You
should also expect that the compilation times will be worse if you use
this option._


## How to migrate projects from v2 to v3

To migrate to v3, there are two basic approaches to do so.

1. Use `catch_amalgamated.hpp` and `catch_amalgamated.cpp`.
2. Build Catch2 as a proper (static) library, and move to piecewise headers

Doing 1 means downloading the [amalgamated header](/extras/catch_amalgamated.hpp)
and the [amalgamated sources](/extras/catch_amalgamated.cpp) from `extras`,
dropping them into your test project, and rewriting your includes from
`<catch2/catch.hpp>` to `"catch_amalgamated.hpp"` (or something similar,
based on how you set up your paths).

The disadvantage of using this approach are increased compilation times,
at least compared to the second approach, but it does let you avoid
dealing with consuming libraries in your build system of choice.


However, we recommend doing 2, and taking extra time to migrate to v3
properly. This lets you reap the benefits of significantly improved
compilation times in the v3 version. The basic steps to do so are:

1. Change your CMakeLists.txt to link against `Catch2WithMain` target if
you use Catch2's default main. (If you do not, keep linking against
the `Catch2` target.). If you use pkg-config, change `pkg-config catch2` to
`pkg-config catch2-with-main`.
2. Delete TU with `CATCH_CONFIG_RUNNER` or `CATCH_CONFIG_MAIN` defined,
as it is no longer needed.
3. Change `#include <catch2/catch.hpp>` to `#include <catch2/catch_all.hpp>`
4. Check that everything compiles. You might have to modify namespaces,
or perform some other changes (see the
[Things that can break during porting](#things-that-can-break-during-porting)
section for the most common things).
5. Start migrating your test TUs from including `<catch2/catch_all.hpp>`
to piecemeal includes. You will likely want to start by including
`<catch2/catch_test_macros.hpp>`, and then go from there. (see
[other notes](#other-notes) for further ideas)

## Other notes

* The main test include is now `<catch2/catch_test_macros.hpp>`

* Big "subparts" like Matchers, or Generators, have their own folder, and
also their own "big header", so if you just want to include all matchers,
you can include `<catch2/matchers/catch_matchers_all.hpp>`,
or `<catch2/generators/catch_generators_all.hpp>`


## Things that can break during porting

* The namespaces of Matchers were flattened and cleaned up.

Matchers are no longer declared deep within an internal namespace and
then brought up into `Catch` namespace. All Matchers now live in the
`Catch::Matchers` namespace.

* The `Contains` string matcher was renamed to `ContainsSubstring`.

* The reporter interfaces changed in a breaking manner.

If you are using a custom reporter or listener, you will likely need to
modify them to conform to the new interfaces. Unlike before in v2,
the [interfaces](reporters.md#top) and the [events](reporter-events.md#top)
are now documented.


---

[Home](Readme.md#top)



docs/opensource-users.md
--------------------------------------
<a id="top"></a>
# Open Source projects using Catch2

Catch2 is great for open source. It is licensed under the [Boost Software
License (BSL)](../LICENSE.txt), has no further dependencies and supports
two file distribution.

As a result, Catch2 is used for testing in many different Open Source
projects. This page lists at least some of them, even though it will
obviously never be complete (and does not have the ambition to be
complete). Note that the list below is intended to be in alphabetical
order, to avoid implications of relative importance of the projects.

_Please only add projects here if you are their maintainer, or have the
maintainer's explicit consent._


## Libraries & Frameworks

### [accessorpp](https://github.com/wqking/accessorpp)
C++ library for implementing property and data binding.

### [alpaka](https://github.com/alpaka-group/alpaka)
A header-only C++14 abstraction library for accelerator development.

### [ApprovalTests.cpp](https://github.com/approvals/ApprovalTests.cpp)
C++11 implementation of Approval Tests, for quick, convenient testing of legacy code.

### [args](https://github.com/Taywee/args)
A simple header-only C++ argument parser library.

### [Azmq](https://github.com/zeromq/azmq)
Boost Asio style bindings for ZeroMQ.

### [Cataclysm: Dark Days Ahead](https://github.com/CleverRaven/Cataclysm-DDA)
Post-apocalyptic survival RPG.

### [ChaiScript](https://github.com/ChaiScript/ChaiScript)
A, header-only, embedded scripting language designed from the ground up to directly target C++ and take advantage of modern C++ development techniques.

### [ChakraCore](https://github.com/Microsoft/ChakraCore)
The core part of the Chakra JavaScript engine that powers Microsoft Edge.

### [Clara](https://github.com/philsquared/Clara)
A, single-header-only, type-safe, command line parser - which also prints formatted usage strings.

### [Couchbase-lite-core](https://github.com/couchbase/couchbase-lite-core)
The next-generation core storage and query engine for Couchbase Lite.

### [cppcodec](https://github.com/tplgy/cppcodec)
Header-only C++11 library to encode/decode base64, base64url, base32, base32hex and hex (a.k.a. base16) as specified in RFC 4648, plus Crockford's base32.

### [DtCraft](https://github.com/twhuang-uiuc/DtCraft)
A High-performance Cluster Computing Engine.

### [eventpp](https://github.com/wqking/eventpp)
C++ event library for callbacks, event dispatcher, and event queue. With eventpp you can easily implement signal and slot mechanism, publisher and subscriber pattern, or observer pattern.

### [forest](https://github.com/xorz57/forest)
Template Library of Tree Data Structures.

### [Fuxedo](https://github.com/fuxedo/fuxedo)
Open source Oracle Tuxedo-like XATMI middleware for C and C++.

### [HIP CPU Runtime](https://github.com/ROCm-Developer-Tools/HIP-CPU)
A header-only library that allows CPUs to execute unmodified HIP code. It is generic and does not assume a particular CPU vendor or architecture.

### [Inja](https://github.com/pantor/inja)
A header-only template engine for modern C++.

### [LLAMA](https://github.com/alpaka-group/llama)
A C++17 template header-only library for the abstraction of memory access patterns.

### [libcluon](https://github.com/chrberger/libcluon)
A single-header-only library written in C++14 to glue distributed software components (UDP, TCP, shared memory) supporting natively Protobuf, LCM/ZCM, MsgPack, and JSON for dynamic message transformations in-between.

### [MNMLSTC Core](https://github.com/mnmlstc/core)
A small and easy to use C++11 library that adds a functionality set that will be available in C++14 and later, as well as some useful additions.

### [nanodbc](https://github.com/lexicalunit/nanodbc/)
A small C++ library wrapper for the native C ODBC API.

### [Nonius](https://github.com/libnonius/nonius)
A header-only framework for benchmarking small snippets of C++ code.

### [OpenALpp](https://github.com/Laguna1989/OpenALpp)
A modern OOP C++14 audio library built on OpenAL for Windows, Linux and web (emscripten).

### [polymorphic_value](https://github.com/jbcoe/polymorphic_value)
A polymorphic value-type for C++.

### [Ppconsul](https://github.com/oliora/ppconsul)
A C++ client library for Consul. Consul is a distributed tool for discovering and configuring services in your infrastructure.

### [Reactive-Extensions/ RxCpp](https://github.com/Reactive-Extensions/RxCpp)
A library of algorithms for values-distributed-in-time.

### [SFML](https://github.com/SFML/SFML)
Simple and Fast Multimedia Library.

### [SOCI](https://github.com/SOCI/soci)
The C++ Database Access Library.

### [TextFlowCpp](https://github.com/philsquared/textflowcpp)
A small, single-header-only, library for wrapping and composing columns of text.

### [thor](https://github.com/xorz57/thor)
Wrapper Library for CUDA.

### [toml++](https://github.com/marzer/tomlplusplus)
A header-only TOML parser and serializer for modern C++.

### [Trompeloeil](https://github.com/rollbear/trompeloeil)
A thread-safe header-only mocking framework for C++14.

### [wxWidgets](https://www.wxwidgets.org/)
Cross-Platform C++ GUI Library.

### [xmlwrapp](https://github.com/vslavik/xmlwrapp)
C++ XML parsing library using libxml2.

## Applications & Tools

### [App Mesh](https://github.com/laoshanxi/app-mesh)
A high available cloud native micro-service application management platform implemented by modern C++.

### [ArangoDB](https://github.com/arangodb/arangodb)
ArangoDB is a native multi-model database with flexible data models for documents, graphs, and key-values.

### [Cytopia](https://github.com/CytopiaTeam/Cytopia)
Cytopia is a free, open source retro pixel-art city building game with a big focus on mods. It utilizes a custom isometric rendering engine based on SDL2.

### [d-SEAMS](https://github.com/d-SEAMS/seams-core)
Open source molecular dynamics simulation structure analysis suite of tools in modern C++.

### [Giada - Your Hardcore Loop Machine](https://github.com/monocasual/giada)
Minimal, open-source and cross-platform audio tool for live music production.

### [MAME](https://github.com/mamedev/mame)
MAME originally stood for Multiple Arcade Machine Emulator.

### [Newsbeuter](https://github.com/akrennmair/newsbeuter)
Newsbeuter is an open-source RSS/Atom feed reader for text terminals.

### [PopHead](https://github.com/SPC-Some-Polish-Coders/PopHead)
A 2D, Zombie, RPG game which is being made on our own engine.

### [raspigcd](https://github.com/pantadeusz/raspigcd)
Low level CLI app and library for execution of GCODE on Raspberry Pi without any additional microcontrollers (just RPi + Stepsticks).

### [SpECTRE](https://github.com/sxs-collaboration/spectre)
SpECTRE is a code for multi-scale, multi-physics problems in astrophysics and gravitational physics.

### [Standardese](https://github.com/foonathan/standardese)
Standardese aims to be a nextgen Doxygen.

---

[Home](Readme.md#top)



docs/other-macros.md
--------------------------------------
<a id="top"></a>
# Other macros

This page serves as a reference for macros that are not documented
elsewhere. For now, these macros are separated into 2 rough categories,
"assertion related macros" and "test case related macros".

## Assertion related macros

* `CHECKED_IF` and `CHECKED_ELSE`

`CHECKED_IF( expr )` is an `if` replacement, that also applies Catch2's
stringification machinery to the _expr_ and records the result. As with
`if`, the block after a `CHECKED_IF` is entered only if the expression
evaluates to `true`. `CHECKED_ELSE( expr )` work similarly, but the block
is entered only if the _expr_ evaluated to `false`.

> `CHECKED_X` macros were changed to not count as failure in Catch2 3.0.1.

Example:
```cpp
int a = ...;
int b = ...;
CHECKED_IF( a == b ) {
    // This block is entered when a == b
} CHECKED_ELSE ( a == b ) {
    // This block is entered when a != b
}
```

* `CHECK_NOFAIL`

`CHECK_NOFAIL( expr )` is a variant of `CHECK` that does not fail the test
case if _expr_ evaluates to `false`. This can be useful for checking some
assumption, that might be violated without the test necessarily failing.

Example output:
```
main.cpp:6:
FAILED - but was ok:
  CHECK_NOFAIL( 1 == 2 )

main.cpp:7:
PASSED:
  CHECK( 2 == 2 )
```

* `SUCCEED`

`SUCCEED( msg )` is mostly equivalent with `INFO( msg ); REQUIRE( true );`.
In other words, `SUCCEED` is for cases where just reaching a certain line
means that the test has been a success.

Example usage:
```cpp
TEST_CASE( "SUCCEED showcase" ) {
    int I = 1;
    SUCCEED( "I is " << I );
}
```

* `STATIC_REQUIRE` and `STATIC_CHECK`

> `STATIC_REQUIRE` was [introduced](https://github.com/catchorg/Catch2/issues/1362) in Catch2 2.4.2.

`STATIC_REQUIRE( expr )` is a macro that can be used the same way as a
`static_assert`, but also registers the success with Catch2, so it is
reported as a success at runtime. The whole check can also be deferred
to the runtime, by defining `CATCH_CONFIG_RUNTIME_STATIC_REQUIRE` before
including the Catch2 header.

Example:
```cpp
TEST_CASE("STATIC_REQUIRE showcase", "[traits]") {
    STATIC_REQUIRE( std::is_void<void>::value );
    STATIC_REQUIRE_FALSE( std::is_void<int>::value );
}
```

> `STATIC_CHECK` was [introduced](https://github.com/catchorg/Catch2/pull/2318) in Catch2 3.0.1.

`STATIC_CHECK( expr )` is equivalent to `STATIC_REQUIRE( expr )`, with the
difference that when `CATCH_CONFIG_RUNTIME_STATIC_REQUIRE` is defined, it
becomes equivalent to `CHECK` instead of `REQUIRE`.

Example:
```cpp
TEST_CASE("STATIC_CHECK showcase", "[traits]") {
    STATIC_CHECK( std::is_void<void>::value );
    STATIC_CHECK_FALSE( std::is_void<int>::value );
}
```

## Test case related macros

* `REGISTER_TEST_CASE`

`REGISTER_TEST_CASE( function, description )` let's you register
a `function` as a test case. The function has to have `void()` signature,
the description can contain both name and tags.

Example:
```cpp
REGISTER_TEST_CASE( someFunction, "ManuallyRegistered", "[tags]" );
```

_Note that the registration still has to happen before Catch2's session
is initiated. This means that it either needs to be done in a global
constructor, or before Catch2's session is created in user's own main._


* `DYNAMIC_SECTION`

> Introduced in Catch2 2.3.0.

`DYNAMIC_SECTION` is a `SECTION` where the user can use `operator<<` to
create the final name for that section. This can be useful with e.g.
generators, or when creating a `SECTION` dynamically, within a loop.

Example:
```cpp
TEST_CASE( "looped SECTION tests" ) {
    int a = 1;

    for( int b = 0; b < 10; ++b ) {
        DYNAMIC_SECTION( "b is currently: " << b ) {
            CHECK( b > a );
        }
    }
}
```



docs/own-main.md
--------------------------------------
<a id="top"></a>
# Supplying main() yourself

**Contents**<br>
[Let Catch2 take full control of args and config](#let-catch2-take-full-control-of-args-and-config)<br>
[Amending the Catch2 config](#amending-the-catch2-config)<br>
[Adding your own command line options](#adding-your-own-command-line-options)<br>
[Version detection](#version-detection)<br>

The easiest way to use Catch2 is to use its own `main` function, and let
it handle the command line arguments. This is done by linking against
Catch2Main library, e.g. through the [CMake target](cmake-integration.md#cmake-targets),
or pkg-config files.

If you want to provide your own `main`, then you should link against
the static library (target) only, without the main part. You will then
have to write your own `main` and call into Catch2 test runner manually.

Below are some basic recipes on what you can do supplying your own main.


## Let Catch2 take full control of args and config

This is useful if you just need to have code that executes before/after
Catch2 runs tests.

```cpp
#include <catch2/catch_session.hpp>

int main( int argc, char* argv[] ) {
  // your setup ...

  int result = Catch::Session().run( argc, argv );

  // your clean-up...

  return result;
}
```

_Note that if you only want to run some set up before tests are run, it
might be simpler to use [event listeners](event-listeners.md#top) instead._


## Amending the Catch2 config

If you want Catch2 to process command line arguments, but also want to
programmatically change the resulting configuration of Catch2 run,
you can do it in two ways:

```c++
int main( int argc, char* argv[] ) {
  Catch::Session session; // There must be exactly one instance

  // writing to session.configData() here sets defaults
  // this is the preferred way to set them

  int returnCode = session.applyCommandLine( argc, argv );
  if( returnCode != 0 ) // Indicates a command line error
        return returnCode;

  // writing to session.configData() or session.Config() here
  // overrides command line args
  // only do this if you know you need to

  int numFailed = session.run();

  // numFailed is clamped to 255 as some unices only use the lower 8 bits.
  // This clamping has already been applied, so just return it here
  // You can also do any post run clean-up here
  return numFailed;
}
```

If you want full control of the configuration, don't call `applyCommandLine`.


## Adding your own command line options

You can add new command line options to Catch2, by composing the premade
CLI parser (called Clara), and add your own options.

```cpp
int main( int argc, char* argv[] ) {
  Catch::Session session; // There must be exactly one instance

  int height = 0; // Some user variable you want to be able to set

  // Build a new parser on top of Catch2's
  using namespace Catch::Clara;
  auto cli
    = session.cli()           // Get Catch2's command line parser
    | Opt( height, "height" ) // bind variable to a new option, with a hint string
        ["-g"]["--height"]    // the option names it will respond to
        ("how high?");        // description string for the help output

  // Now pass the new composite back to Catch2 so it uses that
  session.cli( cli );

  // Let Catch2 (using Clara) parse the command line
  int returnCode = session.applyCommandLine( argc, argv );
  if( returnCode != 0 ) // Indicates a command line error
      return returnCode;

  // if set on the command line then 'height' is now set at this point
  if( height > 0 )
      std::cout << "height: " << height << std::endl;

  return session.run();
}
```

See the [Clara documentation](https://github.com/catchorg/Clara/blob/master/README.md)
for more details on how to use the Clara parser.


## Version detection

Catch2 provides a triplet of macros providing the header's version,

* `CATCH_VERSION_MAJOR`
* `CATCH_VERSION_MINOR`
* `CATCH_VERSION_PATCH`

these macros expand into a single number, that corresponds to the appropriate
part of the version. As an example, given single header version v2.3.4,
the macros would expand into `2`, `3`, and `4` respectively.


---

[Home](Readme.md#top)



docs/release-notes.md
--------------------------------------
<a id="top"></a>

# Release notes
**Contents**<br>
[3.7.1](#371)<br>
[3.7.0](#370)<br>
[3.6.0](#360)<br>
[3.5.4](#354)<br>
[3.5.3](#353)<br>
[3.5.2](#352)<br>
[3.5.1](#351)<br>
[3.5.0](#350)<br>
[3.4.0](#340)<br>
[3.3.2](#332)<br>
[3.3.1](#331)<br>
[3.3.0](#330)<br>
[3.2.1](#321)<br>
[3.2.0](#320)<br>
[3.1.1](#311)<br>
[3.1.0](#310)<br>
[3.0.1](#301)<br>
[2.13.7](#2137)<br>
[2.13.6](#2136)<br>
[2.13.5](#2135)<br>
[2.13.4](#2134)<br>
[2.13.3](#2133)<br>
[2.13.2](#2132)<br>
[2.13.1](#2131)<br>
[2.13.0](#2130)<br>
[2.12.4](#2124)<br>
[2.12.3](#2123)<br>
[2.12.2](#2122)<br>
[2.12.1](#2121)<br>
[2.12.0](#2120)<br>
[2.11.3](#2113)<br>
[2.11.2](#2112)<br>
[2.11.1](#2111)<br>
[2.11.0](#2110)<br>
[2.10.2](#2102)<br>
[2.10.1](#2101)<br>
[2.10.0](#2100)<br>
[2.9.2](#292)<br>
[2.9.1](#291)<br>
[2.9.0](#290)<br>
[2.8.0](#280)<br>
[2.7.2](#272)<br>
[2.7.1](#271)<br>
[2.7.0](#270)<br>
[2.6.1](#261)<br>
[2.6.0](#260)<br>
[2.5.0](#250)<br>
[2.4.2](#242)<br>
[2.4.1](#241)<br>
[2.4.0](#240)<br>
[2.3.0](#230)<br>
[2.2.3](#223)<br>
[2.2.2](#222)<br>
[2.2.1](#221)<br>
[2.2.0](#220)<br>
[2.1.2](#212)<br>
[2.1.1](#211)<br>
[2.1.0](#210)<br>
[2.0.1](#201)<br>
[Older versions](#older-versions)<br>
[Even Older versions](#even-older-versions)<br>


## 3.7.1

### Improvements
* Applied the JUnit reporter's optimization from last release to the SonarQube reporter
* Suppressed `-Wuseless-cast` in `CHECK_THROWS_MATCHES` (#2904)
* Standardize exit codes for various failures
  * Running no tests is now guaranteed to exit with 2 (without the `--allow-running-no-tests` flag)
  * All tests skipped is now always 4 (...)
  * Assertion failures are now always 42
  * and so on

### Fixes
* Fixed out-of-bounds access when the arg parser encounters single `-` as an argument (#2905)

### Miscellaneous
* Added `catch_config_prefix_messages.hpp` to meson build (#2903)
* `catch_discover_tests` now supports skipped tests (#2873)
  * You can get the old behaviour by calling `catch_discover_tests` with `SKIP_IS_FAILURE` option.


## 3.7.0

### Improvements
* Slightly improved compile times of benchmarks
* Made the resolution estimation in benchmarks slightly more precise
* Added new test case macro, `TEST_CASE_PERSISTENT_FIXTURE` (#2885, #1602)
  * Unlike `TEST_CASE_METHOD`, the same underlying instance is used for all partial runs of that test case
* **MASSIVELY** improved performance of the JUnit reporter when handling successful assertions (#2897)
  * For 1 test case and 10M assertions, the new reporter runs 3x faster and uses up only 8 MB of memory, while the old one needs 7 GB of memory.
* Reworked how output redirects works.
  * Combining a reporter writing to stdout with capturing reporter no longer leads to the capturing reporter seeing all of the other reporter's output.
  * The file based redirect no longer opens up a new temporary file for each partial test case run, so it will not run out of temporary files when running many tests in single process.

### Miscellaneous
* Better documentation for matchers on thrown exceptions (`REQUIRE_THROWS_MATCHES`)
* Improved `catch_discover_tests`'s handling of environment paths (#2878)
  * It won't reorder paths in `DL_PATHS` or `DYLD_FRAMEWORK_PATHS` args
  * It won't overwrite the environment paths for test discovery


## 3.6.0

### Fixes
* Fixed Windows ARM64 build by fixing the preprocessor condition guarding use `_umul128` intrinsic.
* Fixed Windows ARM64EC build by removing intrinsic pragma it does not understand. (#2858)
  * Why doesn't the x64-emulation build mode understand x64 pragmas? Don't ask me, ask the MSVC guys.
* Fixed the JUnit reporter sometimes crashing when reporting a fatal error. (#1210, #2855)
  * The binary will still exit, but through the original error, rather than secondary error inside the reporter.
  * The underlying fix applies to all reporters, not just the JUnit one, but only JUnit was currently causing troubles.

### Improvements
* Disable `-Wnon-virtual-dtor` in Decomposer and Matchers (#2854)
* `precision` in floating point stringmakers defaults to `max_digits10`.
  * This means that floating point values will be printed with enough precision to disambiguate any two floats.
* Column wrapping ignores ansi colour codes when calculating string width (#2833, #2849)
  * This makes the output much more readable when the provided messages contain colour codes.

### Miscellaneous
* Conan support improvements
  * `compatibility_cppstr` is set to False. (#2860)
    * This means that Conan won't let you mix library and project with different C++ standard settings.
  * The implementation library CMake target name through Conan is properly set to `Catch2::Catch2` (#2861)
* `SelfTest` target can be built through Bazel (#2857)


## 3.5.4

### Fixes
* Fixed potential compilation error when asked to generate random integers whose type did not match `std::(u)int*_t`.
  * This manifested itself when generating random `size_t`s on MacOS
* Added missing outlined destructor causing `Wdelete-incomplete` when compiling against libstdc++ in C++23 mode (#2852)
* Fixed regression where decomposing assertion with const instance of `std::foo_ordering` would not compile

### Improvements
* Reintroduced support for GCC 5 and 6 (#2836)
  * As with VS2017, if they start causing trouble again, they will be dropped again.
* Added workaround for targetting newest MacOS (Sonoma) using GCC (#2837, #2839)
* `CATCH_CONFIG_DEFAULT_REPORTER` can now be an arbitrary reporter spec
  * Previously it could only be a plain reporter name, so it was impossible to compile in custom arguments to the reporter.
* Improved performance of generating 64bit random integers by 20+%

### Miscellaneous
* Significantly improved Conan in-tree recipe (#2831)
* `DL_PATHS` in `catch_discover_tests` now supports multiple arguments (#2852, #2736)
* Fixed preprocessor logic for checking whether we expect reproducible floating point results in tests.
* Improved the floating point tests structure to avoid `Wunused` when the reproducibility tests are disabled (#2845)


## 3.5.3

### Fixes
* Fixed OOB access when computing filename tag (from the `-#` flag) for file without extension (#2798)
* Fixed the linking against `log` on Android to be `PRIVATE` (#2815)
* Fixed `Wuseless-cast` in benchmarking internals (#2823)

### Improvements
* Restored compatibility with VS2017 (#2792, #2822)
  * The baseline for Catch2 is still C++14 with some reasonable workarounds for specific compilers, so if VS2017 starts acting up again, the support will be dropped again.
* Suppressed clang-tidy's `bugprone-chained-comparison` in assertions (#2801)
* Improved the static analysis mode to evaluate arguments to `TEST_CASE` and `SECTION` (#2817)
  * Clang-tidy should no longer warn about runtime arguments to these macros being unused in static analysis mode.
  * Clang-tidy can warn on issues involved arguments to these macros.
* Added support for literal-zero detectors based on `consteval` constructors
  * This is required for compiling `REQUIRE((a <=> b) == 0)` against MSVC's stdlib.
  * Sadly, MSVC still cannot compile this assertion as it does not implement C++20 correctly.
  * You can use `clang-cl` with MSVC's stdlib instead.
  * If for some godforsaken reasons you want to understand this better, read the two relevant commits: [`dc51386b9fd61f99ea9c660d01867e6ad489b403`](https://github.com/catchorg/Catch2/commit/dc51386b9fd61f99ea9c660d01867e6ad489b403), and [`0787132fc82a75e3fb255aa9484ca1dc1eff2a30`](https://github.com/catchorg/Catch2/commit/0787132fc82a75e3fb255aa9484ca1dc1eff2a30).

### Miscellaneous
* Disabled tests for FP random generator reproducibility on non-SSE2 x86 targets (#2796)
* Modified the in-tree Conan recipe to support Conan 2 (#2805)


## 3.5.2

### Fixes
* Fixed `-Wsubobject-linkage` in the Console reporter (#2794)
* Fixed adding new CLI Options to lvalue parser using `|` (#2787)


## 3.5.1

### Improvements
* Significantly improved performance of the CLI parsing.
  * This includes the cost of preparing the CLI parser, so Catch2's binaries start much faster.

### Miscellaneous
* Added support for Bazel modules (#2781)
* Added CMake option to disable the build reproducibility settings (#2785)
* Added `log` library linking to the Meson build (#2784)


## 3.5.0

### Improvements
* Introduced `CATCH_CONFIG_PREFIX_MESSAGES` to prefix only logging macros (#2544)
  * This means `INFO`, `UNSCOPED_INFO`, `WARN` and `CAPTURE`.
* Section hints in static analysis mode are now `const`
  * This prevents Clang-Tidy from complaining about `misc-const-correctness`.
* `from_range` generator supports C arrays and ranges that require ADL (#2737)
* Stringification support for `std::optional` now also includes `std::nullopt` (#2740)
* The Console reporter flushes output after writing benchmark runtime estimate.
  * This means that you can immediately see for how long the benchmark is expected to run.
* Added workaround to enable compilation with ICC 19.1 (#2551, #2766)
* Compiling Catch2 for XBox should work out of the box (#2772)
  * Catch2 should automatically disable getenv when compiled for XBox.
* Compiling Catch2 with exceptions disabled no longer triggers `Wunused-function` (#2726)
* **`random` Generators for integral types are now reproducible across different platforms**
  * Unlike `<random>`, Catch2's generators also support 1 byte integral types (`char`, `bool`, ...)
* **`random` Generators for `float` and `double` are now reproducible across different platforms**
  * `long double` varies across different platforms too much to be reproducible
  * This guarantee applies only to platforms with IEEE 754 floats.

### Fixes
* UDL declaration inside Catch2 are now strictly conforming to the standard
  * `operator "" _a` is UB, `operator ""_a` is fine. Seriously.
* Fixed `CAPTURE` tests failing to compile in C++23 mode (#2744)
* Fixed missing include in `catch_message.hpp` (#2758)
* Fixed `CHECK_ELSE` suppressing failure from uncaught exceptions(#2723)

### Miscellaneous
* The documentation for specifying which tests to run through commandline has been completely rewritten (#2738)
* Fixed installation when building Catch2 with meson (#2722, #2742)
* Fixed `catch_discover_tests` when using custom reporter and `PRE_TEST` discovery mode (#2747)
* `catch_discover_tests` supports multi-config CMake generator in `PRE_TEST` discovery mode (#2739, #2746)


## 3.4.0

### Improvements
* `VectorEquals` supports elements that provide only `==` and not `!=` (#2648)
* Catch2 supports compiling with IAR compiler (#2651)
* Various small internal performance improvements
* Various small internal compilation time improvements
* XMLReporter now reports location info for INFO and WARN (#1251)
  * This bumps up the xml format version to 3
* Documented that `SKIP` in generator constructor can be used to handle empty  generator (#1593)
* Added experimental static analysis support to `TEST_CASE` and `SECTION` macros (#2681)
  * The two macros are redefined in a way that helps the SA tools reason about the possible paths through a test case with sections.
  * The support is controlled by the `CATCH_CONFIG_EXPERIMENTAL_STATIC_ANALYSIS_SUPPORT` option and autodetects clang-tidy and Coverity.
* `*_THROWS`, `*_THROWS_AS`, etc now suppress warning coming from `__attribute__((warn_unused_result))` on GCC  (#2691)
  * Unlike plain `[[nodiscard]]`, this warning is not silenced by void cast. WTF GCC?

### Fixes
* Fixed `assertionStarting` events being sent after the expr is evaluated (#2678)
* Errors in `TEST_CASE` tags are now reported nicely (#2650)

### Miscellaneous
* Bunch of improvements to `catch_discover_tests`
  * Added DISCOVERY_MODE option, so the discovery can happen either post build or pre-run.
  * Fixed handling of semicolons and backslashes in test names (#2674, #2676)
* meson build can disable building tests (#2693)
* meson build properly sets meson version 0.54.1 as the minimal supported version (#2688)


## 3.3.2

### Improvements
* Further reduced allocations
  * The compact, console, TAP and XML reporters perform less allocations in various cases
  * Removed 1 allocation per entered `SECTION`/`TEST_CASE`.
  * Removed 2 allocations per test case exit, if stdout/stderr is captured
* Improved performance
  * Section tracking is 10%-25% faster than in v3.3.0
  * Assertion handling is 5%-10% faster than in v3.3.0
  * Test case registration is 1%-2% faster than in v3.3.0
  * Tiny speedup for registering listeners
  * Tiny speedup for `CAPTURE`, `TEST_CASE_METHOD`, `METHOD_AS_TEST_CASE`, and `TEMPLATE_LIST_TEST_*` macros.
* `Contains`, `RangeEquals` and `UnorderedRangeEquals` matchers now support ranges with iterator + sentinel pair
* Added `IsNaN` matcher
  * Unlike `REQUIRE(isnan(x))`, `REQUIRE_THAT(x, IsNaN())` shows you the value of `x`.
* Suppressed `declared_but_not_referenced` warning for NVHPC (#2637)

### Fixes
* Fixed performance regression in section tracking introduced in v3.3.1
  * Extreme cases would cause the tracking to run about 4x slower than in 3.3.0


## 3.3.1

### Improvements
* Reduced allocations and improved performance
  * The exact improvements are dependent on your usage of Catch2.
  * For example running Catch2's SelfTest binary performs 8k less allocations.
  * The main improvement comes from smarter handling of `SECTION`s, especially sibling `SECTION`s


## 3.3.0

### Improvements

* Added `MessageMatches` exception matcher (#2570)
* Added `RangeEquals` and `UnorderedRangeEquals` generic range matchers (#2377)
* Added `SKIP` macro for skipping tests from within the test body (#2360)
  * All built-in reporters have been extended to handle it properly, whether your custom reporter needs changes depends on how it was written
  * `skipTest` reporter event **is unrelated** to this, and has been deprecated since it has practically no uses
* Restored support for PPC Macs in the break-into-debugger functionality (#2619)
* Made our warning suppression compatible with CUDA toolkit pre 11.5 (#2626)
* Cleaned out some static analysis complaints


### Fixes

* Fixed macro redefinition warning when NVCC was reporting as MSVC (#2603)
* Fixed throws in generator constructor causing the whole binary to abort (#2615)
  * Now it just fails the test
* Fixed missing transitive include with libstdc++13 (#2611)


### Miscellaneous

* Improved support for dynamic library build with non-MSVC compilers on Windows (#2630)
* When used as a subproject, Catch2 keeps its generated header in a separate directory from the main project (#2604)



## 3.2.1

### Improvements
* Fix the reworked decomposer to work with older (pre 9) GCC versions (#2571)
  * **This required more significant changes to properly support C++20, there might be bugs.**


## 3.2.0

### Improvements
* Catch2 now compiles on PlayStation (#2562)
* Added `CATCH_CONFIG_GETENV` compile-time toggle (#2562)
  * This toggle guards whether Catch2 calls `std::getenv` when reading env variables
* Added support for more Bazel test environment variables
  * `TESTBRIDGE_TEST_ONLY` is now supported (#2490)
  * Sharding variables, `TEST_SHARD_INDEX`, `TEST_TOTAL_SHARDS`, `TEST_SHARD_STATUS_FILE`, are now all supported (#2491)
* Bunch of small tweaks and improvements in reporters
  * The TAP and SonarQube reporters output the used test filters
  * The XML reporter now also reports the version of its output format
  * The compact reporter now uses the same summary output as the console reporter (#878, #2554)
* Added support for asserting on types that can only be compared with literal 0 (#2555)
  * A canonical example is C++20's `std::*_ordering` types, which cannot be compared with an `int` variable, only `0`
  * The support extends to any type with this property, not just the ones in stdlib
  * This change imposes 2-3% slowdown on compiling files that are heavy on `REQUIRE` and friends
  * **This required significant rewrite of decomposition, there might be bugs**
* Simplified internals of matcher related macros
  * This provides about ~2% speed up compiling files that are heavy on `REQUIRE_THAT` and friends


### Fixes
* Cleaned out some warnings and static analysis issues
  * Suppressed `-Wcomma` warning rarely occurring in templated test cases (#2543)
  * Constified implementation details in `INFO` (#2564)
  * Made `MatcherGenericBase` copy constructor const (#2566)
* Fixed serialization of test filters so the output roundtrips
  * This means that e.g. `./tests/SelfTest "aaa bbb", [approx]` outputs `Filters: "aaa bbb",[approx]`


### Miscellaneous
* Catch2's build no longer leaks `-ffile-prefix-map` setting  to dependees (#2533)



## 3.1.1

### Improvements
* Added `Catch::getSeed` function that user code can call to retrieve current rng-seed
* Better detection of compiler support for `-ffile-prefix-map` (#2517)
* Catch2's shared libraries now have `SOVERSION` set (#2516)
* `catch2/catch_all.hpp` convenience header no longer transitively includes `windows.h` (#2432, #2526)


### Fixes
* Fixed compilation on Universal Windows Platform
* Fixed compilation on VxWorks (#2515)
* Fixed compilation on Cygwin (#2540)
* Remove unused variable in reporter registration (#2538)
* Fixed some symbol visibility issues with dynamic library on Windows (#2527)
* Suppressed `-Wuseless-cast` warnings in `REQUIRE_THROWS*` macros (#2520, #2521)
  * This was triggered when the potentially throwing expression evaluates to `void`
* Fixed "warning: storage class is not first" with `nvc++` (#2533)
* Fixed handling of `DL_PATHS` argument to `catch_discover_tests` on MacOS (#2483)
* Suppressed `*-avoid-c-arrays` clang-tidy warning in `TEMPLATE_TEST_CASE` (#2095, #2536)


### Miscellaneous
* Fixed CMake install step for Catch2 build as dynamic library (#2485)
* Raised minimum CMake version to 3.10 (#2523)
  * Expect the minimum CMake version to increase once more in next few releases.
* Whole bunch of doc updates and fixes
  * #1444, #2497, #2547, #2549, and more
* Added support for building Catch2 with Meson (#2530, #2539)



## 3.1.0

### Improvements
* Improved suppression of `-Wparentheses` for older GCCs
  * Turns out that even GCC 9 does not properly handle `_Pragma`s in the C++ frontend.
* Added type constraints onto `random` generator (#2433)
  * These constraints copy what the standard says for the underlying `std::uniform_int_distribution`
* Suppressed -Wunused-variable from nvcc (#2306, #2427)
* Suppressed -Wunused-variable from MinGW (#2132)
* Added All/Any/NoneTrue range matchers (#2319)
  * These check that all/any/none of boolean values in a range are true.
* The JUnit reporter now normalizes classnames from C++ namespaces to Java-like namespaces (#2468)
  * This provides better support for other JUnit based tools.
* The Bazel support now understands `BAZEL_TEST` environment variable (#2459)
  * The `CATCH_CONFIG_BAZEL_SUPPORT` configuration option is also still supported.
* Returned support for compiling Catch2 with GCC 5 (#2448)
  * This required removing inherited constructors from Catch2's internals.
  * I recommend updating to a newer GCC anyway.
* `catch_discover_tests` now has a new options for setting library load path(s) when running the Catch2 binary (#2467)


### Fixes
* Fixed crash when listing listeners without any registered listeners (#2442)
* Fixed nvcc compilation error in constructor benchmarking helper (#2477)
* Catch2's CMakeList supports pre-3.12 CMake again (#2428)
  * The gain from requiring CMake 3.12 was very minor, but y'all should really update to newer CMake


### Miscellaneous
* Fixed SelfTest build on MinGW (#2447)
* The in-repo conan recipe exports the CMake helper (#2460)
* Added experimental CMake script to showcase using test case sharding together with CTest
  * Compared to `catch_discover_tests`, it supports very limited number of options and customization
* Added documentation page on best practices when running Catch2 tests
* Catch2 can be built as a dynamic library (#2397, #2398)
  * Note that Catch2 does not have visibility annotations, and you are responsible for ensuring correct visibility built into the resulting library.



## 3.0.1

**Catch2 now uses statically compiled library as its distribution model.
This also means that to get all of Catch2's functionality in a test file,
you have to include multiple headers.**

You probably want to look into the [migration docs](migrate-v2-to-v3.md#top),
which were written to help people coming from v2.x.x versions to the
v3 releases.


### FAQ

* Why is Catch2 moving to separate headers?
  * The short answer is future extensibility and scalability. The long answer is complex and can be found on my blog, but at the most basic level, it is that providing single-header distribution is at odds with providing variety of useful features. When Catch2 was distributed in a single header, adding a new Matcher would cause overhead for everyone, but was useful only to a subset of users. This meant that the barrier to entry for new Matchers/Generators/etc is high in single header model, but much smaller in the new model.
* Will Catch2 again distribute single-header version in the future?
  * No. But we do provide sqlite-style amalgamated distribution option. This means that you can download just 1 .cpp file and 1 header and place them next to your own sources. However, doing this has downsides similar to using the `catch_all.hpp` header.
* Why the big breaking change caused by replacing `catch.hpp` with `catch_all.hpp`?
  * The convenience header `catch_all.hpp` exists for two reasons. One of them is to provide a way for quick migration from Catch2, the second one is to provide a simple way to test things with Catch2. Using it for migration has one drawback in that it is **big**. This means that including it _will_ cause significant compile time drag, and so using it to migrate should be a conscious decision by the user, not something they can just stumble into unknowingly.


### (Potentially) Breaking changes
* **Catch2 now uses statically compiled library as its distribution model**
  * **Including `catch.hpp` no longer works**
* **Catch2 now uses C++14 as the minimum support language version**
* `ANON_TEST_CASE` has been removed, use `TEST_CASE` with no arguments instead (#1220)
* `--list*` commands no longer have non-zero return code (#1410)
* `--list-test-names-only` has been removed (#1190)
  * You should use verbosity-modifiers for `--list-tests` instead
* `--list*` commands are now piped through the reporters
  * The top-level reporter interface provides default implementation that works just as the old one
  * XmlReporter outputs a machine-parseable XML
* `TEST_CASE` description support has been removed
  * If the second argument has text outside tags, the text will be ignored.
* Hidden test cases are no longer included just because they don't match an exclusion tag
  * Previously, a `TEST_CASE("A", "[.foo]")` would be included by asking for `~[bar]`.
* `PredicateMatcher` is no longer type erased.
  * This means that the type of the provided predicate is part of the `PredicateMatcher`'s type
* `SectionInfo` no longer contains section description as a member (#1319)
  * You can still write `SECTION("ShortName", "Long and wordy description")`, but the description is thrown away
  * The description type now must be a `const char*` or be implicitly convertible to it
* The `[!hide]` tag has been removed.
  * Use `[.]` or `[.foo]` instead.
* Lvalues of composed matchers cannot be composed further
* Uses of `REGISTER_TEST_CASE` macro need to be followed by a semicolon
  * This does not change `TEST_CASE` and friends in any way
* `IStreamingReporter::IsMulti` member function was removed
  * This is _very_ unlikely to actually affect anyone, as it was default-implemented in the interface, and only used internally
* Various classes not designed for user-extension have been made final
  * `ListeningReporter` is now `final`
  * Concrete Matchers (e.g. `UnorderedEquals` vector matcher) are now `final`
  * All Generators are now `final`
* Matcher namespacing has been redone
  * Matcher types are no longer in deeply nested namespaces
  * Matcher factory functions are no longer brought into `Catch` namespace
  * This means that all public-facing matcher-related functionality is now in `Catch::Matchers` namespace
* Defining `CATCH_CONFIG_MAIN` will no longer create main in that TU.
  * Link with `libCatch2Main.a`, or the proper CMake/pkg-config target
  * If you want to write custom main, include `catch2/catch_session.hpp`
* `CATCH_CONFIG_EXTERNAL_INTERFACES` has been removed.
  * You should instead include the appropriate headers as needed.
* `CATCH_CONFIG_IMPL` has been removed.
  * The implementation is now compiled into a static library.
* Event Listener interface has changed
  * `TestEventListenerBase` was renamed to `EventListenerBase`
  * `EventListenerBase` now directly derives from `IStreamingReporter`, instead of deriving from `StreamingReporterBase`
* `GENERATE` decays its arguments (#2012, #2040)
  * This means that `str` in `auto str = GENERATE("aa", "bb", "cc");` is inferred to `char const*` rather than `const char[2]`.
* `--list-*` flags write their output to file specified by the `-o` flag
* Many changes to reporter interfaces
  * With the exception of the XmlReporter, the outputs of first party reporters should remain the same
  * New pair of events were added
  * One obsolete event was removed
  * The base class has been renamed
  * The built-in reporter class hierarchy has been redone
* Catch2 generates a random seed if one hasn't been specified by the user
* The short flag for `--list-tests`, `-l`, has been removed.
  * This is not a commonly used flag and does not need to use up valuable single-letter space.
* The short flag for `--list-tags`, `-t`, has been removed.
  * This is not a commonly used flag and does not need to use up valuable single-letter space.
* The `--colour` option has been replaced with `--colour-mode` option


### Improvements
* Matchers have been extended with the ability to use different signatures of `match` (#1307, #1553, #1554, #1843)
  * This includes having templated `match` member function
  * See the [rewritten Matchers documentation](matchers.md#top) for details
  * Catch2 currently provides _some_ generic matchers, but there should be more before final release of v3
    * `IsEmpty`, `SizeIs` which check that the range has specific properties
    * `Contains`, which checks whether a range contains a specific element
    * `AllMatch`, `AnyMatch`, `NoneMatch` range matchers, which apply matchers over a range of elements
* Significant compilation time improvements
  * including `catch_test_macros.hpp` is 80% cheaper than including `catch.hpp`
* Some runtime performance optimizations
  * In all tested cases the v3 branch was faster, so the table below shows the speedup of v3 to v2 at the same task
<a id="v3-runtime-optimization-table"></a>

|                   task                      |  debug build | release build |
|:------------------------------------------- | ------------:| -------------:|
| Run 1M `REQUIRE(true)`                      |  1.10 ± 0.01 |   1.02 ± 0.06 |
| Run 100 tests, 3^3 sections, 1 REQUIRE each |  1.27 ± 0.01 |   1.04 ± 0.01 |
| Run 3k tests, no names, no tags             |  1.29 ± 0.01 |   1.05 ± 0.01 |
| Run 3k tests, names, tags                   |  1.49 ± 0.01 |   1.22 ± 0.01 |
| Run 1 out of 3k tests no names, no tags     |  1.68 ± 0.02 |   1.19 ± 0.22 |
| Run 1 out of 3k tests, names, tags          |  1.79 ± 0.02 |   2.06 ± 0.23 |


* POSIX platforms use `gmtime_r`, rather than `gmtime` when constructing a date string (#2008, #2165)
* `--list-*` flags write their output to file specified by the `-o` flag (#2061, #2163)
* `Approx::operator()` is now properly `const`
* Catch2's internal helper variables no longer use reserved identifiers (#578)
* `--rng-seed` now accepts string `"random-device"` to generate random seed using `std::random_device`
* Catch2 now supports test sharding (#2257)
  * You can ask for the tests to be split into N groups and only run one of them.
  * This greatly simplifies parallelization of tests in a binary through external runner.
* The embedded CLI parser now supports repeatedly callable lambdas
  * A lambda-based option parser can opt into being repeatedly specifiable.
* Added `STATIC_CHECK` macro, similar to `STATIC_REQUIRE` (#2318)
  * When deferred tu runtime, it behaves like `CHECK`, and not like `REQUIRE`.
* You can have multiple tests with the same name, as long as other parts of the test identity differ (#1915, #1999, #2175)
  * Test identity includes test's name, test's tags and test's class name if applicable.
* Added new warning, `UnmatchedTestSpec`, to error on test specs with no matching tests
* The `-w`, `--warn` warning flags can now be provided multiple times to enable multiple warnings
* The case-insensitive handling of tags is now more reliable and takes up less memory
* Test case and assertion counting can no longer reasonably overflow on 32 bit systems
  * The count is now kept in `uint64_t` on all platforms, instead of using `size_t` type.
* The `-o`, `--out` output destination specifiers recognize `-` as stdout
  * You have to provide it as `--out=-` to avoid CLI error about missing option
  * The new reporter specification also recognizes `-` as stdout
* Multiple reporters can now run at the same time and write to different files (#1712, #2183)
  * To support this, the `-r`, `--reporter` flag now also accepts optional output destination
  * For full overview of the semantics of using multiple reporters, look into the reporter documentation
  * To enable the new syntax, reporter names can no longer contain `::`.
* Console colour support has been rewritten and significantly improved
  * The colour implementation based on ANSI colour codes is always available
  * Colour implementations respect their associated stream
    * previously e.g. Win32 impl would change console colour even if Catch2 was writing to a file
  * The colour API is resilient against changing evaluation order of expressions
  * The associated CLI flag and compile-time configuration options have changed
    * For details see the docs for command-line and compile-time Catch2 configuration
* Added a support for Bazel integration with `XML_OUTPUT_FILE` env var (#2399)
  * This has to be enabled during compilation.
* Added `--skip-benchmarks` flag to run tests without any `BENCHMARK`s (#2392, #2408)
* Added option to list all listeners in the binary via `--list-listeners`


### Fixes
* The `INFO` macro no longer contains superfluous semicolon (#1456)
* The `--list*` family of command line flags now return 0 on success (#1410, #1146)
* Various ways of failing a benchmark are now counted and reporter properly
* The ULP matcher now handles comparing numbers with different signs properly (#2152)
* Universal ADL-found operators should no longer break decomposition (#2121)
* Reporter selection is properly case-insensitive
  * Previously it forced lower cased name, which would fail for reporters with upper case characters in name
* The cumulative reporter base stores benchmark results alongside assertion results
* Catch2's SE handling should no longer interferes with ASan on Windows (#2334)
* Fixed Windows console colour handling for tests that redirect stdout (#2345)
* Fixed issue with the `random` generators returning the same value over and over again


### Other changes
* `CATCH_CONFIG_DISABLE_MATCHERS` no longer exists.
  * If you do not want to use Matchers in a TU, do not include their header.
* `CATCH_CONFIG_ENABLE_CHRONO_STRINGMAKER` no longer exists.
  * `StringMaker` specializations for `<chrono>` are always provided
* Catch2's CMake now provides 2 targets, `Catch2` and `Catch2WithMain`.
  * `Catch2` is the statically compiled implementation by itself
  * `Catch2WithMain` also links in the default main
* Catch2's pkg-config integration also provides 2 packages
  * `catch2` is the statically compiled implementation by itself
  * `catch2-with-main` also links in the default main
* Passing invalid test specifications passed to Catch2 are now reported before tests are run, and are a hard error.
* Running 0 tests (e.g. due to empty binary, or test spec not matching anything) returns non-0 exit code
  * Flag `--allow-running-no-tests` overrides this behaviour.
  * `NoTests` warning has been removed because it is fully subsumed by this change.
* Catch2's compile-time configuration options (`CATCH_CONFIG_FOO`) can be set through CMake options of the same name
  * They use the same semantics as C++ defines, including the `CATCH_CONFIG_NO_FOO` overrides,
    * `-DCATCH_CONFIG_DEFAULT_REPORTER=compact` changes default reporter to "compact"
    * `-DCATCH_CONFIG_NO_ANDROID_LOGWRITE=ON` forces android logwrite to off
    * `-DCATCH_CONFIG_ANDROID_LOGWRITE=OFF` does nothing (the define will not exist)



## 2.13.7

### Fixes
* Added missing `<iterator>` include in benchmarking. (#2231)
* Fixed noexcept build with benchmarking enabled (#2235)
* Fixed build for compilers with C++17 support but without C++17 library support (#2195)
* JUnit only uses 3 decimal places when reporting durations (#2221)
* `!mayfail` tagged tests are now marked as `skipped` in JUnit reporter output (#2116)


## 2.13.6

### Fixes
* Disabling all signal handlers no longer breaks compilation  (#2212, #2213)

### Miscellaneous
* `catch_discover_tests` should handle escaped semicolon (`;`) better (#2214, #2215)


## 2.13.5

### Improvements
* Detection of MAC and IPHONE platforms has been improved (#2140, #2157)
* Added workaround for bug in XLC 16.1.0.1 (#2155)
* Add detection for LCC when it is masquerading as GCC (#2199)
* Modified posix signal handling so it supports newer libcs (#2178)
  * `MINSIGSTKSZ` was no longer usable in constexpr context.

### Fixes
* Fixed compilation of benchmarking when `min` and `max` macros are defined (#2159)
  * Including `windows.h` without `NOMINMAX` remains a really bad idea, don't do it

### Miscellaneous
* The check whether Catch2 is being built as a subproject is now more reliable (#2202, #2204)
  * The problem was that if the variable name used internally was defined the project including Catch2 as subproject, it would not be properly overwritten for Catch2's CMake.


## 2.13.4

### Improvements
* Improved the hashing algorithm used for shuffling test cases (#2070)
  * `TEST_CASE`s that differ only in the last character should be properly shuffled
  * Note that this means that v2.13.4 gives you a different order of test cases than 2.13.3, even given the same seed.

### Miscellaneous
* Deprecated `ParseAndAddCatchTests` CMake integration (#2092)
  * It is impossible to implement it properly for all the different test case variants Catch2 provides, and there are better options provided.
  * Use `catch_discover_tests` instead, which uses runtime information about available tests.
* Fixed bug in `catch_discover_tests` that would cause it to fail when used in specific project structures (#2119)
* Added Bazel build file
* Added an experimental static library target to CMake


## 2.13.3

### Fixes
* Fixed possible infinite loop when combining generators with section filter (`-c` option) (#2025)

### Miscellaneous
* Fixed `ParseAndAddCatchTests` not finding `TEST_CASE`s without tags (#2055, #2056)
* `ParseAndAddCatchTests` supports `CMP0110` policy for changing behaviour of `add_test` (#2057)
  * This was the shortlived change in CMake 3.18.0 that temporarily broke `ParseAndAddCatchTests`


## 2.13.2

### Improvements
* Implemented workaround for AppleClang shadowing bug (#2030)
* Implemented workaround for NVCC ICE (#2005, #2027)

### Fixes
* Fixed detection of `std::uncaught_exceptions` support under non-msvc platforms (#2021)
* Fixed the experimental stdout/stderr capture under Windows (#2013)

### Miscellaneous
* `catch_discover_tests` has been improved significantly (#2023, #2039)
  * You can now specify which reporter should be used
  * You can now modify where the output will be written
  * `WORKING_DIRECTORY` setting is respected
* `ParseAndAddCatchTests` now supports `TEMPLATE_TEST_CASE` macros (#2031)
* Various documentation fixes and improvements (#2022, #2028, #2034)


## 2.13.1

### Improvements
* `ParseAndAddCatchTests` handles CMake v3.18.0 correctly (#1984)
* Improved autodetection of `std::byte` (#1992)
* Simplified implementation of templated test cases (#2007)
  * This should have a tiny positive effect on its compilation throughput

### Fixes
* Automatic stringification of ranges handles sentinel ranges properly (#2004)


## 2.13.0

### Improvements
* `GENERATE` can now follow a `SECTION` at the same level of nesting (#1938)
  * The `SECTION`(s) before the `GENERATE` will not be run multiple times, the following ones will.
* Added `-D`/`--min-duration` command line flag (#1910)
  * If a test takes longer to finish than the provided value, its name and duration will be printed.
  * This flag is overridden by setting `-d`/`--duration`.

### Fixes
* `TAPReporter` no longer skips successful assertions (#1983)


## 2.12.4

### Improvements
* Added support for MacOS on ARM (#1971)


## 2.12.3

### Fixes
* `GENERATE` nested in a for loop no longer creates multiple generators (#1913)
* Fixed copy paste error breaking `TEMPLATE_TEST_CASE_SIG` for 6 or more arguments (#1954)
* Fixed potential UB when handling non-ASCII characters in CLI args (#1943)

### Improvements
* There can be multiple calls to `GENERATE` on a single line
* Improved `fno-except` support for platforms that do not provide shims for exception-related std functions (#1950)
  * E.g. the Green Hills C++ compiler
* XmlReporter now also reports test-case-level statistics (#1958)
  * This is done via a new element, `OverallResultsCases`

### Miscellaneous
* Added `.clang-format` file to the repo (#1182, #1920)
* Rewrote contributing docs
  * They should explain the different levels of testing and so on much better


## 2.12.2

### Fixes
* Fixed compilation failure if `is_range` ADL found deleted function (#1929)
* Fixed potential UB in `CAPTURE` if the expression contained non-ASCII characters (#1925)

### Improvements
* `std::result_of` is not used if `std::invoke_result` is available (#1934)
* JUnit reporter writes out `status` attribute for tests (#1899)
* Suppressed clang-tidy's `hicpp-vararg` warning (#1921)
  * Catch2 was already suppressing the `cppcoreguidelines-pro-type-vararg` alias of the warning


## 2.12.1

### Fixes
* Vector matchers now support initializer list literals better

### Improvements
* Added support for `^` (bitwise xor) to `CHECK` and `REQUIRE`



## 2.12.0

### Improvements
* Running tests in random order (`--order rand`) has been reworked significantly (#1908)
  * Given same seed, all platforms now produce the same order
  * Given same seed, the relative order of tests does not change if you select only a subset of them
* Vector matchers support custom allocators (#1909)
* `|` and `&` (bitwise or and bitwise and) are now supported in `CHECK` and `REQUIRE`
  * The resulting type must be convertible to `bool`

### Fixes
* Fixed computation of benchmarking column widths in ConsoleReporter (#1885, #1886)
* Suppressed clang-tidy's `cppcoreguidelines-pro-type-vararg` in assertions (#1901)
  * It was a false positive triggered by the new warning support workaround
* Fixed bug in test specification parser handling of OR'd patterns using escaping (#1905)

### Miscellaneous
* Worked around IBM XL's codegen bug (#1907)
  * It would emit code for _destructors_ of temporaries in an unevaluated context
* Improved detection of stdlib's support for `std::uncaught_exceptions` (#1911)



## 2.11.3

### Fixes
* Fixed compilation error caused by lambdas in assertions under MSVC


## 2.11.2

### Improvements
* GCC and Clang now issue warnings for suspicious code in assertions (#1880)
  * E.g. `REQUIRE( int != unsigned int )` will now issue mixed signedness comparison warning
  * This has always worked on MSVC, but it now also works for GCC and current Clang versions
* Colorization of "Test filters" output should be more robust now
* `--wait-for-keypress` now also accepts `never` as an option (#1866)
* Reporters no longer round-off nanoseconds when reporting benchmarking results (#1876)
* Catch2's debug break now supports iOS while using Thumb instruction set (#1862)
* It is now possible to customize benchmark's warm-up time when running the test binary (#1844)
  * `--benchmark-warmup-time {ms}`
* User can now specify how Catch2 should break into debugger (#1846)

### Fixes
* Fixes missing `<random>` include in benchmarking (#1831)
* Fixed missing `<iterator>` include in benchmarking (#1874)
* Hidden test cases are now also tagged with `[!hide]` as per documentation (#1847)
* Detection of whether libc provides `std::nextafter` has been improved (#1854)
* Detection of `wmain` no longer incorrectly looks for `WIN32` macro (#1849)
  * Now it just detects Windows platform
* Composing already-composed matchers no longer modifies the partially-composed matcher expression
  * This bug has been present for the last ~2 years and nobody reported it



## 2.11.1

### Improvements
* Breaking into debugger is supported on iOS (#1817)
* `google-build-using-namespace` clang-tidy warning is suppressed (#1799)

### Fixes
* Clang on Windows is no longer assumed to implement MSVC's traditional preprocessor (#1806)
* `ObjectStorage` now behaves properly in `const` contexts (#1820)
* `GENERATE_COPY(a, b)` now compiles properly (#1809, #1815)
* Some more cleanups in the benchmarking support



## 2.11.0

### Improvements
* JUnit reporter output now contains more details in case of failure (#1347, #1719)
* Added SonarQube Test Data reporter (#1738)
  * It is in a separate header, just like the TAP, Automake, and TeamCity reporters
* `range` generator now allows floating point numbers (#1776)
* Reworked part of internals to increase throughput


### Fixes
* The single header version should contain full benchmarking support (#1800)
* `[.foo]` is now properly parsed as `[.][foo]` when used on the command line (#1798)
* Fixed compilation of benchmarking on platforms where `steady_clock::period` is not `std::nano` (#1794)



## 2.10.2

### Improvements
* Catch2 will now compile on platform where `INFINITY` is double (#1782)


### Fixes
* Warning suppressed during listener registration will no longer leak



## 2.10.1

### Improvements
* Catch2 now guards itself against `min` and `max` macros from `windows.h` (#1772)
* Templated tests will now compile with ICC (#1748)
* `WithinULP` matcher now uses scientific notation for stringification (#1760)


### Fixes
* Templated tests no longer trigger `-Wunused-templates` (#1762)
* Suppressed clang-analyzer false positive in context getter (#1230, #1735)


### Miscellaneous
* CMake no longer prohibits in-tree build when Catch2 is used as a subproject (#1773, #1774)



## 2.10.0

### Fixes
* `TEMPLATE_LIST_TEST_CASE` now properly handles non-copyable and non-movable types (#1729)
* Fixed compilation error on Solaris caused by a system header defining macro `TT` (#1722, #1723)
* `REGISTER_ENUM` will now fail at compilation time if the registered enum is too large
* Removed use of `std::is_same_v` in C++17 mode (#1757)
* Fixed parsing of escaped special characters when reading test specs from a file (#1767, #1769)


### Improvements
* Trailing and leading whitespace in test/section specs are now ignored.
* Writing to Android debug log now uses `__android_log_write` instead of `__android_log_print`
* Android logging support can now be turned on/off at compile time (#1743)
  * The toggle is `CATCH_CONFIG_ANDROID_LOGWRITE`
* Added a generator that returns elements of a range
  * Use via `from_range(from, to)` or `from_range(container)`
* Added support for CRTs that do not provide `std::nextafter` (#1739)
  * They must still provide global `nextafter{f,l,}`
  * Enabled via `CATCH_CONFIG_GLOBAL_NEXTAFTER`
* Special cased `Approx(inf)` not to match non-infinite values
  * Very strictly speaking this might be a breaking change, but it should match user expectations better
* The output of benchmarking through the Console reporter when `--benchmark-no-analysis` is set is now much simpler (#1768)
* Added a matcher that can be used for checking an exceptions message (#1649, #1728)
  * The matcher helper function is called `Message`
  * The exception must publicly derive from `std::exception`
  * The matching is done exactly, including case and whitespace
* Added a matcher that can be used for checking relative equality of floating point numbers (#1746)
  * Unlike `Approx`, it considers both sides when determining the allowed margin
  * Special cases `NaN` and `INFINITY` to match user expectations
  * The matcher helper function is called `WithinRel`
* The ULP matcher now allows for any possible distance between the two numbers
* The random number generators now use Catch-global instance of RNG (#1734, #1736)
  * This means that nested random number generators actually generate different numbers


### Miscellaneous
* In-repo PNGs have been optimized to lower overhead of using Catch2 via git clone
* Catch2 now uses its own implementation of the URBG concept
  * In the future we also plan to use our own implementation of the distributions from `<random>` to provide cross-platform repeatability of random results



## 2.9.2

### Fixes
* `ChunkGenerator` can now be used with chunks of size 0 (#1671)
* Nested subsections are now run properly when specific section is run via the `-c` argument (#1670, #1673)
* Catch2 now consistently uses `_WIN32` to detect Windows platform (#1676)
* `TEMPLATE_LIST_TEST_CASE` now support non-default constructible type lists (#1697)
* Fixed a crash in the XMLReporter when a benchmark throws exception during warmup (#1706)
* Fixed a possible infinite loop in CompactReporter (#1715)
* Fixed `-w NoTests` returning 0 even when no tests were matched (#1449, #1683, #1684)
* Fixed matcher compilation under Obj-C++ (#1661)

### Improvements
* `RepeatGenerator` and `FixedValuesGenerator` now fail to compile when used with `bool` (#1692)
  * Previously they would fail at runtime.
* Catch2 now supports Android's debug logging for its debug output (#1710)
* Catch2 now detects and configures itself for the RTX platform (#1693)
  * You still need to pass `--benchmark-no-analysis` if you are using benchmarking under RTX
* Removed a "storage class is not first" warning when compiling Catch2 with PGI compiler (#1717)

### Miscellaneous
* Documentation now contains indication when a specific feature was introduced (#1695)
  * These start with Catch2 v2.3.0, (a bit over a year ago).
  * `docs/contributing.md` has been updated to provide contributors guidance on how to add these to newly written documentation
* Various other documentation improvements
  * ToC fixes
  * Documented `--order` and `--rng-seed` command line options
  * Benchmarking documentation now clearly states that it requires opt-in
  * Documented `CATCH_CONFIG_CPP17_OPTIONAL` and `CATCH_CONFIG_CPP17_BYTE` macros
  * Properly documented built-in vector matchers
  * Improved `*_THROWS_MATCHES` documentation a bit
* CMake config file is now arch-independent even if `CMAKE_SIZEOF_VOID_P` is in CMake cache (#1660)
* `CatchAddTests` now properly escapes `[` and `]` in test names (#1634, #1698)
* Reverted `CatchAddTests` adding tags as CTest labels (#1658)
  * The script broke when test names were too long
  * Overwriting `LABELS` caused trouble for users who set them manually
  * CMake does not let users append to `LABELS` if the test name has spaces


## 2.9.1

### Fixes
* Fix benchmarking compilation failure in files without `CATCH_CONFIG_EXTERNAL_INTERFACES` (or implementation)


## 2.9.0

### Improvements
* The experimental benchmarking support has been replaced by integrating Nonius code (#1616)
  * This provides a much more featurefull micro-benchmarking support.
  * Due to the compilation cost, it is disabled by default. See the documentation for details.
  * As far as backwards compatibility is concerned, this feature is still considered experimental in that we might change the interface based on user feedback.
* `WithinULP` matcher now shows the acceptable range (#1581)
* Template test cases now support type lists (#1627)


## 2.8.0

### Improvements
* Templated test cases no longer check whether the provided types are unique (#1628)
  * This allows you to e.g. test over `uint32_t`, `uint64_t`, and `size_t` without compilation failing
* The precision of floating point stringification can be modified by user (#1612, #1614)
* We now provide `REGISTER_ENUM` convenience macro for generating `StringMaker` specializations for enums
  * See the "String conversion" documentation for details
* Added new set of macros for template test cases that enables the use of NTTPs (#1531, #1609)
  * See "Test cases and sections" documentation for details

### Fixes
* `UNSCOPED_INFO` macro now has a prefixed/disabled/prefixed+disabled versions (#1611)
* Reporting errors at startup should no longer cause a segfault under certain circumstances (#1626)


### Miscellaneous
* CMake will now prevent you from attempting in-tree build (#1636, #1638)
  * Previously it would break with an obscure error message during the build step


## 2.7.2

### Improvements
* Added an approximate vector matcher (#1499)

### Fixes
* Filters will no longer be shown if there were none
* Fixed compilation error when using Homebrew GCC on OS X (#1588, #1589)
* Fixed the console reporter not showing messages that start with a newline (#1455, #1470)
* Modified JUnit reporter's output so that rng seed and filters are reported according to the JUnit schema (#1598)
* Fixed some obscure warnings and static analysis passes

### Miscellaneous
* Various improvements to `ParseAndAddCatchTests` (#1559, #1601)
  * When a target is parsed, it receives `ParseAndAddCatchTests_TESTS` property which summarizes found tests
  * Fixed problem with tests not being found if the `OptionalCatchTestLauncher` variables is used
  * Including the script will no longer forcefully modify `CMAKE_MINIMUM_REQUIRED_VERSION`
  * CMake object libraries are ignored when parsing to avoid needless warnings
* `CatchAddTests` now adds test's tags to their CTest labels (#1600)
* Added basic CPack support to our build

## 2.7.1

### Improvements
* Reporters now print out the filters applied to test cases (#1550, #1585)
* Added `GENERATE_COPY` and `GENERATE_REF` macros that can use variables inside the generator expression
  * Because of the significant danger of lifetime issues, the default `GENERATE` macro still does not allow variables
* The `map` generator helper now deduces the mapped return type (#1576)

### Fixes
* Fixed ObjC++ compilation (#1571)
* Fixed test tag parsing so that `[.foo]` is now parsed as `[.][foo]`.
* Suppressed warning caused by the Windows headers defining SE codes in different manners (#1575)

## 2.7.0

### Improvements
* `TEMPLATE_PRODUCT_TEST_CASE` now uses the resulting type in the name, instead of the serial number (#1544)
* Catch2's single header is now strictly ASCII (#1542)
* Added generator for random integral/floating point types
  * The types are inferred within the `random` helper
* Added back RangeGenerator (#1526)
  * RangeGenerator returns elements within a certain range
* Added ChunkGenerator generic transform (#1538)
  * A ChunkGenerator returns the elements from different generator in chunks of n elements
* Added `UNSCOPED_INFO` (#415, #983, #1522)
  * This is a variant of `INFO` that lives until next assertion/end of the test case.


### Fixes
* All calls to C stdlib functions are now `std::` qualified (#1541)
  * Code brought in from Clara was also updated.
* Running tests will no longer open the specified output file twice (#1545)
  * This would cause trouble when the file was not a file, but rather a named pipe
  * Fixes the CLion/Resharper integration with Catch
* Fixed `-Wunreachable-code` occurring with (old) ccache+cmake+clang combination (#1540)
* Fixed `-Wdefaulted-function-deleted` warning with Clang 8 (#1537)
* Catch2's type traits and helpers are now properly namespaced inside `Catch::` (#1548)
* Fixed std{out,err} redirection for failing test (#1514, #1525)
  * Somehow, this bug has been present for well over a year before it was reported


### Contrib
* `ParseAndAddCatchTests` now properly escapes commas in the test name



## 2.6.1

### Improvements
* The JUnit reporter now also reports random seed (#1520, #1521)

### Fixes
* The TAP reporter now formats comments with test name properly (#1529)
* `CATCH_REQUIRE_THROWS`'s internals were unified with `REQUIRE_THROWS` (#1536)
  * This fixes a potential `-Wunused-value` warning when used
* Fixed a potential segfault when using any of the `--list-*` options (#1533, #1534)


## 2.6.0

**With this release the data generator feature is now fully supported.**


### Improvements
* Added `TEMPLATE_PRODUCT_TEST_CASE` (#1454, #1468)
  * This allows you to easily test various type combinations, see documentation for details
* The error message for `&&` and `||` inside assertions has been improved (#1273, #1480)
* The error message for chained comparisons inside assertions has been improved (#1481)
* Added `StringMaker` specialization for `std::optional` (#1510)
* The generator interface has been redone once again (#1516)
  * It is no longer considered experimental and is fully supported
  * The new interface supports "Input" generators
  * The generator documentation has been fully updated
  * We also added 2 generator examples


### Fixes
* Fixed `-Wredundant-move` on newer Clang (#1474)
* Removed unreachable mentions `std::current_exception`, `std::rethrow_exception` in no-exceptions mode (#1462)
  * This should fix compilation with IAR
* Fixed missing `<type_traits>` include (#1494)
* Fixed various static analysis warnings
  * Unrestored stream state in `XmlWriter` (#1489)
  * Potential division by zero in `estimateClockResolution` (#1490)
  * Uninitialized member in `RunContext` (#1491)
  * `SourceLineInfo` move ops are now marked `noexcept`
  * `CATCH_BREAK_INTO_DEBUGGER` is now always a function
* Fix double run of a test case if user asks for a specific section (#1394, #1492)
* ANSI colour code output now respects `-o` flag and writes to the file as well (#1502)
* Fixed detection of `std::variant` support for compilers other than Clang (#1511)


### Contrib
* `ParseAndAddCatchTests` has learned how to use `DISABLED` CTest property (#1452)
* `ParseAndAddCatchTests` now works when there is a whitespace before the test name (#1493)


### Miscellaneous
* We added new issue templates for reporting issues on GitHub
* `contributing.md` has been updated to reflect the current test status (#1484)



## 2.5.0

### Improvements
* Added support for templated tests via `TEMPLATE_TEST_CASE` (#1437)


### Fixes
* Fixed compilation of `PredicateMatcher<const char*>` by removing partial specialization of `MatcherMethod<T*>`
* Listeners now implicitly support any verbosity (#1426)
* Fixed compilation with Embarcadero builder by introducing `Catch::isnan` polyfill (#1438)
* Fixed `CAPTURE` asserting for non-trivial captures (#1436, #1448)


### Miscellaneous
* We should now be providing first party Conan support via https://bintray.com/catchorg/Catch2 (#1443)
* Added new section "deprecations and planned changes" to the documentation
  * It contains summary of what is deprecated and might change with next major version
* From this release forward, the released headers should be pgp signed (#430)
  * KeyID `E29C 46F3 B8A7 5028 6079 3B7D ECC9 C20E 314B 2360`
  * or https://codingnest.com/files/horenmar-publickey.asc


## 2.4.2

### Improvements
* XmlReporter now also outputs the RNG seed that was used in a run (#1404)
* `Catch::Session::applyCommandLine` now also accepts `wchar_t` arguments.
  * However, Catch2 still does not support unicode.
* Added `STATIC_REQUIRE` macro (#1356, #1362)
* Catch2's singleton's are now cleaned up even if tests are run (#1411)
  * This is mostly useful as a FP prevention for users who define their own main.
* Specifying an invalid reporter via `-r` is now reported sooner (#1351, #1422)


### Fixes
* Stringification no longer assumes that `char` is signed (#1399, #1407)
  * This caused a `Wtautological-compare` warning.
* SFINAE for `operator<<` no longer sees different overload set than the actual insertion (#1403)


### Contrib
* `catch_discover_tests` correctly adds tests with comma in name (#1327, #1409)
* Added a new customization point in how the tests are launched to `catch_discover_tests`


## 2.4.1

### Improvements
* Added a StringMaker for `std::(w)string_view` (#1375, #1376)
* Added a StringMaker for `std::variant` (#1380)
  * This one is disabled by default to avoid increased compile-time drag
* Added detection for cygwin environment without `std::to_string` (#1396, #1397)

### Fixes
* `UnorderedEqualsMatcher` will no longer accept erroneously accept
vectors that share suffix, but are not permutation of the desired vector
* Abort after (`-x N`) can no longer be overshot by nested `REQUIRES` and
subsequently ignored (#1391, #1392)


## 2.4.0

**This release brings two new experimental features, generator support
and a `-fno-exceptions` support. Being experimental means that they
will not be subject to the usual stability guarantees provided by semver.**

### Improvements
* Various small runtime performance improvements
* `CAPTURE` macro is now variadic
* Added `AND_GIVEN` macro (#1360)
* Added experimental support for data generators
  * See [their documentation](generators.md) for details
* Added support for compiling and running Catch without exceptions
  * Doing so limits the functionality somewhat
  * Look [into the documentation](configuration.md#disablingexceptions) for details

### Fixes
* Suppressed `-Wnon-virtual-dtor` warnings in Matchers (#1357)
* Suppressed `-Wunreachable-code` warnings in floating point matchers (#1350)

### CMake
* It is now possible to override which Python is used to run Catch's tests (#1365)
* Catch now provides infrastructure for adding tests that check compile-time configuration
* Catch no longer tries to install itself when used as a subproject (#1373)
* Catch2ConfigVersion.cmake is now generated as arch-independent (#1368)
  * This means that installing Catch from 32-bit machine and copying it to 64-bit one works
  * This fixes conan installation of Catch


## 2.3.0

**This release changes the include paths provided by our CMake and
pkg-config integration. The proper include path for the single-header
when using one of the above is now `<catch2/catch.hpp>`. This change
also necessitated changes to paths inside the repository, so that the
single-header version is now at `single_include/catch2/catch.hpp`, rather
than `single_include/catch.hpp`.**



### Fixes
* Fixed Objective-C++ build
* `-Wunused-variable` suppression no longer leaks from Catch's header under Clang
* Implementation of the experimental new output capture can now be disabled (#1335)
  * This allows building Catch2 on platforms that do not provide things like `dup` or `tmpfile`.
* The JUnit and XML reporters will no longer skip over successful tests when running without `-s`  (#1264, #1267, #1310)
  * See improvements for more details

### Improvements
* pkg-config and CMake integration has been rewritten
  * If you use them, the new include path is `#include <catch2/catch.hpp>`
  * CMake installation now also installs scripts from `contrib/`
  * For details see the [new documentation](cmake-integration.md#top)
* Reporters now have a new customization point, `ReporterPreferences::shouldReportAllAssertions`
  * When this is set to `false` and the tests are run without `-s`, passing assertions are not sent to the reporter.
  * Defaults to `false`.
* Added `DYNAMIC_SECTION`, a section variant that constructs its name using stream
  * This means that you can do `DYNAMIC_SECTION("For X := " << x)`.


## 2.2.3

**To fix some of the bugs, some behavior had to change in potentially breaking manner.**
**This means that even though this is a patch release, it might not be a drop-in replacement.**

### Fixes
* Listeners are now called before reporter
  * This was always documented to be the case, now it actually works that way
* Catch's commandline will no longer accept multiple reporters
  * This was done because multiple reporters never worked properly and broke things in non-obvious ways
  * **This has potential to be a breaking change**
* MinGW is now detected as Windows platform w/o SEH support (#1257)
  * This means that Catch2 no longer tries to use POSIX signal handling when compiled with MinGW
* Fixed potential UB in parsing tags using non-ASCII characters (#1266)
  * Note that Catch2 still supports only ASCII test names/tags/etc
* `TEST_CASE_METHOD` can now be used on classnames containing commas (#1245)
  * You have to enclose the classname in extra set of parentheses
* Fixed insufficient alt stack size for POSIX signal handling (#1225)
* Fixed compilation error on Android due to missing `std::to_string` in C++11 mode (#1280)
* Fixed the order of user-provided `FALLBACK_STRINGIFIER` in stringification machinery (#1024)
  * It was intended to be replacement for built-in fallbacks, but it was used _after_ them.
  * **This has potential to be a breaking change**
* Fixed compilation error when a type has an `operator<<` with templated lhs (#1285, #1306)

### Improvements
* Added a new, experimental, output capture (#1243)
  * This capture can also redirect output written via C apis, e.g. `printf`
  * To opt-in, define `CATCH_CONFIG_EXPERIMENTAL_REDIRECT` in the implementation file
* Added a new fallback stringifier for classes derived from `std::exception`
  * Both `StringMaker` specialization and `operator<<` overload are given priority

### Miscellaneous
* `contrib/` now contains dbg scripts that skip over Catch's internals (#904, #1283)
  * `gdbinit` for gdb `lldbinit` for lldb
* `CatchAddTests.cmake` no longer strips whitespace from tests (#1265, #1281)
* Online documentation now describes `--use-colour` option (#1263)


## 2.2.2

### Fixes
* Fixed bug in `WithinAbs::match()` failing spuriously (#1228)
* Fixed clang-tidy diagnostic about virtual call in destructor (#1226)
* Reduced the number of GCC warnings suppression leaking out of the header (#1090, #1091)
  * Only `-Wparentheses` should be leaking now
* Added upper bound on the time benchmark timer calibration is allowed to take (#1237)
  * On platforms where `std::chrono::high_resolution_clock`'s resolution is low, the calibration would appear stuck
* Fixed compilation error when stringifying static arrays of `unsigned char`s (#1238)

### Improvements
* XML encoder now hex-encodes invalid UTF-8 sequences (#1207)
  * This affects xml and junit reporters
  * Some invalid UTF-8 parts are left as is, e.g. surrogate pairs. This is because certain extensions of UTF-8 allow them, such as WTF-8.
* CLR objects (`T^`) can now be stringified (#1216)
  * This affects code compiled as C++/CLI
* Added `PredicateMatcher`, a matcher that takes an arbitrary predicate function (#1236)
  * See [documentation for details](https://github.com/catchorg/Catch2/blob/devel/docs/matchers.md)

### Others
* Modified CMake-installed pkg-config to allow `#include <catch.hpp>`(#1239)
  * The plans to standardize on `#include <catch2/catch.hpp>` are still in effect


## 2.2.1

### Fixes
* Fixed compilation error when compiling Catch2 with `std=c++17` against libc++ (#1214)
  * Clara (Catch2's CLI parsing library) used `std::optional` without including it explicitly
* Fixed Catch2 return code always being 0 (#1215)
  * In the words of STL, "We feel superbad about letting this in"


## 2.2.0

### Fixes
* Hidden tests are not listed by default when listing tests (#1175)
  * This makes `catch_discover_tests` CMake script work better
* Fixed regression that meant `<windows.h>` could potentially not be included properly (#1197)
* Fixed installing `Catch2ConfigVersion.cmake` when Catch2 is a subproject.

### Improvements
* Added an option to warn (+ exit with error) when no tests were ran (#1158)
  * Use as `-w NoTests`
* Added provisional support for Emscripten (#1114)
* [Added a way to override the fallback stringifier](https://github.com/catchorg/Catch2/blob/devel/docs/configuration.md#fallback-stringifier) (#1024)
  * This allows project's own stringification machinery to be easily reused for Catch
* `Catch::Session::run()` now accepts `char const * const *`, allowing it to accept array of string literals (#1031, #1178)
  * The embedded version of Clara was bumped to v1.1.3
* Various minor performance improvements
* Added support for DJGPP DOS crosscompiler (#1206)


## 2.1.2

### Fixes
* Fixed compilation error with `-fno-rtti` (#1165)
* Fixed NoAssertion warnings
* `operator<<` is used before range-based stringification (#1172)
* Fixed `-Wpedantic` warnings (extra semicolons and binary literals) (#1173)


### Improvements
* Added `CATCH_VERSION_{MAJOR,MINOR,PATCH}` macros (#1131)
* Added `BrightYellow` colour for use in reporters (#979)
  * It is also used by ConsoleReporter for reconstructed expressions

### Other changes
* Catch is now exported as a CMake package and linkable target (#1170)

## 2.1.1

### Improvements
* Static arrays are now properly stringified like ranges across MSVC/GCC/Clang
* Embedded newer version of Clara -- v1.1.1
  * This should fix some warnings dragged in from Clara
* MSVC's CLR exceptions are supported


### Fixes
* Fixed compilation when comparison operators do not return bool (#1147)
* Fixed CLR exceptions blowing up the executable during translation (#1138)


### Other changes
* Many CMake changes
  * `NO_SELFTEST` option is deprecated, use `BUILD_TESTING` instead.
  * Catch specific CMake options were prefixed with `CATCH_` for namespacing purposes
  * Other changes to simplify Catch2's packaging



## 2.1.0

### Improvements
* Various performance improvements
  * On top of the performance regression fixes
* Experimental support for PCH was added (#1061)
* `CATCH_CONFIG_EXTERNAL_INTERFACES` now brings in declarations of Console, Compact, XML and JUnit reporters
* `MatcherBase` no longer has a pointless second template argument
* Reduced the number of warning suppressions that leak into user's code
  * Bugs in g++ 4.x and 5.x mean that some of them have to be left in


### Fixes
* Fixed performance regression from Catch classic
  * One of the performance improvement patches for Catch classic was not applied to Catch2
* Fixed platform detection for iOS (#1084)
* Fixed compilation when `g++` is used together with `libc++` (#1110)
* Fixed TeamCity reporter compilation with the single header version
  * To fix the underlying issue we will be versioning reporters in single_include folder per release
* The XML reporter will now report `WARN` messages even when not used with `-s`
* Fixed compilation when `VectorContains` matcher was combined using `&&` (#1092)
* Fixed test duration overflowing after 10 seconds (#1125, #1129)
* Fixed `std::uncaught_exception` deprecation warning (#1124)


### New features
* New Matchers
  * Regex matcher for strings, `Matches`.
  * Set-equal matcher for vectors, `UnorderedEquals`
  * Floating point matchers, `WithinAbs` and `WithinULP`.
* Stringification now attempts to decompose all containers (#606)
  * Containers are objects that respond to ADL `begin(T)` and `end(T)`.


### Other changes
* Reporters will now be versioned in the `single_include` folder to ensure their compatibility with the last released version




## 2.0.1

### Breaking changes
* Removed C++98 support
* Removed legacy reporter support
* Removed legacy generator support
  * Generator support will come back later, reworked
* Removed `Catch::toString` support
  * The new stringification machinery uses `Catch::StringMaker` specializations first and `operator<<` overloads second.
* Removed legacy `SCOPED_MSG` and `SCOPED_INFO` macros
* Removed `INTERNAL_CATCH_REGISTER_REPORTER`
  * `CATCH_REGISTER_REPORTER` should be used to register reporters
* Removed legacy `[hide]` tag
  * `[.]`, `[.foo]` and `[!hide]` are still supported
* Output into debugger is now colourized
* `*_THROWS_AS(expr, exception_type)` now unconditionally appends `const&` to the exception type.
* `CATCH_CONFIG_FAST_COMPILE` now affects the `CHECK_` family of assertions as well as `REQUIRE_` family of assertions
  * This is most noticeable in `CHECK(throws())`, which would previously report failure, properly stringify the exception and continue. Now it will report failure and stop executing current section.
* Removed deprecated matcher utility functions `Not`, `AllOf` and `AnyOf`.
  * They are superseded by operators `!`, `&&` and `||`, which are natural and do not have limited arity
* Removed support for non-const comparison operators
  * Non-const comparison operators are an abomination that should not exist
  * They were breaking support for comparing function to function pointer
* `std::pair` and `std::tuple` are no longer stringified by default
  * This is done to avoid dragging in `<tuple>` and `<utility>` headers in common path
  * Their stringification can be enabled per-file via new configuration macros
* `Approx` is subtly different and hopefully behaves more as users would expect
  * `Approx::scale` defaults to `0.0`
  * `Approx::epsilon` no longer applies to the larger of the two compared values, but only to the `Approx`'s value
  * `INFINITY == Approx(INFINITY)` returns true


### Improvements
* Reporters and Listeners can be defined in files different from the main file
  * The file has to define `CATCH_CONFIG_EXTERNAL_INTERFACES` before including catch.hpp.
* Errors that happen during set up before main are now caught and properly reported once main is entered
  * If you are providing your own main, you can access and use these as well.
* New assertion macros, *_THROWS_MATCHES(expr, exception_type, matcher) are provided
  * As the arguments suggest, these allow you to assert that an expression throws desired type of exception and pass the exception to a matcher.
* JUnit reporter no longer has significantly different output for test cases with and without sections
* Most assertions now support expressions containing commas (ie `REQUIRE(foo() == std::vector<int>{1, 2, 3});`)
* Catch now contains experimental micro benchmarking support
  * See `projects/SelfTest/Benchmark.tests.cpp` for examples
  * The support being experiment means that it can be changed without prior notice
* Catch uses new CLI parsing library (Clara)
  * Users can now easily add new command line options to the final executable
  * This also leads to some changes in `Catch::Session` interface
* All parts of matchers can be removed from a TU by defining `CATCH_CONFIG_DISABLE_MATCHERS`
  * This can be used to somewhat speed up compilation times
* An experimental implementation of `CATCH_CONFIG_DISABLE` has been added
  * Inspired by Doctest's `DOCTEST_CONFIG_DISABLE`
  * Useful for implementing tests in source files
    * ie for functions in anonymous namespaces
  * Removes all assertions
  * Prevents `TEST_CASE` registrations
  * Exception translators are not registered
  * Reporters are not registered
  * Listeners are not registered
* Reporters/Listeners are now notified of fatal errors
  * This means specific signals or structured exceptions
  * The Reporter/Listener interface provides default, empty, implementation to preserve backward compatibility
* Stringification of `std::chrono::duration` and `std::chrono::time_point` is now supported
  * Needs to be enabled by a per-file compile time configuration option
* Add `pkg-config` support to CMake install command


### Fixes
* Don't use console colour if running in XCode
* Explicit constructor in reporter base class
* Swept out `-Wweak-vtables`, `-Wexit-time-destructors`, `-Wglobal-constructors` warnings
* Compilation for Universal Windows Platform (UWP) is supported
  * SEH handling and colorized output are disabled when compiling for UWP
* Implemented a workaround for `std::uncaught_exception` issues in libcxxrt
  * These issues caused incorrect section traversals
  * The workaround is only partial, user's test can still trigger the issue by using `throw;` to rethrow an exception
* Suppressed C4061 warning under MSVC


### Internal changes
* The development version now uses .cpp files instead of header files containing implementation.
  * This makes partial rebuilds much faster during development
* The expression decomposition layer has been rewritten
* The evaluation layer has been rewritten
* New library (TextFlow) is used for formatting text to output


## Older versions

### 1.12.x

#### 1.12.2
##### Fixes
* Fixed missing <cassert> include

#### 1.12.1

##### Fixes
* Fixed deprecation warning in `ScopedMessage::~ScopedMessage`
* All uses of `min` or `max` identifiers are now wrapped in parentheses
  * This avoids problems when Windows headers define `min` and `max` macros

#### 1.12.0

##### Fixes
* Fixed compilation for strict C++98 mode (ie not gnu++98) and older compilers (#1103)
* `INFO` messages are included in the `xml` reporter output even without `-s` specified.


### 1.11.x

#### 1.11.0

##### Fixes
* The original expression in `REQUIRE_FALSE( expr )` is now reporter properly as `!( expr )` (#1051)
  * Previously the parentheses were missing and `x != y` would be expanded as `!x != x`
* `Approx::Margin` is now inclusive (#952)
  * Previously it was meant and documented as inclusive, but the check itself wasn't
  * This means that `REQUIRE( 0.25f == Approx( 0.0f ).margin( 0.25f ) )` passes, instead of fails
* `RandomNumberGenerator::result_type` is now unsigned (#1050)

##### Improvements
* `__JETBRAINS_IDE__` macro handling is now CLion version specific (#1017)
  * When CLion 2017.3 or newer is detected, `__COUNTER__` is used instead of
* TeamCity reporter now explicitly flushes output stream after each report (#1057)
  * On some platforms, output from redirected streams would show up only after the tests finished running
* `ParseAndAddCatchTests` now can add test files as dependency to CMake configuration
  * This means you do not have to manually rerun CMake configuration step to detect new tests

### 1.10.x

#### 1.10.0

##### Fixes
* Evaluation layer has been rewritten (backported from Catch 2)
  * The new layer is much simpler and fixes some issues (#981)
* Implemented workaround for VS 2017 raw string literal stringification bug (#995)
* Fixed interaction between `[!shouldfail]` and `[!mayfail]` tags and sections
  * Previously sections with failing assertions would be marked as failed, not failed-but-ok

##### Improvements
* Added [libidentify](https://github.com/janwilmans/LibIdentify) support
* Added "wait-for-keypress" option

### 1.9.x

#### 1.9.6

##### Improvements
* Catch's runtime overhead has been significantly decreased (#937, #939)
* Added `--list-extra-info` cli option (#934).
  * It lists all tests together with extra information, ie filename, line number and description.



#### 1.9.5

##### Fixes
* Truthy expressions are now reconstructed properly, not as booleans (#914)
* Various warnings are no longer erroneously suppressed in test files (files that include `catch.hpp`, but do not define `CATCH_CONFIG_MAIN` or `CATCH_CONFIG_RUNNER`) (#871)
* Catch no longer fails to link when main is compiled as C++, but linked against Objective-C (#855)
* Fixed incorrect gcc version detection when deciding to use `__COUNTER__` (#928)
  * Previously any GCC with minor version less than 3 would be incorrectly classified as not supporting `__COUNTER__`.
* Suppressed C4996 warning caused by upcoming updated to MSVC 2017, marking `std::uncaught_exception` as deprecated. (#927)

##### Improvements
* CMake integration script now incorporates debug messages and registers tests in an improved way (#911)
* Various documentation improvements



#### 1.9.4

##### Fixes
* `CATCH_FAIL` macro no longer causes compilation error without variadic macro support
* `INFO` messages are no longer cleared after being reported once

##### Improvements and minor changes
* Catch now uses `wmain` when compiled under Windows and `UNICODE` is defined.
  * Note that Catch still officially supports only ASCII

#### 1.9.3

##### Fixes
* Completed the fix for (lack of) uint64_t in earlier Visual Studios

#### 1.9.2

##### Improvements and minor changes
* All of `Approx`'s member functions now accept strong typedefs in C++11 mode (#888)
  * Previously `Approx::scale`, `Approx::epsilon`, `Approx::margin` and `Approx::operator()` didn't.


##### Fixes
* POSIX signals are now disabled by default under QNX (#889)
  * QNX does not support current enough (2001) POSIX specification
* JUnit no longer counts exceptions as failures if given test case is marked as ok to fail.
* `Catch::Option` should now have its storage properly aligned.
* Catch no longer attempts to define `uint64_t` on windows (#862)
  * This was causing trouble when compiled under Cygwin

##### Other
* Catch is now compiled under MSVC 2017 using `std:c++latest` (C++17 mode) in CI
* We now provide cmake script that autoregisters Catch tests into ctest.
  * See `contrib` folder.


#### 1.9.1

##### Fixes
* Unexpected exceptions are no longer ignored by default (#885, #887)


#### 1.9.0


##### Improvements and minor changes
* Catch no longer attempts to ensure the exception type passed by user in `REQUIRE_THROWS_AS` is a constant reference.
  * It was causing trouble when `REQUIRE_THROWS_AS` was used inside templated functions
  * This actually reverts changes made in v1.7.2
* Catch's `Version` struct should no longer be double freed when multiple instances of Catch tests are loaded into single program (#858)
  * It is now a static variable in an inline function instead of being an `extern`ed struct.
* Attempt to register invalid tag or tag alias now throws instead of calling `exit()`.
  * Because this happen before entering main, it still aborts execution
  * Further improvements to this are coming
* `CATCH_CONFIG_FAST_COMPILE` now speeds-up compilation of `REQUIRE*` assertions by further ~15%.
  * The trade-off is disabling translation of unexpected exceptions into text.
* When Catch is compiled using C++11, `Approx` is now constructible with anything that can be explicitly converted to `double`.
* Captured messages are now printed on unexpected exceptions

##### Fixes:
* Clang's `-Wexit-time-destructors` should be suppressed for Catch's internals
* GCC's `-Wparentheses` is now suppressed for all TU's that include `catch.hpp`.
  * This is functionally a revert of changes made in 1.8.0, where we tried using `_Pragma` based suppression. This should have kept the suppression local to Catch's assertions, but bugs in GCC's handling of `_Pragma`s in C++ mode meant that it did not always work.
* You can now tell Catch to use C++11-based check when checking whether a type can be streamed to output.
  * This fixes cases when an unstreamable type has streamable private base (#877)
  * [Details can be found in documentation](configuration.md#catch_config_cpp11_stream_insertable_check)


##### Other notes:
* We have added VS 2017 to our CI
* Work on Catch 2 should start soon



### 1.8.x

#### 1.8.2


##### Improvements and minor changes
* TAP reporter now behaves as if `-s` was always set
  * This should be more consistent with the protocol desired behaviour.
* Compact reporter now obeys `-d yes` argument (#780)
  * The format is "XXX.123 s: <section-name>" (3 decimal places are always present).
  * Before it did not report the durations at all.
* XML reporter now behaves the same way as Console reporter in regards to `INFO`
  * This means it reports `INFO` messages on success, if output on success (`-s`) is enabled.
  * Previously it only reported `INFO` messages on failure.
* `CAPTURE(expr)` now stringifies `expr` in the same way assertion macros do (#639)
* Listeners are now finally [documented](event-listeners.md#top).
  * Listeners provide a way to hook into events generated by running your tests, including start and end of run, every test case, every section and every assertion.


##### Fixes:
* Catch no longer attempts to reconstruct expression that led to a fatal error  (#810)
  * This fixes possible signal/SEH loop when processing expressions, where the signal was triggered by expression decomposition.
* Fixed (C4265) missing virtual destructor warning in Matchers (#844)
* `std::string`s are now taken by `const&` everywhere (#842).
  * Previously some places were taking them by-value.
* Catch should no longer change errno (#835).
  * This was caused by libstdc++ bug that we now work around.
* Catch now provides `FAIL_CHECK( ... )` macro (#765).
  * Same as `FAIL( ... )`, but does not abort the test.
* Functions like `fabs`, `tolower`, `memset`, `isalnum` are now used with `std::` qualification (#543).
* Clara no longer assumes first argument (binary name) is always present (#729)
  * If it is missing, empty string is used as default.
* Clara no longer reads 1 character past argument string (#830)
* Regression in Objective-C bindings (Matchers) fixed (#854)


##### Other notes:
* We have added VS 2013 and 2015 to our CI
* Catch Classic (1.x.x) now contains its own, forked, version of Clara (the argument parser).



#### 1.8.1

##### Fixes

Cygwin issue with `gettimeofday` - `#define` was not early enough

#### 1.8.0

##### New features/ minor changes

* Matchers have new, simpler (and documented) interface.
  * Catch provides string and vector matchers.
  * For details see [Matchers documentation](matchers.md#top).
* Changed console reporter test duration reporting format (#322)
  * Old format: `Some simple comparisons between doubles completed in 0.000123s`
  * New format: `xxx.123s: Some simple comparisons between doubles` _(There will always be exactly 3 decimal places)_
* Added opt-in leak detection under MSVC + Windows (#439)
  * Enable it by compiling Catch's main with `CATCH_CONFIG_WINDOWS_CRTDBG`
* Introduced new compile-time flag, `CATCH_CONFIG_FAST_COMPILE`, trading features for compilation speed.
  * Moves debug breaks out of tests and into implementation, speeding up test compilation time (~10% on linux).
  * _More changes are coming_
* Added [TAP (Test Anything Protocol)](https://testanything.org/) and [Automake](https://www.gnu.org/software/automake/manual/html_node/Log-files-generation-and-test-results-recording.html#Log-files-generation-and-test-results-recording) reporters.
  * These are not present in the default single-include header and need to be downloaded from GitHub separately.
  * For details see [documentation about integrating with build systems](build-systems.md#top).
*  XML reporter now reports filename as part of the `Section` and `TestCase` tags.
* `Approx` now supports an optional margin of absolute error
  * It has also received [new documentation](assertions.md#top).

##### Fixes
* Silenced C4312 ("conversion from int to 'ClassName *") warnings in the evaluate layer.
* Fixed C4512 ("assignment operator could not be generated") warnings under VS2013.
* Cygwin compatibility fixes
  * Signal handling is no longer compiled by default.
  * Usage of `gettimeofday` inside Catch should no longer cause compilation errors.
* Improved `-Wparentheses` suppression for gcc (#674)
  * When compiled with gcc 4.8 or newer, the suppression is localized to assertions only
  * Otherwise it is suppressed for the whole TU
* Fixed test spec parser issue (with escapes in multiple names)

##### Other
* Various documentation fixes and improvements


### 1.7.x

#### 1.7.2

##### Fixes and minor improvements
Xml:

(technically the first two are breaking changes but are also fixes and arguably break few if any people)
* C-escape control characters instead of XML encoding them (which requires XML 1.1)
* Revert XML output to XML 1.0
* Can provide stylesheet references by extending the XML reporter
* Added description and tags attributes to XML Reporter
* Tags are closed and the stream flushed more eagerly to avoid stdout interpolation


Other:
* `REQUIRE_THROWS_AS` now catches exception by `const&` and reports expected type
* In `SECTION`s the file/ line is now of the `SECTION`. not the `TEST_CASE`
* Added std:: qualification to some functions from C stdlib
* Removed use of RTTI (`dynamic_cast`) that had crept back in
* Silenced a few more warnings in different circumstances
* Travis improvements

#### 1.7.1

##### Fixes:
* Fixed inconsistency in defining `NOMINMAX` and `WIN32_LEAN_AND_MEAN` inside `catch.hpp`.
* Fixed SEH-related compilation error under older MinGW compilers, by making Windows SEH handling opt-in for compilers other than MSVC.
  * For specifics, look into the [documentation](configuration.md#top).
* Fixed compilation error under MinGW caused by improper compiler detection.
* Fixed XML reporter sometimes leaving an empty output file when a test ends with signal/structured exception.
* Fixed XML reporter not reporting captured stdout/stderr.
* Fixed possible infinite recursion in Windows SEH.
* Fixed possible compilation error caused by Catch's operator overloads being ambiguous in regards to user-defined templated operators.

#### 1.7.0

##### Features/ Changes:
* Catch now runs significantly faster for passing tests
  * Microbenchmark focused on Catch's overhead went from ~3.4s to ~0.7s.
  * Real world test using [JSON for Modern C++](https://github.com/nlohmann/json)'s test suite went from ~6m 25s to ~4m 14s.
* Catch can now run specific sections within test cases.
  * For now the support is only basic (no wildcards or tags), for details see the [documentation](command-line.md#top).
* Catch now supports SEH on Windows as well as signals on Linux.
  * After receiving a signal, Catch reports failing assertion and then passes the signal onto the previous handler.
* Approx can be used to compare values against strong typedefs (available in C++11 mode only).
  * Strong typedefs mean types that are explicitly convertible to double.
* CHECK macro no longer stops executing section if an exception happens.
* Certain characters (space, tab, etc) are now pretty printed.
  * This means that a `char c = ' '; REQUIRE(c == '\t');` would be printed as `' ' == '\t'`, instead of ` == 9`.

##### Fixes:
* Text formatting no longer attempts to access out-of-bounds characters under certain conditions.
* THROW family of assertions no longer trigger `-Wunused-value` on expressions containing explicit cast.
* Breaking into debugger under OS X works again and no longer required `DEBUG` to be defined.
* Compilation no longer breaks under certain compiler if a lambda is used inside assertion macro.

##### Other:
* Catch's CMakeLists now defines install command.
* Catch's CMakeLists now generates projects with warnings enabled.


### 1.6.x

#### 1.6.1

##### Features/ Changes:
* Catch now supports breaking into debugger on Linux

##### Fixes:
* Generators no longer leak memory (generators are still unsupported in general)
* JUnit reporter now reports UTC timestamps, instead of "tbd"
* `CHECK_THAT` macro is now properly defined as `CATCH_CHECK_THAT` when using `CATCH_` prefixed macros

##### Other:
* Types with overloaded `&&` operator are no longer evaluated twice when used in an assertion macro.
* The use of `__COUNTER__` is suppressed when Catch is parsed by CLion
  * This change is not active when compiling a binary
* Approval tests can now be run on Windows
* CMake will now warn if a file is present in the `include` folder but not is not enumerated as part of the project
* Catch now defines `NOMINMAX` and `WIN32_LEAN_AND_MEAN` before including `windows.h`
  * This can be disabled if needed, see [documentation](configuration.md#top) for details.


#### 1.6.0

##### Cmake/ projects:
* Moved CMakeLists.txt to root, made it friendlier for CLion and generating XCode and VS projects, and removed the manually maintained XCode and VS projects.

##### Features/ Changes:
* Approx now supports `>=` and `<=`
* Can now use `\` to escape chars in test names on command line
* Standardize C++11 feature toggles

##### Fixes:
* Blue shell colour
* Missing argument to `CATCH_CHECK_THROWS`
* Don't encode extended ASCII in XML
* use `std::shuffle` on more compilers (fixes deprecation warning/error)
* Use `__COUNTER__` more consistently (where available)

##### Other:
* Tweaks and changes to scripts - particularly for Approval test - to make them more portable


## Even Older versions
Release notes were not maintained prior to v1.6.0, but you should be able to work them out from the Git history

---

[Home](Readme.md#top)



docs/release-process.md
--------------------------------------
<a id="top"></a>
# How to release

When enough changes have accumulated, it is time to release new version of Catch. This document describes the process in doing so, that no steps are forgotten. Note that all referenced scripts can be found in the `tools/scripts/` directory.

## Necessary steps

These steps are necessary and have to be performed before each new release. They serve to make sure that the new release is correct and linked-to from the standard places.


### Testing

All of the tests are currently run in our CI setup based on TravisCI and
AppVeyor. As long as the last commit tested green, the release can
proceed.


### Incrementing version number

Catch uses a variant of [semantic versioning](http://semver.org/), with breaking API changes (and thus major version increments) being very rare. Thus, the release will usually increment the patch version, when it only contains couple of bugfixes, or minor version, when it contains new functionality, or larger changes in implementation of current functionality.

After deciding which part of version number should be incremented, you can use one of the `*Release.py` scripts to perform the required changes to Catch.

This will take care of generating the single include header, updating
version numbers everywhere and pushing the new version to Wandbox.


### Release notes

Once a release is ready, release notes need to be written. They should summarize changes done since last release. For rough idea of expected notes see previous releases. Once written, release notes should be added to `docs/release-notes.md`.


### Commit and push update to GitHub

After version number is incremented, single-include header is regenerated and release notes are updated, changes should be committed and pushed to GitHub.


### Release on GitHub

After pushing changes to GitHub, GitHub release *needs* to be created.
Tag version and release title should be same as the new version,
description should contain the release notes for the current release.
We also attach the two amalgamated files as "binaries".

Since 2.5.0, the release tag and the "binaries" (amalgamated files) should
be PGP signed.

#### Signing a tag

To create a signed tag, use `git tag -s <VERSION>`, where `<VERSION>`
is the version being released, e.g. `git tag -s v2.6.0`.

Use the version name as the short message and the release notes as
the body (long) message.

#### Signing the amalgamated files

This will create ASCII-armored signatures for the two amalgamated files
that are uploaded to the GitHub release:

```
gpg --armor --output extras/catch_amalgamated.hpp.asc --detach-sig extras/catch_amalgamated.hpp
gpg --armor --output extras/catch_amalgamated.cpp.asc --detach-sig extras/catch_amalgamated.cpp
```

_GPG does not support signing multiple files in single invocation._



docs/reporter-events.md
--------------------------------------
<a id="top"></a>
# Reporter events

**Contents**<br>
[Test running events](#test-running-events)<br>
[Benchmarking events](#benchmarking-events)<br>
[Listings events](#listings-events)<br>
[Miscellaneous events](#miscellaneous-events)<br>

Reporter events are one of the customization points for user code. They
are used by [reporters](reporters.md#top) to customize Catch2's output,
and by [event listeners](event-listeners.md#top) to perform in-process
actions under some conditions.

There are currently 21 reporter events in Catch2, split between 4 distinct
event groups:
* test running events (10 events)
* benchmarking (4 events)
* listings (3 events)
* miscellaneous (4 events)

## Test running events

Test running events are always paired so that for each `fooStarting` event,
there is a `fooEnded` event. This means that the 10 test running events
consist of 5 pairs of events:

* `testRunStarting` and `testRunEnded`,
* `testCaseStarting` and `testCaseEnded`,
* `testCasePartialStarting` and `testCasePartialEnded`,
* `sectionStarting` and `sectionEnded`,
* `assertionStarting` and `assertionEnded`

### `testRun` events

```cpp
void testRunStarting( TestRunInfo const& testRunInfo );
void testRunEnded( TestRunStats const& testRunStats );
```

The `testRun` events bookend the entire test run. `testRunStarting` is
emitted before the first test case is executed, and `testRunEnded` is
emitted after all the test cases have been executed.

### `testCase` events

```cpp
void testCaseStarting( TestCaseInfo const& testInfo );
void testCaseEnded( TestCaseStats const& testCaseStats );
```

The `testCase` events bookend one _full_ run of a specific test case.
Individual runs through a test case, e.g. due to `SECTION`s or `GENERATE`s,
are handled by a different event.


### `testCasePartial` events

> Introduced in Catch2 3.0.1

```cpp
void testCasePartialStarting( TestCaseInfo const& testInfo, uint64_t partNumber );
void testCasePartialEnded(TestCaseStats const& testCaseStats, uint64_t partNumber );
```

`testCasePartial` events bookend one _partial_ run of a specific test case.
This means that for any given test case, these events can be emitted
multiple times, e.g. due to multiple leaf sections.

In regards to nesting with `testCase` events, `testCasePartialStarting`
will never be emitted before the corresponding `testCaseStarting`, and
`testCasePartialEnded` will always be emitted before the corresponding
`testCaseEnded`.


### `section` events

```cpp
void sectionStarting( SectionInfo const& sectionInfo );
void sectionEnded( SectionStats const& sectionStats );
```

`section` events are emitted only for active `SECTION`s, that is, sections
that are entered. Sections that are skipped in this test case run-through
do not cause events to be emitted.

_Note that test cases always contain one implicit section. The event for
this section is emitted after the corresponding `testCasePartialStarting`
event._


### `assertion` events

```cpp
void assertionStarting( AssertionInfo const& assertionInfo );
void assertionEnded( AssertionStats const& assertionStats );
```

The `assertionStarting` event is emitted before the expression in the
assertion is captured or evaluated and `assertionEnded` is emitted
afterwards. This means that given assertion like `REQUIRE(a + b == c + d)`,
Catch2 first emits `assertionStarting` event, then `a + b` and `c + d`
are evaluated, then their results are captured, the comparison is evaluated,
and then `assertionEnded` event is emitted.


## Benchmarking events

> [Introduced](https://github.com/catchorg/Catch2/issues/1616) in Catch2 2.9.0.

```cpp
void benchmarkPreparing( StringRef name ) override;
void benchmarkStarting( BenchmarkInfo const& benchmarkInfo ) override;
void benchmarkEnded( BenchmarkStats<> const& benchmarkStats ) override;
void benchmarkFailed( StringRef error ) override;
```

Due to the benchmark lifecycle being bit more complicated, the benchmarking
events have their own category, even though they could be seen as parallel
to the `assertion*` events. You should expect running a benchmark to
generate at least 2 of the events above.

To understand the explanation below, you should read the [benchmarking
documentation](benchmarks.md#top) first.

* `benchmarkPreparing` event is sent after the environmental probe
finishes, but before the user code is first estimated.
* `benchmarkStarting` event is sent after the user code is estimated,
but has not been benchmarked yet.
* `benchmarkEnded` event is sent after the user code has been benchmarked,
and contains the benchmarking results.
* `benchmarkFailed` event is sent if either the estimation or the
benchmarking itself fails.


## Listings events

> Introduced in Catch2 3.0.1.

Listings events are events that correspond to the test binary being
invoked with `--list-foo` flag.

There are currently 3 listing events, one for reporters, one for tests,
and one for tags. Note that they are not exclusive to each other.

```cpp
void listReporters( std::vector<ReporterDescription> const& descriptions );
void listTests( std::vector<TestCaseHandle> const& tests );
void listTags( std::vector<TagInfo> const& tagInfos );
```


## Miscellaneous events

```cpp
void reportInvalidTestSpec( StringRef unmatchedSpec );
void fatalErrorEncountered( StringRef error );
void noMatchingTestCases( StringRef unmatchedSpec );
```

These are one-off events that do not neatly fit into other categories.

`reportInvalidTestSpec` is sent for each [test specification command line
argument](command-line.md#specifying-which-tests-to-run) that wasn't
parsed into a valid spec.

`fatalErrorEncountered` is sent when Catch2's POSIX signal handling
or Windows SE handler is called into with a fatal signal/exception.

`noMatchingTestCases` is sent for each user provided test specification
that did not match any registered tests.

---

[Home](Readme.md#top)



docs/reporters.md
--------------------------------------
<a id="top"></a>
# Reporters

Reporters are a customization point for most of Catch2's output, e.g.
formatting and writing out [assertions (whether passing or failing),
sections, test cases, benchmarks, and so on](reporter-events.md#top).

Catch2 comes with a bunch of reporters by default (currently 9), and
you can also write your own reporter. Because multiple reporters can
be active at the same time, your own reporters do not even have to handle
all reporter event, just the ones you are interested in, e.g. benchmarks.


## Using different reporters

You can see which reporters are available by running the test binary
with `--list-reporters`. You can then pick one of them with the [`-r`,
`--reporter` option](command-line.md#choosing-a-reporter-to-use), followed
by the name of the desired reporter, like so:

```
--reporter xml
```

You can also select multiple reporters to be used at the same time.
In that case you should read the [section on using multiple
reporters](#multiple-reporters) to avoid any surprises from doing so.


<a id="multiple-reporters"></a>
## Using multiple reporters

> Support for having multiple parallel reporters was [introduced](https://github.com/catchorg/Catch2/pull/2183) in Catch2 3.0.1

Catch2 supports using multiple reporters at the same time while having
them write into different destinations. The two main uses of this are

* having both human-friendly and machine-parseable (e.g. in JUnit format)
  output from one run of binary
* having "partial" reporters that are highly specialized, e.g. having one
  reporter that writes out benchmark results as markdown tables and does
  nothing else, while also having standard testing output separately

Specifying multiple reporter looks like this:
```
--reporter JUnit::out=result-junit.xml --reporter console::out=-::colour-mode=ansi
```

This tells Catch2 to use two reporters, `JUnit` reporter that writes
its machine-readable XML output to file `result-junit.xml`, and the
`console` reporter that writes its user-friendly output to stdout and
uses ANSI colour codes for colouring the output.

Using multiple reporters (or one reporter and one-or-more [event
listeners](event-listeners.md#top)) can have surprisingly complex semantics
when using customization points provided to reporters by Catch2, namely
capturing stdout/stderr from test cases.

As long as at least one reporter (or listener) asks Catch2 to capture
stdout/stderr, captured stdout and stderr will be available to all
reporters and listeners.

Because this might be surprising to the users, if at least one active
_reporter_ is non-capturing, then Catch2 tries to roughly emulate
non-capturing behaviour by printing out the captured stdout/stderr
just before `testCasePartialEnded` event is sent out to the active
reporters and listeners. This means that stdout/stderr is no longer
printed out from tests as it is being written, but instead it is written
out in batch after each runthrough of a test case is finished.



## Writing your own reporter

You can also write your own custom reporter and tell Catch2 to use it.
When writing your reporter, you have two options:

* Derive from `Catch::ReporterBase`. When doing this, you will have
  to provide handling for all [reporter events](reporter-events.md#top).
* Derive from one of the provided [utility reporter bases in
  Catch2](#utility-reporter-bases).

Generally we recommend doing the latter, as it is less work.

Apart from overriding handling of the individual reporter events, reporters
have access to some extra customization points, described below.


### Utility reporter bases

Catch2 currently provides two utility reporter bases:

* `Catch::StreamingReporterBase`
* `Catch::CumulativeReporterBase`

`StreamingReporterBase` is useful for reporters that can format and write
out the events as they come in. It provides (usually empty) implementation
for all reporter events, and if you let it handle the relevant events,
it also handles storing information about active test run and test case.

`CumulativeReporterBase` is a base for reporters that need to see the whole
test run, before they can start writing the output, such as the JUnit
and SonarQube reporters. This post-facto approach requires the assertions
to be stringified when it is finished, so that the assertion can be written
out later. Because the stringification can be expensive, and not all
cumulative reporters need the assertions, this base provides customization
point to change whether the assertions are saved or not, separate for
passing and failing assertions.


_Generally we recommend that if you override a member function from either
of the bases, you call into the base's implementation first. This is not
necessarily in all cases, but it is safer and easier._


Writing your own reporter then looks like this:

```cpp
#include <catch2/reporters/catch_reporter_streaming_base.hpp>
#include <catch2/catch_test_case_info.hpp>
#include <catch2/reporters/catch_reporter_registrars.hpp>

#include <iostream>

class PartialReporter : public Catch::StreamingReporterBase {
public:
    using StreamingReporterBase::StreamingReporterBase;

    static std::string getDescription() {
        return "Reporter for testing TestCasePartialStarting/Ended events";
    }

    void testCasePartialStarting(Catch::TestCaseInfo const& testInfo,
                                 uint64_t partNumber) override {
        std::cout << "TestCaseStartingPartial: " << testInfo.name << '#' << partNumber << '\n';
    }

    void testCasePartialEnded(Catch::TestCaseStats const& testCaseStats,
                              uint64_t partNumber) override {
        std::cout << "TestCasePartialEnded: " << testCaseStats.testInfo->name << '#' << partNumber << '\n';
    }
};


CATCH_REGISTER_REPORTER("partial", PartialReporter)
```

This create a simple reporter that responds to `testCasePartial*` events,
and calls itself "partial" reporter, so it can be invoked with
`--reporter partial` command line flag.


### `ReporterPreferences`

Each reporter instance contains instance of `ReporterPreferences`, a type
that holds flags for the behaviour of Catch2 when this reporter run.
Currently there are two customization options:

* `shouldRedirectStdOut` - whether the reporter wants to handle
   writes to stdout/stderr from user code, or not. This is useful for
   reporters that output machine-parseable output, e.g. the JUnit
   reporter, or the XML reporter.
* `shouldReportAllAssertions` - whether the reporter wants to handle
  `assertionEnded` events for passing assertions as well as failing
   assertions. Usually reporters do not report successful assertions
   and don't need them for their output, but sometimes the desired output
   format includes passing assertions even without the `-s` flag.


### Per-reporter configuration

> Per-reporter configuration was introduced in Catch2 3.0.1

Catch2 supports some configuration to happen per reporter. The configuration
options fall into one of two categories:

* Catch2-recognized options
* Reporter-specific options

The former is a small set of universal options that Catch2 handles for
the reporters, e.g. output file or console colour mode. The latter are
options that the reporters have to handle themselves, but the keys and
values can be arbitrary strings, as long as they don't contain `::`. This
allows writing reporters that can be significantly customized at runtime.

Reporter-specific options always have to be prefixed with "X" (large
letter X).


### Other expected functionality of a reporter

When writing a custom reporter, there are few more things that you should
keep in mind. These are not important for correctness, but they are
important for the reporter to work _nicely_.

* Catch2 provides a simple verbosity option for users. There are three
  verbosity levels, "quiet", "normal", and "high", and if it makes sense
  for reporter's output format, it should respond to these by changing
  what, and how much, it writes out.

* Catch2 operates with an rng-seed. Knowing what seed a test run had
  is important if you want to replicate it, so your reporter should
  report the rng-seed, if at all possible given the target output format.

* Catch2 also operates with test filters, or test specs. If a filter
  is present, you should also report the filter, if at all possible given
  the target output format.



---

[Home](Readme.md#top)



docs/skipping-passing-failing.md
--------------------------------------
<a id="top"></a>
# Explicitly skipping, passing, and failing tests at runtime

## Skipping Test Cases at Runtime

> [Introduced](https://github.com/catchorg/Catch2/pull/2360) in Catch2 3.3.0.

In some situations it may not be possible to meaningfully execute a test case,
for example when the system under test is missing certain hardware capabilities.
If the required conditions can only be determined at runtime, it often
doesn't make sense to consider such a test case as either passed or failed,
because it simply cannot run at all.

To properly express such scenarios, Catch2 provides a way to explicitly
_skip_ test cases, using the `SKIP` macro:

```
SKIP( [streamable expression] )
```

Example usage:

```c++
TEST_CASE("copy files between drives") {
    if(getNumberOfHardDrives() < 2) {
        SKIP("at least two hard drives required");
    }
    // ...
}
```

This test case is then reported as _skipped_ instead of _passed_ or _failed_.

The `SKIP` macro behaves similarly to an explicit [`FAIL`](#passing-and-failing-test-cases),
in that it is the last expression that will be executed:

```c++
TEST_CASE("my test") {
    printf("foo");
    SKIP();
    printf("bar"); // not printed
}
```

However a failed assertion _before_ a `SKIP` still causes the entire
test case to fail:

```c++
TEST_CASE("failing test") {
    CHECK(1 == 2);
    SKIP();
}
```

### Interaction with Sections and Generators

Sections, nested sections as well as specific outputs from [generators](generators.md#top)
can all be individually skipped, with the rest executing as usual:

```c++
TEST_CASE("complex test case") {
  int value = GENERATE(2, 4, 6);
  SECTION("a") {
    SECTION("a1") { CHECK(value < 8); }
    SECTION("a2") {
      if (value == 4) {
        SKIP();
      }
      CHECK(value % 2 == 0);
    }
  }
}
```

This test case will report 5 passing assertions; one for each of the three
values in section `a1`, and then two in section `a2`, from values 2 and 4.

Note that as soon as one section is skipped, the entire test case will
be reported as _skipped_ (unless there is a failing assertion, in which
case the test is handled as _failed_ instead).

Note that if all test cases in a run are skipped, Catch2 returns a non-zero
exit code, same as it does if no test cases have run. This behaviour can
be overridden using the [--allow-running-no-tests](command-line.md#no-tests-override)
flag.

### `SKIP` inside generators

You can also use the `SKIP` macro inside generator's constructor to handle
cases where the generator is empty, but you do not want to fail the test
case.


## Passing and failing test cases

Test cases can also be explicitly passed or failed, without the use of
assertions, and with a specific message. This can be useful to handle
complex preconditions/postconditions and give useful error messages
when they fail.

* `SUCCEED( [streamable expression] )`

`SUCCEED` is morally equivalent with `INFO( [streamable expression] ); REQUIRE( true );`.
Note that it does not stop further test execution, so it cannot be used
to guard failing assertions from being executed.

_In practice, `SUCCEED` is usually used as a test placeholder, to avoid
[failing a test case due to missing assertions](command-line.md#warnings)._

```cpp
TEST_CASE( "SUCCEED showcase" ) {
    int I = 1;
    SUCCEED( "I is " << I );
    // ... execution continues here ...
}
```

* `FAIL( [streamable expression] )`

`FAIL` is morally equivalent with `INFO( [streamable expression] ); REQUIRE( false );`.

_In practice, `FAIL` is usually used to stop executing test that is currently
known to be broken, but has to be fixed later._

```cpp
TEST_CASE( "FAIL showcase" ) {
    FAIL( "This test case causes segfault, which breaks CI." );
    // ... this will not be executed ...
}
```


---

[Home](Readme.md#top)



docs/test-cases-and-sections.md
--------------------------------------
<a id="top"></a>
# Test cases and sections

**Contents**<br>
[Tags](#tags)<br>
[Tag aliases](#tag-aliases)<br>
[BDD-style test cases](#bdd-style-test-cases)<br>
[Type parametrised test cases](#type-parametrised-test-cases)<br>
[Signature based parametrised test cases](#signature-based-parametrised-test-cases)<br>

While Catch fully supports the traditional, xUnit, style of class-based fixtures containing test case methods this is not the preferred style.

Instead Catch provides a powerful mechanism for nesting test case sections within a test case. For a more detailed discussion see the [tutorial](tutorial.md#test-cases-and-sections).

Test cases and sections are very easy to use in practice:

* **TEST_CASE(** _test name_ \[, _tags_ \] **)**
* **SECTION(** _section name_, \[, _section description_ \] **)**


_test name_ and _section name_ are free form, quoted, strings.
The optional _tags_ argument is a quoted string containing one or more
tags enclosed in square brackets, and are discussed below.
_section description_ can be used to provide long form description
of a section while keeping the _section name_ short for use with the
[`-c` command line parameter](command-line.md#specify-the-section-to-run).

**The combination of test names and tags must be unique within the Catch2
executable.**

For examples see the [Tutorial](tutorial.md#top)

## Tags

Tags allow an arbitrary number of additional strings to be associated with a test case. Test cases can be selected (for running, or just for listing) by tag - or even by an expression that combines several tags. At their most basic level they provide a simple way to group several related tests together.

As an example - given the following test cases:

    TEST_CASE( "A", "[widget]" ) { /* ... */ }
    TEST_CASE( "B", "[widget]" ) { /* ... */ }
    TEST_CASE( "C", "[gadget]" ) { /* ... */ }
    TEST_CASE( "D", "[widget][gadget]" ) { /* ... */ }

The tag expression, ```"[widget]"``` selects A, B & D. ```"[gadget]"``` selects C & D. ```"[widget][gadget]"``` selects just D and ```"[widget],[gadget]"``` selects all four test cases.

For more detail on command line selection see [the command line docs](command-line.md#specifying-which-tests-to-run)

Tag names are not case sensitive and can contain any ASCII characters.
This means that tags `[tag with spaces]` and `[I said "good day"]`
are both allowed tags and can be filtered on. However, escapes are not
supported and `[\]]` is not a valid tag.

The same tag can be specified multiple times for a single test case,
but only one of the instances of identical tags will be kept. Which one
is kept is functionally random.


### Special Tags

All tag names beginning with non-alphanumeric characters are reserved by Catch. Catch defines a number of "special" tags, which have meaning to the test runner itself. These special tags all begin with a symbol character. Following is a list of currently defined special tags and their meanings.

* `[.]` - causes test cases to be skipped from the default list (i.e. when no test cases have been explicitly selected through tag expressions or name wildcards). The hide tag is often combined with another, user, tag (for example `[.][integration]` - so all integration tests are excluded from the default run but can be run by passing `[integration]` on the command line). As a short-cut you can combine these by simply prefixing your user tag with a `.` - e.g. `[.integration]`.

* `[!throws]` - lets Catch know that this test is likely to throw an exception even if successful. This causes the test to be excluded when running with `-e` or `--nothrow`.

* `[!mayfail]` - doesn't fail the test if any given assertion fails (but still reports it). This can be useful to flag a work-in-progress, or a known issue that you don't want to immediately fix but still want to track in your tests.

* `[!shouldfail]` - like `[!mayfail]` but *fails* the test if it *passes*. This can be useful if you want to be notified of accidental, or third-party, fixes.

* `[!nonportable]` - Indicates that behaviour may vary between platforms or compilers.

* `[#<filename>]` - these tags are added to test cases when you run Catch2
                    with [`-#` or `--filenames-as-tags`](command-line.md#filenames-as-tags).

* `[@<alias>]` - tag aliases all begin with `@` (see below).

* `[!benchmark]` - this test case is actually a benchmark. Currently this only serves to hide the test case by default, to avoid the execution time costs.


## Tag aliases

Between tag expressions and wildcarded test names (as well as combinations of the two) quite complex patterns can be constructed to direct which test cases are run. If a complex pattern is used often it is convenient to be able to create an alias for the expression. This can be done, in code, using the following form:

    CATCH_REGISTER_TAG_ALIAS( <alias string>, <tag expression> )

Aliases must begin with the `@` character. An example of a tag alias is:

    CATCH_REGISTER_TAG_ALIAS( "[@nhf]", "[failing]~[.]" )

Now when `[@nhf]` is used on the command line this matches all tests that are tagged `[failing]`, but which are not also hidden.

## BDD-style test cases

In addition to Catch's take on the classic style of test cases, Catch supports an alternative syntax that allow tests to be written as "executable specifications" (one of the early goals of [Behaviour Driven Development](http://dannorth.net/introducing-bdd/)). This set of macros map on to ```TEST_CASE```s and ```SECTION```s, with a little internal support to make them smoother to work with.

* **SCENARIO(** _scenario name_ \[, _tags_ \] **)**

This macro maps onto ```TEST_CASE``` and works in the same way, except that the test case name will be prefixed by "Scenario: "

* **GIVEN(** _something_ **)**
* **WHEN(** _something_ **)**
* **THEN(** _something_ **)**

These macros map onto ```SECTION```s except that the section names are the _something_ texts prefixed by
"given: ", "when: " or "then: " respectively. These macros also map onto the AAA or A<sup>3</sup> test pattern
(standing either for [Assemble-Activate-Assert](http://wiki.c2.com/?AssembleActivateAssert) or
[Arrange-Act-Assert](http://wiki.c2.com/?ArrangeActAssert)), and in this context, the macros provide both code
documentation and reporting of these parts of a test case without the need for extra comments or code to do so.

Semantically, a `GIVEN` clause may have multiple _independent_ `WHEN` clauses within it. This allows a test
to have, e.g., one set of "given" objects and multiple subtests using those objects in various ways in each
of the `WHEN` clauses without repeating the initialisation from the `GIVEN` clause. When there are _dependent_
clauses -- such as a second `WHEN` clause that should only happen _after_ the previous `WHEN` clause has been
executed and validated -- there are additional macros starting with `AND_`:

* **AND_GIVEN(** _something_ **)**
* **AND_WHEN(** _something_ **)**
* **AND_THEN(** _something_ **)**

These are used to chain ```GIVEN```s, ```WHEN```s and ```THEN```s together. The `AND_*` clause is placed
_inside_ the clause on which it depends. There can be multiple _independent_ clauses that are all _dependent_
on a single outer clause.
```cpp
SCENARIO( "vector can be sized and resized" ) {
    GIVEN( "An empty vector" ) {
        auto v = std::vector<std::string>{};

        // Validate assumption of the GIVEN clause
        THEN( "The size and capacity start at 0" ) {
            REQUIRE( v.size() == 0 );
            REQUIRE( v.capacity() == 0 );
        }

        // Validate one use case for the GIVEN object
        WHEN( "push_back() is called" ) {
            v.push_back("hullo");

            THEN( "The size changes" ) {
                REQUIRE( v.size() == 1 );
                REQUIRE( v.capacity() >= 1 );
            }
        }
    }
}
```

This code will result in two runs through the scenario:
```
Scenario : vector can be sized and resized
  Given  : An empty vector
  Then   : The size and capacity start at 0

Scenario : vector can be sized and resized
  Given  : An empty vector
  When   : push_back() is called
  Then   : The size changes
```

See also [runnable example on godbolt](https://godbolt.org/z/eY5a64r99),
with a more complicated (and failing) example.

> `AND_GIVEN` was [introduced](https://github.com/catchorg/Catch2/issues/1360) in Catch2 2.4.0.

When any of these macros are used the console reporter recognises them and formats the test case header such that the Givens, Whens and Thens are aligned to aid readability.

Other than the additional prefixes and the formatting in the console reporter these macros behave exactly as ```TEST_CASE```s and ```SECTION```s. As such there is nothing enforcing the correct sequencing of these macros - that's up to the programmer!

## Type parametrised test cases

In addition to `TEST_CASE`s, Catch2 also supports test cases parametrised
by types, in the form of `TEMPLATE_TEST_CASE`,
`TEMPLATE_PRODUCT_TEST_CASE` and `TEMPLATE_LIST_TEST_CASE`. These macros
are defined in the `catch_template_test_macros.hpp` header, so compiling
the code examples below also requires
`#include <catch2/catch_template_test_macros.hpp>`.


* **TEMPLATE_TEST_CASE(** _test name_ , _tags_,  _type1_, _type2_, ..., _typen_ **)**

> [Introduced](https://github.com/catchorg/Catch2/issues/1437) in Catch2 2.5.0.

_test name_ and _tag_ are exactly the same as they are in `TEST_CASE`,
with the difference that the tag string must be provided (however, it
can be empty). _type1_ through _typen_ is the list of types for which
this test case should run, and, inside the test code, the current type
is available as the `TestType` type.

Because of limitations of the C++ preprocessor, if you want to specify
a type with multiple template parameters, you need to enclose it in
parentheses, e.g. `std::map<int, std::string>` needs to be passed as
`(std::map<int, std::string>)`.

Example:
```cpp
TEMPLATE_TEST_CASE( "vectors can be sized and resized", "[vector][template]", int, std::string, (std::tuple<int,float>) ) {

    std::vector<TestType> v( 5 );

    REQUIRE( v.size() == 5 );
    REQUIRE( v.capacity() >= 5 );

    SECTION( "resizing bigger changes size and capacity" ) {
        v.resize( 10 );

        REQUIRE( v.size() == 10 );
        REQUIRE( v.capacity() >= 10 );
    }
    SECTION( "resizing smaller changes size but not capacity" ) {
        v.resize( 0 );

        REQUIRE( v.size() == 0 );
        REQUIRE( v.capacity() >= 5 );

        SECTION( "We can use the 'swap trick' to reset the capacity" ) {
            std::vector<TestType> empty;
            empty.swap( v );

            REQUIRE( v.capacity() == 0 );
        }
    }
    SECTION( "reserving smaller does not change size or capacity" ) {
        v.reserve( 0 );

        REQUIRE( v.size() == 5 );
        REQUIRE( v.capacity() >= 5 );
    }
}
```

* **TEMPLATE_PRODUCT_TEST_CASE(** _test name_ , _tags_, (_template-type1_, _template-type2_, ..., _template-typen_), (_template-arg1_, _template-arg2_, ..., _template-argm_) **)**

> [Introduced](https://github.com/catchorg/Catch2/issues/1468) in Catch2 2.6.0.

_template-type1_ through _template-typen_ is list of template
types which should be combined with each of _template-arg1_ through
 _template-argm_, resulting in _n * m_ test cases. Inside the test case,
the resulting type is available under the name of `TestType`.

To specify more than 1 type as a single _template-type_ or _template-arg_,
you must enclose the types in an additional set of parentheses, e.g.
`((int, float), (char, double))` specifies 2 template-args, each
consisting of 2 concrete types (`int`, `float` and `char`, `double`
respectively). You can also omit the outer set of parentheses if you
specify only one type as the full set of either the _template-types_,
or the _template-args_.


Example:
```cpp
template< typename T>
struct Foo {
    size_t size() {
        return 0;
    }
};

TEMPLATE_PRODUCT_TEST_CASE("A Template product test case", "[template][product]", (std::vector, Foo), (int, float)) {
    TestType x;
    REQUIRE(x.size() == 0);
}
```

You can also have different arities in the _template-arg_ packs:
```cpp
TEMPLATE_PRODUCT_TEST_CASE("Product with differing arities", "[template][product]", std::tuple, (int, (int, double), (int, double, float))) {
    TestType x;
    REQUIRE(std::tuple_size<TestType>::value >= 1);
}
```

* **TEMPLATE_LIST_TEST_CASE(** _test name_, _tags_, _type list_ **)**

> [Introduced](https://github.com/catchorg/Catch2/issues/1627) in Catch2 2.9.0.

_type list_ is a generic list of types on which test case should be instantiated.
List can be `std::tuple`, `boost::mpl::list`, `boost::mp11::mp_list` or anything with
`template <typename...>` signature.

This allows you to reuse the _type list_ in multiple test cases.

Example:
```cpp
using MyTypes = std::tuple<int, char, float>;
TEMPLATE_LIST_TEST_CASE("Template test case with test types specified inside std::tuple", "[template][list]", MyTypes)
{
    REQUIRE(sizeof(TestType) > 0);
}
```


## Signature based parametrised test cases

> [Introduced](https://github.com/catchorg/Catch2/issues/1609) in Catch2 2.8.0.

In addition to [type parametrised test cases](#type-parametrised-test-cases) Catch2 also supports
signature base parametrised test cases, in form of `TEMPLATE_TEST_CASE_SIG` and `TEMPLATE_PRODUCT_TEST_CASE_SIG`.
These test cases have similar syntax like [type parametrised test cases](#type-parametrised-test-cases), with one
additional positional argument which specifies the signature. These macros are defined in the
`catch_template_test_macros.hpp` header, so compiling the code examples below also requires
`#include <catch2/catch_template_test_macros.hpp>`.

### Signature
Signature has some strict rules for these tests cases to work properly:
* signature with multiple template parameters e.g. `typename T, size_t S` must have this format in test case declaration
  `((typename T, size_t S), T, S)`
* signature with variadic template arguments e.g. `typename T, size_t S, typename...Ts` must have this format in test case declaration
  `((typename T, size_t S, typename...Ts), T, S, Ts...)`
* signature with single non type template parameter e.g. `int V` must have this format in test case declaration `((int V), V)`
* signature with single type template parameter e.g. `typename T` should not be used as it is in fact `TEMPLATE_TEST_CASE`

Currently Catch2 support up to 11 template parameters in signature

### Examples

* **TEMPLATE_TEST_CASE_SIG(** _test name_ , _tags_,  _signature_, _type1_, _type2_, ..., _typen_ **)**

Inside `TEMPLATE_TEST_CASE_SIG` test case you can use the names of template parameters as defined in _signature_.

```cpp
TEMPLATE_TEST_CASE_SIG("TemplateTestSig: arrays can be created from NTTP arguments", "[vector][template][nttp]",
  ((typename T, int V), T, V), (int,5), (float,4), (std::string,15), ((std::tuple<int, float>), 6)) {

    std::array<T, V> v;
    REQUIRE(v.size() > 1);
}
```

* **TEMPLATE_PRODUCT_TEST_CASE_SIG(** _test name_ , _tags_, _signature_, (_template-type1_, _template-type2_, ..., _template-typen_), (_template-arg1_, _template-arg2_, ..., _template-argm_) **)**

```cpp

template<typename T, size_t S>
struct Bar {
    size_t size() { return S; }
};

TEMPLATE_PRODUCT_TEST_CASE_SIG("A Template product test case with array signature", "[template][product][nttp]", ((typename T, size_t S), T, S), (std::array, Bar), ((int, 9), (float, 42))) {
    TestType x;
    REQUIRE(x.size() > 0);
}
```


---

[Home](Readme.md#top)



docs/test-fixtures.md
--------------------------------------
<a id="top"></a>
# Test fixtures

**Contents**<br>
[Non-Templated test fixtures](#non-templated-test-fixtures)<br>
[Templated test fixtures](#templated-test-fixtures)<br>
[Signature-based parameterised test fixtures](#signature-based-parametrised-test-fixtures)<br>
[Template fixtures with types specified in template type lists](#template-fixtures-with-types-specified-in-template-type-lists)<br>

## Non-Templated test fixtures

Although Catch2 allows you to group tests together as 
[sections within a test case](test-cases-and-sections.md), it can still 
be convenient, sometimes, to group them using a more traditional test. 
Catch2 fully supports this too with 3 different macros for 
non-templated test fixtures. They are: 

| Macro    | Description |
|----------|-------------|
|1. `TEST_CASE_METHOD(className, ...)`| Creates a uniquely named class which inherits from the class specified by `className`. The test function will be a member of this derived class. An instance of the derived class will be created for every partial run of the test case. |
|2. `METHOD_AS_TEST_CASE(member-function, ...)`| Uses `member-function` as the test function. An instance of the class will be created for each partial run of the test case. |
|3. `TEST_CASE_PERSISTENT_FIXTURE(className, ...)`| Creates a uniquely named class which inherits from the class specified by `className`. The test function will be a member of this derived class. An instance of the derived class will be created at the start of the test run. That instance will be destroyed once the entire test case has ended. |

### 1. `TEST_CASE_METHOD`


You define a `TEST_CASE_METHOD` test fixture as a simple structure:

```c++
class UniqueTestsFixture {
  private:
   static int uniqueID;
  protected:
   DBConnection conn;
  public:
   UniqueTestsFixture() : conn(DBConnection::createConnection("myDB")) {
   }
  protected:
   int getID() {
     return ++uniqueID;
   }
 };

 int UniqueTestsFixture::uniqueID = 0;

 TEST_CASE_METHOD(UniqueTestsFixture, "Create Employee/No Name", "[create]") {
   REQUIRE_THROWS(conn.executeSQL("INSERT INTO employee (id, name) VALUES (?, ?)", getID(), ""));
 }
 TEST_CASE_METHOD(UniqueTestsFixture, "Create Employee/Normal", "[create]") {
   REQUIRE(conn.executeSQL("INSERT INTO employee (id, name) VALUES (?, ?)", getID(), "Joe Bloggs"));
 }
```

The two test cases here will create uniquely-named derived classes of 
UniqueTestsFixture and thus can access the `getID()` protected method 
and `conn` member variables. This ensures that both the test cases 
are able to create a DBConnection using the same method 
(DRY principle) and that any ID's created are unique such that the 
order that tests are executed does not matter. 

### 2. `METHOD_AS_TEST_CASE`

`METHOD_AS_TEST_CASE` lets you register a member function of a class 
as a Catch2 test case. The class will be separately instantiated 
for each method registered in this way.

```cpp
class TestClass {
    std::string s;

public:
    TestClass()
        :s( "hello" )
    {}

    void testCase() {
        REQUIRE( s == "hello" );
    }
};


METHOD_AS_TEST_CASE( TestClass::testCase, "Use class's method as a test case", "[class]" )
```

This type of fixture is similar to [TEST_CASE_METHOD](#1-test_case_method) except in this 
case it will directly use the provided class to create an object rather than a derived 
class.

### 3. `TEST_CASE_PERSISTENT_FIXTURE`

> [Introduced](https://github.com/catchorg/Catch2/pull/2885) in Catch2 3.7.0

`TEST_CASE_PERSISTENT_FIXTURE` behaves in the same way as
[TEST_CASE_METHOD](#1-test_case_method) except that there will only be
one instance created throughout the entire run of a test case. To 
demonstrate this have a look at the following example:

```cpp
class ClassWithExpensiveSetup {
public:
    ClassWithExpensiveSetup() {
        // expensive construction
        std::this_thread::sleep_for( std::chrono::seconds( 2 ) );
    }

    ~ClassWithExpensiveSetup() noexcept {
        // expensive destruction
        std::this_thread::sleep_for( std::chrono::seconds( 1 ) );
    }

    int getInt() const { return 42; }
};

struct MyFixture {
    mutable int myInt = 0;
    ClassWithExpensiveSetup expensive;
};

TEST_CASE_PERSISTENT_FIXTURE( MyFixture, "Tests with MyFixture" ) {

    const int val = myInt++;

    SECTION( "First partial run" ) {
        const auto otherValue = expensive.getInt();
        REQUIRE( val == 0 );
        REQUIRE( otherValue == 42 );
    }

    SECTION( "Second partial run" ) { REQUIRE( val == 1 ); }
}
```

This example demonstates two possible use-cases of this fixture type:
1. Improve test run times by reducing the amount of expensive and 
redundant setup and tear-down required.
2. Reusing results from the previous partial run, in the current
partial run.

This test case will be executed twice as there are two leaf sections.
On the first run `val` will be `0` and on the second run `val` will be 
`1`. This demonstrates that we were able to use the results of the
previous partial run in subsequent partial runs.

Additionally, we are simulating an expensive object using 
`std::this_thread::sleep_for`, but real world use-cases could be:
1. Creating a D3D12/Vulkan device
2. Connecting to a database
3. Loading a file.

The fixture object (`MyFixture`) will be constructed just before the
test case begins, and it will be destroyed just after the test case 
ends. Therefore, this expensive object will only be created and 
destroyed once during the execution of this test case. If we had used 
`TEST_CASE_METHOD`, `MyFixture` would have been created and destroyed 
twice during the execution of this test case.

NOTE: The member function which runs the test case is `const`. Therefore 
if you want to mutate any member of the fixture it must be marked as
`mutable` as shown in this example. This is to make it clear that
the initial state of the fixture is intended to mutate during the
execution of the test case.

## Templated test fixtures

Catch2 also provides `TEMPLATE_TEST_CASE_METHOD` and
`TEMPLATE_PRODUCT_TEST_CASE_METHOD` that can be used together
with templated fixtures and templated template fixtures to perform
tests for multiple different types. Unlike `TEST_CASE_METHOD`,
`TEMPLATE_TEST_CASE_METHOD` and `TEMPLATE_PRODUCT_TEST_CASE_METHOD` do
require the tag specification to be non-empty, as it is followed by
further macro arguments.

Also note that, because of limitations of the C++ preprocessor, if you
want to specify a type with multiple template parameters, you need to
enclose it in parentheses, e.g. `std::map<int, std::string>` needs to be
passed as `(std::map<int, std::string>)`.
In the case of `TEMPLATE_PRODUCT_TEST_CASE_METHOD`, if a member of the
type list should consist of more than single type, it needs to be enclosed
in another pair of parentheses, e.g. `(std::map, std::pair)` and
`((int, float), (char, double))`.

Example:
```cpp
template< typename T >
struct Template_Fixture {
    Template_Fixture(): m_a(1) {}

    T m_a;
};

TEMPLATE_TEST_CASE_METHOD(Template_Fixture,
                          "A TEMPLATE_TEST_CASE_METHOD based test run that succeeds",
                          "[class][template]",
                          int, float, double) {
    REQUIRE( Template_Fixture<TestType>::m_a == 1 );
}

template<typename T>
struct Template_Template_Fixture {
    Template_Template_Fixture() {}

    T m_a;
};

template<typename T>
struct Foo_class {
    size_t size() {
        return 0;
    }
};

TEMPLATE_PRODUCT_TEST_CASE_METHOD(Template_Template_Fixture,
                                  "A TEMPLATE_PRODUCT_TEST_CASE_METHOD based test succeeds",
                                  "[class][template]",
                                  (Foo_class, std::vector),
                                  int) {
    REQUIRE( Template_Template_Fixture<TestType>::m_a.size() == 0 );
}
```

_While there is an upper limit on the number of types you can specify
in single `TEMPLATE_TEST_CASE_METHOD` or `TEMPLATE_PRODUCT_TEST_CASE_METHOD`,
the limit is very high and should not be encountered in practice._

## Signature-based parameterised test fixtures

> [Introduced](https://github.com/catchorg/Catch2/issues/1609) in Catch2 2.8.0.

Catch2 also provides `TEMPLATE_TEST_CASE_METHOD_SIG` and `TEMPLATE_PRODUCT_TEST_CASE_METHOD_SIG` to support
fixtures using non-type template parameters. These test cases work similar to `TEMPLATE_TEST_CASE_METHOD` and `TEMPLATE_PRODUCT_TEST_CASE_METHOD`,
with additional positional argument for [signature](test-cases-and-sections.md#signature-based-parametrised-test-cases).

Example:
```cpp
template <int V>
struct Nttp_Fixture{
    int value = V;
};

TEMPLATE_TEST_CASE_METHOD_SIG(
    Nttp_Fixture,
    "A TEMPLATE_TEST_CASE_METHOD_SIG based test run that succeeds",
    "[class][template][nttp]",
    ((int V), V),
    1, 3, 6) {
    REQUIRE(Nttp_Fixture<V>::value > 0);
}

template<typename T>
struct Template_Fixture_2 {
    Template_Fixture_2() {}

    T m_a;
};

template< typename T, size_t V>
struct Template_Foo_2 {
    size_t size() { return V; }
};

TEMPLATE_PRODUCT_TEST_CASE_METHOD_SIG(
    Template_Fixture_2,
    "A TEMPLATE_PRODUCT_TEST_CASE_METHOD_SIG based test run that succeeds",
    "[class][template][product][nttp]",
    ((typename T, size_t S), T, S),
    (std::array, Template_Foo_2),
    ((int,2), (float,6))) {
    REQUIRE(Template_Fixture_2<TestType>{}.m_a.size() >= 2);
}
```

## Template fixtures with types specified in template type lists

Catch2 also provides `TEMPLATE_LIST_TEST_CASE_METHOD` to support template fixtures with types specified in
template type lists like `std::tuple`, `boost::mpl::list` or `boost::mp11::mp_list`. This test case works the same as `TEMPLATE_TEST_CASE_METHOD`,
only difference is the source of types. This allows you to reuse the template type list in multiple test cases.

Example:
```cpp
using MyTypes = std::tuple<int, char, double>;
TEMPLATE_LIST_TEST_CASE_METHOD(Template_Fixture,
                               "Template test case method with test types specified inside std::tuple",
                               "[class][template][list]",
                               MyTypes) {
    REQUIRE( Template_Fixture<TestType>::m_a == 1 );
}
```

---

[Home](Readme.md#top)



docs/tostring.md
--------------------------------------
<a id="top"></a>
# String conversions

**Contents**<br>
[operator << overload for std::ostream](#operator--overload-for-stdostream)<br>
[Catch::StringMaker specialisation](#catchstringmaker-specialisation)<br>
[Catch::is_range specialisation](#catchis_range-specialisation)<br>
[Exceptions](#exceptions)<br>
[Enums](#enums)<br>
[Floating point precision](#floating-point-precision)<br>


Catch needs to be able to convert types you use in assertions and logging expressions into strings (for logging and reporting purposes).
Most built-in or std types are supported out of the box but there are two ways that you can tell Catch how to convert your own types (or other, third-party types) into strings.

## operator << overload for std::ostream

This is the standard way of providing string conversions in C++ - and the chances are you may already provide this for your own purposes. If you're not familiar with this idiom it involves writing a free function of the form:

```cpp
std::ostream& operator << ( std::ostream& os, T const& value ) {
    os << convertMyTypeToString( value );
    return os;
}
```

(where ```T``` is your type and ```convertMyTypeToString``` is where you'll write whatever code is necessary to make your type printable - it doesn't have to be in another function).

You should put this function in the same namespace as your type, or the global namespace, and have it declared before including Catch's header.

## Catch::StringMaker specialisation
If you don't want to provide an ```operator <<``` overload, or you want to convert your type differently for testing purposes, you can provide a specialization for `Catch::StringMaker<T>`:

```cpp
namespace Catch {
    template<>
    struct StringMaker<T> {
        static std::string convert( T const& value ) {
            return convertMyTypeToString( value );
        }
    };
}
```

## Catch::is_range specialisation
As a fallback, Catch attempts to detect if the type can be iterated
(`begin(T)` and `end(T)` are valid) and if it can be, it is stringified
as a range. For certain types this can lead to infinite recursion, so
it can be disabled by specializing `Catch::is_range` like so:

```cpp
namespace Catch {
    template<>
    struct is_range<T> {
        static const bool value = false;
    };
}

```


## Exceptions

By default all exceptions deriving from `std::exception` will be translated to strings by calling the `what()` method. For exception types that do not derive from `std::exception` - or if `what()` does not return a suitable string - use `CATCH_TRANSLATE_EXCEPTION`. This defines a function that takes your exception type, by reference, and returns a string. It can appear anywhere in the code - it doesn't have to be in the same translation unit. For example:

```cpp
CATCH_TRANSLATE_EXCEPTION( MyType const& ex ) {
    return ex.message();
}
```

## Enums

> Introduced in Catch2 2.8.0.

Enums that already have a `<<` overload for `std::ostream` will convert to strings as expected.
If you only need to convert enums to strings for test reporting purposes you can provide a `StringMaker` specialisations as any other type.
However, as a convenience, Catch provides the `REGISTER_ENUM` helper macro that will generate the `StringMaker` specialisation for you with minimal code.
Simply provide it the (qualified) enum name, followed by all the enum values, and you're done!

E.g.

```cpp
enum class Fruits { Banana, Apple, Mango };

CATCH_REGISTER_ENUM( Fruits, Fruits::Banana, Fruits::Apple, Fruits::Mango )

TEST_CASE() {
    REQUIRE( Fruits::Mango == Fruits::Apple );
}
```

... or if the enum is in a namespace:
```cpp
namespace Bikeshed {
    enum class Colours { Red, Green, Blue };
}

// Important!: This macro must appear at top level scope - not inside a namespace
// You can fully qualify the names, or use a using if you prefer
CATCH_REGISTER_ENUM( Bikeshed::Colours,
    Bikeshed::Colours::Red,
    Bikeshed::Colours::Green,
    Bikeshed::Colours::Blue )

TEST_CASE() {
    REQUIRE( Bikeshed::Colours::Red == Bikeshed::Colours::Blue );
}
```

## Floating point precision

> [Introduced](https://github.com/catchorg/Catch2/issues/1614) in Catch2 2.8.0.

Catch provides a built-in `StringMaker` specialization for both `float`
and `double`. By default, it uses what we think is a reasonable precision,
but you can customize it by modifying the `precision` static variable
inside the `StringMaker` specialization, like so:

```cpp
        Catch::StringMaker<float>::precision = 15;
        const float testFloat1 = 1.12345678901234567899f;
        const float testFloat2 = 1.12345678991234567899f;
        REQUIRE(testFloat1 == testFloat2);
```

This assertion will fail and print out the `testFloat1` and `testFloat2`
to 15 decimal places.

---

[Home](Readme.md#top)



docs/tutorial.md
--------------------------------------
<a id="top"></a>
# Tutorial

**Contents**<br>
[Getting Catch2](#getting-catch2)<br>
[Writing tests](#writing-tests)<br>
[Test cases and sections](#test-cases-and-sections)<br>
[BDD style testing](#bdd-style-testing)<br>
[Data and Type driven tests](#data-and-type-driven-tests)<br>
[Next steps](#next-steps)<br>


## Getting Catch2

Ideally you should be using Catch2 through its [CMake integration](cmake-integration.md#top).
Catch2 also provides pkg-config files and two file (header + cpp)
distribution, but this documentation will assume you are using CMake. If
you are using the two file distribution instead, remember to replace
the included header with `catch_amalgamated.hpp` ([step by step instructions](migrate-v2-to-v3.md#how-to-migrate-projects-from-v2-to-v3)).


## Writing tests

Let's start with a really simple example ([code](../examples/010-TestCase.cpp)). Say you have written a function to calculate factorials and now you want to test it (let's leave aside TDD for now).

```c++
unsigned int Factorial( unsigned int number ) {
    return number <= 1 ? number : Factorial(number-1)*number;
}
```

```c++
#include <catch2/catch_test_macros.hpp>

unsigned int Factorial( unsigned int number ) {
    return number <= 1 ? number : Factorial(number-1)*number;
}

TEST_CASE( "Factorials are computed", "[factorial]" ) {
    REQUIRE( Factorial(1) == 1 );
    REQUIRE( Factorial(2) == 2 );
    REQUIRE( Factorial(3) == 6 );
    REQUIRE( Factorial(10) == 3628800 );
}
```

This will compile to a complete executable which responds to [command line arguments](command-line.md#top). If you just run it with no arguments it will execute all test cases (in this case there is just one), report any failures, report a summary of how many tests passed and failed and return the number of failed tests (useful for if you just want a yes/ no answer to: "did it work").

Anyway, as the tests above as written will pass, but there is a bug.
The problem is that `Factorial(0)` should return 1 (due to [its
definition](https://en.wikipedia.org/wiki/Factorial#Factorial_of_zero)).
Let's add that as an assertion to the test case:

```c++
TEST_CASE( "Factorials are computed", "[factorial]" ) {
    REQUIRE( Factorial(0) == 1 );
    REQUIRE( Factorial(1) == 1 );
    REQUIRE( Factorial(2) == 2 );
    REQUIRE( Factorial(3) == 6 );
    REQUIRE( Factorial(10) == 3628800 );
}
```

After another compile & run cycle, we will see a test failure. The output
will look something like:

```
Example.cpp:9: FAILED:
  REQUIRE( Factorial(0) == 1 )
with expansion:
  0 == 1
```

Note that the output contains both the original expression,
`REQUIRE( Factorial(0) == 1 )` and the actual value returned by the call
to the `Factorial` function: `0`.

We can fix this bug by slightly modifying the `Factorial` function to:
```c++
unsigned int Factorial( unsigned int number ) {
  return number > 1 ? Factorial(number-1)*number : 1;
}
```


### What did we do here?

Although this was a simple test it's been enough to demonstrate a few
things about how Catch2 is used. Let's take a moment to consider those
before we move on.

* We introduce test cases with the `TEST_CASE` macro. This macro takes
  one or two string arguments - a free form test name and, optionally,
  one or more tags (for more see [Test cases and Sections](#test-cases-and-sections)).
* The test automatically self-registers with the test runner, and user
  does not have do anything more to ensure that it is picked up by the test
  framework. _Note that you can run specific test, or set of tests,
  through the [command line](command-line.md#top)._
* The individual test assertions are written using the `REQUIRE` macro.
  It accepts a boolean expression, and uses expression templates to
  internally decompose it, so that it can be individually stringified
  on test failure.

On the last point, note that there are more testing macros available,
because not all useful checks can be expressed as a simple boolean
expression. As an example, checking that an expression throws an exception
is done with the `REQUIRE_THROWS` macro. More on that later.


## Test cases and sections

Like most test frameworks, Catch2 supports a class-based fixture mechanism,
where individual tests are methods on class and setup/teardown can be
done in constructor/destructor of the type.

However, their use in Catch2 is rare, because idiomatic Catch2 tests
instead use _sections_ to share setup and teardown code between test code.
This is best explained through an example ([code](../examples/100-Fix-Section.cpp)):

```c++
TEST_CASE( "vectors can be sized and resized", "[vector]" ) {
    // This setup will be done 4 times in total, once for each section
    std::vector<int> v( 5 );

    REQUIRE( v.size() == 5 );
    REQUIRE( v.capacity() >= 5 );

    SECTION( "resizing bigger changes size and capacity" ) {
        v.resize( 10 );

        REQUIRE( v.size() == 10 );
        REQUIRE( v.capacity() >= 10 );
    }
    SECTION( "resizing smaller changes size but not capacity" ) {
        v.resize( 0 );

        REQUIRE( v.size() == 0 );
        REQUIRE( v.capacity() >= 5 );
    }
    SECTION( "reserving bigger changes capacity but not size" ) {
        v.reserve( 10 );

        REQUIRE( v.size() == 5 );
        REQUIRE( v.capacity() >= 10 );
    }
    SECTION( "reserving smaller does not change size or capacity" ) {
        v.reserve( 0 );

        REQUIRE( v.size() == 5 );
        REQUIRE( v.capacity() >= 5 );
    }
}
```

For each `SECTION` the `TEST_CASE` is **executed from the start**. This means
that each section is entered with a freshly constructed vector `v`, that
we know has size 5 and capacity at least 5, because the two assertions
are also checked before the section is entered. This behaviour may not be
ideal for tests where setup is expensive. Each run through a test case will
execute one, and only one, leaf section.

Section can also be nested, in which case the parent section can be
entered multiple times, once for each leaf section. Nested sections are
most useful when you have multiple tests that share part of the set up.
To continue on the vector example above, you could add a check that
`std::vector::reserve` does not remove unused excess capacity, like this:

```cpp
    SECTION( "reserving bigger changes capacity but not size" ) {
        v.reserve( 10 );

        REQUIRE( v.size() == 5 );
        REQUIRE( v.capacity() >= 10 );
        SECTION( "reserving down unused capacity does not change capacity" ) {
            v.reserve( 7 );
            REQUIRE( v.size() == 5 );
            REQUIRE( v.capacity() >= 10 );
        }
    }
```

Another way to look at sections is that they are a way to define a tree
of paths through the test. Each section represents a node, and the final
tree is walked in depth-first manner, with each path only visiting only
one leaf node.

There is no practical limit on nesting sections, as long as your compiler
can handle them, but keep in mind that overly nested sections can become
unreadable. From experience, having section nest more than 3 levels is
usually very hard to follow and not worth the removed duplication.


## BDD style testing

Catch2 also provides some basic support for BDD-style testing. There are
macro aliases for `TEST_CASE` and `SECTIONS` that you can use so that
the resulting tests read as BDD spec. `SCENARIO` acts as a `TEST_CASE`
with "Scenario: " name prefix. Then there are `GIVEN`, `WHEN`, `THEN`
(and their variants with `AND_` prefix), which act as a `SECTION`,
similarly prefixed with the macro name.

For more details on the macros look at the [test cases and
sections](test-cases-and-sections.md#top) part of the reference docs,
or at the [vector example done with BDD macros](../examples/120-Bdd-ScenarioGivenWhenThen.cpp).


## Data and Type driven tests

Test cases in Catch2 can also be driven by types, input data, or both
at the same time.

For more details look into the Catch2 reference, either at the
[type parametrized test cases](test-cases-and-sections.md#type-parametrised-test-cases),
or [data generators](generators.md#top).


## Next steps

This page is a brief introduction to get you up and running with Catch2,
and to show the basic features of Catch2. The features mentioned here
can get you quite far, but there are many more. However, you can read
about these as you go, in the ever-growing [reference section](Readme.md#top)
of the documentation.


---

[Home](Readme.md#top)



docs/usage-tips.md
--------------------------------------
<a id="top"></a>
# Best practices and other tips on using Catch2

## Running tests

Your tests should be run in a manner roughly equivalent with:

```
./tests --order rand --warn NoAssertions
```

Notice that all the tests are run in a large batch, their relative order
is randomized, and that you ask Catch2 to fail test whose leaf-path
does not contain an assertion.

The reason I recommend running all your tests in the same process is that
this exposes your tests to interference from their runs. This can be both
positive interference, where the changes in global state from previous
test allow later tests to pass, but also negative interference, where
changes in global state from previous test causes later tests to fail.

In my experience, interference, especially destructive interference,
usually comes from errors in the code under test, rather than the tests
themselves. This means that by allowing interference to happen, our tests
can find these issues. Obviously, to shake out interference coming from
different orderings of tests, the test order also need to be shuffled
between runs.

However, running all tests in a single batch eventually becomes impractical
as they will take too long to run, and you will want to run your tests
in parallel.


<a id="parallel-tests"></a>
## Running tests in parallel

There are multiple ways of running tests in parallel, with various level
of structure. If you are using CMake and CTest, then we provide a helper
function [`catch_discover_tests`](cmake-integration.md#automatic-test-registration)
that registers each Catch2 `TEST_CASE` as a single CTest test, which
is then run in a separate process. This is an easy way to set up parallel
tests if you are already using CMake & CTest to run your tests, but you
will lose the advantage of running tests in batches.


Catch2 also supports [splitting tests in a binary into multiple
shards](command-line.md#test-sharding). This can be used by any test
runner to run batches of tests in parallel. Do note that when selecting
on the number of shards, you should have more shards than there are cores,
to avoid issues with long-running tests getting accidentally grouped in
the same shard, and causing long-tailed execution time.

**Note that naively composing sharding and random ordering of tests will break.**

Invoking Catch2 test executable like this

```text
./tests --order rand --shard-index 0 --shard-count 3
./tests --order rand --shard-index 1 --shard-count 3
./tests --order rand --shard-index 2 --shard-count 3
```

does not guarantee covering all tests inside the executable, because
each invocation will have its own random seed, thus it will have its own
random order of tests and thus the partitioning of tests into shards will
be different as well.

To do this properly, you need the individual shards to share the random
seed, e.g.
```text
./tests --order rand --shard-index 0 --shard-count 3 --rng-seed 0xBEEF
./tests --order rand --shard-index 1 --shard-count 3 --rng-seed 0xBEEF
./tests --order rand --shard-index 2 --shard-count 3 --rng-seed 0xBEEF
```

Catch2 actually provides a helper to automatically register multiple shards
as CTest tests, with shared random seed that changes each CTest invocation.
For details look at the documentation of
[`CatchShardTests.cmake` CMake script](cmake-integration.md#catchshardtestscmake).


## Organizing tests into binaries

Both overly large and overly small test binaries can cause issues. Overly
large test binaries have to be recompiled and relinked often, and the
link times are usually also long. Overly small test binaries in turn pay
significant overhead from linking against Catch2 more often per compiled
test case, and also make it hard/impossible to run tests in batches.

Because there is no hard and fast rule for the right size of a test binary,
I recommend having 1:1 correspondence between libraries in project and test
binaries. (At least if it is possible, in some cases it is not.) Having
a test binary for each library in project keeps related tests together,
and makes tests easy to navigate by reflecting the project's organizational
structure.


---

[Home](Readme.md#top)



docs/why-catch.md
--------------------------------------
<a id="top"></a>
# Why do we need yet another C++ test framework?

Good question. For C++ there are quite a number of established frameworks,
including (but not limited to),
[Google Test](http://code.google.com/p/googletest/),
[Boost.Test](http://www.boost.org/doc/libs/1_49_0/libs/test/doc/html/index.html),
[CppUnit](http://sourceforge.net/apps/mediawiki/cppunit/index.php?title=Main_Page),
[Cute](http://www.cute-test.com), and
[many, many more](http://en.wikipedia.org/wiki/List_of_unit_testing_frameworks#C.2B.2B).

So what does Catch2 bring to the party that differentiates it from these? Apart from the catchy name, of course.


## Key Features

* Quick and easy to get started. Just download two files, add them into your project and you're away.
* No external dependencies. As long as you can compile C++14 and have the C++ standard library available.
* Write test cases as, self-registering, functions (or methods, if you prefer).
* Divide test cases into sections, each of which is run in isolation (eliminates the need for fixtures).
* Use BDD-style Given-When-Then sections as well as traditional unit test cases.
* Only one core assertion macro for comparisons. Standard C/C++ operators are used for the comparison - yet the full expression is decomposed and lhs and rhs values are logged.
* Tests are named using free-form strings - no more couching names in legal identifiers.


## Other core features

* Tests can be tagged for easily running ad-hoc groups of tests.
* Failures can (optionally) break into the debugger on common platforms.
* Output is through modular reporter objects. Basic textual and XML reporters are included. Custom reporters can easily be added.
* JUnit xml output is supported for integration with third-party tools, such as CI servers.
* A default main() function is provided, but you can supply your own for complete control (e.g. integration into your own test runner GUI).
* A command line parser is provided and can still be used if you choose to provide your own main() function.
* Alternative assertion macro(s) report failures but don't abort the test case
* Good set of facilities for floating point comparisons (`Catch::Approx` and full set of matchers)
* Internal and friendly macros are isolated so name clashes can be managed
* Data generators (data driven test support)
* Hamcrest-style Matchers for testing complex properties
* Microbenchmarking support


## Who else is using Catch2?

A whole lot of people. According to [the 2022 JetBrains C++ ecosystem survey](https://www.jetbrains.com/lp/devecosystem-2022/cpp/#Which-unit-testing-frameworks-do-you-regularly-use),
about 12% of C++ programmers use Catch2 for unit testing, making it the
second most popular unit testing framework.

You can also take a look at the (incomplete) list of [open source projects](opensource-users.md#top)
or the (very incomplete) list of [commercial users of Catch2](commercial-users.md#top)
for some idea on who else also uses Catch2.

---

See the [tutorial](tutorial.md#top) to get more of a taste of using
Catch2 in practice.

---

[Home](Readme.md#top)



gcr-cmake/README.md
--------------------------------------
# GCR_CMake
A CMake extension supporting the **glib-compile-resources** tool.

About
-----

Inspired from the macros for Vala that I used to build a GTK application, I came
to the point where I needed resources for it. For that purpose the
**glib-compile-resources** tool seemed to be the right choice, but the extra XML
file you need to write bothered me. If I add a resource I don't want to mark it
explicitly in CMake and in the XML file. So I came up with this macro that does
everything for you. You just add your resource to a resource list (with
eventually some attributes like compression) and invoke the resource compilation
function. It generates the XML automatically for you. Quite simple, isn't it?

Clone
-----

To clone this repository, just type

```shell
git clone https://github.com/Makman2/GCR_CMake
```

Usage
-----

Just add the macro directory to your `CMAKE_MODULE_PATH`, include the CMake
file and you are ready to go.

```cmake
list(APPEND CMAKE_MODULE_PATH
     ${PATH_TO_GCR_CMAKE_PARENT_DIR}/GCR_CMake/macros)

include(GlibCompileResourcesSupport)
```

Reference
---------

The resource compiling macro is quite powerful and handles as much errors as
possible to make error-finding easier. The function is defined as follows:

```
compile_gresources(<output>
                   <xml_out>
                   [TYPE output_type]
                   [TARGET output_name]
                   [RESOURCES [resources...]]
                   [OPTIONS [command_line_options...]]
                   [PREFIX resource_prefix]
                   [C_PREFIX c_names_prefix]
                   [SOURCE_DIR resource_directory]
                   [COMPRESS_ALL] [NO_COMPRESS_ALL]
                   [STRIPBLANKS_ALL] [NO_STRIPBLANKS_ALL]
                   [TOPIXDATA_ALL] [NO_TOPIXDATA_ALL])
```

- ***output***

  The variable name where to set the output file names. Pass this variable to a
  target as a dependency (i.e. `add_custom_target`).

- ***xml_out***

  The variable name where to store the output file name of the intermediately
  generated gresource-XML-file.

- **TYPE** ***output_type***

  The resource type to generate. Valid values are `EMBED_C`, `EMBED_H`, `BUNDLE`
  or `AUTO`. Anything else will default to `AUTO`.

  - `EMBED_C`: Generate a C code file that can be compiled with your program.

  - `EMBED_H`: Generate a header file to include in your program.

  - `BUNDLE`: Generates a resource disk file to load at runtime.

  - `AUTO` (or anything else): Extract mode from file ending specified in
    `TARGET`.

    If `TARGET` contains
    an invalid file or file ending not detectable, the function results in a
    **FATAL_ERROR**.

    Recognized file formats are: *.gresource*, *.c*, *.h*.

- **TARGET** ***output_name***

  Overrides the default output file name. If not specified (and not
  `AUTO`-**TYPE** is set) the output name is *resources.[type-ending]*.

- **RESOURCES** ***[resources...]***

  The resource list to process. Each resource must be a relative path inside the
  source directory. Each file can be preceded with resource flags.

  - `COMPRESS` flag

    Compress the following file. Effectively sets the attribute *compressed* in
    the XML file to true.

  - `STRIPBLANKS` flag

    Strip whitespace characters in XML files. Sets the *preprocess* attribute in
    XML with *xml-stripblanks* (requires XMLLint).

  - `TOPIXDATA` flag

    Generates a pixdata ready to use in Gdk. Sets the *preprocess* attribute in
    XML with *to-pixdata*.

  Note: Using `STRIPBLANKS` and `TOPIXDATA` together results in a
  **FATAL_ERROR**.

- **OPTIONS** ***command_line_options***

  Extra command line arguments passed to **glib_compile_resources**. For example
  `--internal` or `--manual-register`.

- **PREFIX** ***resource_prefix***

  Overrides the resource prefix. The resource entries get inside the XML a
  prefix that is prepended to each resource file and represents as a whole the
  resource path.

- **C_PREFIX** ***c_names_prefix***

  Specifies the prefix used for the C identifiers in the code generated when
  *EMBED_C* or *EMBED_H* are specified for *TYPE*.

- **SOURCE_DIR** ***resource_directory***

  The source directory where the resource files are. If not overridden, this
  value is set to `CMAKE_CURRENT_SOURCE_DIR`.

- **COMPRESS_ALL**, **NO_COMPRESS_ALL**

  Overrides the `COMPRESS` flag in all resources. If **COMPRESS_ALL** is
  specified, `COMPRESS` is set everywhere regardless of the specified resource
  flags. If **NO_COMPRESS_ALL** is specified, compression is deactivated in all
  resources.

  Specifying **COMPRESS_ALL** and **NO_COMPRESS_ALL** together results in a
  **FATAL_ERROR**.

- **STRIPBLANKS_ALL**, **NO_STRIPBLANKS_ALL**

  Overrides the `STRIPBLANKS` flag in all resources. If **STRIPBLANKS_ALL** is
  specified, `STRIPBLANKS` is set everywhere regardless of the specified
  resource flags. If **NO_STRIPBLANKS_ALL** is specified, stripping away
  whitespaces is deactivated in all resources.

  Specifying **STRIPBLANKS_ALL** and **NO_STRIPBLANKS_ALL** together results in
  a **FATAL_ERROR**.

- **TOPIXDATA_ALL**, **NO_TOPIXDATA_ALL**

  Overrides the `TOPIXDATA` flag in all resources. If **TOPIXDATA_ALL** is
  specified, `TOPIXDATA` is set everywhere regardless of the specified resource
  flags. If **NO_TOPIXDATA_ALL** is specified, converting into pixbufs is
  deactivated in all resources.

  Specifying **TOPIXDATA_ALL** and **NO_TOPIXDATA_ALL** together results in a
  **FATAL_ERROR**.

Kickstart
---------

This is a quick start guide to provide you an easy start with this macro.

Starting with a simple example:

```cmake
set(RESOURCE_LIST
    info.txt
    img/image1.jpg
    img/image2.jpg
    data.xml)

compile_gresources(RESOURCE_FILE
                   XML_OUT
                   TYPE BUNDLE
                   RESOURCES ${RESOURCE_LIST})
```

What does this snippet do? First it sets some resource files to pack into a
resource file. They are located in the source directory passed to CMake at
invocation (`CMAKE_CURENT_SOURCE_DIR`).
After that we compile the resources. Means we generate a *.gresource.xml*-file
(it's path is put inside the `XML_OUT` variable) automatically from our
`RESOURCE_LIST` and create a custom command that compiles the generated
*.gresource.xml*-file with the provided resources into a resource bundle. Since
no specific output file is specified via **TARGET** the output file is placed
into the `CMAKE_CURENT_BINARY_DIR` with the name *resources.gresource*. The
first argument `RESOURCE_FILE` is a variable that is filled with the output file
name, so with *resources.gresource* inside the build directory. This variable is
helpful to create makefile targets (or to process the output file differently).

So here comes a full *CMakeLists.txt* that creates the resources from before.

```cmake
# Minimum support is guaranteed for CMake 2.8. Everything below needs to be
# tested.
cmake_minimum_required(2.8)

project(MyResourceFile)

# Include the extension module.
list(APPEND CMAKE_MODULE_PATH
     ${PATH_TO_GCR_CMAKE_PARENT_DIR}/GCR_CMake/macros)

include(GlibCompileResourcesSupport)

# Set the resources to bundle.
set(RESOURCE_LIST
    info.txt
    img/image1.jpg
    img/image2.jpg
    data.xml)

# Compile the resources.
compile_gresources(RESOURCE_FILE
                   XML_OUT
                   TYPE BUNDLE
                   RESOURCES ${RESOURCE_LIST})

# Add a custom target to the makefile. Now make builds our resource file.
# It depends on the output RESOURCE_FILE.
add_custom_target(resource ALL DEPENDS ${RESOURCE_FILE})
```

A nice feature of the `compile_gresources`-macro is that it supports
individually setting flags on the resources. So we can extend our resource list
like that:

```cmake
set(RESOURCE_LIST
    info.txt
    COMPRESS img/image1.jpg
    COMPRESS img/image2.jpg
    STRIPBLANKS data.xml)
```

This resource list not only simply includes the resources, it specifies also
that the two images should be compressed and in *data.xml* the whitespaces
should be removed. This resource list will include the same files but will
preprocess some of them.

Very handy are the `COMPRESS_ALL`, `STRIPBLANKS_ALL` or `TOPIXDATA_ALL`
parameters (and their `NO_`-equivalents). If you are too lazy to write before
every file the flag, just invoke `compile_gresources` with them.

```cmake
# Compile the resources. Compresses all files regardless if you specified it
# explicitly or not.
compile_gresources(RESOURCE_FILE
                   XML_OUT
                   TYPE BUNDLE
                   RESOURCES ${RESOURCE_LIST}
                   COMPRESS_ALL)
```

So that's a short introduction into the operating mode of the
`compile-gresources` macro.



libinterchange/README.md
--------------------------------------
# FPGA interchange schema definitions

This repository contains the capnp schema for the FPGA interchange.

## Repositories that implement tools around the FPGA interchange

[RapidWright](https://github.com/Xilinx/RapidWright/):
 - Provides support for 7-series, UltraScale and UltraScale+ parts
 - Generate device database for parts
 - Can convert DCPs to logical and physical FPGA interchange files.
 - Can convert logical and physical FPGA interchange files to DCPs

[python-fpga-interchange](https://github.com/SymbiFlow/python-fpga-interchange):
 - Implements textual format conversions for FPGA interchange files.
 - Provides (partial) generator for nextpnr to generate a nextpnr architecture
   for a FPGA interchange device database.



docs/bel_and_site_design.md
--------------------------------------
# Cell, BEL and Site Design

One of the key concepts within the FPGA interchange device resources is the
relationship between the cell library and the device BEL and site definitions.
A well designed cell library and a flexible but concise BEL and site
definition is important for exposing the hardware in an efficient way that
enables a place and route tool to succeed.

Good design is hard to capture, but this document will talk about some of the
considerations.

### Assumptions about cell placement and driver BEL pins

One important note is that BELs represent a placable location for a cell, and
only one cell should be placable at a given BEL.  This means that the cell
library design and BEL design strongly affects what is expressable by the
place and route tool.  There will be some examples highlighted below that
expand on how this is important and relevant when discussing concrete
examples.

## Granularity of the cell library

It is important to divide the place and route problem and the synthesis
problem, at least as defined for the purpose of the FPGA interchange.  The
synthesis tool operates on the **cell library**, which should be designed to
expose logic elements at a useful level of granularity.

As a concrete example, a LUT4 element is technically just two LUT3 elements,
connected by a mux (e.g. MUXF4), a LUT3 element is just two LUT2 elements,
connected by a mux (e.g. MUXF3), etc. If the outputs of those interior muxes
are not accessible to the place and route tool, then exposing those interior
function muxes as cells in the cell library is not as useful.

Cell definitions should be granular enough that the synthesis can map to
them, but not so granular that the place and route tool will be making few if
any choices.  If there is only one legal placement of the cell, it's value is
relatively low.

## Drawing site boundaries

When designing an FPGA interchange device resource for a new fabric, one
important consideration is where to draw the site boundary.  The primary goal
of lumping BELs within a site is to capture some local congestion due to
fanout limitations.  Interior static routing muxes and output muxes may
accommodate significantly fewer signals than the possible number of BELs that
drive them.  In this case, it is important to draw the site boundary large
enough to capture these cases so as to enable the local congestion to be
resolved during either packing for clustered approaches, or during placement
during unclustered approaches.  In either case, local congestion that is
strongly placement dependant must be resolved prior to general routing,
unless a fused placement and routing algorithm is used.

### FF control sets routing

A common case worth exploring is FF control sets, e.g. SR type signals and CE
type signals.  In most fabric SLICE types, the SR and CE control signals are
shared among multiple rows of the SLICE.  This is a common example of local
site congestion, and the site boundary should typically encompass all BELs
that share this kind of local routing for all the reasons discussed above.

Another consideration with control signals is the presence of control signal
constraints that cannot be expressed as local routing congestion.  For
example, if a set of BELs share whether the SR control line is a set or reset
(or async set or async reset), it is common to expand the site boundary to
cover the BELs that share these implicit configurations.  The constraint
system in the device resources is designed to handle this kind of non-routing
driven configuration.

## Drawing BEL boundaries

BEL definitions require creating a boundary around primitive elements of
the fabric.  The choice of where to place that boundary has a strong influence
on the design of the cell library in the FPGA interchange.

In general, the smaller the BEL boundary, the more complexity is exposed to
the place and route tool.  In some cases exposing this complexity is
important, because it enables some goal.  For example, leaving static routing
muxes outside of BELs enables a place and route tool to have greater
flexibility when resolving site congestion.  But as a counter point, if only
a handful of static mux configurations are useful and those choices can be
made at synthesis time, then lumping those muxes into synthesis reduces the
complexity required in the place and route tool.

The most common case where the static routing muxes are typically lumped into
the BEL is BRAM's and FIFO's address and routing configuration.  At synthesis
time, a choice is made about the address and data widths, which are encoded as
parameters on the cell.  The place and route tool does not typically make
meaningful choices on the configuration of those static routing muxes, but
they do exist in the hardware.

The most common case where the static routing muxes are almost never lumped
into the BEL is SLICE-type situations.  The remainder of this document will
show examples of why the BEL boundary should typically exclude the static
routing muxes, and leave the choice to the place and route tooling.

## Static routing muxes and bitstream formats

Something to keep in mind when drawing BEL boundaries to include or exclude
static routing muxes is the degree of configurability present in the
underlying bitstream.  Some static routing muxes share configuration bits in
the bitstream, and so expressing them as two seperate static routing muxes
potentially gives the place and route tool flexibility than the underlying
fabric cannot express.  This will result in physical netlists that cannot be
converted to bitstream.

In some cases this can be handled through tight coupling of the cell and
BEL library.  The idea is to limit cell port to BEL pin mappings that avoid
illegal static routing mux configurations.  This approach has its limits.
In general, considering how the bitstream expresses static routing muxes must
be accounted for when drawing BEL boundaries.

### Stratix II and Stratix 10 ALM

![Stratix II](stratix2_slice.png-026_rotate.png)

![Stratix 10](stratix10_slice.png-11.png)

Consider both Stratix II and Stratix 10 logic sites.  The first thing to note
is that the architectures at this level are actually mostly the same.  Though
it isn't immediately apparent, both designs are structured around 4 4-LUT
elements.

Take note that of the following structure:

![Stratix II fractured LUT4](frac_lut4.png)

This is actually just two LUT4 elements, where the top select line is
independent.

See the following two figures:

![Stratix II fractured LUT4 Top](frac_lut4_a.png)
![Stratix II fractured LUT4 Bottom](frac_lut4_b.png)

In Stratix 10, the LUT4 element is still present, but the top select line
fracturing was removed.

So now consider the output paths from the the 4 LUT4 elements in the Stratix
II site.  Some of the LUT4 outputs route directly to the carry element, so it
will be important for the place and route tool be able to place a LUT4 or
smaller to access that direct connection.  But if the output is not used in
the carry element, then it can only be accessed in Stratix II via the MUXF5
(blue below) and MUXF6 (red below) elements.

![Stratix II Highlight MUXF5 and MUXF6](highlight_muxf5_muxf6.png)

So given the Stratix II site layout, the following BELs will be required:

 - 4 LUT4 BELs that connect to the carry
 - 2 LUT6 BELs that connect to the output FF or output MUX.

The two LUT6 BELs are shown below:

![Stratix II Top LUT6](highlight_top_lut6.png)
![Stratix II Top LUT6](highlight_bottom_lut6.png)

Drawing a smaller BEL boundary has little value, because a LUT5 element would
still always require routing through the MUXF6 element.

Now consider the Stratix 10 output arrangement.  The LUT4 elements direct to
the carry element is the same, so those BELs would be identical.  The Stratix
10 site now has an output tap directly on the top LUT5, similiar to the Xilinx
Versal LUT6 / LUT5 fracture setup.  See diagram below.  LUT5 element is shown
in blue, and LUT6 element is shown in red.

![Stratix 10 2 LUT5](stratix10_highlight_lut5.png)
![Stratix 10 LUT6](stratix10_highlight_lut6.png)

So given the Stratix 10 site layout, the following BELs will be required:

 - 4 LUT4 BELs that connect to the carry
 - 2 LUT5 BELs that connect to the output FF or output MUX
 - 1 LUT6 BELs that connect to the output FF or output MUX

### Versal ACAP

The Versal ACAP LUT structure is fairly similiar to the Stratix 10 combitorial
elements.

![Versal ACAP LUTs](versal_luts.png)

Unlike the Stratix 10 ALM, it appears only 1 of the LUT4's connects to the
carry element (the prop signal).  The O6 output also has a dedicate
connection to the carry.  See image below:

![Versal SLICE row](versal_row.png)

The Versal LUT structure likely should be decomposed into 4 BELs, shown in
the next figures:

![Versal ACAP LUT4](versal_lut4.png)
![Versal ACAP two LUT5](versal_lut5.png)
![Versal ACAP LUT6](versal_lut6.png)

So given the Versal site layout, the following BELs will be required (per SLICE row):

 - 1 LUT4 BELs that connect to the carry
 - 2 LUT5 BELs that connect to the output FF or output MUX
 - 1 LUT6 BELs that connect to the output FF or output MUX

#### Implication of a wider BEL definition

Consider the Versal structure, but instead of drawing four BELs per row, only have two
BELs per row.  One BEL has the `O5_1` and `prop` output BEL pins and the
other BEL has the `O6` and `O5_2` BEL pin.  In this configuration, if the cell
library does not expose a cell that maps to both the `O5_1` and `prop` output
BEL pins, then it will not be possible to map LUTs that leverage both output
BEL pins.

In theory, the cell port to BEL pin map could map the output pin of a LUT4
element to both the `prop` and `O5_1` output BEL pins, but then there will be
two output BEL pins driving the net connected to the cell port.  Having
multiple BEL pins driving one net is not legal, except for the global logic 0
and 1.


### Quicklogic EOS S3 logic cell

The Quicklogic EOS S3 logic cell has an interesting LUT design because there
is not LUT element specifically.  Instead, the fabric exposes a 8x3 mux, with
inverters at each of the mux inputs, see figure below:

![Quicklogic EOS S3 logic cell](eos_slice.png)

The way to approach this fabric is to first draw BEL boundaries around the 4x2
mux and 8x3 mux present in the fabric:

![Quicklogic MUX4x2](eos_slice_mux4x2.png)
![Quicklogic MUX8x3](eos_slice_mux8x3.png)

The cell library should have
3 MUX cell types:
 - 4-input 1-output 2-select MUX4x2 (maps to MUX4x2 BEL and MUX8x3 BEL)
 - 8-input 1-output 3-select MUX8x3 (maps to MUX8x3 BEL)
 - A macro cell that is 2x (4-input 1-output 2-select MUX4x2) 2xMUX4x2 (maps to MUX4x2 *and* MUX8x3 BEL)

A fourth most general cell type is possible, which is to add a cell that also
has a cell port that maps to `TBS`, instead of tying `TBS` high as the
2xMUX4x2 cell would do. It is unclear how useful such a cell would be.
However given the BEL boundaries, adding such a cell would be easy after the
fact.

In all of the cells above, all inputs to the muxes have statically
configured inverters.

So the question becomes, how to model LUT cells in this fabric?  The LUT cells
should be the regular LUT1, LUT2 and LUT3 cells.  The LUT1 and LUT2 can map to
either the MUX4x2 or MUX8x3 BEL.  The LUT3 can map to only the MUX8x3 BEL.
The question is only what is the cell port to BEL pin map?

The solution is when mapping a LUT cell, to tie all of the MUX BEL pins to VCC
(or GND, whatever the default is) before the inverter.  The place and route
tool can treat the BEL as a regular LUT, and only the bitstream generation
step will need to be aware that the inversion control is being used to
encode the LUT equation.

This configuration allows most (if not all) of the logic to be available
to the place and route tool, without exposing unneeded complexity.



docs/device_resources.md
--------------------------------------
# Device Resources

The device resources schema is intended to be a complete description of an
island-based FPGA design.  It is made of many components, but the core
description of the device is shown below.

```
┌─────────────────┐
│  Device         │
│ ┌─────────────┐ │
│ │ Tile        │ │
│ │ ┌─────────┐ │ │
│ │ │ Site    │ │ │
│ │ │ ┌─────┐ │ │ │
│ │ │ │ BEL │ │ │ │
│ │ │ └─────┘ │ │ │
│ │ │         │ │ │
│ │ └─────────┘ │ │
│ │             │ │
│ └─────────────┘ │
│                 │
└─────────────────┘
```

That is:
 - A device contains tiles
 - Tiles contains sites
 - Sites contains BELs

The schema contains the required information to answer questions such as:
 - Where are tiles located?
 - How are sites connected to the routing graph?
 - How are BELs connected to the boundary of the site?
 - How can cells be placed at BELs?

## Terminology

- Device - A set of tiles and package pins.
- Tiles - An instance of a tile type which contains wires and sites
- Package pin - A boundry between the "interior" of the device and what is "outside" the package.  Generally corresponds to a pin on a package, e.g. pin 1 on SOP-8 or A1 on CSG324.
- Wire - Also known as a "tile wire" .  A wire is a piece of conductive material totally contained within a tile.  Wires can be part of nodes.  Wires can connect to PIPs or site pins.
- Node - A node is a set of 1 or more wires that are connected.  Nodes can span multiple tiles.  Nodes connect to PIPs or site pins via the wires that are part of the node.
- PIP - PIP is an abbreviation for programable interconnect point.  A PIP provides a connection between two wires.  PIPs can be unidirectional or bidirectional.  Unidirectional PIPs always connect wire0 to wire1.  Bidirectional PIPs can connect wire0 to wire1 or wire1 to wire0.
- Site - A collection of site pins, site wires and BELs.
- Site pin - The connection between a site and a wire. Site pins may connect to 0 or more site port BELs.
- Site wire - A piece of conductive material that connects to at most 1 output BEL pin and 0 or more input or inout BEL pins.
- BEL - BEL is an abbreviation of basic logic element.  A BEL can be one of 3 types, site port, logic, routing.  A BEL contains 1 or more BEL pins.
- BEL pin - A connection between a BEL and a site wire.
- Logic BEL - A placable logic element.  May be subject to 0 or more placement constraints.
- Site port BEL - A site port BEL represents a connection to a site pin contained within the parent tile of the site.  See [Site port BEL](#site_port_bel).
- Routing BEL - A routing BEL connects at most 1 input BEL pin to the output BEL pin.  See [Routing BEL](#routing_bel).
- Site PIP - A pair of input and output BEL pins belonging to a BEL that represents a logically connection.
- Cell - A logical element of a design that contains some number of cell ports and some number of cell instances, and some number of nets.
- Cell port - The boundary between the interior of a cell and the containing cell (if any).
- Cell instance - A instance of a cell.  The cell ports of may be connected to nets within the parent cell.
- Net - A set of logically connected cell ports.

## The place and route problem

The device resources schema is intended to provide a description for a tool
solving the place and route problem. The definition of the problem used by
this schema is described below:

There exists exactly 1 **cell instance** (the **top** instance) that contains 1 or
more leaf **cell instances** that must be placed at **BELs**, such that no
**constraints** are violated and the **nets** between the **cell instances** are
**routable**. **Routable** means that **site wires**, **site PIPs**,  **nodes**, **PIPs**
can be assigned to at most 1 **net** such that each **net driver BEL pin** can
reach each every **net sink BEL pin** on the **net**.

The device resource format describes how **cell instances** can be legally
placed at **BELs** and how **cell pins** relate to **BEL pins**.  When a
**cell instance** is placed at a **BEL**, it may be subject to 0 or more
**constraints**.

**Nets** are divided into 3 categories.  A **signal** net represents a signal
that is not either the constant logical 0 or constant logical 1 net.
The constant logical 0 and constant logical 1 nets are special because they
can have multiple drivers in the device description.  Routing resources that
are always part of the constant logical 0 or constant logical 1 net are
explicitly defined in the device resources schema.  The constant logical 0 net
is listed in the schema as the "gnd" type.  The constant logical 1 net is
listed in the schema as the "vcc" type.

### Rules for routing

Fully routed signal nets always begin at a output/inout BEL pin, and always
end at an input/inout BEL pin.  If a net enters a site, that net **must** end
at an input/inout BEL pin.  It is not legal for a net to enter and leave a
site.  If such a path is required, a pseudo PIP should be added to the schema.

## Site example

The following is an example site for a SLICE for a non-existent device.

```
                                ▲
                              CO│                                     │
     ┌──────────────────────────┼─────────────────────────────────────┼──────┐
     │                          │                                     │      │
   B0│   I0┌───────┐            │                  BLUT ┌───────┐     ▼ CLK  │
─────┼────►│       │   ┌────────o─────────────────┬────►│       │   ┌────┐   │
   B1│   I1│       │O  │        │                 │XOR  │       │D D│    │Q  │ FFOUT
─────┼────►│ BLUT3 ├───┤        │     ┌────────┬──o────►│ FFMUX ├──►│ FF ├───┼───►
   B2│   I2│       │   │        │     │        │  |ALUT │       │   │    │   │
─────┼────►│       │   │        │     │  ┌──┬──o──o────►│       │   └────┘   │
     │     └───────┘   │    ┌───┴───┐ │  │  │  │  │     └───────┘            │
     │                 │  SI│       │ │  │  │  │  │                          │
     │                 └───►│       ├─┘  │  │  │  │                          │
     │                    DX│ CARRY │ O  │  │  │  │                          │
     │                 ┌───►│       │    │  │  │  │                          │
   A0│   I0┌───────┐   │    └───┬───┘    │  │  │  │      ┌────────┐          │
─────┼────►│       │   │        │        │  │  │  │ BLUT │        │          │
   A1│   I1│       │O  │        │        │  │  │  └─────►│        │OUT       │ OUT
─────┼────►│ ALUT3 ├───┴────────o────────┘  │  │    XOR  │ OUTMUX ├──────────┼────►
   B2│   I2│       │            │           │  └────────►│        │          │
─────┼────►│       │            │           │       ALUT │        │          │
     │     └───────┘            │           └───────────►└────────┘          │
     │                          │                                            │
     │                          │                                            │
     └──────────────────────────┼────────────────────────────────────────────┘
                              CI│
                                │
```

In the above example, there are 17 BELs:

| BEL Name | Category  | # Input | # Output |
|----------|-----------|---------|----------|
| ALUT3    | Logic     | 3       | 1        |
| BLUT3    | Logic     | 3       | 1        |
| CARRY    | Logic     | 3       | 2        |
| FF       | Logic     | 2       | 1        |
| FFMUX    | Routing   | 3       | 1        |
| OUTMUX   | Routing   | 3       | 1        |
| A0       | Site port | 0       | 1        |
| A1       | Site port | 0       | 1        |
| A2       | Site port | 0       | 1        |
| B0       | Site port | 0       | 1        |
| B1       | Site port | 0       | 1        |
| B2       | Site port | 0       | 1        |
| CI       | Site port | 0       | 1        |
| CLK      | Site port | 0       | 1        |
| CO       | Site port | 1       | 0        |
| FFOUT    | Site port | 1       | 0        |
| OUT      | Site port | 1       | 0        |

Each site port BEL has a site pin, so the site pins are:

| Site Pin Name | Direction |
|---------------|-----------|
| A0            | In        |
| A1            | In        |
| A2            | In        |
| B0            | In        |
| B1            | In        |
| B2            | In        |
| CI            | In        |
| CLK           | In        |
| CO            | Out       |
| FFOUT         | Out       |
| OUT           | Out       |

The BEL BLUT3 has 4 BEL pins:

| BEL pin name | Direction |
|--------------|-----------|
| I0           | In        |
| I1           | In        |
| I2           | In        |
| O            | Out       |

The BEL A0 has 1 BEL pin:

| BEL pin name | Direction |
|--------------|-----------|
| A0           | Out       |

The BEL OUTMUX has 4 BEL pins:

| BEL pin name | Direction |
|--------------|-----------|
| BLUT         | In        |
| XOR          | In        |
| ALUT         | In        |
| OUT          | Out       |


There are 12 site PIPs:

| BEL name | In pin | Out pin |
|----------|--------|---------|
| BLUT3    | I0     | O       |
| BLUT3    | I1     | O       |
| BLUT3    | I2     | O       |
| ALUT3    | I0     | O       |
| ALUT3    | I1     | O       |
| ALUT3    | I2     | O       |
| FFMUX    | ALUT   | D       |
| FFMUX    | XOR    | D       |
| FFMUX    | BLUT   | D       |
| OUTMUX   | ALUT   | OUT     |
| OUTMUX   | XOR    | OUT     |
| OUTMUX   | BLUT   | OUT     |


The site wire BLUT3\_O has 4 BEL pins:

| BEL Name | Pin  |
|----------|------|
| BLUT3    | O    |
| CARRY    | SI   |
| FFMUX    | BLUT |
| OUTMUX   | BLUT |

The site wire B0 has 2 BEL pins:

| BEL Name | Pin |
|----------|-----|
| B0       | B0  |
| BLUT3    | I0  |

## Details

### Net routing summary

Each net start at the driver output/inout BEL pin.  The BEL pin will be
connected to exactly 1 site wire.  If the net sink can be reached within the
site, then the net can use site PIPs to reach a site wire connected to the
input/inout BEL pin.

If the net sink is in another site, then the net must first reach a site port
input BEL pin using site PIPs to reach the site wire connected to the site
port. From there the net leaves the site via the site port and is now on the
first node via the site pin matching the site port.

From there the net must use PIPs to expand to new nodes until arriving at a
node attached to valid site pin for the sink.  This would be a site pin that
is part of the same site that the sink BEL is part of, and that the site port
wire can reach the sink BEL pin (via 0 or more site PIPs). The site can be
entered via the site port corresponding to the site pin. The first site wire
in the site will be the site wire attached to the output BEL pin of the site
port.  From there site routing continues per above.

![Wire and nodes](https://symbiflow.readthedocs.io/projects/arch-defs/en/latest/_images/rrgraph-wire.svg)

### Use of site PIPs

It is important to note that site PIPs can only be used to access placed cells
inside that site. Site PIPs cannot be used as general route-thrus, to route
from site input to output. General route-thrus across entire sites should use
tile pseudo PIPs as described below - even if a site pin is being validly used
for one sink pin of a net that is located inside the site; site PIPs cannot
also be used to leave the site again to reach other sinks.

LUT route-thrus, for example, might require both site PIPs and tile pseudo
PIPs. The site PIP would be used to route through the LUT in order to access
an associated flipflop input inside the site. The tile PIP would be used to
route across the entire site as part of the general, inter-tile, routing
problem.

A diagram illustrating the legal and illegal uses is shown below.

![Site PIP usage](site_pip_usage.svg)

### Tile Types and site types

To reduce data duplication in the device schema, both tiles and sites have a
type.  Most of the definition of the tile and site is in the type rather than
repeated at each instance.  This does cause some more complicated
indirection, so the following section provides some additional explanation
here.

#### Sites, site types and alternative site types

The most complicated relationship in the schema is likely the relationship
between sites, site types and alternative site types.

Most of the site type description is independent of the tile / tile type that
the site type is within.  See the "SiteType" struct definition for
the independent portion of the schema. The important exception is the
relationship between wires and site pins.

Each site within a tile has a "primary site type", which is found in the
"SiteTypeInTileType" struct definition, contained in the "TileType" struct.
The site within the tile will specify which "SiteTypeInTileType" to use for
that particular site.

The primary site type contains a list of "alternative" site types that may be
used ("altSiteTypes" in "SiteType").  The primary site type must always
contains the complete list of site pins used in any of the alternative site
types.

The site pins to wire relationship is always done via the primary site type.
When an alternative site type is used, the site pins of that alternative site
type must be first be mapped to a site pin of the primary site type.

The "SiteTypeInTileType" defines the relationship between the primary site
type and the wires.  It also defines the relationship from the alternative
site type to the primary site type.

It first defines the primary site type ("primaryType").  It defines a map
between the site pins in the primary site type and wires in the tile type that
contains the site ("primaryPinsToTileWires").  Last it defines
the map between the alternative site pin and the primary site pin
("altPinsToPrimaryPins").

Important: When solving the place and route problem, only the primary or one
of the alternative site types can be used at a time for a particular site.

### <a name="routing_bel">Routing BEL

A routing BEL represents statically configurable site routing by connecting a
site wire connected to one of the input BEL pins to the output BEL pin of
the BEL.  Routing BELs must have 1 output BEL pin.

#### Inverting routing BELs

Some routing BELs can invert signals that pass through them.  Defined the
"inverting" field in the "BEL" struct with the BEL pins that invert and do
not invert.

### <a name="site_port_bel">Site port BEL

A site port BEL represents a connection to a site pin contained within the
parent tile of the site. Site port BELs have exactly 1 BEL pin.  The BEL name
and pin name should be the same.  The name of the BEL should match the name of
the site pin that the site port connects too.  The direction of the BEL pin
should be the opposite of the site pin direction.

Examples:

An input site pin "I0" would have a site port BEL named "I0" with 1 BEL
output pin named "I0".

## Additional topics

The device resources schema also covers some important data required for
handling common cases found in island based FPGAs.

### Signal inversions

It is fairly common for fabrics to contain site local signal inverters.
Depending on the architecture, the place and route tool may be expected to
leverage inverters or may even require it.  The device resources schemas
provides a description for how cells express inversion and how to use site
local inverters to implement the requested inversion.

### LUT definitions

LUTs are common to every island-based FPGA, and many place and route tasks
depend on having knowledge of how the LUTs are arranged.  The LUT definition
section of the device resources defines where LUTs exist as BELs and what
cells can be placed at those BELs.  This is important is at least two place
and route tasks.  The first is that LUTs can be trivially turned into site pips
from the input of the LUT to the output of the LUT, subject to **constraints**
and LUT equation sharing.  The second is that LUTs can be trivially turned
into constant sources from the output pin.

### Parameters

Some parameters attached to cell instances may be relevant for the place and
route problem.  A common example is LUT equation sharing, which can happen on
fracturable LUTs.  See the schema for details.

### Pseudo PIPs

It may be important within a device to represent PIPs that "route-thru"
one or more BELs.  This can be modelled as placing a cell in a particular
configuration at a BEL, subject to the normal cell placement rules.  The
"PseudoCell" struct defines what resources are used by using PIPs.

All pseudo PIPs must define at least 1 pseudo cell.  Pseudo cells should
include the site port BEL that the pseudo PIP used to enter the site.



docs/index.rst
--------------------------------------
Welcome to FPGA Interchange documentation!
==========================================

.. toctree::
   :maxdepth: 2
   :caption: Contents:

   device_resources
   bel_and_site_design
   pseudo_cells


Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`



docs/pseudo_cells.md
--------------------------------------
# Pseudo Cells

Pseudo PIPs and site pseudo PIPs are edges in the device graph that route
through other sites and/or BELs.  Pseudo cells are the expression of what
routing resources are "blocked" by the use of a pseudo PIP.

The device database currently expresses pseudo PIPs as using BEL pins within
the site that the pseudo PIP is attached too.  Both input and output BEL pins
are included in the pseudo cell definition, but only output BEL pins "block"
the site wire.

### Example

In the case of a Xilinx 7-series `CLBLL`'s `CLBLL_LL_A1` to `CLBLL_LL_A`
pseudo PIP, this PIP connects a signal from an input site port to an output
site port.  Each site wire that is consumed has **both** a output BEL pin
(from the site wire source) and a input BEL pin (connected to either a logic
BEL, e.g. `A6LUT` or routing BEL, e.g. `AUSED` or output site port BEL, e.g.
`A`).

In the case of a `CLBLL`'s `CLBLL_LL_A` to `CLBLL_LL_AMUX` pseudo PIP, this
PIP connects a signal from an output site port to an output site port.  In
this case, it is assumed and required that some of the site wires are already
bound to the relevant net (by virtue of the wire `CLBLL_LL_A` already being
part of the net).  In this case, the first BEL pin will be an input BEL pin
(specifically `AOUTMUX/O6`) that indicates that a site PIP is used as part of
the pseudo PIP.  However in this case, this edge does **not** block the site
wire, instead it only requires it.  The following output BEL pin (specifically
 `AOUTMUX/OUT`) blocks the site wire `AMUX`.

### Future enhancements

Pseudo cells right now only have BEL pins used by the pseudo PIP.  This
ignores the fact that some BEL's when route through may have constraint
implications.  For example, routing through a LUT BEL requires that it be in
LUT mode.  If that BEL is in either a SRL or LUT-RAM mode, the LUT route
through may not operate properly.



libsdcparse/README.md
--------------------------------------
libsdcparse
----------------------------------

This library provides a parser for a subset of the Synopsys Design Constriant (SDC)
language. See comments at the top of 'src/sdc.hpp' for more detailed information and
usage.




libtatum/README.md
--------------------------------------
# Tatum: A Fast, Flexible Static Timing Analysis (STA) Engine for Digital Circuits

[![Build Status](https://travis-ci.org/kmurray/tatum.svg?branch=master)](https://travis-ci.org/kmurray/tatum)

## Overview
Tatum is a block-based Static Timing Analysis (STA) engine suitable for integration with Computer-Aided Design (CAD) tools, which analyze, implement and optimize digital circuits.
Tatum supports both setup (max-delay) and hold (min-delay) analysis, clock skew, multiple clocks and a variety of timing exceptions.

Tatum is provided as a library (`libtatum`) which can be easily integrated into the host application.
Tatum operates on an abstract *timing graph* constructed by the host application, and can be configured to use an application defined delay calculator.

Tatum is optimized for high performance, as required by optimizing CAD tools.
In particular:
  * Tatum performs only a *single* set of graph traversals to calculate timing information for all clocks and analyses (setup and hold).
  * Tatum's data structures are cache optimized
  * Tatum supports parallel analysis using multiple CPU cores

## How to Cite
If your work uses Tatum please cite the following as a general citation:

K. E. Murray and V. Betz, "Tatum: Parallel Timing Analysis for Faster Design Cycles and Improved Optimization", *IEEE International Conference on Field-Programmable Technology (FPT)*, 2018

**Bibtex:**
```
@inproceedings{c:tatum,
    author = {Murray, Kevin E. and Betz, Vaughn},
    title = {Tatum: Parallel Timing Analysis for Faster Design Cycles and Improved Optimization},
    booktitle = {IEEE International Conference on Field-Programmable Technology (FPT)},
    year = {2018}
}
```

## Documentation
Comming soon.

## Download
Comming soon.

## Projects using Tatum

Tatum is designed to be re-usable in a variety of appliations.

Some of the known uses are:
  * The [Verilog to Routing (VTR)](https://verilogtorouting.org) project for Field-Programmable Gate Array (FPGA) Architecture and CAD research. Tatum is used as the STA engine in the VPR placement and routing tool.
  * The [CGRA-ME](http://cgra-me.ece.utoronto.ca/) framework for Coarse-Grained Reconfigurable Array (CGRA) Architecture research.

*If your project is using Tatum please let us know!*

## History

### Why was Tatum created?
I had need for a high performance, flexible STA engine for my research into FPGA architecture and CAD tools.
I could find no suitable open source STA engines, and wrote my own.

### Name Origin
A *tatum* is a unit of time used in the computational analysis of music \[[1]\], named after Jazz pianist [Art Tatum](https://en.wikipedia.org/wiki/Art_Tatum).

[1]: http://web.media.mit.edu/~tristan/phd/dissertation/chapter3.html#x1-390003.4.3



sockpp/CHANGELOG.md
--------------------------------------
# Change Log for _sockpp_

## [Version 1.0.0](https://github.com/fpagliughi/sockpp/compare/v0.8.3..v1.0.0) - (2023-12-17)

This is a release of the previous 0.8.x line as the initial, stable API.

## [Version 0.8.3](https://github.com/fpagliughi/sockpp/compare/v0.8.2..v0.8.3) - (2023-12-11)

- [#64](https://github.com/fpagliughi/sockpp/pull/84) Added support for Catch2 v3.x for unit tests. (v2.x still supported)


## [Version 0.8.2](https://github.com/fpagliughi/sockpp/compare/v0.8.1..v0.8.2) - (2023-12-05)

- [#89](https://github.com/fpagliughi/sockpp/issue/89) Fixed generator expression for older CMake
- [#91](https://github.com/fpagliughi/sockpp/issue/91) Fixed uniform_int_distribution<> in UNIX socket example


## [Version 0.8.1](https://github.com/fpagliughi/sockpp/compare/v0.8.0..v0.8.1) - (2023-01-30)

- Cherry picked most of the non-TLS commits in PR [#17](https://github.com/fpagliughi/sockpp/pull/17)
    - Connector timeouts
    - Stateless reads & writes for streaming sockets w/ functions returning `ioresult`
    - Some small bug fixes
    - No shutdown on invalid sockets
- [#38](https://github.com/fpagliughi/sockpp/issues/38) Made system libs public for static builds to fix Windows
- [#73](https://github.com/fpagliughi/sockpp/issue/73) Clone a datagram (UDP) socket
- [#74](https://github.com/fpagliughi/sockpp/issue/74) Added `<sys/time.h>` to properly get `timeval` in *nix builds.
- [#56](https://github.com/fpagliughi/sockpp/issue/56) handling unix paths with maximum length (no NUL term)
- Fixed outstanding build warnings on Windows when using MSVC

## [Version 0.8.0](https://github.com/fpagliughi/sockpp/compare/v0.7.1..v0.8.0) - (2023-01-17)

- [Breaking] Library initializer now uses a static singleton created via `socket_initializer::initialize()` call, which can be called repeatedly with no ill effect. Also added global `socketpp::initialize()` function as shortcut.
- Improvements to CMake to better follow modern standards.
    - CMake required version bumped up to 3.12
    - Generating CMake files for downstream projects (config, target, version)
    - Windows builds default to shared DLL, not static library
    - Lots of cleanup

## [Version 0.7.1](https://github.com/fpagliughi/sockpp/compare/v0.7..v0.7.1)

Released: 2022-01-24

- [Experimental] **SocketCAN**, CAN bus support on Linux
- [#37](https://github.com/fpagliughi/sockpp/pull/37) socket::get_option() not returning length on Windows
- [#39](https://github.com/fpagliughi/sockpp/pull/39) Using *SSIZE_T* for *ssize_t* in Windows
- [#53](https://github.com/fpagliughi/sockpp/pull/53) Add Conan support
- [#55](https://github.com/fpagliughi/sockpp/pull/55) Fix Android strerror
- [#60](https://github.com/fpagliughi/sockpp/pull/60) Add missing move constructor for connector template.
- Now `acceptor::open()` uses the *SO_REUSEPORT* option instead of *SO_REUSEADDR* on non-Windows systems. Also made reuse optional.

## Version 0.7

- Base `socket` class
    - `shutdown()` added
    - `create()` added
    - `bind()` moved into base socket (from `acceptor`)
- Unix-domain socket pairs (stream and datagram)
- Non-blocking I/O
- Scatter/Gather I/O
- `stream_socket` cloning.
- Set and get socket options using template types.
- `stream_socket::read_n()` and `write_n()` now properly handle EINTR return.
- `to_timeval()` can convert from any `std::chrono::duration` type.
- `socket::close()` and `shutdown()` check for errors, set last error, and return a bool.
- _tcpechomt.cpp_: Example of a client sharing a socket between read and write threads - using `clone()`.
- Windows enhancements:
    - Implemented socket timeouts on Windows
    - Fixed bug in Windows socket cloning.
    - Fixed bug in Windows `socket::last_error_string`.
    - Unit tests working on Windows
- More unit tests

##  Version 0.6

- UDP support
    - The base `datagram_socket` added to the Windows build
    - The `datagram_socket` cleaned up for proper parameter and return types.
    - New `datagram_socket_tmpl` template class for defining UDP sockets for the different address families.
    - New datagram classes for IPv4 (`udp_socket`), IPv6 (`udp6_socket`), and Unix-domain (`unix_dgram_socket`)
- Windows support
    - Windows support was broken in release v0.5. It is now fixed, and includes the UDP features.
- Proper move semantics for stream sockets and connectors.
- Separate tcp socket header files for each address family (`tcp_socket.h`, `tcp6_socket.h`, etc).
- Proper implementation of Unix-domain streaming socket.
- CMake auto-generates a version header file, _version.h_
- CI dropped tests for gcc-4.9, and added support for clang-7 and 8.

## Version 0.5

- (Breaking change) Updated the hierarchy of network address classes, now derived from a common base class.
    - Removed `sock_address_ref` class. Now a C++ reference to `sock_address` will replace it (i.e. `sock_address&`).
    - `sock_address` is now an abstract base class.
    - All the network address classes now derive from `sock_address`
    - Consolidates a number of overloaded functions that took different forms of addresses to just take a `const sock_address&`
    - Adds a new `sock_address_any` class that can contain any address, and is used by base classes that need a generic address.
- The `acceptor` and `connector` classes are still concrete, generic classes, but now a template derives from each of them to specialize.
- The connector and acceptor classes for each address family (`tcp_connector`, `tcp_acceptor`, `tcp6_connector`, etc) are now typedef'ed to template specializations.
- The `acceptor::bind()` and `acceptor::listen()` methods are now public.
- CMake build now honors the `CMAKE_BUILD_TYPE` flag.

## Version 0.4

The work in this branch is proceeding to add support for IPv6 and refactor the class hierarchies to better support the different address families without so much redundant code.

 - IPv6 support: `inet6_address`, `tcp6_acceptor`, `tcp_connector`, etc.
 - (Breaking change) The `sock_address` class is now contains storage for any type of address and follows copy semantics. Previously it was a non-owning reference class. That reference class now exists as `sock_addresss_ref`.
 - Generic base classses are being re-implemented to use _sock_address_ and _sock_address_ref_ as generic addresses.
 - (Breaking change) In the `socket` class(es) the `bool address(address&)` and `bool peer_address(addr&)` forms of getting the socket addresses have been removed in favor of the ones that simply return the address.
 Added `get_option()` and `set_option()` methods to the base `socket`class.
 - The GNU Make build system (Makefile) was deprecated and removed.

## Version 0.3

 - Socket class hierarcy now splits out for streaming and datagram sockets.
 - Support for UNIX-domain sockets.
 - New modern CMake build system.
 - GNU Make system marked for deprecation.

## Version 0.2

 - Initial working version for IPv4.
 - API using boolean return values for pass/fail functions instead of syscall-style integers.


sockpp/CONTRIBUTING.md
--------------------------------------
# Contributing to _sockpp_

Thank you for your interest in the _sockpp_ library!

Contributions are accepted and much appreciated. You can contribute updates, bug fixes, and bug reports through the GitHub site for the project.

1. New and unstable development is done in the `develop` branch. Please make all pull requests against the `develop` branch.

1. Please follow the naming and format conventions of the existing code.

1. New features should be zero cost. Existing applications that do not use the feature(s) should not pay a cost in speed or size due to the new additions.

1. Prefer smaller, targeted pull requests (PR's). 
    1. Put each different new feature in a separate PR. 
    1. Separate bug fixes and new features in individual PR's.
 
1. Include unit tests for new features.
 
1. Please indicate the system, OS, and compiler used for development and whether there are any known incompatibilities with other supported systems.

1. Please only contribute code for which you have legal right to ownership. **Do not** contribute any code written at an employer site or on equipment ownd by an employer, if you have not been given explicit, written consent by the employer to contribute to open-source projects.



sockpp/README.md
--------------------------------------
# sockpp

Simple, modern, C++ socket library.

This is a fairly low-level C++ wrapper around the Berkeley sockets library using `socket`, `acceptor,` and `connector` classes that are familiar concepts from other languages.

The base `socket` class wraps a system socket handle, and maintains its lifetime. When the C++ object goes out of scope, it closes the underlying socket handle. Socket objects are generally _moveable_ but not _copyable_. A socket can be transferred from one scope (or thread) to another using `std::move()`.

Currently supports: IPv4, IPv6, and Unix-Domain Sockets on Linux, Mac, and Windows. Other *nix and POSIX systems should work with little or no modification.

There is also some experimental support for CAN bus programming on Linux using the SocketCAN package. This gives CAN bus adapters a network interface, with limitations dictated by the CAN message protocol.

All code in the library lives within the `sockpp` C++ namespace.

## Latest News

To keep up with the latest announcements for this project, follow me at:

**Mastodon/Fosstodon:** [@fpagliughi@fosstodon.org](https://fosstodon.org/@fpagliughi)

**Twitter:** [@fmpagliughi](https://twitter.com/fmpagliughi)

If you're using this library, send me a message, and let me know how you're using it.  I'm always curious to see where it winds up!


## Contributing

Contributions are accepted and appreciated. New and unstable work is done in the `develop` branch. Please submit all pull requests against that branch, not _master_.

For more information, refer to: [CONTRIBUTING.md](https://github.com/fpagliughi/sockpp/blob/master/CONTRIBUTING.md)


## Building the Library

CMake is the supported build system.

### Requirements:

- A conforming C++-14 compiler.
    - _gcc_ v5.0 or later (or) _clang_ v3.8 or later.
    - _Visual Studio 2015_, or later on WIndows.
- _CMake_ v3.12 or newer.
- _Doxygen_ (optional) to generate API docs.
- _Catch2_ (optional) v2.x or v3.x to build and run unit tests.

To build with default options:

```
$ cd sockpp
$ cmake -Bbuild .
$ cmake --build build/
```

To install:

```
$ cmake --build build/ --target install
```

### Build Options

The library has several build options via CMake to choose between creating a static or shared (dynamic) library - or both. It also allows you to build the example options, and if Doxygen is

Variable | Default Value | Description
------------ | ------------- | -------------
SOCKPP_BUILD_SHARED | ON | Whether to build the shared library
SOCKPP_BUILD_STATIC | OFF | Whether to build the static library
SOCKPP_BUILD_DOCUMENTATION | OFF | Create and install the HTML based API documentation (requires _Doxygen)_
SOCKPP_BUILD_EXAMPLES | OFF | Build example programs
SOCKPP_BUILD_TESTS | OFF | Build the unit tests (requires _Catch2_, v2.x or 3.x)
SOCKPP_BUILD_CAN | OFF | Build SocketCAN support. (Linux only)

Set these using the '-D' switch in the CMake configuration command. For example, to build documentation and example apps:

```
$ cd sockpp
$ cmake -Bbuild -DSOCKPP_BUILD_DOCUMENTATION=ON -DSOCKPP_BUILD_EXAMPLES=ON .
$ cmake --build build/
```

## TCP Sockets

TCP and other "streaming" network applications are usually set up as either servers or clients. An acceptor is used to create a TCP/streaming server. It binds an address and listens on a known port to accept incoming connections. When a connection is accepted, a new, streaming socket is created. That new socket can be handled directly or moved to a thread (or thread pool) for processing.

Conversely, to create a TCP client, a connector object is created and connected to a server at a known address (typically host and socket). When connected, the socket is a streaming one which can be used to read and write, directly.

For IPv4 the `tcp_acceptor` and `tcp_connector` classes are used to create servers and clients, respectively. These use the `inet_address` class to specify endpoint addresses composed of a 32-bit host address and a 16-bit port number.

### TCP Server: `tcp_acceptor`

The `tcp_acceptor` is used to set up a server and listen for incoming connections.

    int16_t port = 12345;
    sockpp::tcp_acceptor acc(port);

    if (!acc)
        report_error(acc.last_error_str());

    // Accept a new client connection
    sockpp::tcp_socket sock = acc.accept();

The acceptor normally sits in a loop accepting new connections, and passes them off to another process, thread, or thread pool to interact with the client. In standard C++, this could look like:

    while (true) {
        // Accept a new client connection
        sockpp::tcp_socket sock = acc.accept();

        if (!sock) {
            cerr << "Error accepting incoming connection: "
                << acc.last_error_str() << endl;
        }
        else {
            // Create a thread and transfer the new stream to it.
            thread thr(run_echo, std::move(sock));
            thr.detach();
        }
    }

The hazards of a thread-per-connection design is well documented, but the same technique can be used to pass the socket into a thread pool, if one is available.

See the [tcpechosvr.cpp](https://github.com/fpagliughi/sockpp/blob/master/examples/tcp/tcpechosvr.cpp) example.

### TCP Client: `tcp_connector`

The TCP client is somewhat simpler in that a `tcp_connector` object is created and connected, then can be used to read and write data directly.

    sockpp::tcp_connector conn;
    int16_t port = 12345;

    if (!conn.connect(sockpp::inet_address("localhost", port)))
        report_error(conn.last_error_str());

    conn.write_n("Hello", 5);

    char buf[16];
    ssize_t n = conn.read(buf, sizeof(buf));

See the [tcpecho.cpp](https://github.com/fpagliughi/sockpp/blob/master/examples/tcp/tcpecho.cpp) example.

### UDP Socket: `udp_socket`

UDP sockets can be used for connectionless communications:

    sockpp::udp_socket sock;
    sockpp::inet_address addr("localhost", 12345);

    std::string msg("Hello there!");
    sock.send_to(msg, addr);

    sockpp::inet_address srcAddr;

    char buf[16];
    ssize_t n = sock.recv(buf, sizeof(buf), &srcAddr);

See the [udpecho.cpp](https://github.com/fpagliughi/sockpp/blob/master/examples/udp/udpecho.cpp) and [udpechosvr.cpp](https://github.com/fpagliughi/sockpp/blob/master/examples/udp/udpechosvr.cpp) examples.
### IPv6

The same style of  connectors and acceptors can be used for TCP connections over IPv6 using the classes:

    inet6_address
    tcp6_connector
    tcp6_acceptor
    tcp6_socket
    udp6_socket

Examples are in the [examples/tcp](https://github.com/fpagliughi/sockpp/tree/master/examples/tcp) directory.

### Unix Domain Sockets

The same is true for local connection on *nix systems that implement Unix Domain Sockets. For that use the classes:

    unix_address
    unix_connector
    unix_acceptor
    unix_socket  (unix_stream_socket)
    unix_dgram_socket

Examples are in the [examples/unix](https://github.com/fpagliughi/sockpp/tree/master/examples/unix) directory.

### SocketCAN (CAN bus on Linux)

The Controller Area Network (CAN bus) is a relatively simple protocol typically used by microcontrollers to communicate inside an automobile or industrial machine. Linux has the _SocketCAN_ package which allows processes to share acces to a physical CAN bus interface using sockets in user space. See: [Linux SocketCAN](https://www.kernel.org/doc/html/latest/networking/can.html)

At the lowest level, CAN devices write individual packets, called "frames" to a specific numeric addresses on the bus. 

For examle a device with a temperature sensor might read the temperature persoidically and write it to the bus as a raw 32-bit integer, like:

```
can_address addr("CAN0");
can_socket sock(addr);

// The agreed ID to broadcast temperature on the bus
canid_t canID = 0x40;

while (true) {
    this_thread::sleep_for(1s);

    // Write the time to the CAN bus as a 32-bit int
    int32_t t = read_temperature();

    can_frame frame { canID, &t, sizeof(t) };
    sock.send(frame);
}
```

A receiver to get a frame might look like this:

```
can_address addr("CAN0");
can_socket sock(addr);

can_frame frame;
sock.recv(&frame);
```

## Implementation Details

The socket class hierarchy is built upon a base `socket` class. Most simple applications will probably not use `socket` directly, but rather use derived classes defined for a specific address family like `tcp_connector` and `tcp_acceptor`.

The socket objects keep a handle to an underlying OS socket handle and a cached value for the last error that occurred for that socket. The socket handle is typically an integer file descriptor, with values >=0 for open sockets, and -1 for an unopened or invalid socket. The value used for unopened sockets is defined as a constant, `INVALID_SOCKET`, although it usually doesn't need to be tested directly, as the object itself will evaluate to _false_ if it's uninitialized or in an error state. A typical error check would be like this:

    tcp_connector conn({"localhost", 12345});

    if (!conn)
        cerr << conn.last_error_str() << std::endl;

The default constructors for each of the socket classes do nothing, and simply set the underlying handle to `INVALID_SOCKET`. They do not create a socket object. The call to actively connect a `connector` object or open an `acceptor` object will create an underlying OS socket and then perform the requested operation.

An application can generally perform most low-level operations with the library. Unconnected and unbound sockets can be created with the static `create()` function in most of the classes, and then manually bind and listen on those sockets.

The `socket::handle()` method exposes the underlying OS handle which can then be sent to any platform API call that is not exposed by the library.

### Thread Safety

A socket object is not thread-safe. Applications that want to have multiple threads reading from a socket or writing to a socket should use some form of serialization, such as a `std::mutex` to protect access.

A `socket` can be _moved_ from one thread to another safely. This is a common pattern for a server which uses one thread to accept incoming connections and then passes off the new socket to another thread or thread pool for handling. This can be done like:

    sockpp::tcp6_socket sock = acc.accept(&peer);

    // Create a thread and transfer the new socket to it.
    std::thread thr(handle_connection, std::move(sock));

In this case, _handle_connection_ would be a function that takes a socket by value, like:

    void handle_connection(sockpp::tcp6_socket sock) { ... }

Since a `socket` can not be copied, the only choice would be to move the socket to a function like this.

It is a common patern, especially in client applications, to have one thread to read from a socket and another thread to write to the socket. In this case the underlying socket handle can be considered thread safe (one read thread and one write thread). But even in this scenario, a `sockpp::socket` object is still not thread-safe due especially to the cached error value. The write thread might see an error that happened on the read thread and visa versa.

The solution for this case is to use the `socket::clone()` method to make a copy of the socket. This will use the system's `dup()` function or similar create another socket with a duplicated copy of the socket handle. This has the added benefit that each copy of the socket can maintain an independent lifetime. The underlying socket will not be closed until both objects go out of scope.

    sockpp::tcp_connector conn({host, port});

    auto rdSock = conn.clone();
    std::thread rdThr(read_thread_func, std::move(rdSock));

The `socket::shutdown()` method can be used to communicate the intent to close the socket from one of these objects to the other without needing another thread signaling mechanism.

See the [tcpechomt.cpp](https://github.com/fpagliughi/sockpp/blob/master/examples/tcp/tcpechomt.cpp) example.


base/SCHEMA_GENERATOR.md
--------------------------------------
## Schema generator instructions

The rrgraph reader and writer is now generated via uxsdcxx and the
`rr_graph.xsd` file.  The interface between the generated code and the VPR is
mediated via RrGraphBase located in `rr_graph_uxsdcxx_interface.h`.

If `rr_graph.xsd` is modified, then the following files must be updated:

 - `libs/librrgraph/src/io/gen/rr_graph_uxsdcxx.h`
 - `libs/librrgraph/src/io/gen/rr_graph_uxsdcxx_capnp.h`
 - `libs/librrgraph/src/io/gen/rr_graph_uxsdcxx_interface.h`
 - `libs/libvtrcapnproto/rr_graph_uxsdcxx.capnp`

### Instructions to update generated files (using CMake)

1. Run target `generate_rr_graph_serializers`, e.g. run `make generate_rr_graph_serializers`.
2. Run target `format`, e.g. run `make format`.
3. Update `libs/librrgraph/src/io/rr_graph_uxsdcxx_interface_impl.h`, implement or
   update interfaces that are new or are changed.  The compiler will complain
   that virtual methods are missing if the schema has changed.

### Instructions to update generated files (manually)

1. Clone https://github.com/duck2/uxsdcxx/
2. Run `python3 -mpip install --user -r requirements.txt`
3. Run `python3 uxsdcxx.py libs/librrgraph/src/io/rr_graph.xsd`
3. Run `python3 uxsdcap.py libs/librrgraph/src/io/rr_graph.xsd`
4. Copy `rr_graph_uxsdcxx.h`, `rr_graph_uxsdcxx_capnp.h`,
   `rr_graph_uxsdcxx_interface.h` to `libs/librrgraph/src/io/`
5. Copy `rr_graph_uxsdcxx.capnp` to `../libvtrcapnproto/`
6. Run `make format`
7. Update `libs/librrgraph/src/io/rr_graph_uxsdcxx_interface_impl.h`, implement or
   update interfaces that are new or are changed.  The compiler will complain
   that virtual methods are missing if the schema has changed.

### Defining the serialization interface object

The code generator emits an abstract base class in
`rr_graph_uxsdcxx_interface.h` that must be implemented to bind VPR to the
generated code.  Roughly half of the methods are for the reader interface
(from disk to VPR) and half of the methods are for the writer interface (from
VPR to disk).

The methods starting with `preallocate_`, `add_`, `init_`, and `finish_`
are associated with the reader interface.  The reader interface has three additionals methods, `start_load`, `finish_load` and `error_encountered`.

The methods starting with `get_`, `has_` and `size_` are associated with
the writer interface.  The writer interface has two additionals methods, `start_write`, and `finish_write`.

#### Load interface

##### Start, finish, and error handling

The first method called upon a read is `start_load`.  `start_load` has one parameter, a
`std::function` called `report_error`.

This function should be called if an error is detected in the file.  Calling
this function will pass control the reader implementation.  The reader
implementation can then add context about where the error occur (e.g. file and
line number).

The `error_encountered` method is used by the reader implementation to
declare that an error has occurred during parsing.  That error may be generated
via the reader implementation itself, or by the load interface class by calling
the `report_error` function.

`error_encountered` is expected to throw some form of C++ exception to unwind
the parsing stack.  If `error_encountered` does not throw an exception, the
reader implementation will throw a `std::runtime_error` exception.

The `std::function` based into `start_load` will have a lifetime that exists
until after the `finish_load` method is called.  The `finish_load` method
will be called after loading is complete and no error was encountered.

If an error is detected by either the reader implementation or the
implementation class, the `error_encountered` will be invoked. After
`error_encountered` is invoked, then no further methods will be invoked until
a new load is started.

##### Method invocation order during read

During a load, the reader interface will invoke the following methods in order:

1. (optional) The reader implementation may call `preallocate_` on instances
   that can have size.  The implementation class should not require this
   method be called, in the event that the reader implementation does not have
   size information available.
2. The reader implementation will call with `init_` or `add_` with any
   required non-string attributes.  Required string attributes and other
   optional attributes will be set next.
3. The reader implementation will call `set_` with any required string or
   optional attributes present on the object.
4. If the object has children, the reader will return to #1 with the child
   objects.
5. Once all children of the current object have been read and the
   implementation class methods have been invoked, the `finish_` method will
   be called.

##### Context variables

Context variables are available to allow the implementation class to keep
state between method invocations.  By default all context variables are
`void *`, but they can be changed to any type via the template parameter
`ContextTypes`.

The root tag context variable is passed to the first load function
`load_rr_graph_xml`.  Context variables are then introduced whenever an
`init_` or `add_` call is made, used during `set_` calls, and the `finish_`
method call.  After the `finish_` method, the context variable leaves scope
and is destroyed.

When descending into a child object, the parents context variable is passed to
the child's `preallocate_`, `init_` or `add_` method calls.

##### Load Example

For example:

Consider an object tree like:

```
<rr_graph tool_comment="Comment">
  <rr_nodes>
    <node id="0" type="CHANX"/>
    <node id="1" type="CHANX"/>
    <node id="2" type="CHANX">
      <metadata>
        <meta name="fasm_feature">CLB</meta>
      </metadata>
    </node>
  </rr_nodes>
</rr_graph>
```

The method call order will be:

1. `start_load` invoked with a `report_error` `std::function`.
2. `set_rr_graph_tool_comment` with the initial context variable passed to
   `load_rr_graph_xml`.
3. `init_rr_graph_rr_nodes` with the initial context variable passed to
   `load_rr_graph_xml`.  The `init_rr_graph_rr_nodes` method returns the
   new context variable to be used for `rr_nodes`, lets call it
   `rr_nodes_context`.
4. (optional) `preallocate_rr_nodes_node` with size=3 and the context variable
   `rr_nodes_context`.
5. `add_rr_nodes_node` with the context variable `rr_nodes_context`, with
   id=0, and type=CHANX.  `add_rr_nodes_node` will return a new context
   variable, `node0_context`.
6. `finish_rr_nodes_node` with the context variable `node0_context`.
7. `node0_context` will leave scope and be destroyed.
8. `add_rr_nodes_node` with the context variable `rr_nodes_context`, with
   id=1, and type=CHANX.  `add_rr_nodes_node` will return a new context
   variable, `node1_context`.
9. `finish_rr_nodes_node` with the context variable `node1_context`.
10. `node1_context` will leave scope and be destroyed.
11. `add_rr_nodes_node` with the context variable `rr_nodes_context`, with
   id=1, and type=CHANX.  `add_rr_nodes_node` will return a new context
   variable, `node2_context`.
12. `init_node_metadata` with the context variable `node2_context`, and will
    return new context variable `node2_metadata_context`.
13. `add_metadata_meta` with the context variable `node2_metadata_context`,
    returning `node2_metadata_meta_context`.
14. `set_meta_name` with the context variable `node2_metadata_meta_context`,
    and name="fasm_feature".
14. `set_meta_value` with the context variable
    `node2_metadata_meta_context` and value="CLB".
15. `finish_metadata_meta` with the context variable  `node2_metadata_meta_context`.
16. `node2_metadata_meta_context` will leave scope and be destroyed.
17. `finish_node_metadata` with the context variable `node2_metadata_context`.
18. `finish_rr_nodes_node` with the context variable `node2_context`.
19. `node2_context` will leave scope and be destroyed.
20. `finish_rr_graph_rr_nodes` with the context variable with the initial
    context variable passed to `load_rr_graph_xml`.
21. `finish_load` invoked

#### Write interface

##### Method invocation order during write

During a writer, the writer interface will invoke the following methods in order:

1. For scalar optional fields, the `has_` method will be invoked.  If the
   `has_` method returns false, that field and/or object will be skipped
   during writing.
   For non-scalar fields, the `num_` method will be invoked.
2. The `get_` method will be invoked with the parents context variable.  The
   `get_` method should return a new context variable for the relevant object.
3. For each required attributes, a `get_` method will be called with the
   current context variable.
4. For each optional attribute, a `has_` method will be called with the
   current context variable, followed by a `get_` method if the `has_`
   returns true.
5. For any children, 1-4 repeats.
6. If this object is part of a non-scalar field, 2-5 repeats for each remaining
   item.

##### Write Example

For example:

Consider writing an object tree like:

```
<rr_graph tool_comment="Comment">
  <rr_nodes>
    <node id="10" type="CHANX"/>
    <node id="23" type="CHANX"/>
    <node id="50" type="CHANX">
      <metadata>
        <meta name="fasm_feature">CLB</meta>
      </metadata>
    </node>
  </rr_nodes>
</rr_graph>
```

The method call order will be:

1. `start_write` invoked
2. `get_rr_graph_tool_comment` invoked with the initial context variable passed
    to `write_rr_graph_xml`, returning "Comment".  Note that the returned
    string pointer must live at least until the next implementation method
    call.
3. `get_rr_graph_rr_nodes` invoked with the initial context variable passed
    to `write_rr_graph_xml`, returning a new context variable
    `rr_nodes_context`.
4. `num_rr_nodes_node` invoked with context variable `rr_nodes_context`,
   returns 3.
5. `get_rr_nodes_node` invoked with n=0 and context variable
   `rr_nodes_context`, returns context variable `node0_context`.
6. `get_node_id` invoked with context variable `node0_context`, returns 10.
7. `get_node_type` invoked with context variable `node0_context`, returns CHANX.
8. `has_node_metadata` invoked with context variable `node0_context`, returns false.
9. `node0_context` leaves scope and is destroyed.
10. `get_rr_nodes_node` invoked with n=1 and context variable
   `rr_nodes_context`, returns context variable `node1_context`.
11. `get_node_id` invoked with context variable `node1_context`, returns 23.
12. `get_node_type` invoked with context variable `node1_context`, returns CHANX.
13. `has_node_metadata` invoked with context variable `node1_context`, returns false.
14. `node1_context` leaves scope and is destroyed.
15. `get_rr_nodes_node` invoked with n=2 and context variable
   `rr_nodes_context`, returns context variable `node2_context`.
16. `get_node_id` invoked with context variable `node2_context`, returns 50.
17. `get_node_type` invoked with context variable `node2_context`, returns CHANX.
18. `has_node_metadata` invoked with context variable `node2_context`, returns true.
19. `get_node_metadata` invoked with context variable `node2_context`, returning `node2_metadata_context`.
20. `num_metadata_meta` invoked with context variable `node2_metadata_context`, returns 1.
21. `get_metadata_meta` invoked with n=0 and context variable `node2_metadata_context`, returns `node2_metadata_meta_context`.
22. `get_meta_name` invoked with context variable `node2_metadata_meta_context`, returns "fasm_feature".
23. `get_meta_value` invoked with context variable `node2_metadata_meta_context`, returns "CLB".
24. `node2_metadata_meta_context` leaves scope and is destroyed.
25. `node2_metadata_context` leaves scope and is destroyed.
26. `node2_context` leaves scope and is destroyed.
27. `finish_write` invoked



gen/README.gen.md
--------------------------------------
`rr_graph_uxsdcxx.h`, `rr_graph_uxsdcxx_capnp.h` and
`rr_graph_uxsdcxx_interface.h` are generated via uxsdcxx and are checked in to
avoid requiring python3 and the uxsdcxx depedencies to build VPR.

See `$VTR_DIR/libs/librrgraph/src/base/SCHEMA_GENERATOR.md` for details.



librtlnumber/README.md
--------------------------------------
librtlnumber - Register Transfer Level (RTL) Verilog Number Library

Authors: Aaron Graham (aaron.graham@unb.ca, aarongraham9@gmail.com),
         Jean-Philippe Legault (jlegault@unb.ca, jeanphilippe.legault@gmail.com)
		  and Dr. Kenneth B. Kent (ken@unb.ca)
           for the Reconfigurable Computing Research Lab at the
            Univerity of New Brunswick in Fredericton, New Brunswick, Canada

Arbitrary Length Verilog Number Library that can Handle `X` and `Z` inputs.



libvtrcapnproto/README.md
--------------------------------------
Capnproto usage in VTR
======================

Capnproto is a data serialization framework designed for portabliity and speed.
In VPR, capnproto is used to provide binary formats for internal data
structures that can be computed once, and used many times.  Specific examples:
 - rrgraph
 - Router lookahead data
 - Place matrix delay estimates

What is capnproto?
==================

capnproto can be broken down into 3 parts:
 - A schema language
 - A code generator
 - A library

The schema language is used to define messages.  Each message must have an
explcit capnproto schema, which are stored in files suffixed with ".capnp".
The capnproto documentation for how to write these schema files can be found
here: https://capnproto.org/language.html

The schema by itself is not especially useful.  In order to read and write
messages defined by the schema in a target language (e.g. C++), a code
generation step is required.  Capnproto provides a cmake function for this
purpose, `capnp_generate_cpp`.  This generates C++ source and header files.
These source and header files combined with the capnproto C++ library, enables
C++ code to read and write the messages matching a particular schema.  The C++
library API can be found here: https://capnproto.org/cxx.html

Contents of libvtrcapnproto
===========================

libvtrcapnproto should contain two elements:
 - Utilities for working capnproto messages in VTR
 - Generate source and header files of all capnproto messages used in VTR

I/O Utilities
-------------

Capnproto does not provide IO support, instead it works from arrays (or file
descriptors).  To avoid re-writing this code, libvtrcapnproto provides two
utilities that should be used whenever reading or writing capnproto message to
disk:
 - `serdes_utils.h` provides the writeMessageToFile function - Writes a
   capnproto message to disk.
 - `mmap_file.h` provides MmapFile object - Maps a capnproto message from the
   disk as a flat array.

NdMatrix Utilities
------------------

A common datatype which appears in many data structures that VPR might want to
serialize is the generic type `vtr::NdMatrix`.  `ndmatrix_serdes.h` provides
generic functions ToNdMatrix and FromNdMatrix, which can be used to generically
convert between the provideid capnproto message `Matrix` and `vtr::NdMatrix`.

Capnproto schemas
-----------------

libvtrcapnproto should contain all capnproto schema definitions used within
VTR.  To add a new schema:
1. Add the schema to git in `libs/libvtrcapnproto/`
2. Add the schema file name to `capnp_generate_cpp` invocation in
   `libs/libvtrcapnproto/CMakeLists.txt`.

The schema will be available in the header file `schema filename>.h`.  The
actual header file will appear in the CMake build directory
`libs/libvtrcapnproto` after `libvtrcapnproto` has been rebuilt.

Writing capnproto binary files to text
======================================

The `capnp` tool (found in the CMake build directiory
`libs/EXTERNAL/capnproto/c++/src/capnp`) can be used to convert from a binary
capnp message to a textual form.

Example converting VprOverrideDelayModel from binary to text:

```
capnp convert binary:text place_delay_model.capnp VprOverrideDelayModel \
  < place_delay.bin > place_delay.txt
```



gen/README.gen.md
--------------------------------------
`rr_graph_uxsdcxx.capnp` is generated via uxsdcxx and is checked in to
avoid requiring python3 and the uxsdcxx depedencies to build VPR.

See `vpr/src/route/gen/SCHEMA_GENERATOR.md` for details.



odin_ii/README.md
--------------------------------------
odin_ii
=======

Odin II is used for logic synthesis and elaboration, converting a subset of the Verilog Hardware Description Language (HDL) into a BLIF netlist.

-----------------

* **[Quick Start](../doc/src/odin/quickstart.md)**
* **[User Guide](../doc/src/odin/user_guide.md)**
* **[Verilog Support](../doc/src/odin/verilog_support.md)**
* **Developer guide**
  * [Contributing](../doc/src/odin/dev_guide/contributing)
  * [Regression Tests](../doc/src/odin/dev_guide/regression_test)
  * [Regression Tool](../doc/src/odin/dev_guide/verify_script)
  * [Validating Tests](../doc/src/odin/dev_guide/testing)

Maintained by:  [Ph.D. - alirezazd](https://github.com/alirezazd) and [M.Sc. - poname](https://github.com/poname)
Owned by: [KennethKent](https://github.com/kennethkent)



parmys/README.md
--------------------------------------
# Parmys (Partial Mapper for Yosys) Plugin

This repository contains intellignet partial mapper plugin from [Odin-II](https://github.com/verilog-to-routing/vtr-verilog-to-routing/tree/master/odin_ii) for [Yosys](https://github.com/YosysHQ/yosys.git).

The project build skeleton is based on [Yosys F4PGA Plugins](https://github.com/chipsalliance/yosys-f4pga-plugins.git) project.

It is highly recommended to utilize this plugin through the [Verilog to Routing (VTR)](https://github.com/verilog-to-routing/vtr-verilog-to-routing.git) project.

## Build as a plugin

- Clone, make, and install [Yosys](https://github.com/YosysHQ/yosys.git)
- Clone, make, and install [VTR](https://github.com/verilog-to-routing/vtr-verilog-to-routing.git)
- Build and install Parmys

```makefile
make VTR_INSTALL_DIR=`path to VTR build/install directory` plugins -j`nproc`
sudo make install
```

## Parameters
Available parameters are:
```
    -a ARCHITECTURE_FILE
        VTR FPGA architecture description file (XML)

    -c XML_CONFIGURATION_FILE
        Configuration file

    -top top_module
        set the specified module as design top module

    -nopass
        No additional passes will be executed.

    -exact_mults int_value
        To enable mixing hard block and soft logic implementation of adders

    -mults_ratio float_value
        To enable mixing hard block and soft logic implementation of adders

    -vtr_prim
        loads vtr primitives as modules, if the design uses vtr prmitives then this flag is mandatory for first run
```

## Usage (without VTR)

Example for simple partial mapping with parmys:

```sh
# run yosys
yosys

# load the plugin
plugin -i parmys

# read verilog files
read_verilog my_verilog.v

# use parmys to read the architecture file and partial mapping
parmys -a simple_vtr_fpga_architecture.xml

```

## Usage (within VTR flow)

- Clone [VTR](https://github.com/verilog-to-routing/vtr-verilog-to-routing.git)
- [Set up the Environment](https://docs.verilogtorouting.org/en/latest/BUILDING/#setting-up-your-environment)
- Build with the following command to enable both Yosys as the frontend and Parmys as the partial mapper:
```makefile
make CMAKE_PARAMS="-DWITH_YOSYS=on -DYOSYS_PARMYS_PLUGIN=on"
```
- Run vtr flow

```makefile
cd vtr_flow/scripts/

# this command runs the vtr flow [yosys+parmys, abc, vpr]
./run_vtr_flow.py my_verilog.v fpga_architecture.xml -start yosys -end vpr
```

For detailed information please refer to the [VTR documentation](https://docs.verilogtorouting.org/en/latest/).

Detailed help on the supported command(s) can be obtained by running `help parmys` in Yosys.



vtr_flow/LICENSE.md
--------------------------------------
# VTR License

The software package "VTR" includes the software tools ODIN II, ABC, and VPR as
well as additional benchmarks, documentation, libraries and scripts. The authors
of the various components of VTR retain their ownership of their tools.

* Unless otherwise noted (in particular ABC, the benchmark circuits and some libraries),
all software, documents, and scripts in VTR, follows the standard MIT license described
[here](http://www.opensource.org/licenses/mit-license.php) copied below for
your convenience:

> The MIT License (MIT)
>
> Copyright 2012 VTR Developers
>
> Permission is hereby granted, free of charge, to any person obtaining a copy of
> this software and associated documentation files (the "Software"), to deal in
> the Software without restriction, including without limitation the rights to
> use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
> of the Software, and to permit persons to whom the Software is furnished to do
> so, subject to the following conditions:
>
> The above copyright notice and this permission notice shall be included in all
> copies or substantial portions of the Software.
>
> THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
> IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
> FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
> AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
> LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
> OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
> SOFTWARE.

* Terms and conditions for ABC is found
[here](http://www.eecs.berkeley.edu/~alanmi/abc/copyright.htm) copied below
for your convenience:

> Copyright (c) The Regents of the University of California. All rights reserved.
>
> Permission is hereby granted, without written agreement and without license or
> royalty fees, to use, copy, modify, and distribute this software and its
> documentation for any purpose, provided that the above copyright notice and the
> following two paragraphs appear in all copies of this software.
>
> IN NO EVENT SHALL THE UNIVERSITY OF CALIFORNIA BE LIABLE TO ANY PARTY FOR
> DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF
> THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF THE UNIVERSITY OF
> CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
>
> THE UNIVERSITY OF CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING,
> BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
> A PARTICULAR PURPOSE. THE SOFTWARE PROVIDED HEREUNDER IS ON AN "AS IS" BASIS,
> AND THE UNIVERSITY OF CALIFORNIA HAS NO OBLIGATION TO PROVIDE MAINTENANCE,
> SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.

The benchmark circuits are all open source but each have their own
individual terms and conditions which are listed in the source code of each
benchmark.

Subject to these conditions, the software is provided free of charge to all
interested parties.

If you do decide to use this tool, please reference our work as references are
important in academia.

Donations in the form of research grants to promote further research and
development on the tools will be gladly accepted, either anonymously or with
attribution on our future publications.




vtr_flow/README.md
--------------------------------------
# VTR Flow

This folder contains architecture files, benchmark circuits and scripts for running the VTR flow.

For a description see the ['Running the VTR Flow' documentation at https://docs.verilogtorouting.org](https://docs.verilogtorouting.org/en/latest/vtr/running_vtr/),
or the [`run_vtr_flow.rst` file](../doc/src/vtr/run_vtr_flow.rst) in the [`doc` folder](../doc)
in the [root of the VTR source tree](https://github.com/SymbiFlow/vtr-verilog-to-routing).



gen/README.gen.md
--------------------------------------
`vpr_constraints_uxsdcxx.h`and
`vpr_constraints_uxsdcxx_interface.h` are generated via uxsdcxx and are checked in to
avoid requiring python3 and the uxsdcxx depedencies to build VPR.



vtr_flow/README.md
--------------------------------------
# VTR Flow

This folder contains architecture files, benchmark circuits and scripts for running the VTR flow.

For a description see the ['Running the VTR Flow' documentation at https://docs.verilogtorouting.org](https://docs.verilogtorouting.org/en/latest/vtr/running_vtr/),
or the [`run_vtr_flow.rst` file](../doc/src/vtr/run_vtr_flow.rst) in the [`doc` folder](../doc)
in the [root of the VTR source tree](https://github.com/SymbiFlow/vtr-verilog-to-routing).



multi_die/README.md
--------------------------------------
# 3D FPGA Architectures

This directory contains architecture files for 3D FPGAs. The architectures are divided into three sub-directories:

1. **koios_3d:**
- Contains architecture files based on the [k6FracN10LB_mem20K_complexDSP_customSB_22nm](../COFFE_22nm/k6FracN10LB_mem20K_complexDSP_customSB_22nm.clustered.xml) architecture, utilized in Koios benchmarks.
- Inside the architecture file, the fabric with multiple sizes based on the sector size is defined.
- Routing resource and switch delays in this architecture are configured for 7 nm technology. The inter-die connection delay is 73 ps.
- The empty blocks in the middle of the fabric at the base die, which have the chess pattern, represent the through-silicon via (TSV) holes used to deliver power and ground to the upper die.
- Detailed information on how these delays are obtained can be found in the paper "Into the Third Dimension: Architecture Exploration Tools for 3D Reconfigurable Acceleration Devices," presented at FPT '23.
- **Architectures:**
   - `3d_full_OPIN_inter_die_k6FracN10LB_mem20k_complexDSP_customSB_7nm.xml`
     - The inter-die connection is only from output pins, with all output pins connected to the channel on the same die and the other.
     - Inside the architecture file, the fabric with multiple sizes based on the sector size is defined.
   - `3d_limited_OPIN_inter_die_k6FracN10LB_mem20k_complexDSP_customSB_7nm.xml`
   - The inter-die connection is only from output pins, with all OPINs of BRAMs and DSPs connected to the channels on the same die and the other die. 
   - Only 60% of the OPINs of CLBs are connected to the channels on the other die and all of them can get connected to the channels on the same die.

2. **stratixiv_3d:**
   - Contains architecture files based on a [StratixIV-like](../titan/stratixiv_arch.timing.xml) architecture.
   - Delays of switches and routing resources are similar to those reported in the capture of the StratixIV architecture.
   - For the inter-die connection, we multiply the L4 wire delay of SIV by the ratio of the inter-die connection delay to the L4 wire delay of the Koios_3d benchmark.
   - **Architectures:**
     - `3d_10x10_noc_base_stratixiv_up.xml`
       - A 10x10 NoC mech is put on the base die.
       - The upper die is SIV-like FPGA fabric.
     - `3d_full_inter_die_stratixiv_arch.timing.xml`
       - The architecture has two dice.
       - Both dice are SIV-like FPGA fabric.
       - All pins can cross die.
       - This is a completely hypothetical architecture, as the area required to place drivers on every channel segment to drive an IPIN on the other die would be too large. For the purpose of this scenario, we assume an inter-die connection delay of 0.
     - `3d_full_OPIN_inter_die_stratixiv_arch.timing.xml`
       - The architecture has two dice.
       - Both dice are SIV-like FPGA fabric.
       - Only OPINs can cross die.

3. **simple_arch:**
   - Simple architectures primarily used for quick testing in the flow.
   - The inter-die delay of architectures in this file is considered to be zero.
   - Contains two dice and both have the simple fabric.
   - All pins can cross die.



symbiflow/README.md
--------------------------------------
SymbiFlow architectures
=======================

This directory holds all the SymbiFlow architectures that are generated in the [SymbiFlow-arch-defs](https://github.com/SymbiFlow/symbiflow-arch-defs) repository.

The data files needed to successfully run VPR are:
- Architecture XML definition
- RR Graph
- Router Lookahead
- Place delay matrix lookup

All the data files can be downloaded with the `make get_symbiflow_benchmarks` target in the root directory.



titan/README.rst
--------------------------------------
Titan Benchmarks
--------------------------------------------------
This folder contains architecture files for use with Titan.

The `Titan <http://www.eecg.utoronto.ca/~vaughn/titan/>` benchmarks are distributed
separately from VTR due to their large size.

To integrate them into VTR run:

    $ make get_titan_benchmarks

from the root of the VTR source tree.

This will download and extract the benchmark netlists to:

    <vtr>/vtr_flow/benchmarks/titan_blif/



Directory Structure
--------------------------------------------------

stratix10_arch.timing.xml:
    A detailed capture of Intel's Stratix 10 FPGA architecture, with 
    small architectural adjustments to ensure compatibility with VPR. It 
    incorporates a timing model that has been fine-tuned to match the 
    Stratix 10 timing model integrated into Intel's Quartus Prime CAD tools.

stratixiv_arch.timing.xml:
    An enhanced capture of Altera's Stratix IV FPGA architecture. It makes some 
    relatively minor architectural approximations to be compatible with VPR. It
    includes a timing model which has been calibrated to the Stratix IV timing
    model included in Altera's Quartus II CAD tools.
    
    Compared to the original (legacy) capture of Stratix IV, this architecture
    better captures the routing architecture by modeling the hierarchical connections
    between the L16 and L4 networks, and the custom switch patterns between them.
    The L4 wires have 12-input muxes driving them, and the L16 wires have 40:1 muxes
    driving them, roughly matching the Stratix IV mux sizes from the information available.

    This is the architecture used to generate results in the VTR 8.0 TRETS paper,
    and corresponds to architecture E in Table 3 in that paper.

    Use this architecture file, unless you have specific reasons to use the
    other ones in this directory.

stratixiv_arch.timing.complex_sb.12to1.xml:
    A variant of the above architecture capture using customized switch-block
    with hierarchical wire connectivity and fixed 12:1 L4 and L16 driver muxes.

stratixiv_arch.timing.complex_sb.L16_40to1.L4_turn-straight_rand_L4_L16.xml:
    A variant like stratixiv_arch.timing.complex_sb.12to1.xml, but increased the
    L16 driver muxes to 40:1.

stratixiv_arch.timing.complex_sb.L16_72to1.L4_turn-straight_rand_L4_L16.xml:
    A variant like stratixiv_arch.timing.complex_sb.12to1.xml, but increased the
    L16 driver muxes to 72:1.

stratixiv_arch.timing.complex_sb.L4_16to1.L16_72to1.L4_turn-straight_rand_L4_L16.xml:
    A variant like stratixiv_arch.timing.complex_sb.12to1.xml, but increased the
    L4 driver muxes to 16:1 and the L16 driver muxes to 72:1.

For more details about the Altera's Stratix IV FPGA architecture capture variants,
see Section 5.4 of the paper, "VTR 8: High Performance CAD and Customizable FPGA Architecture Modelling": 
<https://www.eecg.utoronto.ca/~kmurray/vtr/vtr8_trets.pdf>

**legacy subdirectory:**

    stratixiv_arch.timing.legacy.xml:
        The old capture of Altera's Stratix IV FPGA architecture.
        This architecture was used to generate the results in the Timing-driven Titan paper:
        K. Murray et al, "Timing-Driven Titan: Enabling Large Benchmarks and Exploring the
        Gap between Academic and Commercial CAD," ACM TRETS, March 2015,
        <https://dl.acm.org/doi/10.1145/2629579>

    stratixiv_arch.timing.no_chain.xml (experimental):
        Like stratixiv_arch.timing.xml, but with carry chains disabled.

    stratixiv_arch.timing.no_directlink.xml (experimental):
        Like stratixiv_arch.timing.xml, but with direct-links disabled.

    stratixiv_arch.timing.no_pack_patterns.xml (experimental):
        Like stratixiv_arch.timing.xml, but with DSP pack patterns disabled.
    
    
Modifying Architecture Files
--------------------------------------------------
Due the length of these architecture files, it may be useful to use a text 
editor which support "folding" of XML tags (e.g. vim).  This can greatly
simplify the editing process, allowing you to focus only on the sections of
interest.

Most parts of the Architecture can be modified by hand. However care must
be taken when changing some fields (e.g. a <pb_type>'s num_pb) to ensure
that the the interconnect between pb_types remains correct.

If you wish to modify the memory architecture (i.e. RAM blocks), it is 
probably best to use gen_stratixiv_memories_class.py, rather than attempt 
to do so by hand because of the large number of possible operating modes.

Adding Support for New Architectures
--------------------------------------------------
Support can be added for additional Quartus II supported FPGA architectures 
(Cyclone III, Stratix II etc), by defining models for the architecture's VQM
primitives.  Good places to look for this information include:
   * Altera's Quartus Univeristy Interface Program (QUIP) documentation
   * The 'fv_lib' directory under a Quartus installation

For more details see vqm_to_blif's README.txt



include/README.md
--------------------------------------
# include files

This folder contains _include_ files for running the modified version of ch_intrisinc.

_include_ files can be either a Verilog file or a Verilog header file. The main point worth mentioning is that the union of include files and the circuit design includes the top module should not result in any conflict like having multiple top modules or declaring different variables with the same name.

To create a task config file, the syntax for _include_ files is pretty much like the circuits or architectures. 
In the beginning, _includes_dir_, a path to _include_ files should be specified. In the following, specifiers include_add_list adds specific _include_ files, using a relative path that will be pre-pended by the _includes_dir_ path.
If config.txt file lists multiple benchmark circuits and multiple include files, all include files will be considered (unioned into) each circuit design. In other words, _include_ files are shared among given benchmark circuits.

___________________________hdl_include task config file____________________________
##############################################
\# Configuration file for running experiments
##############################################

\# Path to directory of circuits to use
circuits_dir=benchmarks/hdl_include

\# Path to directory of includes circuits to use
includes_dir=benchmarks/hdl_include/include

\# Path to directory of architectures to use
archs_dir=arch/no_timing/memory_sweep

\# Add circuits to list to sweep
circuit_list_add=ch_intrinsics_top.v

\# Add circuits to includes list to sweep
include_list_add=generic_definitions1.vh
include_list_add=generic_definitions2.vh
include_list_add=memory_controller.vh

\# Add architectures to list to sweep
arch_list_add=k4_N10_memSize16384_memData64.xml

\# Parse info and how to parse
parse_file=vpr_no_timing.txt

\# How to parse QoR info
qor_parse_file=qor_no_timing.txt

\# Script parameters
script_params_common=-track_memory_usage --timing_analysis off
___________________________________________________________________________________



ispd_blif/README.rst
--------------------------------------
The `ISPD16 <http://www.ispd.cc/contests/16>` and `ISPD17 <http://www.ispd.cc/contests/17>` benchmarks 
(converted to be compatible with VPR) are distributed seperately from VTR due to their large size.

To integrate them into VTR run:

    $ make get_ispd_benchmarks

from the root of the VTR source tree.

This will download and extract the benchmark netlists to:

    <vtr>/vtr_flow/benchmarks/ispd_blif/

The relevant architecture to use is:
    <vtr>/vtr_flow/arch/ispd/ultrascale_ispd.xml

where <vtr> is the root of the VTR source tree.

However note that the ISPD contest architectures have no timing model (so no timing-driven optimization) and focused on placement only (no routing architecture).



symbiflow/README.md
--------------------------------------
SymbiFlow benchmarks
====================

This directory holds all the SymbiFlow benchmarks that are generated in the [SymbiFlow-arch-defs](https://github.com/SymbiFlow/symbiflow-arch-defs) repository.

The circuits come along with the SDC constraints file, if present, and have been produced with yosys.
They are compatible with the symbiflow architectures produced in the same Symbiflow-arch-defs build.

Some of the circuites require also the place constraint files to correctly place some IOs and clock tiles
in the correct location, so not to incur in routability issues.

All the data files can be downloaded with the `make get_symbiflow_benchmarks` target in the root directory.



place_constr/README.md
--------------------------------------
Place constraints
=================

This directory contains the Place constraints files corresponding to the homonym circuit file.



sdc/README.md
--------------------------------------
SDC constraints
===============

This directory contains the SDC constraints files corresponding to the homonym circuit file.



f4pga/README.md
--------------------------------------
# F4PGA SystemVerilog Benchmarks

This folder contains the `button_controller`, `pulse_width_led` and `timer` benchmarks directly copied from the master branch of the F4PGA GitHub repository at commit [cad8afe](https://github.com/chipsalliance/f4pga/commits/cad8afe0842cd73f5b73949fa12eab1fda326055).
The benchmarks are directly copied to avoid dealing with a significant amount of code by adding the F4PGA repository as a subtree to the VTR repository.
The primary purpose of these benchmarks is to utilize them in VTR GitHub CI tests to continuously monitor the functionality of the Yosys SystemVerilog and UHDM plugins.

For more information please see the ['ChipsAlliance/F4PGA'](https://github.com/chipsalliance/f4pga) Github repository.


fx68k/README.md
--------------------------------------
# fx68k
FX68K 68000 cycle accurate SystemVerilog core

Copyright (c) 2018 by Jorge Cwik
fx68k@fxatari.com

FX68K is a 68000 cycle exact compatible core. At least in theory, it should be impossible to distinguish functionally from a real 68K processor.

On Cyclone families it uses just over 5,100 LEs and about 5KB internal ram, reaching a max effective clock frequency close to 40MHz. Some optimizations are still possible to implement and increase the performance.

The core is fully synchronous. Considerable effort was made to avoid any asynchronous logic.

Written in SystemVerilog.

The timing of the external bus signals is exactly as the original processor. The only feature that is not implemented yet is bus retry using the external HALT input signal.

It was designed to replace an actual chip on a real board. This wasn't yet tested however and not all necessary output enable control signals are fully implemented.



hdmi/README.md
--------------------------------------
# hdmi

[English](./README.md) | [Français](./README_fr.md) | [Help translate](https://github.com/hdl-util/hdmi/issues/11)

![hdmi](https://github.com/hdl-util/hdmi/workflows/hdmi/badge.svg)

SystemVerilog code for HDMI 1.4b video/audio output on an [FPGA](https://simple.wikipedia.org/wiki/Field-programmable_gate_array).

## Why?

Most free and open source HDMI source (computer/gaming console) implementations actually output a DVI signal, which HDMI sinks (TVs/monitors) are backwards compatible with. To support audio and other HDMI-only functionality, a true HDMI signal must be sent. The code in this repository lets you do that without having to license an HDMI IP block from anyone.

### Demo: VGA-compatible text mode, 720x480p on a Dell Ultrasharp 1080p Monitor

![GIF showing VGA-compatible text mode on a monitor](demo.gif)

## Usage

1. Take files from `src/` and add them to your own project. If you use [hdlmake](https://hdlmake.readthedocs.io/en/master/), you can add this repository itself as a remote module.
1. Other helpful modules for displaying text / generating sound are also available in this GitHub organization.
1. Consult the simple usage example in `top/top.sv`.
1. See [hdmi-demo](https://github.com/hdl-util/hdmi-demo) for code that runs the demo as seen the demo GIF.
1. Read through the parameters in `hdmi.sv` and tailor any instantiations to your situation.
1. Please create an issue if you run into a problem or have any questions. Make sure you have consulted the troubleshooting section first.

### Platform Support

- [x] Altera (tested on [MKR Vidor 4000](https://store.arduino.cc/usa/mkr-vidor-4000))
- [x] Xilinx (tested on [Spartan Edge Accelerator Board](https://www.seeedstudio.com/Spartan-Edge-Accelerator-Board-p-4261.html))
- [ ] Lattice (unknown)

### To-do List (upon request)
- [x] 24-bit color
- [x] Data island packets
	- [x] Null packet
	- [x] ECC with BCH systematic encoding GF(2^8)
	- [x] Audio clock regeneration
	- [x] L-PCM audio
		- [x] 2-channel
		- [ ] 3-channel to 8-channel
	- [ ] 1-bit audio
	- [x] Audio InfoFrame
	- [x] Auxiliary Video Information InfoFrame
	- [x] Source Product Descriptor InfoFrame
	- [ ] MPEG Source InfoFrame
		- NOTE—Problems with the MPEG Source Infoframe have been identified that were not able to be fixed in time for CEA-861-D. Implementation is strongly discouraged until a future revision fixes the problems
	- [ ] Gamut Metadata
- [x] Video formats 1, 2, 3, 4, 16, 17, 18, and 19
- [x] VGA-compatible text mode
	- [x] IBM 8x16 font
	- [ ] Alternate fonts
- [ ] Other color formats (YCbCr, deep color, etc.)
- [ ] Support other video id codes
	- [ ] Interlaced video
	- [ ] Pixel repetition


### Pixel Clock

You'll need to set up a PLL for producing the two HDMI clocks. The pixel clock for each supported format is shown below:

|Video Resolution|Video ID Code(s)|Refresh Rate|Pixel Clock Frequency|[Progressive](https://en.wikipedia.org/wiki/Progressive_scan)/[Interlaced](https://en.wikipedia.org/wiki/Interlaced_video)|
|---|---|---|---|---|
|640x480|1|60Hz|25.2MHz|P|
|640x480|1|59.94Hz|25.175MHz|P|
|720x480|2, 3|60Hz|27.027MHz|P|
|720x480|2, 3|59.94Hz|27MHz|P|
|720x576|17, 18|50Hz|27MHz|P|
|1280x720|4|60Hz|74.25MHz|P|
|1280x720|4|59.94Hz|74.176MHz|P|
|1280x720|19|50Hz|74.25MHz|P|
|1920x1080|16|60Hz|148.5MHz|P|
|1920x1080|16|59.94Hz|148.352MHz|P|
|1920x1080|34|30Hz|74.25MHz|P|
|1920x1080|34|29.97Hz|74.176MHz|P|
|3840x2160 (not ready)|97, 107|60Hz|594MHz|P|
|3840x2160|95, 105|30Hz|297MHz|P|

The second clock is a clock 5 times as fast as the pixel clock. Even if your FPGA only has a single PLL, the Altera MegaWizard (or the Xilinx equivalent) should still be able to produce both. See [hdl-util/hdmi-demo](https://github.com/hdl-util/hdmi-demo/) for example PLLs.

### L-PCM Audio Bitrate / Sampling Frequency

Both audio bitrate and frequency are specified as parameters of the HDMI module. Bitrate can be any value from 16 through 24. Below is a simple mapping of sample frequency to the appropriate parameter

**WARNING: the audio can be REALLY LOUD if you use the full dynamic range with hand-generated waveforms! Using less dynamic range means you won't be deafened! (i.e. audio_sample >> 8 )**

|Sampling Frequency|AUDIO_RATE value|
|---|---|
|32 kHz|32000|
|44.1 kHz|44100|
|88.2 kHz|88200|
|176.4 kHz|176400|
|48 kHz|48000|
|96 kHz|96000|
|192 kHz|192000|


### Source Device Information Code

This code is sent in the Source Product Description InfoFrame via `SOURCE_DEVICE_INFORMATION` to give HDMI sinks an idea of what capabilities an HDMI source might have. It may be used for displaying a relevant icon in an input list (i.e. DVD logo for a DVD player).

|Code|Source Device Information|
|---|---|
|0x00|Unknown|
|0x01|Digital Set-top Box|
|0x02|DVD Player|
|0x03|Digital VHS|
|0x04|HDD Videorecorder|
|0x05|Digital Video Camera|
|0x06|Digital Still Camera|
|0x07|Video CD|
|0x08|Game|
|0x09|PC General|
|0x0a|Blu-Ray Disc|
|0x0b|Super Audio CD|
|0x0c|HD DVD|
|0x0d|Portable Media Player|

### Things to be aware of / Troubleshooting

* Limited resolution: some FPGAs don't support I/O at speeds high enough to achieve 720p/1080p
	* Workaround: Altera FPGA users can try to specify speed grade C6 and see if it works, though yours may be C7 or C8. Beware that this might introduce some system instability.
* FPGA does not support TMDS: many FPGAs without a dedicated HDMI output don't support TMDS
    * You should be able to directly use LVDS (3.3v) instead, tested up to 720x480
    * This might not work if your video has a high number of transitions or you plan to use higher resolutions
    * Solution: AC-couple the 3.3v LVDS wires to by adding 100nF capacitors in series, as close to the transmitter as possible
        * Why? TMDS is current mode logic, and driving a CML receiver with LVDS is detailed in [Figure 9 of Interfacing LVDS with other differential-I/O types](https://web.archive.org/web/20151123084833/https://m.eet.com/media/1135468/330072.pdf)
            * Resistors are not needed since Vcc = 3.3v for both the transmitter and receiver
        * Example: See `J13`, on the [Arduino MKR Vivado 4000 schematic](https://content.arduino.cc/assets/vidor_c10_sch.zip), where LVDS IO Standard pins on a Cyclone 10 FPGA have 100nF series capacitors
* Poor wiring: if you're using a breakout board or long lengths of untwisted wire, there might be a few pixels that jitter due to interference
    * Make sure you have all the necessary pins connected (GND pins, etc.)
    * Try switching your HDMI cable; some cheap cables like [these I got from Amazon](https://www.amazon.com/gp/product/B01JO9PB7E/) have poor shielding
* Hot-Plug unaware: all modules are unaware of hotplug
    * This shouldn't affect anything in the long term; the only stateful value is `hdmi.tmds_channel[2:0].acc`
    * You should decide hotplug behavior (i.e. pause/resume on disconnect/connect, or ignore it)
* EDID not implemented: it is assumed you know what format you want at synthesis time, so there is no dynamic decision on video format
    * To be implemented in a display protocol independent manner
* SCL/SCA voltage level: though unused by this implementation...it is I2C on a 5V logic level, as confirmed in the [TPD12S016 datasheet](https://www.ti.com/lit/ds/symlink/tpd12s016.pdf), which is unsupported by most FPGAs
    * Solution: use a bidirectional logic level shifter compatible with I2C to convert 3.3v LVTTL to 5v
    * Solution: use 3.3-V LVTTL I/O standard with 6.65k pull-up resistors to 3.3v (as done in `J13` on the [Arduino MKR Vivado 4000 schematic](https://content.arduino.cc/assets/vidor_c10_sch.zip))
	* Emailed Arduino support: safe to use as long as the HDMI slave does not have pull-ups

## Licensing

Dual-licensed under Apache License 2.0 and MIT License.

### HDMI Adoption

I am NOT a lawyer, the below advice is given based on discussion from [a Hacker News post](https://news.ycombinator.com/item?id=22279308) and my research.

HDMI itself is not a royalty free technology, unfortunately. You are free to use it for testing, development, etc. but to receive the HDMI LA's (licensing administration) blessing to create and sell end-user products:


> The manufacturer of the finished end-user product MUST be a licensed HDMI Adopter, and
> The finished end-user product MUST satisfy all requirements as defined in the Adopter Agreement including but not limited to passing compliance testing either at an HDMI ATC or through self-testing.


Becoming an adopter means you have to pay a flat annual fee (~ $1k-$2k) and a per device royalty (~ $0.05). If you are selling an end-user device and DO NOT want to become an adopter, you can turn on the `DVI_OUTPUT` parameter, which will disable any HDMI-only logic, like audio.

Please consult your lawyer if you have any concerns. Here are a few noteworthy cases that may help you make a decision:

* Arduino LLC is not an adopter, yet sells the [Arduino MKR Vidor 4000](https://store.arduino.cc/usa/mkr-vidor-4000) FPGA 
    * It has a micro-HDMI connector
    * [Having an HDMI connector does not require a license](https://electronics.stackexchange.com/questions/28202/legality-of-using-hdmi-connectors-in-non-hdmi-product)
    * Official examples provided by Arduino on GitHub only perform DVI output
    * It is a user's choice to program the FPGA for HDMI output
    * Therefore: the device isn't an end-user product under the purview of HDMI LA
* Unlicensed DisplayPort to HDMI cables (2011)
    * [Articles suggests that the HDMI LA can recall illegal products](https://www.pcmag.com/archive/displayport-to-hdmi-cables-illegal-could-be-recalled-266671?amp=1).
    * But these cables [are still sold on Amazon](https://www.amazon.com/s?k=hdmi+to+displayport+cable)
    * Therefore: the power of HDMI LA to enforce licensing is unclear
* [Terminated Adopters](https://hdmi.org/adopter/terminated)
    * There are currently 1,043 terminated adopters
    * Includes noteworthy companies like Xilinx, Lattice Semiconductor, Cypress Semiconductor, EVGA (!), etc.
    * No conclusion
* Raspberry Pi Trading Ltd is licensed
    * They include the HDMI logo for products
    * Therefore: Raspberry Pi products are legal, licensed end-user products

## Alternative Implementations

- [HDMI Intel FPGA IP Core](https://www.intel.com/content/www/us/en/programmable/products/intellectual-property/ip/interface-protocols/m-alt-hdmi-megacore.html): Stratix/Arria/Cyclone
- [Xilinx HDMI solutions](https://www.xilinx.com/products/intellectual-property/hdmi.html#overview): Virtex/Kintex/Zynq/Artix
- [Artix 7 HDMI Processing](https://github.com/hamsternz/Artix-7-HDMI-processing): VHDL, decode & encode
- [SimpleVOut](https://github.com/cliffordwolf/SimpleVOut): many formats, no auxiliary data

If you know of another good alternative, open an issue and it will be added.

## Reference Documents

*These documents are not hosted here! They are available on Library Genesis and at other locations.*

* [HDMI Specification v1.4b](https://b-ok.cc/book/5499564/fe35f4)
* [HDMI Specification v2.0](https://b-ok.cc/book/5464885/1f0b4c)
* [EIA-CEA861-D.pdf](https://libgen.is/book/index.php?md5=CEE424CA0F098096B6B4EC32C32F80AA)
* [CTA-861-G.pdf](https://b-ok.cc/book/5463292/52859e)
* [DVI Specification v1.0](https://www.cs.unc.edu/~stc/FAQs/Video/dvi_spec-V1_0.pdf)
* [IEC 60958-1](https://ia803003.us.archive.org/30/items/gov.in.is.iec.60958.1.2004/is.iec.60958.1.2004.pdf)
* [IEC 60958-3](https://ia800905.us.archive.org/22/items/gov.in.is.iec.60958.3.2003/is.iec.60958.3.2003.pdf)
* [E-DDC v1.2](https://glenwing.github.io/docs/)

## Special Thanks

* Mike Field's (@hamsternz) demos of DVI and HDMI output for helping me better understand HDMI
	* http://www.hamsterworks.co.nz/mediawiki/index.php/Dvid_test
	* http://www.hamsterworks.co.nz/mediawiki/index.php/Minimal_DVI-D
* Jean P. Nicolle (fpga4fun.com) for sparking my interest in HDMI
	* https://www.fpga4fun.com/HDMI.html
* Bureau of Indian Standards for free equivalents of non-free IEC standards 60958-1, 60958-3, etc.
* @glenwing for [links to many VESA standard documents](https://glenwing.github.io/docs/)



hdmi/README_fr.md
--------------------------------------
# hdmi

[English](./README.md) | [Français](./README_fr.md) | [Nous aider avec la traduction](https://github.com/hdl-util/hdmi/issues/11)

![hdmi](https://github.com/hdl-util/hdmi/workflows/hdmi/badge.svg)

SystemVerilog code pour transmettre vidéo/audio HDMI 1.4a sur un [FPGA](https://fr.wikipedia.org/wiki/Circuit_logique_programmable#FPGA).

## Pourquoi?

La plupart des implementations open source d'un source HDMI transmettre en réalité un signal DVI, avec qui les sinks HDMI (i.e. TVs/moniteurs) sont rétrocompatible. Pour supporter audio et l'autre HDMI seulement fonctionnalité, on doit transmettre un signal HDMI vrai. Le code dans ce dépôt permettez-vous de faire ça sans licencer un bloc HDMI IP de n'importe qui.

### Démo: Mode texte VGA-compatible, 720x480p sur un Moniteur Dell Ultrasharp 1080p

![GIF qui montre mode texte VGA-compatible sur un moniteur](demo.gif)

## Usage

1. Take files from `src/` and add them to your own project. If you use [hdlmake](https://hdlmake.readthedocs.io/en/master/), you can add this repository itself as a remote module.
1. Other helpful modules for displaying text / generating sound are also available in this GitHub organization.
1. Consult the simple usage example in `top/top.sv`.
1. See [hdmi-demo](https://github.com/hdl-util/hdmi-demo) for code that runs the demo as seen the demo GIF.
1. Read through the parameters in `hdmi.sv` and tailor any instantiations to your situation.
1. Please create an issue if you run into a problem or have any questions. Make sure you have consulted the troubleshooting section first.


S'il vous plaît regarder le readme anglais pour l'information à jour.



koios_sv/README.md
--------------------------------------
# SV_benchmarks
Koios Benchmarks for FPGA Research with SystemVerilog features



titan_blif/README.rst
--------------------------------------
The `Titan <http://www.eecg.utoronto.ca/~vaughn/titan/>` benchmarks are distributed separately from VTR due to their large size.

The Titan repo is located at /home/vaughn/titan/titan.git on the U of T EECG network. Members of Vaughn Betz's research lab have read/write privileges.

This repo is where the Titan flow is developed and where any changes to it should be made.

In addition to the titan benchmarks, this repo contains scripts that are used in generation of the architecture description for Stratix IV.

More specifically, they contain scripts that generate memory blocks & complex switch blocks. 


To integrate them into VTR run:

    $ make get_titan_benchmarks

from the root of the VTR source tree.

This will download and extract the benchmark netlists to:

    <vtr>/vtr_flow/benchmarks/titan_blif/


In the directory 'titan_blif', you will find three primary subdirectories that 
correspond to three sets of main benchmarks: 'titan23', 'titan_new', and 'other_benchmarks'. 
Each of these subdirectories contains two further directories, namely 'stratixiv' and 'stratix10'. 
These nested directories house the BLIF files specific to each device. 
The 'titan_other_blif' contains smaller titan-like benchmarks which are useful for 
testing (but should not be used for architecture and CAD evaluation).



koios/README.md
--------------------------------------
# Koios 2.0 Benchmarks

## Introduction
Koios benchmarks are a set of Deep Learning (DL) benchmarks for FPGA architecture and CAD research. They are suitable for DL related architecture and CAD research. There are 40 designs that include several medium-sized benchmarks and some large benchmarks. The designs target different network types (CNNs, RNNs, MLPs, RL) and layer types (fully-connected, convolution, activation, softmax, reduction, eltwise). Some of the designs are generated from HLS tools as well. These designs use many precisions including binary, different fixed point types int8/16/32, brain floating point (bfloat16), and IEEE half-precision floating point (fp16).

## Documentation
A brief documentation of Koios benchmarks is available [here](https://docs.verilogtorouting.org/en/latest/vtr/benchmarks/#koios-benchmarks).

## How to Use
Koios benchmarks are fully compatible with the full VTR flow. They can be used using the standard VTR flow described [here](https://docs.verilogtorouting.org/en/latest/vtr/running_vtr/). 

Koios benchmarks use advanced DSP features that are available in only a few FPGA architectures provided with VTR. These benchmarks instantiate hard DSP macros to implement native FP16 or BF16 multiplications or use the hard dedicated chains, and these are architecture-specific. However, these advanced/complex DSP features can be enabled or disabled. The macro ``complex_dsp`` can be used for this purpose. If `complex_dsp` is defined in a benchmark file (using `` `define complex_dsp``), then advanced DSP features mentioned above will be used. If `complex_dsp` is not defined, then equivalent functionality is obtained through behavioral Verilog that gets mapped to the FPGA soft logic.

Similarly, Koios benchmark instantiate hard memory blocks (single port and dual port BRAM). These blocks are available in most architectures provided with VTR, but may not be available in a user's architecture. These hard memory blocks can be controlled by using the macro ``hard_mem``. If `hard_mem` is defined in a benchmark file (using `` `define hard_mem``), then hard memory blocks will be used. If `hard_mem` is not defined, then equivalent functionality is obtained through behavioral Verilog that the synthesis tools can automatically infer as RAMs.

From a flow perspective, you can enable/disable a macro (like `complex_dsp`) without actually modifying the benchmark file(s). You can specify a separate Verilog header file while running a flow/task that contains these macros. For `run_vtr_flow` users, `-include <filename>` needs to be added. For `run_vtr_task` users, `includes_dir` and `include_list_add` need to be specified in the task file. An example task file can be seen [here](https://github.com/verilog-to-routing/vtr-verilog-to-routing/blob/master/vtr_flow/vtr_reg_basic/hdl_include/config/config.txt).

Using hard macros for DSPs and BRAMs to utilize advanced features (like chaining) is common in modern designs used with contemporary FPGAs. When using these benchmarks and enabling these advanced features, an FPGA architecture that supports these features must be provided. Supporting these features implies that the architecture XML file provided to VTR must describe such features (e.g. by defining a hard block macro DSP slice). We provide such architectures with Koios. The FPGA architectures with advanced DSP that work out-of-the-box with Koios benchmarks are available here: 

    $VTR_ROOT/vtr_flow/arch/COFFE_22nm/k6FracN10LB_mem20K_complexDSP_customSB_22nm.*


When disabling these hard blocks (e.g. by not defining `complex_dsp` or `hard_mem` as mentioned above), users can run these benchmarks with FPGA architectures that don't have these hard blocks in them. That is, an architecture XML file without the required hard macro definitions can be used. For example, the flagship architectures available here: ::

    $VTR_ROOT/vtr_flow/arch/timing/k6_frac_N10_*_mem32K_40nm*

If users want to use a different FPGA architecture file, they can replace the macro instantiations in the benchmarks with their equivalents from the FPGA architectures they wish to use.

## Proxy benchmarks
In Koios 2.0, there are 8 synthetic/proxy benchmarks. These were generated using a framework that is present [here](https://github.com/UT-LCA/koios_proxy_benchmarks). To generate more benchmarks using this framework, use the generate_benchmark.py script.

## SystemVerilog benchmarks
In Koios, there are 3 system-verilog benchmarks. These are based on ARM FixyNN DeepFreeze and accelerate some layers of MobileNet.

## Regressions
Koios benchmarks are tested by the following tasks in VTR:
| Suite         |Test Description      | Target | Complex DSP Features   | Config file   | Frontend   | Parser   |
|---------------|----------------------|---------------|---------------|---------------|---------------|---------------|
| Nightly       | Medium designs     | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_nightly_test4/koios_medium | Parmys | |
| Nightly       | Medium designs     | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml |          | vtr_reg_nightly_test4/koios_medium_no_hb | Parmys | |
| Nightly       | Medium designs     | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_nightly_test4_odin/koios_medium | Odin | |
| Nightly       | Medium designs     | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml |          | vtr_reg_nightly_test4_odin/koios_medium_no_hb | Odin | |
| Weekly        | Large designs      | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_weekly/koios_large | Parmys | |
| Weekly        | Large designs      | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml |          | vtr_reg_weekly/koios_large_no_hb | Parmys | |
| Weekly        | Large designs      | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_weekly/koios_large_odin | Odin | |
| Weekly        | Large designs      | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml |          | vtr_reg_weekly/koios_large_no_hb_odin | Odin | |
| Weekly        | Proxy designs      | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_weekly/koios_proxy | Parmys | |
| Weekly        | Proxy designs      | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml |          | vtr_reg_weekly/koios_proxy_no_hb | Parmys | |
| Weekly        | deepfreeze designs | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_weekly/koios_sv | Parmys | System-Verilog |
| Weekly        | deepfreeze designs | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml |          | vtr_reg_weekly/koios_sv_no_hb | Parmys | System-Verilog |

The following are only for regression testing and less important for benchmarking:
| Suite         |Test Description      | Target | Complex DSP Features   | Config file   | Frontend   |
|---------------|----------------------|---------------|---------------|---------------|---------------|
| Strong        | A test circuit to check the architecture file | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_strong/koios_test | Parmys |
| Strong        | Same test circuit without hard blocks         | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml |          | vtr_reg_strong/koios_test_no_hb | Parmys |
| Strong        | A test circuit to check the architecture file | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_strong_odin/koios_test | Odin |
| Strong        | Same test circuit without hard blocks         | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml |          | vtr_reg_strong_odin/koios_test_no_hb | Odin |
| Nightly       | The `conv_layer.v` design                     | multiple                                        | &#10003; | vtr_reg_nightly_test4/koios_medium_multi_arch | Parmys |
| Nightly       | The `conv_layer.v` design                     | multiple                                        | &#10003; | vtr_reg_nightly_test4_odin/koios_medium_multi_arch | Odin |
| Nightly       | Other designs                                 | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_nightly_test6/koios_other | Parmys |
| Nightly       | Other designs                                 | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml |          | vtr_reg_nightly_test6/koios_other_no_hb | Parmys |
| Nightly       | The `bwave_like.fixed.small.v` design         | multiple                                        | &#10003; | vtr_reg_nightly_test6/koios_other_multi_arch | Parmys |
| Weekly        | The `dla_like.large.v` design                 | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_weekly/koios_dla_large | Parmys |
| Weekly        | The `bwave_like.float.large.v` design         | k6FracN10LB_mem20K_complexDSP_customSB_22nm.xml | &#10003; | vtr_reg_weekly/koios_bwave_float_large | Parmys | |


## Collecting QoR measurements
For collecting QoR measurements on Koios benchmarks, follow the instructions [here](https://docs.verilogtorouting.org/en/latest/README.developers/#example-koios-benchmarks-qor-measurement).

## VTR issues observed

### Fixed
Some of the main issues (and/or pull requests) that were seen and fixed during Koios 2.0 development:

#### Multiple PBs using the same Model
https://github.com/verilog-to-routing/vtr-verilog-to-routing/issues/2103

#### Random net names
https://github.com/verilog-to-routing/vtr-verilog-to-routing/pull/2086

#### Incorrect clock and reset signals
https://github.com/verilog-to-routing/vtr-verilog-to-routing/pull/2118

#### SystemVerilog support
https://github.com/verilog-to-routing/vtr-verilog-to-routing/pull/2015
https://github.com/verilog-to-routing/vtr-verilog-to-routing/pull/2068

#### Auto-name pass after optimization passes
https://github.com/verilog-to-routing/vtr-verilog-to-routing/issues/2031
https://github.com/verilog-to-routing/vtr-verilog-to-routing/pull/2035

#### Padding the multiplication output port if it's size is less than the sum of both input ports
https://github.com/verilog-to-routing/vtr-verilog-to-routing/pull/2013

#### Support for synthesis/mapping of reduce and shift operators
https://github.com/verilog-to-routing/vtr-verilog-to-routing/pull/1923

#### Error during placement with hard blocks with chains
https://github.com/verilog-to-routing/vtr-verilog-to-routing/issues/2036

### Open
Some issues that are still pending to be fixed are:

#### Placement failure with designs with hard block chains:
https://github.com/verilog-to-routing/vtr-verilog-to-routing/issues/2149


## How to Cite
The following paper may be used as a citation for Koios:

A. Arora, A. Boutros, D. Rauch, A. Rajen, A. Borda, S. A. Damghani, S. Mehta, S. Kate, P. Patel, K. B. Kent, V. Betz, L. K. John "Koios: A Deep Learning Benchmark Suite for FPGA Architecture and CAD Research", FPL, 2021.

Bibtex:
```
@inproceedings{koios_benchmarks,
  title={Koios: A Deep Learning Benchmark Suite for FPGA Architecture and CAD Research},
  author={Arora, Aman and Boutros, Andrew and Rauch, Daniel and Rajen, Aishwarya and Borda, Aatman and Damghani, Seyed A. and Mehta, Samidh and Kate, Sangram and Patel, Pragnesh and Kent, Kenneth B. and Betz, Vaughn and John, Lizy K.},
  booktitle={International Conference on Field Programmable Logic and Applications (FPL)},
  year={2021}
}
```

## License
Koios benchmarks are distributed under the [same license as VTR](https://github.com/verilog-to-routing/vtr-verilog-to-routing/blob/master/LICENSE.md). 

## Contact
If you have questions, email [Aman Arora](mailto:aman.kbm@utexas.edu). 

If you find any bugs, please file an issue [here](https://github.com/verilog-to-routing/vtr-verilog-to-routing/issues) and tag @aman26kbm in the description.

## Contributing to Koios
We welcome contributions to this benchmark suite. If you'd like to contribute to Koios, contact [Aman Arora](mailto:aman.kbm@utexas.edu). 

## Contributors
*Please keep this up-to-date*

**Professors:** Lizy K. John, Vaughn Betz, Kenneth B. Kent

**Doctoral Students:** Aman Arora (University of Texas at Austin), Andrew Boutros (University of Toronto), Mohamed Elgammal (University of Toronto)

**Graduate Students:** Seyed Alireza Damghani (University of New Brunswick), Daniel Rauch (University of Texas at Austin), Aishwarya Rajen (University of Texas at Austin), , Sangram Kate (University of Texas at Austin)

**Undergraduate Students:** Karan Mathur (University of Texas at Austin), Vedant Mohanty (University of Texas at Austin), Tanmay Anand (University of Texas at Austin), Aatman Borda (University of Texas at Austin), Samidh Mehta (University of Texas at Austin), Pragnesh Patel (University of Texas at Austin), Helen Dai (University of Toronto), Zack Zheng (University of Toronto)




koios_proxy/README.md
--------------------------------------
# proxy_benchmarks
Generate Synthetic Benchmarks for FPGA Research

More information: [koios_proxy_benchmarks](https://github.com/UT-LCA/koios_proxy_benchmarks.git)


slurm/README.md
--------------------------------------
submission_template.sh is a template used to submit jobs using SLURM on a large cluster
- This script is tested on computecanada cluster (Niagara) system

To use this script:
- First, generate all the shell scripts to run a task using:
    * run_vtr_task.py -system scripts (Sec. 3.9.3 in the documentation)
    * this will create a new run directory under the specified task (e.g. run001)

- Second, edit the submission_template.sh script to match your run
    * All lines starting with #SBATCH go to SLURM
    * Edit all these line depending on the number of cores you want, number of parallel jobs you want,
      the job name, your email and the job time limit
    * Edit the relative path to your task (path_to_task) and the specific run directory where the 
      generated shell scripts are (dir)

- Third, submit these jobs to SLURM using the following command
    * sbatch submission_template.sh

You can check the state of the submitted job (or all the jobs that you submitted) using
squeue --me




tuning_runs/README.md
--------------------------------------
A script used to run tuning experiments with multiple parameters.

Steps to use:
=============
    1) edit the first section of the script by setting `PARAMS_DICT` dictionary to the parameters that you want to sweep and the corresponding values that you want to try. If you want the resulting spreadheet to include specific metrics, set `KEEP_METRICS_ONLY` variable to `True` and the metrics that you care about in `parsed_metrics`. If you want the full parsed result sheet, set `KEEP_METRICS_ONLY` to `False`

    2) run the script as follows:
'''
python control_runs.py --generate <path_to_task_to_run>
'''

This will edit the `config.txt` file of this task adding several lines `script_params_list_add` for each of the combinations of the input params

    3) Launch the task using `run_vtr_task.py` script
    4) When the run is done, run the script to parse the results as follows:
'''
python control_runs.py --parse <path_to_task_to_parse>
'''

The script will generate 3 csv files in the runXXX idrectory of the task as follows:
    - `full_res.csv` that exactly matches parse_results.txt but in csv format
    - `avg_seed.csv` that averages the results of the each circuit with one set of parameters over the different seed values
    - `geomean_res.csv` that geometrically average the results of all the circuits over the same set of parameters
    - `summary.xlsx` that merges all the previously mentioned sheets in a single spreadsheet



regression_tests/README.md
--------------------------------------
# Verilog of Routing Regression Test Infrastructure

There are currently four levels of regression testing as described below. Refer
to [`regression_tests.ods`](./regression_tests.ods) for details on architecture
+ benchmarks run by each test.

## LEVEL ONE - Basic VTR Regression - `vtr_reg_basic`

 * MANDATORY - Must be run by ALL developers before committing.
 * Estimated Runtime: ~2-5 minutes

DO-IT-ALL COMMAND - This command will execute, parse, and check results.
```
./run_reg_test.py vtr_reg_basic
```
To create golden results, use:
```
./run_reg_test.py -create_golden vtr_reg_basic
```

Execute with:
```
<scripts_path>/run_vtr_task.py -l <tasks_path>/regression_tests/vtr_reg_basic/task_list.txt
```

Parse results with:
```
<scripts_path>/parse_vtr_task.py -l <tasks_path>/regression_tests/vtr_reg_basic/task_list.txt
```

Check results with:
```
<scripts_path>/parse_vtr_task.py -check_golden -l <tasks_path>/regression_tests/vtr_reg_basic/task_list.txt
```

Create golden results with:
```
<scripts_path>/parse_vtr_task.py -create_golden -l <tasks_path>/regression_tests/vtr_reg_basic/task_list.txt
```

## LEVEL TWO - Strong VTR Regression - `vtr_reg_strong`

 * OPTIONAL - Can be run by developers before committing.
 * Estimated Runtime: ~5-10 minutes

DO-IT-ALL COMMAND - This command will execute, parse, and check results.
```
./run_reg_test.py vtr_reg_strong
./run_reg_test.py vtr_reg_valgrind_small
```
To create golden results, use:
```
./run_reg_test.py -create_golden vtr_reg_strong
```

Execute with:
```
<scripts_path>/run_vtr_task.py -l <tasks_path>/regression_tests/vtr_reg_strong/task_list.txt
```

Parse results with:
```
<scripts_path>/parse_vtr_task.py -l <tasks_path>/regression_tests/vtr_reg_strong/task_list.txt
```

Check results with:
```
<scripts_path>/parse_vtr_task.py -check_golden -l <tasks_path>/regression_tests/vtr_reg_strong/task_list.txt
```

Create golden results with:
```
<scripts_path>/parse_vtr_task.py -create_golden -l <tasks_path>/regression_tests/vtr_reg_strong/task_list.txt
```

## LEVEL THREE  - Nightly VTR Regression - `vtr_reg_nightly_test#, #:1-3` 

 * To be run by automated build system every night and on every pull request.
 * To keep the wall-clock time of this suite under ~6 hours using -j8, it is divided into multiple sub-suites, and each of them are submitted as different jobs to different kokoro machines. 
 * Estimated runtime: 30-35 hours
 
DO-IT-ALL COMMAND - This command will execute, parse, and check results.
```
./run_reg_test.py vtr_reg_nightly_test1
./run_reg_test.py vtr_reg_nightly_test2
./run_reg_test.py vtr_reg_nightly_test3
./run_reg_test.py vtr_reg_valgrind
```
**The below commands concern a single sub-suite (# is the sub-suite number). They have to be repeated for all sub-suites to cover all tests under Nightly VTR Regression**

To create golden results, use:
```
./run_reg_test.py -create_golden vtr_reg_nightly_test#
```

Execute  a sub-suite with:
```
<scripts_path>/run_vtr_task.py -l <tasks_path>/regression_tests/vtr_reg_nightly_test#/task_list.txt
```

Parse results with:
```
<scripts_path>/parse_vtr_task.py -l <tasks_path>/regression_tests/vtr_reg_nightly_test#/task_list.txt
```

Check results with:
```
<scripts_path>/parse_vtr_task.py -check_golden -l <tasks_path>/regression_tests/vtr_reg_nightly_test#/task_list.txt
```

Create golden results with:
```
<scripts_path>/parse_vtr_task.py -create_golden -l <tasks_path>/regression_tests/vtr_reg_nightly_test#/task_list.txt
```


## LEVEL FOUR - Weekly VTR Regression - `vtr_reg_weekly`

 * To be run by automated build system every weekend.
 * Estimated Runtime: 40+ hours

DO-IT-ALL COMMAND - This command will execute, parse, and check results.
```
./run_reg_test.py vtr_reg_weekly
```

To create golden results, use:
```
./run_reg_test.py -create_golden vtr_reg_weekly
```

Execute with:
```
<scripts_path>/run_vtr_task.py -l <tasks_path>/regression_tests/vtr_reg_weekly/task_list.txt
```

Parse results with:
```
<scripts_path>/parse_vtr_task.py -l <tasks_path>/regression_tests/vtr_reg_weekly/task_list.txt
```

Check results with:
```
<scripts_path>/parse_vtr_task.py -check_golden -l <tasks_path>/regression_tests/vtr_reg_weekly/task_list.txt
```

Create golden results with:
```
<scripts_path>/parse_vtr_task.py -create_golden -l <tasks_path>/regression_tests/vtr_reg_weekly/task_list.txt
```
## Parallesim Startegy for vtr_reg_nightly:
### Current Sub-suites:

  * The nightly regression suite is broken up into multiple sub-suites to minimize the wall-clock when ran by CI using Kokoro machines.
  * The lower bound for the run-time of the nightly regression tests is the longest vtr_flow run in all suites (currently this flow is in vtr_reg_nightly_test2/vtr_reg_qor)
  * To minimize wall-clock time, tasks which have the three longest flow runs are put in seperate directories and other tasks are added to keep the
    run-time for the sub-suite under ~5 hours using -j8 option on the Kokoro machines.
  * The longest tasks are put at the bottom of task_list.txt to get started first (the files are read in backwards in `run_reg_test.py`
  * If tasks that do not have long flow runs are to be added, it is best that they are added under vtr_reg_nightly_test1 as this suite has the smallest run-time
    of all suites (~2 hours using -j8).

### Adding Sub-suites:

  * If tasks with long flows that exceed ~3 hours are to be added, it is best to seperate them from the other suites and put it in a seperate test
    at the bottom of the task list.
  * Adding additional suites to vtr_reg_nightly comprises three steps:
    - a config file (.cfg) has to be added to the config list for Kokoro machines located at `$VTR_ROOT/.github/kokoro/presubmit`. The new config should be indentical to the other config files for nightly tests (e.g. `VTR_ROOT/.github/kokoro/presubmit/nightly_test1.cfg`) , with the only difference being the value for VTR_TEST (i.e. the value should be changed to the directory name for the new suite say vtr_reg_nightly_testX).  
    - `$VTR_ROOT/.github/kokoro/steps/vtr-test.sh` need to be updated to recongize the new suite and zip up the output files (we don't want the machine to run of disk space ...). e.g. if the suite to be added is `vtr_reg_nightly_testX`, the following line should be added to the script in its appropriate place:
    ```
    find vtr_flow/tasks/regression_tests/vtr_reg_nightly_testX/ -type f -print0 | xargs -0 -P $(nproc) gzip
    ```

    - The previous addition of .cfg file sets up the configs from our side of the repo. The new configs need to be submitted on Google's side as well for the Kokoro machines to run the new CI tests. The best person to contact to do this setup is Tim Ansell (@mithro on Github). 



vtr_reg_multiclock/README.md
--------------------------------------
##ODIN-II Regression 

`task_list.txt` in this directory points to a set of tasks in the same directory. These tasks are referred to in  
[`light_suite`](https://github.com/verilog-to-routing/vtr-verilog-to-routing/blob/master/odin_ii/regression_test/benchmark/suite/light_suite/task_list.conf) which is part of a series of ODIN-II regression tests. `light_suite` is currently being run by CI under the name`odin_reg_basic`. First, Odin specific tests are run then the task_list is run by `run_vtr_task.py`. Thus, the entirety of the flow is tested for `vtr_reg_multiclock` tasks by CI. 



func_multiclock/README.md
--------------------------------------
The tasks inside this directory tests that VTR can correctly handle multi-clock designs in different ways. The default way to handle multiple clocks is iterative. 


vtr_reg_multiclock_odin/README.md
--------------------------------------
##ODIN-II Regression 

`task_list.txt` in this directory points to a set of tasks in the same directory. These tasks are referred to in  
[`light_suite`](https://github.com/verilog-to-routing/vtr-verilog-to-routing/blob/master/odin_ii/regression_test/benchmark/suite/light_suite/task_list.conf) which is part of a series of ODIN-II regression tests. `light_suite` is currently being run by CI under the name`odin_reg_basic`. First, Odin specific tests are run then the task_list is run by `run_vtr_task.py`. Thus, the entirety of the flow is tested for `vtr_reg_multiclock` tasks by CI. 



func_multiclock/README.md
--------------------------------------
The tasks inside this directory tests that VTR can correctly handle multi-clock designs in different ways. The default way to handle multiple clocks is iterative. 


yosys/README.md
--------------------------------------
```
yosys -- Yosys Open SYnthesis Suite

Copyright (C) 2012 - 2024  Claire Xenia Wolf <claire@yosyshq.com>

Permission to use, copy, modify, and/or distribute this software for any
purpose with or without fee is hereby granted, provided that the above
copyright notice and this permission notice appear in all copies.

THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
```


yosys – Yosys Open SYnthesis Suite
===================================

This is a framework for RTL synthesis tools. It currently has
extensive Verilog-2005 support and provides a basic set of
synthesis algorithms for various application domains.

Yosys can be adapted to perform any synthesis job by combining
the existing passes (algorithms) using synthesis scripts and
adding additional passes as needed by extending the yosys C++
code base.

Yosys is free software licensed under the ISC license (a GPL
compatible license that is similar in terms to the MIT license
or the 2-clause BSD license).


Web Site and Other Resources
============================

More information and documentation can be found on the Yosys web site:
- https://yosyshq.net/yosys/

The "Documentation" page on the web site contains links to more resources,
including a manual that even describes some of the Yosys internals:
- https://yosyshq.net/yosys/documentation.html

The directory `guidelines` contains additional information
for people interested in using the Yosys C++ APIs.

Users interested in formal verification might want to use the formal verification
front-end for Yosys, SymbiYosys:
- https://symbiyosys.readthedocs.io/en/latest/
- https://github.com/YosysHQ/SymbiYosys


Installation
============

Yosys is part of the [Tabby CAD Suite](https://www.yosyshq.com/tabby-cad-datasheet) and the [OSS CAD Suite](https://github.com/YosysHQ/oss-cad-suite-build)! The easiest way to use yosys is to install the binary software suite, which contains all required dependencies and related tools.

* [Contact YosysHQ](https://www.yosyshq.com/contact) for a [Tabby CAD Suite](https://www.yosyshq.com/tabby-cad-datasheet) Evaluation License and download link
* OR go to https://github.com/YosysHQ/oss-cad-suite-build/releases to download the free OSS CAD Suite
* Follow the [Install Instructions on GitHub](https://github.com/YosysHQ/oss-cad-suite-build#installation)

Make sure to get a Tabby CAD Suite Evaluation License if you need features such as industry-grade SystemVerilog and VHDL parsers!

For more information about the difference between Tabby CAD Suite and the OSS CAD Suite, please visit https://www.yosyshq.com/tabby-cad-datasheet

Many Linux distributions also provide Yosys binaries, some more up to date than others. Check with your package manager!

Building from Source
====================

You need a C++ compiler with C++17 support (up-to-date CLANG or GCC is
recommended) and some standard tools such as GNU Flex, GNU Bison, and GNU Make.
TCL, readline and libffi are optional (see ``ENABLE_*`` settings in Makefile).
Xdot (graphviz) is used by the ``show`` command in yosys to display schematics.

For example on Ubuntu Linux 16.04 LTS the following commands will install all
prerequisites for building yosys:

	$ sudo apt-get install build-essential clang bison flex \
		libreadline-dev gawk tcl-dev libffi-dev git \
		graphviz xdot pkg-config python3 libboost-system-dev \
		libboost-python-dev libboost-filesystem-dev zlib1g-dev

Similarily, on Mac OS X Homebrew can be used to install dependencies (from within cloned yosys repository):

	$ brew tap Homebrew/bundle && brew bundle

or MacPorts:

	$ sudo port install bison flex readline gawk libffi \
		git graphviz pkgconfig python36 boost zlib tcl

On FreeBSD use the following command to install all prerequisites:

	# pkg install bison flex readline gawk libffi\
		git graphviz pkgconf python3 python36 tcl-wrapper boost-libs

On FreeBSD system use gmake instead of make. To run tests use:
    % MAKE=gmake CC=cc gmake test

For Cygwin use the following command to install all prerequisites, or select these additional packages:

	setup-x86_64.exe -q --packages=bison,flex,gcc-core,gcc-g++,git,libffi-devel,libreadline-devel,make,pkg-config,python3,tcl-devel,boost-build,zlib-devel

The environment variable `CXX` can be used to control the C++ compiler used, or
run one of the following:

	$ make config-clang
	$ make config-gcc

Note that these will result in `make` ignoring the `CXX` environment variable,
unless `CXX` is assigned in the call to make, e.g.

  $ make CXX=$CXX

For other compilers and build configurations it might be
necessary to make some changes to the config section of the
Makefile.

	$ vi Makefile            # ..or..
	$ vi Makefile.conf

To build Yosys simply type 'make' in this directory.

	$ make
	$ sudo make install

Note that this also downloads, builds and installs ABC (using yosys-abc
as executable name).

Tests are located in the tests subdirectory and can be executed using the test target. Note that you need gawk as well as a recent version of iverilog (i.e. build from git). Then, execute tests via:

	$ make test

To use a separate (out-of-tree) build directory, provide a path to the Makefile.

	$ mkdir build; cd build
	$ make -f ../Makefile

Out-of-tree builds require a clean source tree.

Getting Started
===============

Yosys can be used with the interactive command shell, with
synthesis scripts or with command line arguments. Let's perform
a simple synthesis job using the interactive command shell:

	$ ./yosys
	yosys>

the command ``help`` can be used to print a list of all available
commands and ``help <command>`` to print details on the specified command:

	yosys> help help

reading and elaborating the design using the Verilog frontend:

	yosys> read -sv tests/simple/fiedler-cooley.v
	yosys> hierarchy -top up3down5

writing the design to the console in the RTLIL format used by Yosys
internally:

	yosys> write_rtlil

convert processes (``always`` blocks) to netlist elements and perform
some simple optimizations:

	yosys> proc; opt

display design netlist using ``xdot``:

	yosys> show

the same thing using ``gv`` as postscript viewer:

	yosys> show -format ps -viewer gv

translating netlist to gate logic and perform some simple optimizations:

	yosys> techmap; opt

write design netlist to a new Verilog file:

	yosys> write_verilog synth.v

or using a simple synthesis script:

	$ cat synth.ys
	read -sv tests/simple/fiedler-cooley.v
	hierarchy -top up3down5
	proc; opt; techmap; opt
	write_verilog synth.v

	$ ./yosys synth.ys

If ABC is enabled in the Yosys build configuration and a cell library is given
in the liberty file ``mycells.lib``, the following synthesis script will
synthesize for the given cell library:

	# read design
	read -sv tests/simple/fiedler-cooley.v
	hierarchy -top up3down5

	# the high-level stuff
	proc; fsm; opt; memory; opt

	# mapping to internal cell library
	techmap; opt

	# mapping flip-flops to mycells.lib
	dfflibmap -liberty mycells.lib

	# mapping logic to mycells.lib
	abc -liberty mycells.lib

	# cleanup
	clean

If you do not have a liberty file but want to test this synthesis script,
you can use the file ``examples/cmos/cmos_cells.lib`` from the yosys sources
as simple example.

Liberty file downloads for and information about free and open ASIC standard
cell libraries can be found here:

- http://www.vlsitechnology.org/html/libraries.html
- http://www.vlsitechnology.org/synopsys/vsclib013.lib

The command ``synth`` provides a good default synthesis script (see
``help synth``):

	read -sv tests/simple/fiedler-cooley.v
	synth -top up3down5

	# mapping to target cells
	dfflibmap -liberty mycells.lib
	abc -liberty mycells.lib
	clean

The command ``prep`` provides a good default word-level synthesis script, as
used in SMT-based formal verification.


Unsupported Verilog-2005 Features
=================================

The following Verilog-2005 features are not supported by
Yosys and there are currently no plans to add support
for them:

- Non-synthesizable language features as defined in
	IEC 62142(E):2005 / IEEE Std. 1364.1(E):2002

- The ``tri``, ``triand`` and ``trior`` net types

- The ``config`` and ``disable`` keywords and library map files


Verilog Attributes and non-standard features
============================================

- The ``full_case`` attribute on case statements is supported
  (also the non-standard ``// synopsys full_case`` directive)

- The ``parallel_case`` attribute on case statements is supported
  (also the non-standard ``// synopsys parallel_case`` directive)

- The ``// synopsys translate_off`` and ``// synopsys translate_on``
  directives are also supported (but the use of ``` `ifdef .. `endif ```
  is strongly recommended instead).

- The ``nomem2reg`` attribute on modules or arrays prohibits the
  automatic early conversion of arrays to separate registers. This
  is potentially dangerous. Usually the front-end has good reasons
  for converting an array to a list of registers. Prohibiting this
  step will likely result in incorrect synthesis results.

- The ``mem2reg`` attribute on modules or arrays forces the early
  conversion of arrays to separate registers.

- The ``nomeminit`` attribute on modules or arrays prohibits the
  creation of initialized memories. This effectively puts ``mem2reg``
  on all memories that are written to in an ``initial`` block and
  are not ROMs.

- The ``nolatches`` attribute on modules or always-blocks
  prohibits the generation of logic-loops for latches. Instead
  all not explicitly assigned values default to x-bits. This does
  not affect clocked storage elements such as flip-flops.

- The ``nosync`` attribute on registers prohibits the generation of a
  storage element. The register itself will always have all bits set
  to 'x' (undefined). The variable may only be used as blocking assigned
  temporary variable within an always block. This is mostly used internally
  by Yosys to synthesize Verilog functions and access arrays.

- The ``nowrshmsk`` attribute on a register prohibits the generation of
  shift-and-mask type circuits for writing to bit slices of that register.

- The ``onehot`` attribute on wires mark them as one-hot state register. This
  is used for example for memory port sharing and set by the fsm_map pass.

- The ``blackbox`` attribute on modules is used to mark empty stub modules
  that have the same ports as the real thing but do not contain information
  on the internal configuration. This modules are only used by the synthesis
  passes to identify input and output ports of cells. The Verilog backend
  also does not output blackbox modules on default. ``read_verilog``, unless
  called with ``-noblackbox`` will automatically set the blackbox attribute
  on any empty module it reads.

- The ``noblackbox`` attribute set on an empty module prevents ``read_verilog``
  from automatically setting the blackbox attribute on the module.

- The ``whitebox`` attribute on modules triggers the same behavior as
  ``blackbox``, but is for whitebox modules, i.e. library modules that
  contain a behavioral model of the cell type.

- The ``lib_whitebox`` attribute overwrites ``whitebox`` when ``read_verilog``
  is run in `-lib` mode. Otherwise it's automatically removed.

- The ``dynports`` attribute is used by the Verilog front-end to mark modules
  that have ports with a width that depends on a parameter.

- The ``hdlname`` attribute is used by some passes to document the original
  (HDL) name of a module when renaming a module. It should contain a single
  name, or, when describing a hierarchical name in a flattened design, multiple
  names separated by a single space character.

- The ``keep`` attribute on cells and wires is used to mark objects that should
  never be removed by the optimizer. This is used for example for cells that
  have hidden connections that are not part of the netlist, such as IO pads.
  Setting the ``keep`` attribute on a module has the same effect as setting it
  on all instances of the module.

- The ``keep_hierarchy`` attribute on cells and modules keeps the ``flatten``
  command from flattening the indicated cells and modules.

- The ``init`` attribute on wires is set by the frontend when a register is
  initialized "FPGA-style" with ``reg foo = val``. It can be used during
  synthesis to add the necessary reset logic.

- The ``top`` attribute on a module marks this module as the top of the
  design hierarchy. The ``hierarchy`` command sets this attribute when called
  with ``-top``. Other commands, such as ``flatten`` and various backends
  use this attribute to determine the top module.

- The ``src`` attribute is set on cells and wires created by to the string
  ``<hdl-file-name>:<line-number>`` by the HDL front-end and is then carried
  through the synthesis. When entities are combined, a new |-separated
  string is created that contains all the string from the original entities.

- The ``defaultvalue`` attribute is used to store default values for
  module inputs. The attribute is attached to the input wire by the HDL
  front-end when the input is declared with a default value.

- The ``parameter`` and ``localparam`` attributes are used to mark wires
  that represent module parameters or localparams (when the HDL front-end
  is run in ``-pwires`` mode).

- Wires marked with the ``hierconn`` attribute are connected to wires with the
  same name (format ``cell_name.identifier``) when they are imported from
  sub-modules by ``flatten``.

- The ``clkbuf_driver`` attribute can be set on an output port of a blackbox
  module to mark it as a clock buffer output, and thus prevent ``clkbufmap``
  from inserting another clock buffer on a net driven by such output.

- The ``clkbuf_sink`` attribute can be set on an input port of a module to
  request clock buffer insertion by the ``clkbufmap`` pass.

- The ``clkbuf_inv`` attribute can be set on an output port of a module
  with the value set to the name of an input port of that module.  When
  the ``clkbufmap`` would otherwise insert a clock buffer on this output,
  it will instead try inserting the clock buffer on the input port (this
  is used to implement clock inverter cells that clock buffer insertion
  will "see through").

- The ``clkbuf_inhibit`` is the default attribute to set on a wire to prevent
  automatic clock buffer insertion by ``clkbufmap``. This behaviour can be
  overridden by providing a custom selection to ``clkbufmap``.

- The ``invertible_pin`` attribute can be set on a port to mark it as
  invertible via a cell parameter.  The name of the inversion parameter
  is specified as the value of this attribute.  The value of the inversion
  parameter must be of the same width as the port, with 1 indicating
  an inverted bit and 0 indicating a non-inverted bit.

- The ``iopad_external_pin`` attribute on a blackbox module's port marks
  it as the external-facing pin of an I/O pad, and prevents ``iopadmap``
  from inserting another pad cell on it.

- The module attribute ``abc9_lut`` is an integer attribute indicating to
  `abc9` that this module describes a LUT with an area cost of this value, and
  propagation delays described using `specify` statements.

- The module attribute ``abc9_box`` is a boolean specifying a black/white-box
  definition, with propagation delays described using `specify` statements, for
  use by `abc9`.

- The port attribute ``abc9_carry`` marks the carry-in (if an input port) and
  carry-out (if output port) ports of a box. This information is necessary for
  `abc9` to preserve the integrity of carry-chains. Specifying this attribute
  onto a bus port will affect only its most significant bit.

- The module attribute ``abc9_flop`` is a boolean marking the module as a
  flip-flop. This allows `abc9` to analyse its contents in order to perform
  sequential synthesis.

- The frontend sets attributes ``always_comb``, ``always_latch`` and
  ``always_ff`` on processes derived from SystemVerilog style always blocks
  according to the type of the always. These are checked for correctness in
  ``proc_dlatch``.

- The cell attribute ``wildcard_port_conns`` represents wildcard port
  connections (SystemVerilog ``.*``). These are resolved to concrete
  connections to matching wires in ``hierarchy``.

- In addition to the ``(* ... *)`` attribute syntax, Yosys supports
  the non-standard ``{* ... *}`` attribute syntax to set default attributes
  for everything that comes after the ``{* ... *}`` statement. (Reset
  by adding an empty ``{* *}`` statement.)

- In module parameter and port declarations, and cell port and parameter
  lists, a trailing comma is ignored. This simplifies writing Verilog code
  generators a bit in some cases.

- Modules can be declared with ``module mod_name(...);`` (with three dots
  instead of a list of module ports). With this syntax it is sufficient
  to simply declare a module port as 'input' or 'output' in the module
  body.

- When defining a macro with `define, all text between triple double quotes
  is interpreted as macro body, even if it contains unescaped newlines. The
  triple double quotes are removed from the macro body. For example:

      `define MY_MACRO(a, b) """
         assign a = 23;
         assign b = 42;
      """

- The attribute ``via_celltype`` can be used to implement a Verilog task or
  function by instantiating the specified cell type. The value is the name
  of the cell type to use. For functions the name of the output port can
  be specified by appending it to the cell type separated by a whitespace.
  The body of the task or function is unused in this case and can be used
  to specify a behavioral model of the cell type for simulation. For example:

      module my_add3(A, B, C, Y);
        parameter WIDTH = 8;
        input [WIDTH-1:0] A, B, C;
        output [WIDTH-1:0] Y;
        ...
      endmodule

      module top;
        ...
        (* via_celltype = "my_add3 Y" *)
        (* via_celltype_defparam_WIDTH = 32 *)
        function [31:0] add3;
          input [31:0] A, B, C;
          begin
            add3 = A + B + C;
          end
        endfunction
        ...
      endmodule

- The ``wiretype`` attribute is added by the verilog parser for wires of a
  typedef'd type to indicate the type identifier.

- Various ``enum_value_{value}`` attributes are added to wires of an enumerated type
  to give a map of possible enum items to their values.

- The ``enum_base_type`` attribute is added to enum items to indicate which
  enum they belong to (enums -- anonymous and otherwise -- are
  automatically named with an auto-incrementing counter). Note that enums
  are currently not strongly typed.

- A limited subset of DPI-C functions is supported. The plugin mechanism
  (see ``help plugin``) can be used to load .so files with implementations
  of DPI-C routines. As a non-standard extension it is possible to specify
  a plugin alias using the ``<alias>:`` syntax. For example:

      module dpitest;
        import "DPI-C" function foo:round = real my_round (real);
        parameter real r = my_round(12.345);
      endmodule

      $ yosys -p 'plugin -a foo -i /lib/libm.so; read_verilog dpitest.v'

- Sized constants (the syntax ``<size>'s?[bodh]<value>``) support constant
  expressions as ``<size>``. If the expression is not a simple identifier, it
  must be put in parentheses. Examples: ``WIDTH'd42``, ``(4+2)'b101010``

- The system tasks ``$finish``, ``$stop`` and ``$display`` are supported in
  initial blocks in an unconditional context (only if/case statements on
  expressions over parameters and constant values are allowed). The intended
  use for this is synthesis-time DRC.

- There is limited support for converting ``specify`` .. ``endspecify``
  statements to special ``$specify2``, ``$specify3``, and ``$specrule`` cells,
  for use in blackboxes and whiteboxes. Use ``read_verilog -specify`` to
  enable this functionality. (By default these blocks are ignored.)

- The ``reprocess_after`` internal attribute is used by the Verilog frontend to
  mark cells with bindings which might depend on the specified instantiated
  module. Modules with such cells will be reprocessed during the ``hierarchy``
  pass once the referenced module definition(s) become available.

- The ``smtlib2_module`` attribute can be set on a blackbox module to specify a
  formal model directly using SMT-LIB 2. For such a module, the
  ``smtlib2_comb_expr`` attribute can be used on output ports to define their
  value using an SMT-LIB 2 expression. For example:

      (* blackbox *)
      (* smtlib2_module *)
      module submod(a, b);
        input [7:0] a;
        (* smtlib2_comb_expr = "(bvnot a)" *)
        output [7:0] b;
      endmodule

Non-standard or SystemVerilog features for formal verification
==============================================================

- Support for ``assert``, ``assume``, ``restrict``, and ``cover`` is enabled
  when ``read_verilog`` is called with ``-formal``.

- The system task ``$initstate`` evaluates to 1 in the initial state and
  to 0 otherwise.

- The system function ``$anyconst`` evaluates to any constant value. This is
  equivalent to declaring a reg as ``rand const``, but also works outside
  of checkers. (Yosys also supports ``rand const`` outside checkers.)

- The system function ``$anyseq`` evaluates to any value, possibly a different
  value in each cycle. This is equivalent to declaring a reg as ``rand``,
  but also works outside of checkers. (Yosys also supports ``rand``
  variables outside checkers.)

- The system functions ``$allconst`` and ``$allseq`` can be used to construct
  formal exist-forall problems. Assumptions only hold if the trace satisfies
  the assumption for all ``$allconst/$allseq`` values. For assertions and cover
  statements it is sufficient if just one ``$allconst/$allseq`` value triggers
  the property (similar to ``$anyconst/$anyseq``).

- Wires/registers declared using the ``anyconst/anyseq/allconst/allseq`` attribute
  (for example ``(* anyconst *) reg [7:0] foobar;``) will behave as if driven
  by a ``$anyconst/$anyseq/$allconst/$allseq`` function.

- The SystemVerilog tasks ``$past``, ``$stable``, ``$rose`` and ``$fell`` are
  supported in any clocked block.

- The syntax ``@($global_clock)`` can be used to create FFs that have no
  explicit clock input (``$ff`` cells). The same can be achieved by using
  ``@(posedge <netname>)`` or ``@(negedge <netname>)`` when ``<netname>``
  is marked with the ``(* gclk *)`` Verilog attribute.


Supported features from SystemVerilog
=====================================

When ``read_verilog`` is called with ``-sv``, it accepts some language features
from SystemVerilog:

- The ``assert`` statement from SystemVerilog is supported in its most basic
  form. In module context: ``assert property (<expression>);`` and within an
  always block: ``assert(<expression>);``. It is transformed to an ``$assert`` cell.

- The ``assume``, ``restrict``, and ``cover`` statements from SystemVerilog are
  also supported. The same limitations as with the ``assert`` statement apply.

- The keywords ``always_comb``, ``always_ff`` and ``always_latch``, ``logic``
  and ``bit`` are supported.

- Declaring free variables with ``rand`` and ``rand const`` is supported.

- Checkers without a port list that do not need to be instantiated (but instead
  behave like a named block) are supported.

- SystemVerilog packages are supported. Once a SystemVerilog file is read
  into a design with ``read_verilog``, all its packages are available to
  SystemVerilog files being read into the same design afterwards.

- typedefs are supported (including inside packages)
	- type casts are currently not supported

- enums are supported (including inside packages)
	- but are currently not strongly typed

- packed structs and unions are supported
	- arrays of packed structs/unions are currently not supported
	- structure literals are currently not supported

- multidimensional arrays are supported
	- array assignment of unpacked arrays is currently not supported
	- array literals are currently not supported

- SystemVerilog interfaces (SVIs) are supported. Modports for specifying whether
  ports are inputs or outputs are supported.

- Assignments within expressions are supported.


Building the documentation
==========================

Note that there is no need to build the manual if you just want to read it.
Simply visit https://yosys.readthedocs.io/en/latest/ instead.

In addition to those packages listed above for building Yosys from source, the
following are used for building the website: 

	$ sudo apt install pdf2svg faketime

PDFLaTeX, included with most LaTeX distributions, is also needed during the
build process for the website.  Or, run the following:

	$ sudo apt install texlive-latex-base texlive-latex-extra latexmk

The Python package, Sphinx, is needed along with those listed in
`docs/source/requirements.txt`:

	$ pip install -U sphinx -r docs/source/requirements.txt

From the root of the repository, run `make docs`.  This will build/rebuild yosys
as necessary before generating the website documentation from the yosys help
commands.  To build for pdf instead of html, call 
`make docs DOC_TARGET=latexpdf`.



.github/PULL_REQUEST_TEMPLATE.md
--------------------------------------
_What are the reasons/motivation for this change?_

_Explain how this is achieved._

_If applicable, please suggest to reviewers how they can test the change._



source/appendix.rst
--------------------------------------
Appendix
========

.. toctree::
	:maxdepth: 2
	:includehidden:

	appendix/primer
	appendix/auxlibs
	appendix/auxprogs

	bib

.. toctree::
	:maxdepth: 1
	:includehidden:

	cmd_ref



source/bib.rst
--------------------------------------
.. only:: html

	Literature references
	=====================

	.. rubric:: Bibliography

.. bibliography:: literature.bib

.. todo:: see if we can get the two hanging appnotes as lit references



source/cmd_ref.rst
--------------------------------------
.. _cmd_ref:

================================================================================
Command line reference
================================================================================

.. literalinclude:: /generated/yosys
    :start-at: Usage

.. toctree::
	:caption: Command reference
	:maxdepth: 1
	:glob:

	/appendix/env_vars
	/cmd/*



source/index.rst
--------------------------------------
================================================================================
Yosys Open SYnthesis Suite
================================================================================

Yosys is an open source framework for RTL synthesis.  To learn more about Yosys,
see :doc:`/introduction`.  For a quick guide on how to get started using Yosys,
check out :doc:`/getting_started/index`.  For the complete list of commands
available, go to :ref:`commandindex`.

.. note::

   This documentation recently went through a major restructure.  If you're
   looking for something from the previous version and can't find it here,
   please `let us know`_.  Documentation from before the restructure can still
   be found by switching to `version 0.36`_ or earlier.  Note that the previous
   theme does not include a version switcher.

.. _let us know: https://github.com/YosysHQ/yosys/issues/new/choose
.. _version 0.36: https://yosyshq.readthedocs.io/projects/yosys/en/0.36/

.. todo:: look into command ref improvements

   - Search bar with live drop down suggestions for matching on title /
     autocompleting commands
   - Scroll the left sidebar to the current location on page load
   - Also the formatting/linking in pdf is broken

.. todolist::

.. only:: html

   Table of contents
   -----------------

.. toctree::
   :maxdepth: 3
   :includehidden:

   introduction
   getting_started/index
   using_yosys/index
   yosys_internals/index
   test_suites

   appendix



source/introduction.rst
--------------------------------------
What is Yosys
=============

Yosys began as a BSc thesis project by Claire Wolf intended to support synthesis
for a CGRA (coarse-grained reconfigurable architecture).  It then expanded into
more general infrastructure for research on synthesis.

Modern Yosys has full support for the synthesizable subset of Verilog-2005 and
has been described as "the GCC of hardware synthesis."  Freely available and
`open source`_, Yosys finds use across hobbyist and commercial applications as
well as academic.

.. _open source: https://github.com/YosysHQ/yosys

.. note:: Yosys is released under the ISC License:

   A permissive license lets people do anything with your code with proper
   attribution and without warranty. The ISC license is functionally equivalent
   to the BSD 2-Clause and MIT licenses, removing some language that is no
   longer necessary.

Together with the place and route tool `nextpnr`_, Yosys can be used to program
some FPGAs with a fully end-to-end open source flow (Lattice iCE40 and ECP5). It
also does the synthesis portion for the `OpenLane flow`_, targeting the SkyWater
130nm open source PDK for fully open source ASIC design.  Yosys can also do
formal verification with backends for solver formats like `SMT2`_.

.. _nextpnr: https://github.com/YosysHQ/nextpnr
.. _OpenLane flow: https://github.com/The-OpenROAD-Project/OpenLane
.. _SMT2: https://smtlib.cs.uiowa.edu/

Yosys, and the accompanying Open Source EDA ecosystem, is currently maintained
by `Yosys Headquarters`_, with many of the core developers employed by `YosysHQ
GmbH`_.  A commercial extension, `Tabby CAD Suite`_, includes the Verific
frontend for industry-grade SystemVerilog and VHDL support, formal verification
with SVA, and formal apps.

.. _Yosys Headquarters: https://github.com/YosysHQ
.. _YosysHQ GmbH: https://www.yosyshq.com/about
.. _Tabby CAD Suite: https://www.yosyshq.com/tabby-cad-datasheet

.. figure:: /_static/logo.png
    :class: width-helper

What you can do with Yosys
--------------------------

- Read and process (most of) modern Verilog-2005 code
- Perform all kinds of operations on netlist (RTL, Logic, Gate)
- Perform logic optimizations and gate mapping with ABC

Typical applications for Yosys
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Synthesis of final production designs
- Pre-production synthesis (trial runs before investing in other tools)
- Conversion of full-featured Verilog to simple Verilog
- Conversion of Verilog to other formats (BLIF, BTOR, etc)
- Demonstrating synthesis algorithms (e.g. for educational purposes)
- Framework for experimenting with new algorithms
- Framework for building custom flows (Not limited to synthesis but also formal
  verification, reverse engineering, ...)

Things you can't do
~~~~~~~~~~~~~~~~~~~

- Process high-level languages such as C/C++/SystemC
- Create physical layouts (place&route)

  - Check out `nextpnr`_ for that

.. todo:: nextpnr for FPGAs, consider mentioning openlane, vpr, coriolis

.. _nextpnr: https://github.com/YosysHQ/nextpnr

The Yosys family
----------------

As mentioned above, `YosysHQ`_ maintains not just Yosys but an entire family of
tools built around it.  In no particular order:

.. _YosysHQ: https://github.com/YosysHQ

SBY for formal verification
   Yosys provides input parsing and conversion to the formats used by the solver
   engines.  Yosys also provides a unified witness framework for providing cover
   traces and counter examples for engines which don't natively support this.
   `SBY source`_ | `SBY docs`_

.. _SBY source: https://github.com/YosysHQ/sby
.. _SBY docs: https://yosyshq.readthedocs.io/projects/sby

EQY for equivalence checking
   In addition to input parsing and preparation, Yosys provides  the plugin
   support enabling EQY to operate on designs directly. `EQY source`_ | `EQY
   docs`_

.. _EQY source: https://github.com/YosysHQ/eqy
.. _EQY docs: https://yosyshq.readthedocs.io/projects/eqy

MCY for mutation coverage
   Yosys is used to read the source design, generate a list of possible
   mutations to maximise design coverage, and then perform selected mutations.
   `MCY source`_ | `MCY docs`_

.. _MCY source: https://github.com/YosysHQ/mcy
.. _MCY docs: https://yosyshq.readthedocs.io/projects/mcy

SCY for deep formal traces
   Since SCY generates and runs SBY, Yosys provides the same utility for SCY as
   it does for SBY.  Yosys additionally provides the trace concatenation needed
   for outputting the deep traces. `SCY source`_

.. _SCY source: https://github.com/YosysHQ/scy

The original thesis abstract
----------------------------

The first version of the Yosys documentation was published as a bachelor thesis
at the Vienna University of Technology :cite:p:`BACC`.

:Abstract:
	Most of today's digital design is done in HDL code (mostly Verilog or 
	VHDL) and with the help of HDL synthesis tools.

	In special cases such as synthesis for coarse-grain cell libraries or
	when testing new synthesis algorithms it might be necessary to write a
	custom HDL synthesis tool or add new features to an existing one. In
	these cases the availability of a Free and Open Source (FOSS) synthesis
	tool that can be used as basis for custom tools would be helpful.

	In the absence of such a tool, the Yosys Open SYnthesis Suite (Yosys)
	was developed. This document covers the design and implementation of
	this tool. At the moment the main focus of Yosys lies on the high-level
	aspects of digital synthesis. The pre-existing FOSS logic-synthesis tool
	ABC is used by Yosys to perform advanced gate-level optimizations.

	An evaluation of Yosys based on real-world designs is included. It is
	shown that Yosys can be used as-is to synthesize such designs. The
	results produced by Yosys in this tests where successfully verified
	using formal verification and are comparable in quality to the results
	produced by a commercial synthesis tool.

Yosys is a Verilog HDL synthesis tool. This means that it takes a behavioural
design description as input and generates an RTL, logical gate or physical gate
level description of the design as output. Yosys' main strengths are behavioural
and RTL synthesis. A wide range of commands (synthesis passes) exist within
Yosys that can be used to perform a wide range of synthesis tasks within the
domain of behavioural, rtl and logic synthesis. Yosys is designed to be
extensible and therefore is a good basis for implementing custom synthesis tools
for specialised tasks.

.. figure:: /_images/primer/levels_of_abstraction.*
    :class: width-helper invert-helper
    :name: fig:Levels_of_abstraction

    Where Yosys exists in the layers of abstraction

Benefits of open source HDL synthesis
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Cost (also applies to ``free as in free beer`` solutions): 
  
  Today the cost for a mask set in 180nm technology is far less than
  the cost for the design tools needed to design the mask layouts. Open Source
  ASIC flows are an important enabler for ASIC-level Open Source Hardware.

- Availability and Reproducibility: 
  
  If you are a researcher who is publishing, you want to use tools that everyone
  else can also use. Even if most universities have access to all major
  commercial tools, you usually do not have easy access to the version that was
  used in a research project a couple of years ago. With Open Source tools you
  can even release the source code of the tool you have used alongside your data.

- Framework: 
  
  Yosys is not only a tool. It is a framework that can be used as basis for other
  developments, so researchers and hackers alike do not need to re-invent the
  basic functionality. Extensibility was one of Yosys' design goals.

- All-in-one: 
  
  Because of the framework characteristics of Yosys, an increasing number of features
  become available in one tool. Yosys not only can be used for circuit synthesis but
  also for formal equivalence checking, SAT solving, and for circuit analysis, to
  name just a few other application domains. With proprietary software one needs to
  learn a new tool for each of these applications.

- Educational Tool: 
  
  Proprietary synthesis tools are at times very secretive about their inner
  workings. They often are ``black boxes``. Yosys is very open about its
  internals and it is easy to observe the different steps of synthesis.

History of Yosys
~~~~~~~~~~~~~~~~

.. todo:: Consider a less academic version of the History of Yosys

A Hardware Description Language (HDL) is a computer language used to describe
circuits. A HDL synthesis tool is a computer program that takes a formal
description of a circuit written in an HDL as input and generates a netlist that
implements the given circuit as output.

Currently the most widely used and supported HDLs for digital circuits are
Verilog :cite:p:`Verilog2005,VerilogSynth` and :abbr:`VHDL (VHSIC HDL, where
VHSIC is an acronym for Very-High-Speed Integrated Circuits)`
:cite:p:`VHDL,VHDLSynth`. Both HDLs are used for test and verification purposes
as well as logic synthesis, resulting in a set of synthesizable and a set of
non-synthesizable language features. In this document we only look at the
synthesizable subset of the language features.

In recent work on heterogeneous coarse-grain reconfigurable logic
:cite:p:`intersynth` the need for a custom application-specific HDL synthesis
tool emerged. It was soon realised that a synthesis tool that understood Verilog
or VHDL would be preferred over a synthesis tool for a custom HDL. Given an
existing Verilog or VHDL front end, the work for writing the necessary
additional features and integrating them in an existing tool can be estimated to
be about the same as writing a new tool with support for a minimalistic custom
HDL.

The proposed custom HDL synthesis tool should be licensed under a Free and Open
Source Software (FOSS) licence. So an existing FOSS Verilog or VHDL synthesis
tool would have been needed as basis to build upon. The main advantages of
choosing Verilog or VHDL is the ability to synthesize existing HDL code and to
mitigate the requirement for circuit-designers to learn a new language. In order
to take full advantage of any existing FOSS Verilog or VHDL tool, such a tool
would have to provide a feature-complete implementation of the synthesizable HDL
subset.

Basic RTL synthesis is a well understood field :cite:p:`LogicSynthesis`. Lexing,
parsing and processing of computer languages :cite:p:`Dragonbook` is a
thoroughly researched field. All the information required to write such tools
has been openly available for a long time, and it is therefore likely that a
FOSS HDL synthesis tool with a feature-complete Verilog or VHDL front end must
exist which can be used as a basis for a custom RTL synthesis tool.

Due to the author's preference for Verilog over VHDL it was decided early on to
go for Verilog instead of VHDL [#]_. So the existing FOSS Verilog synthesis
tools were evaluated. The results of this evaluation are utterly devastating.
Therefore a completely new Verilog synthesis tool was implemented and is
recommended as basis for custom synthesis tools. This is the tool that is
discussed in this document.

.. [#]
   A quick investigation into FOSS VHDL tools yielded similar grim results for
   FOSS VHDL synthesis tools.



source/test_suites.rst
--------------------------------------
Testing Yosys
=============

.. todo:: more about the included test suite

Automatic testing
-----------------

.. only:: html

   The `Yosys Git repo`_ has automatic testing of builds and running of the
   included test suite on the following platforms:

   - Ubuntu |test-linux|
   - macOS |test-macos|

.. _Yosys Git repo: https://github.com/YosysHQ/yosys

.. |test-linux| image:: https://github.com/YosysHQ/yosys/actions/workflows/test-linux.yml/badge.svg?branch=main
.. |test-macos| image:: https://github.com/YosysHQ/yosys/actions/workflows/test-macos.yml/badge.svg?branch=main

For up to date information, including OS versions, refer to `the git actions
page`_.

.. _the git actions page: https://github.com/YosysHQ/yosys/actions



appendix/APPNOTE_010_Verilog_to_BLIF.rst
--------------------------------------
:orphan:

====================================
010: Converting Verilog to BLIF page
====================================

Abstract
========

Verilog-2005 is a powerful Hardware Description Language (HDL) that can be used
to easily create complex designs from small HDL code. It is the preferred method
of design entry for many designers.

The Berkeley Logic Interchange Format (BLIF) is a simple file format for
exchanging sequential logic between programs. It is easy to generate and easy to
parse and is therefore the preferred method of design entry for many authors of
logic synthesis tools.

Yosys is a feature-rich Open-Source Verilog synthesis tool that can be used to
bridge the gap between the two file formats. It implements most of Verilog-2005
and thus can be used to import modern behavioral Verilog designs into BLIF-based
design flows without dependencies on proprietary synthesis tools.

The scope of Yosys goes of course far beyond Verilog logic synthesis. But it is
a useful and important feature and this Application Note will focus on this
aspect of Yosys.

Download
========

This document was originally published in April 2015:
:download:`Converting Verilog to BLIF PDF</_downloads/APPNOTE_010_Verilog_to_BLIF.pdf>`

..
   Installation
   ============

   Yosys written in C++ (using features from C++11) and is tested on modern
   Linux. It should compile fine on most UNIX systems with a C++11
   compiler. The README file contains useful information on building Yosys
   and its prerequisites.

   Yosys is a large and feature-rich program with a couple of dependencies.
   It is, however, possible to deactivate some of the dependencies in the
   Makefile, resulting in features in Yosys becoming unavailable. When
   problems with building Yosys are encountered, a user who is only
   interested in the features of Yosys that are discussed in this
   Application Note may deactivate TCL, Qt and MiniSAT support in the
   Makefile and may opt against building yosys-abc.

   This Application Note is based on `Yosys GIT`_ `Rev. e216e0e`_  from 2013-11-23.
   The Verilog sources used for the examples are taken from `yosys-bigsim`_, a
   collection of real-world designs used for regression testing Yosys.

   .. _Yosys GIT: https://github.com/YosysHQ/yosys

   .. _Rev. e216e0e: https://github.com/YosysHQ/yosys/tree/e216e0e

   .. _yosys-bigsim: https://github.com/YosysHQ/yosys-bigsim

   Getting started
   ===============

   We start our tour with the Navré processor from yosys-bigsim. The `Navré
   processor`_ is an Open Source AVR clone. It is a single module (softusb_navre)
   in a single design file (softusb_navre.v). It also is using only features that
   map nicely to the BLIF format, for example it only uses synchronous resets.

   .. _Navré processor: http://opencores.org/projects/navre

   Converting softusb_navre.v to softusb_navre.blif could not be easier:

   .. code:: sh

      yosys -o softusb_navre.blif -S softusb_navre.v

   Behind the scenes Yosys is controlled by synthesis scripts that execute
   commands that operate on Yosys' internal state. For example, the -o
   softusb_navre.blif option just adds the command write_blif
   softusb_navre.blif to the end of the script. Likewise a file on the
   command line – softusb_navre.v in this case – adds the command
   read_verilog softusb_navre.v to the beginning of the synthesis script.
   In both cases the file type is detected from the file extension.

   Finally the option -S instantiates a built-in default synthesis script.
   Instead of using -S one could also specify the synthesis commands for
   the script on the command line using the -p option, either using
   individual options for each command or by passing one big command string
   with a semicolon-separated list of commands. But in most cases it is
   more convenient to use an actual script file.

   Using a synthesis script
   ========================

   With a script file we have better control over Yosys. The following
   script file replicates what the command from the last section did:

   .. code:: yoscrypt

      read_verilog softusb_navre.v
      hierarchy
      proc; opt; memory; opt; techmap; opt
      write_blif softusb_navre.blif

   The first and last line obviously read the Verilog file and write the
   BLIF file.

   The 2nd line checks the design hierarchy and instantiates parametrized
   versions of the modules in the design, if necessary. In the case of this
   simple design this is a no-op. However, as a general rule a synthesis
   script should always contain this command as first command after reading
   the input files.

   The 3rd line does most of the actual work:

   -  The command opt is the Yosys' built-in optimizer. It can perform some
      simple optimizations such as const-folding and removing unconnected
      parts of the design. It is common practice to call opt after each
      major step in the synthesis procedure. In cases where too much
      optimization is not appreciated (for example when analyzing a
      design), it is recommended to call clean instead of opt.

   -  The command proc converts processes (Yosys' internal representation
      of Verilog always- and initial-blocks) to circuits of multiplexers
      and storage elements (various types of flip-flops).

   -  The command memory converts Yosys' internal representations of arrays
      and array accesses to multi-port block memories, and then maps this
      block memories to address decoders and flip-flops, unless the option
      -nomap is used, in which case the multi-port block memories stay in
      the design and can then be mapped to architecture-specific memory
      primitives using other commands.

   -  The command techmap turns a high-level circuit with coarse grain
      cells such as wide adders and multipliers to a fine-grain circuit of
      simple logic primitives and single-bit storage elements. The command
      does that by substituting the complex cells by circuits of simpler
      cells. It is possible to provide a custom set of rules for this
      process in the form of a Verilog source file, as we will see in the
      next section.

   Now Yosys can be run with the filename of the synthesis script as
   argument:

   .. code:: sh

      yosys softusb_navre.ys

   Now that we are using a synthesis script we can easily modify how Yosys
   synthesizes the design. The first thing we should customize is the call
   to the hierarchy command:

   Whenever it is known that there are no implicit blackboxes in the
   design, i.e. modules that are referenced but are not defined, the
   hierarchy command should be called with the -check option. This will
   then cause synthesis to fail when implicit blackboxes are found in the
   design.

   The 2nd thing we can improve regarding the hierarchy command is that we
   can tell it the name of the top level module of the design hierarchy. It
   will then automatically remove all modules that are not referenced from
   this top level module.

   For many designs it is also desired to optimize the encodings for the
   finite state machines (FSMs) in the design. The fsm command finds FSMs,
   extracts them, performs some basic optimizations and then generate a
   circuit from the extracted and optimized description. It would also be
   possible to tell the fsm command to leave the FSMs in their extracted
   form, so they can be further processed using custom commands. But in
   this case we don't want that.

   So now we have the final synthesis script for generating a BLIF file for
   the Navré CPU:

   .. code:: yoscrypt

      read_verilog softusb_navre.v
      hierarchy -check -top softusb_navre
      proc; opt; memory; opt; fsm; opt; techmap; opt
      write_blif softusb_navre.blif

   Advanced example: The Amber23 ARMv2a CPU
   ========================================

   Our 2nd example is the `Amber23 ARMv2a CPU`_. Once again we base our example on
   the Verilog code that is included in `yosys-bigsim`_.

   .. _Amber23 ARMv2a CPU: http://opencores.org/projects/amber

   .. code-block:: yoscrypt
      :caption: `amber23.ys`
      :name: amber23.ys

      read_verilog a23_alu.v
      read_verilog a23_barrel_shift_fpga.v
      read_verilog a23_barrel_shift.v
      read_verilog a23_cache.v
      read_verilog a23_coprocessor.v
      read_verilog a23_core.v
      read_verilog a23_decode.v
      read_verilog a23_execute.v
      read_verilog a23_fetch.v
      read_verilog a23_multiply.v
      read_verilog a23_ram_register_bank.v
      read_verilog a23_register_bank.v
      read_verilog a23_wishbone.v
      read_verilog generic_sram_byte_en.v
      read_verilog generic_sram_line_en.v
      hierarchy -check -top a23_core
      add -global_input globrst 1
      proc -global_arst globrst
      techmap -map adff2dff.v
      opt; memory; opt; fsm; opt; techmap
      write_blif amber23.blif

   The problem with this core is that it contains no dedicated reset logic. Instead
   the coding techniques shown in :numref:`glob_arst` are used to define reset
   values for the global asynchronous reset in an FPGA implementation. This design
   can not be expressed in BLIF as it is. Instead we need to use a synthesis script
   that transforms this form to synchronous resets that can be expressed in BLIF.

   (Note that there is no problem if this coding techniques are used to model ROM,
   where the register is initialized using this syntax but is never updated
   otherwise.)

   :numref:`amber23.ys` shows the synthesis script for the Amber23 core. In line 17
   the add command is used to add a 1-bit wide global input signal with the name
   ``globrst``. That means that an input with that name is added to each module in the
   design hierarchy and then all module instantiations are altered so that this new
   signal is connected throughout the whole design hierarchy.

   .. code-block:: verilog
      :caption: Implicit coding of global asynchronous resets
      :name: glob_arst

      reg [7:0] a = 13, b;
      initial b = 37;

   .. code-block:: verilog
      :caption: `adff2dff.v`
      :name: adff2dff.v

      (* techmap_celltype = "$adff" *)
      module adff2dff (CLK, ARST, D, Q);

      parameter WIDTH = 1;
      parameter CLK_POLARITY = 1;
      parameter ARST_POLARITY = 1;
      parameter ARST_VALUE = 0;

      input CLK, ARST;
      input [WIDTH-1:0] D;
      output reg [WIDTH-1:0] Q;

      wire [1023:0] _TECHMAP_DO_ = "proc";

      wire _TECHMAP_FAIL_ =
         !CLK_POLARITY || !ARST_POLARITY;

      always @(posedge CLK)
            if (ARST)
                     Q <= ARST_VALUE;
            else
                     Q <= D;

      endmodule

   In line 18 the :cmd:ref:`proc` command is called. But in this script the signal
   name globrst is passed to the command as a global reset signal for resetting the
   registers to their assigned initial values.

   Finally in line 19 the techmap command is used to replace all instances of
   flip-flops with asynchronous resets with flip-flops with synchronous resets. The
   map file used for this is shown in :numref:`adff2dff.v`. Note how the
   ``techmap_celltype`` attribute is used in line 1 to tell the techmap command
   which cells to replace in the design, how the ``_TECHMAP_FAIL_`` wire in lines
   15 and 16 (which evaluates to a constant value) determines if the parameter set
   is compatible with this replacement circuit, and how the ``_TECHMAP_DO_`` wire
   in line 13 provides a mini synthesis-script to be used to process this cell.

   .. code-block:: c
      :caption: Test program for the Amber23 CPU (Sieve of Eratosthenes). Compiled 
               using GCC 4.6.3 for ARM with ``-Os -marm -march=armv2a 
         -mno-thumb-interwork -ffreestanding``, linked with ``--fix-v4bx`` 
         set and booted with a custom setup routine written in ARM assembler.
      :name: sieve

      #include <stdint.h>
      #include <stdbool.h>

      #define BITMAP_SIZE 64
      #define OUTPORT 0x10000000

      static uint32_t bitmap[BITMAP_SIZE/32];

      static void bitmap_set(uint32_t idx) { bitmap[idx/32] |= 1 << (idx % 32); }
      static bool bitmap_get(uint32_t idx) { return (bitmap[idx/32] & (1 << (idx % 32))) != 0; }
      static void output(uint32_t val) { *((volatile uint32_t*)OUTPORT) = val; }

      int main() {
         uint32_t i, j, k;
         output(2);
         for (i = 0; i < BITMAP_SIZE; i++) {
            if (bitmap_get(i)) continue;
            output(3+2*i);
            for (j = 2*(3+2*i);; j += 3+2*i) {
                  if (j%2 == 0) continue;
                  k = (j-3)/2;
                  if (k >= BITMAP_SIZE) break;
                  bitmap_set(k);
            }
         }
         output(0);
         return 0;
      }

   Verification of the Amber23 CPU
   ===============================

   The BLIF file for the Amber23 core, generated using :numref:`amber23.ys` and
   :numref:`adff2dff.v` and the version of the Amber23 RTL source that is bundled
   with yosys-bigsim, was verified using the test-bench from yosys-bigsim. It
   successfully executed the program shown in :numref:`sieve` in the test-bench.

   For simulation the BLIF file was converted back to Verilog using `ABC`_. So this
   test includes the successful transformation of the BLIF file into ABC's internal
   format as well.

   .. _ABC: https://github.com/berkeley-abc/abc

   The only thing left to write about the simulation itself is that it probably was
   one of the most energy inefficient and time consuming ways of successfully
   calculating the first 31 primes the author has ever conducted.

   Limitations
   ===========

   At the time of this writing Yosys does not support multi-dimensional memories,
   does not support writing to individual bits of array elements, does not support
   initialization of arrays with ``$readmemb`` and ``$readmemh``, and has only
   limited support for tristate logic, to name just a few limitations.

   That being said, Yosys can synthesize an overwhelming majority of real-world
   Verilog RTL code. The remaining cases can usually be modified to be compatible
   with Yosys quite easily.

   The various designs in yosys-bigsim are a good place to look for examples of
   what is within the capabilities of Yosys.

   Conclusion
   ==========

   Yosys is a feature-rich Verilog-2005 synthesis tool. It has many uses, but one
   is to provide an easy gateway from high-level Verilog code to low-level logic
   circuits.

   The command line option ``-S`` can be used to quickly synthesize Verilog code to
   BLIF files without a hassle.

   With custom synthesis scripts it becomes possible to easily perform high-level
   optimizations, such as re-encoding FSMs. In some extreme cases, such as the
   Amber23 ARMv2 CPU, the more advanced Yosys features can be used to change a
   design to fit a certain need without actually touching the RTL code.



appendix/APPNOTE_012_Verilog_to_BTOR.rst
--------------------------------------
:orphan:

====================================
012: Converting Verilog to BTOR page
====================================

Abstract
========

Verilog-2005 is a powerful Hardware Description Language (HDL) that can be used
to easily create complex designs from small HDL code. BTOR is a bit-precise
word-level format for model checking. It is a simple format and easy to parse.
It allows to model the model checking problem over the theory of bit-vectors
with one-dimensional arrays, thus enabling to model Verilog designs with
registers and memories. Yosys is an Open-Source Verilog synthesis tool that can
be used to convert Verilog designs with simple assertions to BTOR format.

Download
========

This document was originally published in November 2013: 
:download:`Converting Verilog to BTOR PDF</_downloads/APPNOTE_012_Verilog_to_BTOR.pdf>`

..
   Installation
   ============

   Yosys written in C++ (using features from C++11) and is tested on modern Linux.
   It should compile fine on most UNIX systems with a C++11 compiler. The README
   file contains useful information on building Yosys and its prerequisites.

   Yosys is a large and feature-rich program with some dependencies. For this work,
   we may deactivate other extra features such as TCL and ABC support in the
   Makefile.

   This Application Note is based on `Yosys GIT`_ `Rev. 082550f` from 2015-04-04.

   .. _Yosys GIT: https://github.com/YosysHQ/yosys

   .. _Rev. 082550f: https://github.com/YosysHQ/yosys/tree/082550f

   Quick start
   ===========

   We assume that the Verilog design is synthesizable and we also assume that the
   design does not have multi-dimensional memories. As BTOR implicitly initializes
   registers to zero value and memories stay uninitialized, we assume that the
   Verilog design does not contain initial blocks. For more details about the BTOR
   format, please refer to :cite:p:`btor`.

   We provide a shell script ``verilog2btor.sh`` which can be used to convert a
   Verilog design to BTOR. The script can be found in the ``backends/btor``
   directory. The following example shows its usage:

   .. code:: sh

      verilog2btor.sh fsm.v fsm.btor test

   The script ``verilog2btor.sh`` takes three parameters. In the above example, the
   first parameter ``fsm.v`` is the input design, the second parameter ``fsm.btor``
   is the file name of BTOR output, and the third parameter ``test`` is the name of
   top module in the design.

   To specify the properties (that need to be checked), we have two
   options:

   -  We can use the Verilog ``assert`` statement in the procedural block or module
      body of the Verilog design, as shown in :numref:`specifying_property_assert`.
      This is the preferred option.

   -  We can use a single-bit output wire, whose name starts with ``safety``. The
      value of this output wire needs to be driven low when the property is met,
      i.e. the solver will try to find a model that makes the safety pin go high.
      This is demonstrated in :numref:`specifying_property_output`.

   .. code-block:: verilog
      :caption: Specifying property in Verilog design with ``assert``
      :name: specifying_property_assert

      module test(input clk, input rst, output y);

      reg [2:0] state;

      always @(posedge clk) begin
         if (rst || state == 3) begin
            state <= 0;
         end else begin
            assert(state < 3);
            state <= state + 1;
         end
      end

      assign y = state[2];

      assert property (y !== 1'b1);

      endmodule

   .. code-block:: verilog
      :caption: Specifying property in Verilog design with output wire
      :name: specifying_property_output

      module test(input clk, input rst,
         output y, output safety1);

      reg [2:0] state;

      always @(posedge clk) begin
         if (rst || state == 3)
            state <= 0;
         else
            state <= state + 1;
      end

      assign y = state[2];

      assign safety1 = !(y !== 1'b1);

      endmodule

   We can run `Boolector`_ ``1.4.1`` [1]_ on the generated BTOR file:

   .. _Boolector: http://fmv.jku.at/boolector/

   .. code:: sh

      $ boolector fsm.btor
      unsat

   We can also use `nuXmv`_, but on BTOR designs it does not support memories yet.
   With the next release of nuXmv, we will be also able to verify designs with
   memories.

   .. _nuXmv: https://es-static.fbk.eu/tools/nuxmv/index.php

   Detailed flow
   =============

   Yosys is able to synthesize Verilog designs up to the gate level. We are
   interested in keeping registers and memories when synthesizing the design. For
   this purpose, we describe a customized Yosys synthesis flow, that is also
   provided by the ``verilog2btor.sh`` script. :numref:`btor_script_memory` shows
   the Yosys commands that are executed by ``verilog2btor.sh``.

   .. code-block:: yoscrypt
      :caption: Synthesis Flow for BTOR with memories
      :name: btor_script_memory

      read_verilog -sv $1;
      hierarchy -top $3; hierarchy -libdir $DIR;
      hierarchy -check;
      proc; opt;
      opt_expr -mux_undef; opt;
      rename -hide;;;
      splice; opt;
      memory_dff -wr_only; memory_collect;;
      flatten;;
      memory_unpack;
      splitnets -driver;
      setundef -zero -undriven;
      opt;;;
      write_btor $2;

   Here is short description of what is happening in the script line by
   line:

   #. Reading the input file.

   #. Setting the top module in the hierarchy and trying to read automatically the
      files which are given as ``include`` in the file read in first line.

   #. Checking the design hierarchy.

   #. Converting processes to multiplexers (muxs) and flip-flops.

   #. Removing undef signals from muxs.

   #. Hiding all signal names that are not used as module ports.

   #. Explicit type conversion, by introducing slice and concat cells in the
      circuit.

   #. Converting write memories to synchronous memories, and collecting the
      memories to multi-port memories.

   #. Flattening the design to get only one module.

   #. Separating read and write memories.

   #. Splitting the signals that are partially assigned

   #. Setting undef to zero value.

   #. Final optimization pass.

   #. Writing BTOR file.

   For detailed description of the commands mentioned above, please refer
   to the Yosys documentation, or run ``yosys -h <command_name>``.

   The script presented earlier can be easily modified to have a BTOR file that
   does not contain memories. This is done by removing the line number 8 and 10,
   and introduces a new command :cmd:ref:`memory` at line number 8.
   :numref:`btor_script_without_memory` shows the modified Yosys script file:

   .. code-block:: sh
      :caption: Synthesis Flow for BTOR without memories
      :name: btor_script_without_memory

      read_verilog -sv $1;
      hierarchy -top $3; hierarchy -libdir $DIR;
      hierarchy -check;
      proc; opt;
      opt_expr -mux_undef; opt;
      rename -hide;;;
      splice; opt;
      memory;;
      flatten;;
      splitnets -driver;
      setundef -zero -undriven;
      opt;;;
      write_btor $2;

   Example
   =======

   Here is an example Verilog design that we want to convert to BTOR:

   .. code-block:: verilog
      :caption: Example - Verilog Design
      :name: example_verilog

      module array(input clk);

      reg [7:0] counter;
      reg [7:0] mem [7:0];

      always @(posedge clk) begin
         counter <= counter + 8'd1;
         mem[counter] <= counter;
      end

      assert property (!(counter > 8'd0) ||
         mem[counter - 8'd1] == counter - 8'd1);

      endmodule

   The generated BTOR file that contain memories, using the script shown in
   :numref:`btor_memory`:

   .. code-block::
      :caption: Example - Converted BTOR with memory
      :name: btor_memory

      1 var 1 clk
      2 array 8 3
      3 var 8 $auto$rename.cc:150:execute$20
      4 const 8 00000001
      5 sub 8 3 4
      6 slice 3 5 2 0
      7 read 8 2 6
      8 slice 3 3 2 0
      9 add 8 3 4
      10 const 8 00000000
      11 ugt 1 3 10
      12 not 1 11
      13 const 8 11111111
      14 slice 1 13 0 0
      15 one 1
      16 eq 1 1 15
      17 and 1 16 14
      18 write 8 3 2 8 3
      19 acond 8 3 17 18 2
      20 anext 8 3 2 19
      21 eq 1 7 5
      22 or 1 12 21
      23 const 1 1
      24 one 1
      25 eq 1 23 24
      26 cond 1 25 22 24
      27 root 1 -26
      28 cond 8 1 9 3
      29 next 8 3 28

   And the BTOR file obtained by the script shown in
   :numref:`btor_without_memory`, which expands the memory into individual
   elements:

   .. code-block::
      :caption: Example - Converted BTOR with memory
      :name: btor_without_memory

      1 var 1 clk
      2 var 8 mem[0]
      3 var 8 $auto$rename.cc:150:execute$20
      4 slice 3 3 2 0
      5 slice 1 4 0 0
      6 not 1 5
      7 slice 1 4 1 1
      8 not 1 7
      9 slice 1 4 2 2
      10 not 1 9
      11 and 1 8 10
      12 and 1 6 11
      13 cond 8 12 3 2
      14 cond 8 1 13 2
      15 next 8 2 14
      16 const 8 00000001
      17 add 8 3 16
      18 const 8 00000000
      19 ugt 1 3 18
      20 not 1 19
      21 var 8 mem[2]
      22 and 1 7 10
      23 and 1 6 22
      24 cond 8 23 3 21
      25 cond 8 1 24 21
      26 next 8 21 25
      27 sub 8 3 16

      ...

      54 cond 1 53 50 52
      55 root 1 -54

      ...

      77 cond 8 76 3 44
      78 cond 8 1 77 44
      79 next 8 44 78

   Limitations
   ===========

   BTOR does not support initialization of memories and registers, i.e. they are
   implicitly initialized to value zero, so the initial block for memories need to
   be removed when converting to BTOR. It should also be kept in consideration that
   BTOR does not support the ``x`` or ``z`` values of Verilog.

   Another thing to bear in mind is that Yosys will convert multi-dimensional
   memories to one-dimensional memories and address decoders. Therefore
   out-of-bounds memory accesses can yield unexpected results.

   Conclusion
   ==========

   Using the described flow, we can use Yosys to generate word-level verification
   benchmarks with or without memories from Verilog designs.

   .. [1]
      Newer version of Boolector do not support sequential models.
      Boolector 1.4.1 can be built with picosat-951. Newer versions of
      picosat have an incompatible API.



appendix/auxlibs.rst
--------------------------------------
Auxiliary libraries
===================

The Yosys source distribution contains some auxiliary libraries that are
compiled into Yosys and can be used in plugins.

BigInt
------

The files in ``libs/bigint/`` provide a library for performing arithmetic with
arbitrary length integers. It is written by Matt McCutchen.

The BigInt library is used for evaluating constant expressions, e.g. using the
ConstEval class provided in kernel/consteval.h.

See also: http://mattmccutchen.net/bigint/

dlfcn-win32
-----------

The ``dlfcn`` library enables runtime loading of plugins without requiring
recompilation of Yosys.  The files in ``libs/dlfcn-win32`` provide an
implementation of ``dlfcn`` for Windows.

See also: https://github.com/dlfcn-win32/dlfcn-win32

ezSAT
-----

The files in ``libs/ezsat`` provide a library for simplifying generating CNF
formulas for SAT solvers. It also contains bindings of MiniSAT. The ezSAT
library is written by C. Wolf. It is used by the :cmd:ref:`sat` pass (see
:doc:`/cmd/sat`).

fst
---

``libfst`` files from `gtkwave`_ are included in ``libs/fst`` to support
reading/writing signal traces from/to the GTKWave developed FST format.  This is
primarily used in the :cmd:ref:`sim` command.

.. _gtkwave: https://github.com/gtkwave/gtkwave

json11
------

For reading/writing designs from/to JSON, :cmd:ref:`read_json` and
:cmd:ref:`write_json` should be used.  For everything else there is the `json11
library`_:

   json11 is a tiny JSON library for C++11, providing JSON parsing and
   serialization.

This library is used for outputting machine-readable statistics (:cmd:ref:`stat`
with ``-json`` flag), using the RPC frontend (:cmd:ref:`connect_rpc`), and the
yosys-witness ``yw`` format.

.. _json11 library: https://github.com/dropbox/json11

MiniSAT
-------

The files in ``libs/minisat`` provide a high-performance SAT solver, used by the
:cmd:ref:`sat` command.

SHA1
----

The files in ``libs/sha1/`` provide a public domain SHA1 implementation written
by Steve Reid, Bruce Guenter, and Volker Grabsch. It is used for generating
unique names when specializing parameterized modules.

.. _sec:SubCircuit:

SubCircuit
----------

The files in ``libs/subcircuit`` provide a library for solving the subcircuit
isomorphism problem. It is written by C. Wolf and based on the Ullmann Subgraph
Isomorphism Algorithm :cite:p:`UllmannSubgraphIsomorphism`. It is used by the
extract pass (see :doc:`../cmd/extract`).



appendix/auxprogs.rst
--------------------------------------
Auxiliary programs
==================

Besides the main yosys executable, the Yosys distribution contains a set of
additional helper programs.

yosys-config
------------

The ``yosys-config`` tool (an auto-generated shell-script) can be used to query
compiler options and other information needed for building loadable modules for
Yosys. See :doc:`/yosys_internals/extending_yosys/extensions` for details.

.. literalinclude:: /generated/yosys-config
    :start-at: Usage

.. _sec:filterlib:

yosys-filterlib
---------------

.. todo:: how does a filterlib rules-file work?

The ``yosys-filterlib`` tool is a small utility that can be used to strip or
extract information from a Liberty file.  This can be useful for removing
sensitive or proprietary information such as timing or other trade secrets.

.. literalinclude:: /generated/yosys-filterlib
    :start-at: Usage

yosys-abc
---------

This is a fork of ABC with a small set of custom modifications that have not yet
been accepted upstream. Not all versions of Yosys work with all versions of ABC.
So Yosys comes with its own yosys-abc to avoid compatibility issues between the
two.

.. literalinclude:: /generated/yosys-abc
    :start-at: usage

yosys-smtbmc
------------

The ``yosys-smtbmc`` tool is a utility used by SBY for interacting with smt
solvers.

.. literalinclude:: /generated/yosys-smtbmc

yosys-witness
-------------

``yosys-witness`` is a new tool to inspect and convert yosys witness traces.
This is used in SBY and SCY for producing traces in a consistent format
independent of the solver.

.. literalinclude:: /generated/yosys-witness
    :start-at: Usage

.. note:: ``yosys-witness`` requires `click`_ Python package for use.

.. _click: https://pypi.org/project/click/



appendix/env_vars.rst
--------------------------------------
Yosys environment variables
===========================

``HOME``
   Yosys command history is stored in :file:`$HOME/.yosys_history`.  Graphics
   (from :cmd:ref:`show` and :cmd:ref:`viz` commands) will output to this
   directory by default.  This environment variable is also used in some cases
   for resolving filenames with :file:`~`.

``PATH``
   May be used in OpenBSD builds for finding the location of Yosys executable.

``TMPDIR``
   Used for storing temporary files.

``ABC``
   When compiling Yosys with out-of-tree ABC using :makevar:`ABCEXTERNAL`, this
   variable can be used to override the external ABC executable.

``YOSYS_NOVERIFIC``
   If Yosys was built with Verific, this environment variable can be used to
   temporarily disable Verific support.

``YOSYS_COVER_DIR`` and ``YOSYS_COVER_FILE``
   When using code coverage, these environment variables control the output file
   name/location.

``YOSYS_ABORT_ON_LOG_ERROR``
   Can be used for debugging Yosys internals.  Setting it to 1 causes abort() to
   be called when Yosys terminates with an error message.




appendix/primer.rst
--------------------------------------
.. role:: verilog(code)
	:language: Verilog

.. _chapter:basics:

A primer on digital circuit synthesis
=====================================

This chapter contains a short introduction to the basic principles of digital
circuit synthesis.

Levels of abstraction
---------------------

Digital circuits can be represented at different levels of abstraction. During
the design process a circuit is usually first specified using a higher level
abstraction. Implementation can then be understood as finding a functionally
equivalent representation at a lower abstraction level. When this is done
automatically using software, the term synthesis is used.

So synthesis is the automatic conversion of a high-level representation of a
circuit to a functionally equivalent low-level representation of a circuit.
:numref:`Figure %s <fig:Basics_abstractions>` lists the different levels of
abstraction and how they relate to different kinds of synthesis.

.. figure:: /_images/primer/basics_abstractions.*
	:class: width-helper invert-helper
	:name: fig:Basics_abstractions

	Different levels of abstraction and synthesis.

Regardless of the way a lower level representation of a circuit is obtained
(synthesis or manual design), the lower level representation is usually verified
by comparing simulation results of the lower level and the higher level
representation  [1]_. Therefore even if no synthesis is used, there must still
be a simulatable representation of the circuit in all levels to allow for
verification of the design.

Note: The exact meaning of terminology such as "High-Level" is of course not
fixed over time. For example the HDL "ABEL" was first introduced in 1985 as "A
High-Level Design Language for Programmable Logic Devices" :cite:p:`ABEL`, but
would not be considered a "High-Level Language" today.

System level
~~~~~~~~~~~~

The System Level abstraction of a system only looks at its biggest building
blocks like CPUs and computing cores. At this level the circuit is usually
described using traditional programming languages like C/C++ or Matlab.
Sometimes special software libraries are used that are aimed at simulation
circuits on the system level, such as SystemC.

Usually no synthesis tools are used to automatically transform a system level
representation of a circuit to a lower-level representation. But system level
design tools exist that can be used to connect system level building blocks.

The IEEE 1685-2009 standard defines the IP-XACT file format that can be used to
represent designs on the system level and building blocks that can be used in
such system level designs. :cite:p:`IP-XACT`

High level
~~~~~~~~~~

The high-level abstraction of a system (sometimes referred to as algorithmic
level) is also often represented using traditional programming languages, but
with a reduced feature set. For example when representing a design at the high
level abstraction in C, pointers can only be used to mimic concepts that can be
found in hardware, such as memory interfaces. Full featured dynamic memory
management is not allowed as it has no corresponding concept in digital
circuits.

Tools exist to synthesize high level code (usually in the form of C/C++/SystemC
code with additional metadata) to behavioural HDL code (usually in the form of
Verilog or VHDL code). Aside from the many commercial tools for high level
synthesis there are also a number of FOSS tools for high level synthesis .

Behavioural level
~~~~~~~~~~~~~~~~~

At the behavioural abstraction level a language aimed at hardware description
such as Verilog or VHDL is used to describe the circuit, but so-called
behavioural modelling is used in at least part of the circuit description. In
behavioural modelling there must be a language feature that allows for
imperative programming to be used to describe data paths and registers. This is
the always-block in Verilog and the process-block in VHDL.

In behavioural modelling, code fragments are provided together with a
sensitivity list; a list of signals and conditions. In simulation, the code
fragment is executed whenever a signal in the sensitivity list changes its value
or a condition in the sensitivity list is triggered. A synthesis tool must be
able to transfer this representation into an appropriate datapath followed by
the appropriate types of register.

For example consider the following Verilog code fragment:

.. code:: verilog
   :number-lines:

   always @(posedge clk)
       y <= a + b;

In simulation the statement ``y <= a + b`` is executed whenever a positive edge
on the signal ``clk`` is detected. The synthesis result however will contain an
adder that calculates the sum ``a + b`` all the time, followed by a d-type
flip-flop with the adder output on its D-input and the signal ``y`` on its
Q-output.

Usually the imperative code fragments used in behavioural modelling can contain
statements for conditional execution (``if``- and ``case``-statements in
Verilog) as well as loops, as long as those loops can be completely unrolled.

Interestingly there seems to be no other FOSS Tool that is capable of performing
Verilog or VHDL behavioural syntheses besides Yosys.

Register-Transfer Level (RTL)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

On the Register-Transfer Level the design is represented by combinatorial data
paths and registers (usually d-type flip flops). The following Verilog code
fragment is equivalent to the previous Verilog example, but is in RTL
representation:

.. code:: verilog
   :number-lines:

   assign tmp = a + b;       // combinatorial data path

   always @(posedge clk)     // register
       y <= tmp;

A design in RTL representation is usually stored using HDLs like Verilog and
VHDL. But only a very limited subset of features is used, namely minimalistic
always-blocks (Verilog) or process-blocks (VHDL) that model the register type
used and unconditional assignments for the datapath logic. The use of HDLs on
this level simplifies simulation as no additional tools are required to simulate
a design in RTL representation.

Many optimizations and analyses can be performed best at the RTL level. Examples
include FSM detection and optimization, identification of memories or other
larger building blocks and identification of shareable resources.

Note that RTL is the first abstraction level in which the circuit is represented
as a graph of circuit elements (registers and combinatorial cells) and signals.
Such a graph, when encoded as list of cells and connections, is called a
netlist.

RTL synthesis is easy as each circuit node element in the netlist can simply be
replaced with an equivalent gate-level circuit. However, usually the term RTL
synthesis does not only refer to synthesizing an RTL netlist to a gate level
netlist but also to performing a number of highly sophisticated optimizations
within the RTL representation, such as the examples listed above.

A number of FOSS tools exist that can perform isolated tasks within the domain
of RTL synthesis steps. But there seems to be no FOSS tool that covers a wide
range of RTL synthesis operations.

Logical gate level
~~~~~~~~~~~~~~~~~~

At the logical gate level the design is represented by a netlist that uses only
cells from a small number of single-bit cells, such as basic logic gates (AND,
OR, NOT, XOR, etc.) and registers (usually D-Type Flip-flops).

A number of netlist formats exists that can be used on this level, e.g. the
Electronic Design Interchange Format (EDIF), but for ease of simulation often a
HDL netlist is used. The latter is a HDL file (Verilog or VHDL) that only uses
the most basic language constructs for instantiation and connecting of cells.

There are two challenges in logic synthesis: First finding opportunities for
optimizations within the gate level netlist and second the optimal (or at least
good) mapping of the logic gate netlist to an equivalent netlist of physically
available gate types.

The simplest approach to logic synthesis is two-level logic synthesis, where a
logic function is converted into a sum-of-products representation, e.g. using a
Karnaugh map. This is a simple approach, but has exponential worst-case effort
and cannot make efficient use of physical gates other than AND/NAND-, OR/NOR-
and NOT-Gates.

Therefore modern logic synthesis tools utilize much more complicated multi-level
logic synthesis algorithms :cite:p:`MultiLevelLogicSynth`. Most of these
algorithms convert the logic function to a Binary-Decision-Diagram (BDD) or
And-Inverter-Graph (AIG) and work from that representation. The former has the
advantage that it has a unique normalized form. The latter has much better worst
case performance and is therefore better suited for the synthesis of large logic
functions.

Good FOSS tools exists for multi-level logic synthesis .

Yosys contains basic logic synthesis functionality but can also use ABC for the
logic synthesis step. Using ABC is recommended.

Physical gate level
~~~~~~~~~~~~~~~~~~~

On the physical gate level only gates are used that are physically available on
the target architecture. In some cases this may only be NAND, NOR and NOT gates
as well as D-Type registers. In other cases this might include cells that are
more complex than the cells used at the logical gate level (e.g. complete
half-adders). In the case of an FPGA-based design the physical gate level
representation is a netlist of LUTs with optional output registers, as these are
the basic building blocks of FPGA logic cells.

For the synthesis tool chain this abstraction is usually the lowest level. In
case of an ASIC-based design the cell library might contain further information
on how the physical cells map to individual switches (transistors).

Switch level
~~~~~~~~~~~~

A switch level representation of a circuit is a netlist utilizing single
transistors as cells. Switch level modelling is possible in Verilog and VHDL,
but is seldom used in modern designs, as in modern digital ASIC or FPGA flows
the physical gates are considered the atomic build blocks of the logic circuit.

Yosys
~~~~~

Yosys is a Verilog HDL synthesis tool. This means that it takes a behavioural
design description as input and generates an RTL, logical gate or physical gate
level description of the design as output. Yosys' main strengths are behavioural
and RTL synthesis. A wide range of commands (synthesis passes) exist within
Yosys that can be used to perform a wide range of synthesis tasks within the
domain of behavioural, rtl and logic synthesis. Yosys is designed to be
extensible and therefore is a good basis for implementing custom synthesis tools
for specialised tasks.

Features of synthesizable Verilog
---------------------------------

The subset of Verilog :cite:p:`Verilog2005` that is synthesizable is specified
in a separate IEEE standards document, the IEEE standard 1364.1-2002
:cite:p:`VerilogSynth`. This standard also describes how certain language
constructs are to be interpreted in the scope of synthesis.

This section provides a quick overview of the most important features of
synthesizable Verilog, structured in order of increasing complexity.

Structural Verilog
~~~~~~~~~~~~~~~~~~

Structural Verilog (also known as Verilog Netlists) is a Netlist in Verilog
syntax. Only the following language constructs are used in this
case:

-  Constant values
-  Wire and port declarations
-  Static assignments of signals to other signals
-  Cell instantiations

Many tools (especially at the back end of the synthesis chain) only support
structural Verilog as input. ABC is an example of such a tool. Unfortunately
there is no standard specifying what Structural Verilog actually is, leading to
some confusion about what syntax constructs are supported in structural Verilog
when it comes to features such as attributes or multi-bit signals.

Expressions in Verilog
~~~~~~~~~~~~~~~~~~~~~~

In all situations where Verilog accepts a constant value or signal name,
expressions using arithmetic operations such as ``+``, ``-`` and ``*``, boolean
operations such as ``&`` (AND), ``|`` (OR) and ``^`` (XOR) and many others
(comparison operations, unary operator, etc.) can also be used.

During synthesis these operators are replaced by cells that implement the
respective function.

Many FOSS tools that claim to be able to process Verilog in fact only support
basic structural Verilog and simple expressions. Yosys can be used to convert
full featured synthesizable Verilog to this simpler subset, thus enabling such
applications to be used with a richer set of Verilog features.

Behavioural modelling
~~~~~~~~~~~~~~~~~~~~~

Code that utilizes the Verilog always statement is using Behavioural Modelling.
In behavioural modelling, a circuit is described by means of imperative program
code that is executed on certain events, namely any change, a rising edge, or a
falling edge of a signal. This is a very flexible construct during simulation
but is only synthesizable when one
of the following is modelled:

-  | **Asynchronous or latched logic**
   | In this case the sensitivity list must contain all expressions that
     are used within the always block. The syntax ``@*`` can be used for
     these cases. Examples of this kind include:

   .. code:: verilog
      :number-lines:

      // asynchronous
      always @* begin
          if (add_mode)
              y <= a + b;
          else
              y <= a - b;
      end

      // latched
      always @* begin
          if (!hold)
              y <= a + b;
      end

   Note that latched logic is often considered bad style and in many
   cases just the result of sloppy HDL design. Therefore many synthesis
   tools generate warnings whenever latched logic is generated.

-  | **Synchronous logic (with optional synchronous reset)**
   | This is logic with d-type flip-flops on the output. In this case
     the sensitivity list must only contain the respective clock edge.
     Example:

   .. code:: verilog
      :number-lines:

      // counter with synchronous reset
      always @(posedge clk) begin
          if (reset)
              y <= 0;
          else
              y <= y + 1;
      end

-  | **Synchronous logic with asynchronous reset**
   | This is logic with d-type flip-flops with asynchronous resets on
     the output. In this case the sensitivity list must only contain the
     respective clock and reset edges. The values assigned in the reset
     branch must be constant. Example:

   .. code:: verilog
      :number-lines:

      // counter with asynchronous reset
      always @(posedge clk, posedge reset) begin
          if (reset)
              y <= 0;
          else
              y <= y + 1;
      end

Many synthesis tools support a wider subset of flip-flops that can be modelled
using always-statements (including Yosys). But only the ones listed above are
covered by the Verilog synthesis standard and when writing new designs one
should limit herself or himself to these cases.

In behavioural modelling, blocking assignments (=) and non-blocking assignments
(<=) can be used. The concept of blocking vs. non-blocking assignment is one of
the most misunderstood constructs in Verilog :cite:p:`Cummings00`.

The blocking assignment behaves exactly like an assignment in any imperative
programming language, while with the non-blocking assignment the right hand side
of the assignment is evaluated immediately but the actual update of the left
hand side register is delayed until the end of the time-step. For example the
Verilog code ``a <= b; b <= a;`` exchanges the values of the two registers.


Functions and tasks
~~~~~~~~~~~~~~~~~~~

Verilog supports Functions and Tasks to bundle statements that are used in
multiple places (similar to Procedures in imperative programming). Both
constructs can be implemented easily by substituting the function/task-call with
the body of the function or task.

Conditionals, loops and generate-statements
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Verilog supports ``if-else``-statements and ``for``-loops inside
``always``-statements.

It also supports both features in ``generate``-statements on the module level.
This can be used to selectively enable or disable parts of the module based on
the module parameters (``if-else``) or to generate a set of similar subcircuits
(``for``).

While the ``if-else``-statement inside an always-block is part of behavioural
modelling, the three other cases are (at least for a synthesis tool) part of a
built-in macro processor. Therefore it must be possible for the synthesis tool
to completely unroll all loops and evaluate the condition in all
``if-else``-statement in ``generate``-statements using const-folding..

Arrays and memories
~~~~~~~~~~~~~~~~~~~

Verilog supports arrays. This is in general a synthesizable language feature. In
most cases arrays can be synthesized by generating addressable memories.
However, when complex or asynchronous access patterns are used, it is not
possible to model an array as memory. In these cases the array must be modelled
using individual signals for each word and all accesses to the array must be
implemented using large multiplexers.

In some cases it would be possible to model an array using memories, but it is
not desired. Consider the following delay circuit:

.. code:: verilog
   :number-lines:

   module (clk, in_data, out_data);

   parameter BITS = 8;
   parameter STAGES = 4;

   input clk;
   input [BITS-1:0] in_data;
   output [BITS-1:0] out_data;
   reg [BITS-1:0] ffs [STAGES-1:0];

   integer i;
   always @(posedge clk) begin
       ffs[0] <= in_data;
       for (i = 1; i < STAGES; i = i+1)
           ffs[i] <= ffs[i-1];
   end

   assign out_data = ffs[STAGES-1];

   endmodule

This could be implemented using an addressable memory with STAGES input and
output ports. A better implementation would be to use a simple chain of
flip-flops (a so-called shift register). This better implementation can either
be obtained by first creating a memory-based implementation and then optimizing
it based on the static address signals for all ports or directly identifying
such situations in the language front end and converting all memory accesses to
direct accesses to the correct signals.

Challenges in digital circuit synthesis
---------------------------------------

This section summarizes the most important challenges in digital circuit
synthesis. Tools can be characterized by how well they address these topics.

Standards compliance
~~~~~~~~~~~~~~~~~~~~

The most important challenge is compliance with the HDL standards in question
(in case of Verilog the IEEE Standards 1364.1-2002 and 1364-2005). This can be
broken down in two items:

-  Completeness of implementation of the standard
-  Correctness of implementation of the standard

Completeness is mostly important to guarantee compatibility with existing HDL
code. Once a design has been verified and tested, HDL designers are very
reluctant regarding changes to the design, even if it is only about a few minor
changes to work around a missing feature in a new synthesis tool.

Correctness is crucial. In some areas this is obvious (such as correct synthesis
of basic behavioural models). But it is also crucial for the areas that concern
minor details of the standard, such as the exact rules for handling signed
expressions, even when the HDL code does not target different synthesis tools.
This is because (unlike software source code that is only processed by
compilers), in most design flows HDL code is not only processed by the synthesis
tool but also by one or more simulators and sometimes even a formal verification
tool. It is key for this verification process that all these tools use the same
interpretation for the HDL code.

Optimizations
~~~~~~~~~~~~~

Generally it is hard to give a one-dimensional description of how well a
synthesis tool optimizes the design. First of all because not all optimizations
are applicable to all designs and all synthesis tasks. Some optimizations work
(best) on a coarse-grained level (with complex cells such as adders or
multipliers) and others work (best) on a fine-grained level (single bit gates).
Some optimizations target area and others target speed. Some work well on large
designs while others don't scale well and can only be applied to small designs.

A good tool is capable of applying a wide range of optimizations at different
levels of abstraction and gives the designer control over which optimizations
are performed (or skipped) and what the optimization goals are.

Technology mapping
~~~~~~~~~~~~~~~~~~

Technology mapping is the process of converting the design into a netlist of
cells that are available in the target architecture. In an ASIC flow this might
be the process-specific cell library provided by the fab. In an FPGA flow this
might be LUT cells as well as special function units such as dedicated
multipliers. In a coarse-grain flow this might even be more complex special
function units.

An open and vendor independent tool is especially of interest if it supports a
wide range of different types of target architectures.

Script-based synthesis flows
----------------------------

A digital design is usually started by implementing a high-level or system-level
simulation of the desired function. This description is then manually
transformed (or re-implemented) into a synthesizable lower-level description
(usually at the behavioural level) and the equivalence of the two
representations is verified by simulating both and comparing the simulation
results.

Then the synthesizable description is transformed to lower-level representations
using a series of tools and the results are again verified using simulation.
This process is illustrated in :numref:`Fig. %s <fig:Basics_flow>`.

.. figure:: /_images/primer/basics_flow.*
	:class: width-helper invert-helper
	:name: fig:Basics_flow

	Typical design flow.  Green boxes represent manually created models.
	Orange boxes represent models generated by synthesis tools.


In this example the System Level Model and the Behavioural Model are both
manually written design files. After the equivalence of system level model and
behavioural model has been verified, the lower level representations of the
design can be generated using synthesis tools. Finally the RTL Model and the
Gate-Level Model are verified and the design process is finished.

However, in any real-world design effort there will be multiple iterations for
this design process. The reason for this can be the late change of a design
requirement or the fact that the analysis of a low-abstraction model
(e.g. gate-level timing analysis) revealed that a design change is required in
order to meet the design requirements (e.g. maximum possible clock speed).

Whenever the behavioural model or the system level model is changed their
equivalence must be re-verified by re-running the simulations and comparing the
results. Whenever the behavioural model is changed the synthesis must be re-run
and the synthesis results must be re-verified.

In order to guarantee reproducibility it is important to be able to re-run all
automatic steps in a design project with a fixed set of settings easily. Because
of this, usually all programs used in a synthesis flow can be controlled using
scripts. This means that all functions are available via text commands. When
such a tool provides a GUI, this is complementary to, and not instead of, a
command line interface.

Usually a synthesis flow in an UNIX/Linux environment would be controlled by a
shell script that calls all required tools (synthesis and
simulation/verification in this example) in the correct order. Each of these
tools would be called with a script file containing commands for the respective
tool. All settings required for the tool would be provided by these script files
so that no manual interaction would be necessary. These script files are
considered design sources and should be kept under version control just like the
source code of the system level and the behavioural model.

Methods from compiler design
----------------------------

Some parts of synthesis tools involve problem domains that are traditionally
known from compiler design. This section addresses some of these domains.

Lexing and parsing
~~~~~~~~~~~~~~~~~~

The best known concepts from compiler design are probably lexing and parsing.
These are two methods that together can be used to process complex computer
languages easily. :cite:p:`Dragonbook`

A lexer consumes single characters from the input and generates a stream of
lexical tokens that consist of a type and a value. For example the Verilog input
:verilog:`assign foo = bar + 42;` might be translated by the lexer to the list
of lexical tokens given in :numref:`Tab. %s <tab:Basics_tokens>`.

.. table:: Exemplary token list for the statement :verilog:`assign foo = bar + 42;`
	:name: tab:Basics_tokens

	============== ===============
	Token-Type     Token-Value
	============== ===============
	TOK_ASSIGN     \-
	TOK_IDENTIFIER "foo"
	TOK_EQ         \-
	TOK_IDENTIFIER "bar"
	TOK_PLUS       \-
	TOK_NUMBER     42
	TOK_SEMICOLON  \-
	============== ===============

The lexer is usually generated by a lexer generator (e.g. flex ) from a
description file that is using regular expressions to specify the text pattern
that should match the individual tokens.

The lexer is also responsible for skipping ignored characters (such as
whitespace outside string constants and comments in the case of Verilog) and
converting the original text snippet to a token value.

Note that individual keywords use different token types (instead of a keyword
type with different token values). This is because the parser usually can only
use the Token-Type to make a decision on the grammatical role of a token.

The parser then transforms the list of tokens into a parse tree that closely
resembles the productions from the computer languages grammar. As the lexer, the
parser is also typically generated by a code generator (e.g. bison) from a
grammar description in Backus-Naur Form (BNF).

Let's consider the following BNF (in Bison syntax):

.. code:: none
   :number-lines:

   assign_stmt: TOK_ASSIGN TOK_IDENTIFIER TOK_EQ expr TOK_SEMICOLON;
   expr: TOK_IDENTIFIER | TOK_NUMBER | expr TOK_PLUS expr;

.. figure:: /_images/primer/basics_parsetree.*
	:class: width-helper invert-helper
	:name: fig:Basics_parsetree

	Example parse tree for the Verilog expression 
	:verilog:`assign foo = bar + 42;`

The parser converts the token list to the parse tree in :numref:`Fig. %s
<fig:Basics_parsetree>`. Note that the parse tree never actually exists as a
whole as data structure in memory. Instead the parser calls user-specified code
snippets (so-called reduce-functions) for all inner nodes of the parse tree in
depth-first order.

In some very simple applications (e.g. code generation for stack machines) it is
possible to perform the task at hand directly in the reduce functions. But
usually the reduce functions are only used to build an in-memory data structure
with the relevant information from the parse tree. This data structure is called
an abstract syntax tree (AST).

The exact format for the abstract syntax tree is application specific (while the
format of the parse tree and token list are mostly dictated by the grammar of
the language at hand). :numref:`Figure %s <fig:Basics_ast>` illustrates what an
AST for the parse tree in :numref:`Fig. %s <fig:Basics_parsetree>` could look
like.

Usually the AST is then converted into yet another representation that is more
suitable for further processing. In compilers this is often an assembler-like
three-address-code intermediate representation. :cite:p:`Dragonbook`

.. figure:: /_images/primer/basics_ast.*
	:class: width-helper invert-helper
	:name: fig:Basics_ast

	Example abstract syntax tree for the Verilog expression 
	:verilog:`assign foo = bar + 42;`


Multi-pass compilation
~~~~~~~~~~~~~~~~~~~~~~

Complex problems are often best solved when split up into smaller problems. This
is certainly true for compilers as well as for synthesis tools. The components
responsible for solving the smaller problems can be connected in two different
ways: through Single-Pass Pipelining and by using Multiple Passes.

Traditionally a parser and lexer are connected using the pipelined approach: The
lexer provides a function that is called by the parser. This function reads data
from the input until a complete lexical token has been read. Then this token is
returned to the parser. So the lexer does not first generate a complete list of
lexical tokens and then pass it to the parser. Instead they run concurrently and
the parser can consume tokens as the lexer produces them.

The single-pass pipelining approach has the advantage of lower memory footprint
(at no time must the complete design be kept in memory) but has the disadvantage
of tighter coupling between the interacting components.

Therefore single-pass pipelining should only be used when the lower memory
footprint is required or the components are also conceptually tightly coupled.
The latter certainly is the case for a parser and its lexer. But when data is
passed between two conceptually loosely coupled components it is often
beneficial to use a multi-pass approach.

In the multi-pass approach the first component processes all the data and the
result is stored in a in-memory data structure. Then the second component is
called with this data. This reduces complexity, as only one component is running
at a time. It also improves flexibility as components can be exchanged easier.

Most modern compilers are multi-pass compilers.

Static Single Assignment (SSA) form
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In imperative programming (and behavioural HDL design) it is possible to assign
the same variable multiple times. This can either mean that the variable is
independently used in two different contexts or that the final value of the
variable depends on a condition.

The following examples show C code in which one variable is used independently
in two different contexts:

.. code:: c++
   :number-lines:

   void demo1()
   {
       int a = 1;
       printf("%d\n", a);

       a = 2;
       printf("%d\n", a);
   }

.. code:: c++

   void demo1()
   {
       int a = 1;
       printf("%d\n", a);

       int b = 2;
       printf("%d\n", b);
   }

.. code:: c++
   :number-lines:

   void demo2(bool foo)
   {
       int a;
       if (foo) {
           a = 23;
           printf("%d\n", a);
       } else {
           a = 42;
           printf("%d\n", a);
       }
   }

.. code:: c++

   void demo2(bool foo)
   {
       int a, b;
       if (foo) {
           a = 23;
           printf("%d\n", a);
       } else {
           b = 42;
           printf("%d\n", b);
       }
   }

In both examples the left version (only variable ``a``) and the right version
(variables ``a`` and ``b``) are equivalent. Therefore it is desired for further
processing to bring the code in an equivalent form for both cases.

In the following example the variable is assigned twice but it cannot be easily
replaced by two variables:

.. code:: c++

   void demo3(bool foo)
   {
       int a = 23
       if (foo)
           a = 42;
       printf("%d\n", a);
   }

Static single assignment (SSA) form is a representation of imperative code that
uses identical representations for the left and right version of demos 1 and 2,
but can still represent demo 3. In SSA form each assignment assigns a new
variable (usually written with an index). But it also introduces a special
:math:`\Phi`-function to merge the different instances of a variable when
needed. In C-pseudo-code the demo 3 would be written as follows using SSA from:

.. code:: c++

   void demo3(bool foo)
   {
       int a_1, a_2, a_3;
       a_1 = 23
       if (foo)
           a_2 = 42;
       a_3 = phi(a_1, a_2);
       printf("%d\n", a_3);
   }

The :math:`\Phi`-function is usually interpreted as "these variables must be
stored in the same memory location" during code generation. Most modern
compilers for imperative languages such as C/C++ use SSA form for at least some
of its passes as it is very easy to manipulate and analyse.

.. [1]
   In recent years formal equivalence checking also became an important
   verification method for validating RTL and lower abstraction
   representation of the design.



getting_started/example_synth.rst
--------------------------------------
Synthesis starter
-----------------

This page will be a guided walkthrough of the prepackaged iCE40 FPGA synthesis
script - :cmd:ref:`synth_ice40`.  We will take a simple design through each
step, looking at the commands being called and what they do to the design. While
:cmd:ref:`synth_ice40` is specific to the iCE40 platform, most of the operations
we will be discussing are common across the majority of FPGA synthesis scripts.
Thus, this document will provide a good foundational understanding of how
synthesis in Yosys is performed, regardless of the actual architecture being
used.

.. seealso:: Advanced usage docs for
   :doc:`/using_yosys/synthesis/synth`

Demo design
~~~~~~~~~~~

.. role:: yoscrypt(code)
   :language: yoscrypt

First, let's quickly look at the design we'll be synthesizing:

.. todo:: reconsider including the whole (~77 line) design like this

.. literalinclude:: /code_examples/fifo/fifo.v
   :language: Verilog
   :linenos:
   :caption: :file:`fifo.v`
   :name: fifo-v

.. todo:: fifo.v description

Loading the design
~~~~~~~~~~~~~~~~~~

Let's load the design into Yosys.  From the command line, we can call ``yosys
fifo.v``.  This will open an interactive Yosys shell session and immediately
parse the code from :ref:`fifo-v` and convert it into an Abstract Syntax Tree
(AST).  If you are interested in how this happens, there is more information in
the document, :doc:`/yosys_internals/flow/verilog_frontend`.  For now, suffice
it to say that we do this to simplify further processing of the design.  You
should see something like the following:

.. literalinclude:: /code_examples/fifo/fifo.out
   :language: console
   :start-at: $ yosys fifo.v
   :end-before: echo on

.. seealso:: Advanced usage docs for
   :doc:`/using_yosys/more_scripting/load_design`

Elaboration
~~~~~~~~~~~

Now that we are in the interactive shell, we can call Yosys commands directly.
Our overall goal is to call :yoscrypt:`synth_ice40 -top fifo`, but for now we
can run each of the commands individually for a better sense of how each part
contributes to the flow.  We will also start with just a single module;
``addr_gen``.

At the bottom of the :cmd:ref:`help` output for
:cmd:ref:`synth_ice40` is the complete list of commands called by this script.
Let's start with the section labeled ``begin``:

.. literalinclude:: /cmd/synth_ice40.rst
   :language: yoscrypt
   :start-after: begin:
   :end-before: flatten:
   :dedent:
   :caption: ``begin`` section
   :name: synth_begin

:yoscrypt:`read_verilog -D ICE40_HX -lib -specify +/ice40/cells_sim.v` loads the
iCE40 cell models which allows us to include platform specific IP blocks in our
design.  PLLs are a common example of this, where we might need to reference
``SB_PLL40_CORE`` directly rather than being able to rely on mapping passes
later.  Since our simple design doesn't use any of these IP blocks, we can skip
this command for now.  Because these cell models will also be needed once we
start mapping to hardware we will still need to load them later.

.. note::

   ``+/`` is a dynamic reference to the Yosys ``share`` directory.  By default,
   this is ``/usr/local/share/yosys``.  If using a locally built version of
   Yosys from the source directory, this will be the ``share`` folder in the
   same directory.

.. _addr_gen_example:

The addr_gen module
^^^^^^^^^^^^^^^^^^^

Since we're just getting started, let's instead begin with :yoscrypt:`hierarchy
-top addr_gen`.  This command declares that the top level module is
``addr_gen``, and everything else can be discarded.

.. literalinclude:: /code_examples/fifo/fifo.v
   :language: Verilog
   :start-at: module addr_gen
   :end-at: endmodule //addr_gen
   :lineno-match:
   :caption: ``addr_gen`` module source
   :name: addr_gen-v

.. note::

   :cmd:ref:`hierarchy` should always be the first command after the design has
   been read.  By specifying the top module, :cmd:ref:`hierarchy` will also set
   the ``(* top *)`` attribute on it.  This is used by other commands that need
   to know which module is the top.

.. use doscon for a console-like display that supports the `yosys> [command]` format.

.. literalinclude:: /code_examples/fifo/fifo.out
   :language: doscon
   :start-at: yosys> hierarchy -top addr_gen
   :end-before: yosys> select
   :caption: :yoscrypt:`hierarchy -top addr_gen` output
   :name: hierarchy_output

Our ``addr_gen`` circuit now looks like this:

.. figure:: /_images/code_examples/fifo/addr_gen_hier.*
   :class: width-helper invert-helper
   :name: addr_gen_hier

   ``addr_gen`` module after :cmd:ref:`hierarchy`

Simple operations like ``addr + 1`` and ``addr == MAX_DATA-1`` can be extracted
from our ``always @`` block in :ref:`addr_gen-v`. This gives us the highlighted
``$add`` and ``$eq`` cells we see. But control logic (like the ``if .. else``)
and memory elements (like the ``addr <= 0``) are not so straightforward.  These
get put into "processes", shown in the schematic as ``PROC``.  Note how the
second line refers to the line numbers of the start/end of the corresponding
``always @`` block.  In the case of an ``initial`` block, we instead see the
``PROC`` referring to line 0.

To handle these, let us now introduce the next command: :doc:`/cmd/proc`.
:cmd:ref:`proc` is a macro command like :cmd:ref:`synth_ice40`.  Rather than
modifying the design directly, it instead calls a series of other commands.  In
the case of :cmd:ref:`proc`, these sub-commands work to convert the behavioral
logic of processes into multiplexers and registers.  Let's see what happens when
we run it.  For now, we will call :yoscrypt:`proc -noopt` to prevent some
automatic optimizations which would normally happen.

.. figure:: /_images/code_examples/fifo/addr_gen_proc.*
   :class: width-helper invert-helper
   :name: addr_gen_proc

   ``addr_gen`` module after :yoscrypt:`proc -noopt`

There are now a few new cells from our ``always @``, which have been
highlighted.  The ``if`` statements are now modeled with ``$mux`` cells, while
the register uses an ``$adff`` cell.  If we look at the terminal output we can
also see all of the different ``proc_*`` commands being called.  We will look at
each of these in more detail in :doc:`/using_yosys/synthesis/proc`.

Notice how in the top left of :ref:`addr_gen_proc` we have a floating wire,
generated from the initial assignment of 0 to the ``addr`` wire.  However, this
initial assignment is not synthesizable, so this will need to be cleaned up
before we can generate the physical hardware.  We can do this now by calling
:cmd:ref:`clean`.  We're also going to call :cmd:ref:`opt_expr` now, which would
normally be called at the end of :cmd:ref:`proc`.  We can call both commands at
the same time by separating them with a colon and space: :yoscrypt:`opt_expr;
clean`.

.. figure:: /_images/code_examples/fifo/addr_gen_clean.*
   :class: width-helper invert-helper
   :name: addr_gen_clean

   ``addr_gen`` module after :yoscrypt:`opt_expr; clean`

You may also notice that the highlighted ``$eq`` cell input of ``255`` has
changed to ``8'11111111``.  Constant values are presented in the format
``<bit_width>'<bits>``, with 32-bit values instead using the decimal number.
This indicates that the constant input has been reduced from 32-bit wide to
8-bit wide.  This is a side-effect of running :cmd:ref:`opt_expr`, which
performs constant folding and simple expression rewriting.    For more on why
this happens, refer to :doc:`/using_yosys/synthesis/opt` and the :ref:`section
on opt_expr <adv_opt_expr>`.

.. note::

   :doc:`/cmd/clean` can also be called with two semicolons after any command,
   for example we could have called :yoscrypt:`opt_expr;;` instead of
   :yoscrypt:`opt_expr; clean`.  You may notice some scripts will end each line
   with ``;;``.  It is beneficial to run :cmd:ref:`clean` before inspecting
   intermediate products to remove disconnected parts of the circuit which have
   been left over, and in some cases can reduce the processing required in
   subsequent commands.

.. todo:: consider a brief glossary for terms like adff

.. seealso:: Advanced usage docs for
   
   - :doc:`/using_yosys/synthesis/proc`
   - :doc:`/using_yosys/synthesis/opt`

The full example
^^^^^^^^^^^^^^^^

Let's now go back and check on our full design by using :yoscrypt:`hierarchy
-check -top fifo`.  By passing the ``-check`` option there we are also telling
the :cmd:ref:`hierarchy` command that if the design includes any non-blackbox
modules without an implementation it should return an error.

Note that if we tried to run this command now then we would get an error.  This
is because we already removed all of the modules other than ``addr_gen``.  We
could restart our shell session, but instead let's use two new commands:

- :doc:`/cmd/design`, and
- :doc:`/cmd/read_verilog`.

.. literalinclude:: /code_examples/fifo/fifo.out
   :language: doscon
   :start-at: design -reset
   :end-before: yosys> proc
   :caption: reloading :file:`fifo.v` and running :yoscrypt:`hierarchy -check -top fifo`

Notice how this time we didn't see any of those `$abstract` modules?  That's
because when we ran ``yosys fifo.v``, the first command Yosys called was
:yoscrypt:`read_verilog -defer fifo.v`.  The ``-defer`` option there tells
:cmd:ref:`read_verilog` only read the abstract syntax tree and defer actual
compilation to a later :cmd:ref:`hierarchy` command. This is useful in cases
where the default parameters of modules yield invalid code which is not
synthesizable. This is why Yosys defers compilation automatically and is one of
the reasons why hierarchy should always be the first command after loading the
design.  If we know that our design won't run into this issue, we can skip the
``-defer``.

.. todo:: :cmd:ref:`hierarchy` failure modes

.. note::

   The number before a command's output increments with each command run.  Don't
   worry if your numbers don't match ours!  The output you are seeing comes from
   the same script that was used to generate the images in this document,
   included in the source as :file:`fifo.ys`. There are extra commands being run
   which you don't see, but feel free to try them yourself, or play around with
   different commands.  You can always start over with a clean slate by calling
   ``exit`` or hitting :kbd:`ctrl+d` (i.e. EOF) and re-launching the Yosys
   interactive terminal.  :kbd:`ctrl+c` (i.e. SIGINT) will also end the terminal
   session but will return an error code rather than exiting gracefully.

We can also run :cmd:ref:`proc` now to finish off the full :ref:`synth_begin`.
Because the design schematic is quite large, we will be showing just the data
path for the ``rdata`` output.  If you would like to see the entire design for
yourself, you can do so with :doc:`/cmd/show`.  Note that the :cmd:ref:`show`
command only works with a single module, so you may need to call it with
:yoscrypt:`show fifo`.  :ref:`show_intro` section in
:doc:`/getting_started/scripting_intro` has more on how to use :cmd:ref:`show`.

.. figure:: /_images/code_examples/fifo/rdata_proc.*
   :class: width-helper invert-helper
   :name: rdata_proc

   ``rdata`` output after :cmd:ref:`proc`

The highlighted ``fifo_reader`` block contains an instance of the
:ref:`addr_gen_proc` that we looked at earlier.  Notice how the type is shown as
``$paramod\\addr_gen\\MAX_DATA=s32'...``.  This is a "parametric module": an
instance of the ``addr_gen`` module with the ``MAX_DATA`` parameter set to the
given value.

The other highlighted block is a ``$memrd`` cell.  At this stage of synthesis we
don't yet know what type of memory is going to be implemented, but we *do* know
that ``rdata <= data[raddr];`` could be implemented as a read from memory. Note
that the ``$memrd`` cell here is asynchronous, with both the clock and enable
signal undefined; shown with the ``1'x`` inputs.

.. seealso:: Advanced usage docs for
   :doc:`/using_yosys/synthesis/proc`

Flattening
~~~~~~~~~~

At this stage of a synthesis flow there are a few other commands we could run.
In :cmd:ref:`synth_ice40` we get these:

.. literalinclude:: /cmd/synth_ice40.rst
   :language: yoscrypt
   :start-after: flatten:
   :end-before: coarse:
   :dedent:
   :name: synth_flatten
   :caption: ``flatten`` section

First off is :cmd:ref:`flatten`.  Flattening the design like this can allow for
optimizations between modules which would otherwise be missed.  Let's run
:yoscrypt:`flatten;;` on our design.

.. literalinclude:: /code_examples/fifo/fifo.out
   :language: doscon
   :start-at: yosys> flatten
   :end-before: yosys> select
   :name: flat_clean
   :caption: output of :yoscrypt:`flatten;;`

.. figure:: /_images/code_examples/fifo/rdata_flat.*
   :class: width-helper invert-helper
   :name: rdata_flat

   ``rdata`` output after :yoscrypt:`flatten;;`

.. role:: yoterm(code)
   :language: doscon

The pieces have moved around a bit, but we can see :ref:`addr_gen_proc` from
earlier has replaced the ``fifo_reader`` block in :ref:`rdata_proc`.  We can
also see that the ``addr`` output has been renamed to :file:`fifo_reader.addr`
and merged with the ``raddr`` wire feeding into the ``$memrd`` cell.  This wire
merging happened during the call to :cmd:ref:`clean` which we can see in the
:ref:`flat_clean`.

.. note:: 

   :cmd:ref:`flatten` and :cmd:ref:`clean` would normally be combined into a
   single :yoterm:`yosys> flatten;;` output, but they appear separately here as
   a side effect of using :cmd:ref:`echo` for generating the terminal style
   output.

Depending on the target architecture, this stage of synthesis might also see
commands such as :cmd:ref:`tribuf` with the ``-logic`` option and
:cmd:ref:`deminout`.  These remove tristate and inout constructs respectively,
replacing them with logic suitable for mapping to an FPGA.  Since we do not have
any such constructs in our example running these commands does not change our
design.

The coarse-grain representation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

At this stage, the design is in coarse-grain representation.  It still looks
recognizable, and cells are word-level operators with parametrizable width. This
is the stage of synthesis where we do things like const propagation, expression
rewriting, and trimming unused parts of wires.

This is also where we convert our FSMs and hard blocks like DSPs or memories.
Such elements have to be inferred from patterns in the design and there are
special passes for each.  Detection of these patterns can also be affected by
optimizations and other transformations done previously.

.. note::

   While the iCE40 flow had a :ref:`synth_flatten` and put :cmd:ref:`proc` in
   the :ref:`synth_begin`, some synthesis scripts will instead include these in
   this section.

Part 1
^^^^^^

In the iCE40 flow, we start with the following commands:

.. literalinclude:: /cmd/synth_ice40.rst
   :language: yoscrypt
   :start-after: coarse:
   :end-before: wreduce
   :dedent:
   :caption: ``coarse`` section (part 1)
   :name: synth_coarse1

We've already come across :cmd:ref:`opt_expr`, and :cmd:ref:`opt_clean` is the
same as :cmd:ref:`clean` but with more verbose output.  The :cmd:ref:`check`
pass identifies a few obvious problems which will cause errors later.  Calling
it here lets us fail faster rather than wasting time on something we know is
impossible.

Next up is :yoscrypt:`opt -nodffe -nosdff` performing a set of simple
optimizations on the design.  This command also ensures that only a specific
subset of FF types are included, in preparation for the next command:
:doc:`/cmd/fsm`.  Both :cmd:ref:`opt` and :cmd:ref:`fsm` are macro commands
which are explored in more detail in :doc:`/using_yosys/synthesis/opt` and
:doc:`/using_yosys/synthesis/fsm` respectively.

Up until now, the data path for ``rdata`` has remained the same since
:ref:`rdata_flat`.  However the next call to :cmd:ref:`opt` does cause a change.
Specifically, the call to :cmd:ref:`opt_dff` without the ``-nodffe -nosdff``
options is able to fold one of the ``$mux`` cells into the ``$adff`` to form an
``$adffe`` cell; highlighted below:

.. literalinclude:: /code_examples/fifo/fifo.out
   :language: doscon
   :start-at: yosys> opt_dff
   :end-before: yosys> select
   :caption: output of :cmd:ref:`opt_dff`

.. figure:: /_images/code_examples/fifo/rdata_adffe.*
   :class: width-helper invert-helper
   :name: rdata_adffe

   ``rdata`` output after :cmd:ref:`opt_dff`

.. seealso:: Advanced usage docs for
   
   - :doc:`/using_yosys/synthesis/fsm`
   - :doc:`/using_yosys/synthesis/opt`

Part 2
^^^^^^

The next group of commands performs a series of optimizations:

.. literalinclude:: /cmd/synth_ice40.rst
   :language: yoscrypt
   :start-at: wreduce
   :end-before: t:$mul
   :dedent:
   :caption: ``coarse`` section (part 2)
   :name: synth_coarse2

First up is :doc:`/cmd/wreduce`.  If we run this we get the following:

.. literalinclude:: /code_examples/fifo/fifo.out
   :language: doscon
   :start-at: yosys> wreduce
   :end-before: yosys> select
   :caption: output of :cmd:ref:`wreduce`

Looking at the data path for ``rdata``, the most relevant of these width
reductions are the ones affecting ``fifo.$flatten\fifo_reader.$add$fifo.v``.
That is the ``$add`` cell incrementing the fifo_reader address.  We can look at
the schematic and see the output of that cell has now changed.

.. todo:: pending bugfix in :cmd:ref:`wreduce` and/or :cmd:ref:`opt_clean`

.. figure:: /_images/code_examples/fifo/rdata_wreduce.*
   :class: width-helper invert-helper
   :name: rdata_wreduce

   ``rdata`` output after :cmd:ref:`wreduce`

The next two (new) commands are :doc:`/cmd/peepopt` and :doc:`/cmd/share`.
Neither of these affect our design, and they're explored in more detail in
:doc:`/using_yosys/synthesis/opt`, so let's skip over them.  :yoscrypt:`techmap
-map +/cmp2lut.v -D LUT_WIDTH=4` optimizes certain comparison operators by
converting them to LUTs instead.  The usage of :cmd:ref:`techmap` is explored
more in :doc:`/using_yosys/synthesis/techmap_synth`.

Our next command to run is
:doc:`/cmd/memory_dff`.

.. literalinclude:: /code_examples/fifo/fifo.out
   :language: doscon
   :start-at: yosys> memory_dff
   :end-before: yosys> select
   :caption: output of :cmd:ref:`memory_dff`

.. figure:: /_images/code_examples/fifo/rdata_memrdv2.*
   :class: width-helper invert-helper
   :name: rdata_memrdv2

   ``rdata`` output after :cmd:ref:`memory_dff`

As the title suggests, :cmd:ref:`memory_dff` has merged the output ``$dff`` into
the ``$memrd`` cell and converted it to a ``$memrd_v2`` (highlighted).  This has
also connected the ``CLK`` port to the ``clk`` input as it is now a synchronous
memory read with appropriate enable (``EN=1'1``) and reset (``ARST=1'0`` and
``SRST=1'0``) inputs.

.. seealso:: Advanced usage docs for
   
   - :doc:`/using_yosys/synthesis/opt`
   - :doc:`/using_yosys/synthesis/techmap_synth`
   - :doc:`/using_yosys/synthesis/memory`

Part 3
^^^^^^

The third part of the :cmd:ref:`synth_ice40` flow is a series of commands for
mapping to DSPs.  By default, the iCE40 flow will not map to the hardware DSP
blocks and will only be performed if called with the ``-dsp`` flag:
:yoscrypt:`synth_ice40 -dsp`.  While our example has nothing that could be
mapped to DSPs we can still take a quick look at the commands here and describe
what they do.

.. literalinclude:: /cmd/synth_ice40.rst
   :language: yoscrypt
   :start-at: t:$mul
   :end-before: alumacc
   :dedent:
   :caption: ``coarse`` section (part 3)
   :name: synth_coarse3

:yoscrypt:`wreduce t:$mul` performs width reduction again, this time targetting
only cells of type ``$mul``.  :yoscrypt:`techmap -map +/mul2dsp.v -map
+/ice40/dsp_map.v ... -D DSP_NAME=$__MUL16X16` uses :cmd:ref:`techmap` to map
``$mul`` cells to ``$__MUL16X16`` which are, in turn, mapped to the iCE40
``SB_MAC16``.  Any multipliers which aren't compatible with conversion to
``$__MUL16X16`` are relabelled to ``$__soft_mul`` before :cmd:ref:`chtype`
changes them back to ``$mul``.

During the mul2dsp conversion, some of the intermediate signals are marked with
the attribute ``mul2dsp``.  By calling :yoscrypt:`select a:mul2dsp` we restrict
the following commands to only operate on the cells and wires used for these
signals.  :cmd:ref:`setattr` removes the now unnecessary ``mul2dsp`` attribute.
:cmd:ref:`opt_expr` we've already come across for const folding and simple
expression rewriting, the ``-fine`` option just enables more fine-grain
optimizations.  Then we perform width reduction a final time and clear the
selection.

.. todo:: ``ice40_dsp`` is pmgen

Finally we have :cmd:ref:`ice40_dsp`: similar to the :cmd:ref:`memory_dff`
command we saw in the previous section, this merges any surrounding registers
into the ``SB_MAC16`` cell.  This includes not just the input/output registers,
but also pipeline registers and even a post-adder where applicable: turning a
multiply + add into a single multiply-accumulate.

.. seealso:: Advanced usage docs for
   :doc:`/using_yosys/synthesis/techmap_synth`

Part 4
^^^^^^

That brings us to the fourth and final part for the iCE40 synthesis flow:

.. literalinclude:: /cmd/synth_ice40.rst
   :language: yoscrypt
   :start-at: alumacc
   :end-before: map_ram:
   :dedent:
   :caption: ``coarse`` section (part 4)
   :name: synth_coarse4

Where before each type of arithmetic operation had its own cell, e.g. ``$add``,
we now want to extract these into ``$alu`` and ``$macc`` cells which can help
identify opportunities for reusing logic.  We do this by running
:cmd:ref:`alumacc`, which we can see produce the following changes in our
example design:

.. literalinclude:: /code_examples/fifo/fifo.out
   :language: doscon
   :start-at: yosys> alumacc
   :end-before: yosys> select
   :caption: output of :cmd:ref:`alumacc`

.. figure:: /_images/code_examples/fifo/rdata_alumacc.*
   :class: width-helper invert-helper
   :name: rdata_alumacc

   ``rdata`` output after :cmd:ref:`alumacc`

Once these cells have been inserted, the call to :cmd:ref:`opt` can combine
cells which are now identical but may have been missed due to e.g. the
difference between ``$add`` and ``$sub``.

The other new command in this part is :doc:`/cmd/memory`.  :cmd:ref:`memory` is
another macro command which we examine in more detail in
:doc:`/using_yosys/synthesis/memory`.  For this document, let us focus just on
the step most relevant to our example: :cmd:ref:`memory_collect`. Up until this
point, our memory reads and our memory writes have been totally disjoint cells;
operating on the same memory only in the abstract. :cmd:ref:`memory_collect`
combines all of the reads and writes for a memory block into a single cell.

.. figure:: /_images/code_examples/fifo/rdata_coarse.*
   :class: width-helper invert-helper
   :name: rdata_coarse

   ``rdata`` output after :cmd:ref:`memory_collect`

Looking at the schematic after running :cmd:ref:`memory_collect` we see that our
``$memrd_v2`` cell has been replaced with a ``$mem_v2`` cell named ``data``, the
same name that we used in :ref:`fifo-v`. Where before we had a single set of
signals for address and enable, we now have one set for reading (``RD_*``) and
one for writing (``WR_*``), as well as both ``WR_DATA`` input and ``RD_DATA``
output.

.. seealso:: Advanced usage docs for

   - :doc:`/using_yosys/synthesis/opt`
   - :doc:`/using_yosys/synthesis/memory`

Final note
^^^^^^^^^^

Having now reached the end of the the coarse-grain representation, we could also
have gotten here by running :yoscrypt:`synth_ice40 -top fifo -run :map_ram`
after loading the design.  The :yoscrypt:`-run <from_label>:<to_label>` option
with an empty ``<from_label>`` starts from the :ref:`synth_begin`, while the
``<to_label>`` runs up to but including the :ref:`map_ram`.

Hardware mapping
~~~~~~~~~~~~~~~~

The remaining sections each map a different type of hardware and are much more
architecture dependent than the previous sections.  As such we will only be
looking at each section very briefly.

If you skipped calling :yoscrypt:`read_verilog -D ICE40_HX -lib -specify
+/ice40/cells_sim.v` earlier, do it now.

Memory blocks
^^^^^^^^^^^^^

Mapping to hard memory blocks uses a combination of :cmd:ref:`memory_libmap` and
:cmd:ref:`techmap`.

.. literalinclude:: /cmd/synth_ice40.rst
   :language: yoscrypt
   :start-after: map_ram:
   :end-before: map_ffram:
   :dedent:
   :name: map_ram
   :caption: ``map_ram`` section

.. figure:: /_images/code_examples/fifo/rdata_map_ram.*
   :class: width-helper invert-helper
   :name: rdata_map_ram

   ``rdata`` output after :ref:`map_ram`

The :ref:`map_ram` converts the generic ``$mem_v2`` into the iCE40
``SB_RAM40_4K`` (highlighted). We can also see the memory address has been
remapped, and the data bits have been reordered (or swizzled).  There is also
now a ``$mux`` cell controlling the value of ``rdata``.  In :ref:`fifo-v` we
wrote our memory as read-before-write, however the ``SB_RAM40_4K`` has undefined
behaviour when reading from and writing to the same address in the same cycle.
As a result, extra logic is added so that the generated circuit matches the
behaviour of the verilog.  :ref:`no_rw_check` describes how we could change our
verilog to match our hardware instead.

If we run :cmd:ref:`memory_libmap` under the :cmd:ref:`debug` command we can see
candidates which were identified for mapping, along with the costs of each and
what logic requires emulation.

.. literalinclude:: /code_examples/fifo/fifo.libmap
   :language: doscon
   :lines: 2, 6-

The ``$__ICE40_RAM4K_`` cell is defined in the file |techlibs/ice40/brams.txt|_,
with the mapping to ``SB_RAM40_4K`` done by :cmd:ref:`techmap` using
|techlibs/ice40/brams_map.v|_.  Any leftover memory cells are then converted
into flip flops (the ``logic fallback``) with :cmd:ref:`memory_map`.

.. |techlibs/ice40/brams.txt| replace:: :file:`techlibs/ice40/brams.txt`
.. _techlibs/ice40/brams.txt: https://github.com/YosysHQ/yosys/tree/main/techlibs/ice40/brams.txt
.. |techlibs/ice40/brams_map.v| replace:: :file:`techlibs/ice40/brams_map.v`
.. _techlibs/ice40/brams_map.v: https://github.com/YosysHQ/yosys/tree/main/techlibs/ice40/brams_map.v

.. literalinclude:: /cmd/synth_ice40.rst
   :language: yoscrypt
   :start-after: map_ffram:
   :end-before: map_gates:
   :dedent:
   :name: map_ffram
   :caption: ``map_ffram`` section

.. figure:: /_images/code_examples/fifo/rdata_map_ffram.*
   :class: width-helper invert-helper
   :name: rdata_map_ffram

   ``rdata`` output after :ref:`map_ffram`

.. note::

   The visual clutter on the ``RDATA`` output port (highlighted) is an
   unfortunate side effect of :cmd:ref:`opt_clean` on the swizzled data bits. In
   connecting the ``$mux`` input port directly to ``RDATA`` to reduce the number
   of wires, the ``$techmap579\data.0.0.RDATA`` wire becomes more visually
   complex.

.. seealso:: Advanced usage docs for
   
   - :doc:`/using_yosys/synthesis/techmap_synth`
   - :doc:`/using_yosys/synthesis/memory`

Arithmetic
^^^^^^^^^^

Uses :cmd:ref:`techmap` to map basic arithmetic logic to hardware.  This sees
somewhat of an explosion in cells as multi-bit ``$mux`` and ``$adffe`` are
replaced with single-bit ``$_MUX_`` and ``$_DFFE_PP0P_`` cells, while the
``$alu`` is replaced with primitive ``$_OR_`` and ``$_NOT_`` gates and a
``$lut`` cell.

.. literalinclude:: /cmd/synth_ice40.rst
   :language: yoscrypt
   :start-after: map_gates:
   :end-before: map_ffs:
   :dedent:
   :name: map_gates
   :caption: ``map_gates`` section

.. figure:: /_images/code_examples/fifo/rdata_map_gates.*
   :class: width-helper invert-helper
   :name: rdata_map_gates

   ``rdata`` output after :ref:`map_gates`

.. seealso:: Advanced usage docs for
   :doc:`/using_yosys/synthesis/techmap_synth`

Flip-flops
^^^^^^^^^^

Convert FFs to the types supported in hardware with :cmd:ref:`dfflegalize`, and
then use :cmd:ref:`techmap` to map them.  In our example, this converts the
``$_DFFE_PP0P_`` cells to ``SB_DFFER``.

We also run :cmd:ref:`simplemap` here to convert any remaining cells which could
not be mapped to hardware into gate-level primitives.  This includes optimizing
``$_MUX_`` cells where one of the inputs is a constant ``1'0``, replacing it
instead with an ``$_AND_`` cell.

.. literalinclude:: /cmd/synth_ice40.rst
   :language: yoscrypt
   :start-after: map_ffs:
   :end-before: map_luts:
   :dedent:
   :name: map_ffs
   :caption: ``map_ffs`` section

.. figure:: /_images/code_examples/fifo/rdata_map_ffs.*
   :class: width-helper invert-helper
   :name: rdata_map_ffs

   ``rdata`` output after :ref:`map_ffs`

.. seealso:: Advanced usage docs for
   :doc:`/using_yosys/synthesis/techmap_synth`

LUTs
^^^^

:cmd:ref:`abc` and :cmd:ref:`techmap` are used to map LUTs; converting primitive
cell types to use ``$lut`` and ``SB_CARRY`` cells.  Note that the iCE40 flow
uses :cmd:ref:`abc9` rather than :cmd:ref:`abc`. For more on what these do, and
what the difference between these two commands are, refer to
:doc:`/using_yosys/synthesis/abc`.

.. literalinclude:: /cmd/synth_ice40.rst
   :language: yoscrypt
   :start-after: map_luts:
   :end-before: map_cells:
   :dedent:
   :name: map_luts
   :caption: ``map_luts`` section

.. figure:: /_images/code_examples/fifo/rdata_map_luts.*
   :class: width-helper invert-helper
   :name: rdata_map_luts

   ``rdata`` output after :ref:`map_luts`

Finally we use :cmd:ref:`techmap` to map the generic ``$lut`` cells to iCE40
``SB_LUT4`` cells.

.. literalinclude:: /cmd/synth_ice40.rst
   :language: yoscrypt
   :start-after: map_cells:
   :end-before: check:
   :dedent:
   :name: map_cells
   :caption: ``map_cells`` section

.. figure:: /_images/code_examples/fifo/rdata_map_cells.*
   :class: width-helper invert-helper
   :name: rdata_map_cells

   ``rdata`` output after :ref:`map_cells`

.. seealso:: Advanced usage docs for
   
   - :doc:`/using_yosys/synthesis/techmap_synth`
   - :doc:`/using_yosys/synthesis/abc`

Other cells
^^^^^^^^^^^

The following commands may also be used for mapping other cells:

:cmd:ref:`hilomap`
    Some architectures require special driver cells for driving a constant hi or
    lo value. This command replaces simple constants with instances of such
    driver cells.

:cmd:ref:`iopadmap`
    Top-level input/outputs must usually be implemented using special I/O-pad
    cells. This command inserts such cells to the design.

These commands tend to either be in the :ref:`map_cells` or after the
:ref:`check` depending on the flow.

Final steps
~~~~~~~~~~~~

The next section of the iCE40 synth flow performs some sanity checking and final
tidy up:

.. literalinclude:: /cmd/synth_ice40.rst
   :language: yoscrypt
   :start-after: check:
   :end-before: blif:
   :dedent:
   :name: check
   :caption: ``check`` section

The new commands here are:

- :doc:`/cmd/autoname`,
- :doc:`/cmd/stat`, and
- :doc:`/cmd/blackbox`.

The output from :cmd:ref:`stat` is useful for checking resource utilization;
providing a list of cells used in the design and the number of each, as well as
the number of other resources used such as wires and processes.  For this
design, the final call to :cmd:ref:`stat` should look something like the
following:

.. literalinclude:: /code_examples/fifo/fifo.stat
   :language: doscon
   :start-at: yosys> stat -top fifo

Note that the :yoscrypt:`-top fifo` here is optional.  :cmd:ref:`stat` will
automatically use the module with the ``top`` attribute set, which ``fifo`` was
when we called :cmd:ref:`hierarchy`.  If no module is marked ``top``, then stats
will be shown for each module selected.

The :cmd:ref:`stat` output is also useful as a kind of sanity-check: Since we
have already run :cmd:ref:`proc`, we wouldn't expect there to be any processes.
We also expect ``data`` to use hard memory; if instead of an ``SB_RAM40_4K`` saw
a high number of flip-flops being used we might suspect something was wrong.

If we instead called :cmd:ref:`stat` immediately after :yoscrypt:`read_verilog
fifo.v` we would see something very different:

.. literalinclude:: /code_examples/fifo/fifo.stat
   :language: doscon
   :start-at: yosys> stat
   :end-before: yosys> stat -top fifo

Notice how ``fifo`` and ``addr_gen`` are listed separately, and the statistics
for ``fifo`` show 2 ``addr_gen`` modules.  Because this is before the memory has
been mapped, we also see that there is 1 memory with 2048 memory bits; matching
our 8-bit wide ``data`` memory with 256 values (:math:`8*256=2048`).

Synthesis output
^^^^^^^^^^^^^^^^

The iCE40 synthesis flow has the following output modes available:

- :doc:`/cmd/write_blif`,
- :doc:`/cmd/write_edif`, and
- :doc:`/cmd/write_json`.

As an example, if we called :yoscrypt:`synth_ice40 -top fifo -json fifo.json`,
our synthesized ``fifo`` design will be output as :file:`fifo.json`.  We can
then read the design back into Yosys with :cmd:ref:`read_json`, but make sure
you use :yoscrypt:`design -reset` or open a new interactive terminal first.  The
JSON output we get can also be loaded into `nextpnr`_ to do place and route; but
that is beyond the scope of this documentation.

.. _nextpnr: https://github.com/YosysHQ/nextpnr

.. seealso:: :doc:`/cmd/synth_ice40`



getting_started/index.rst
--------------------------------------
Getting started with Yosys
==========================

This section covers how to get started with Yosys, from installation to a guided
walkthrough of synthesizing a design for hardware, and finishing with an
introduction to writing re-usable Yosys scripts.

.. toctree::
   :maxdepth: 3

   installation
   example_synth
   scripting_intro



getting_started/installation.rst
--------------------------------------
Installation
------------

This document will guide you through the process of installing Yosys.

CAD suite(s)
~~~~~~~~~~~~

Yosys is part of the `Tabby CAD Suite
<https://www.yosyshq.com/tabby-cad-datasheet>`_ and the `OSS CAD Suite
<https://github.com/YosysHQ/oss-cad-suite-build>`_! The easiest way to use yosys
is to install the binary software suite, which contains all required
dependencies and related tools.

* `Contact YosysHQ <https://www.yosyshq.com/contact>`_ for a `Tabby CAD Suite
  <https://www.yosyshq.com/tabby-cad-datasheet>`_ Evaluation License and
  download link
* OR go to https://github.com/YosysHQ/oss-cad-suite-build/releases to download
  the free OSS CAD Suite
* Follow the `Install Instructions on GitHub
  <https://github.com/YosysHQ/oss-cad-suite-build#installation>`_

Make sure to get a Tabby CAD Suite Evaluation License if you need features such
as industry-grade SystemVerilog and VHDL parsers!

For more information about the difference between Tabby CAD Suite and the OSS
CAD Suite, please visit https://www.yosyshq.com/tabby-cad-datasheet

Many Linux distributions also provide Yosys binaries, some more up to date than
others. Check with your package manager!

Targeted architectures
^^^^^^^^^^^^^^^^^^^^^^

The `OSS CAD Suite`_ releases `nightly builds`_ for the following architectures:

.. only:: html

   - linux-x64 |linux-x64|
      - Most personal Linux based computers

   - darwin-x64 |darwin-x64|
      - macOS 12 or later with Intel CPU

   - darwin-arm64 |darwin-arm64|
      - macOS 12 or later with M1/M2 CPU

   - windows-x64 |windows-x64|
      - Targeted for Windows 10 and 11

   - linux-arm64 |linux-arm64|

.. _OSS CAD Suite: https://github.com/YosysHQ/oss-cad-suite-build
.. _nightly builds: https://github.com/YosysHQ/oss-cad-suite-build/releases/latest

.. |linux-x64| image:: https://github.com/YosysHQ/oss-cad-suite-build/actions/workflows/linux-x64.yml/badge.svg
.. |darwin-x64| image:: https://github.com/YosysHQ/oss-cad-suite-build/actions/workflows/darwin-x64.yml/badge.svg
.. |darwin-arm64| image:: https://github.com/YosysHQ/oss-cad-suite-build/actions/workflows/darwin-arm64.yml/badge.svg
.. |windows-x64| image:: https://github.com/YosysHQ/oss-cad-suite-build/actions/workflows/windows-x64.yml/badge.svg
.. |linux-arm64| image:: https://github.com/YosysHQ/oss-cad-suite-build/actions/workflows/linux-arm64.yml/badge.svg

Building from source
~~~~~~~~~~~~~~~~~~~~

Refer to the `readme`_ for the most up-to-date install instructions.

.. _readme: https://github.com/YosysHQ/yosys#building-from-source

Supported platforms
^^^^^^^^^^^^^^^^^^^

The following platforms are supported and regularly tested:

- Linux
- macOS

Other platforms which may work, but instructions may not be up to date and are
not regularly tested:

- FreeBSD
- WSL
- Windows with (e.g.) Cygwin

Build prerequisites
^^^^^^^^^^^^^^^^^^^

A C++ compiler with C++17 support is required as well as some standard tools
such as GNU Flex, GNU Bison, Make and Python.  Some additional tools: readline,
libffi, Tcl and zlib; are optional but enabled by default (see
:makevar:`ENABLE_*` settings in Makefile). Graphviz and Xdot are used by the
:cmd:ref:`show` command to display schematics.

Installing all prerequisites for Ubuntu 20.04:

.. code:: console

   sudo sudo apt-get install build-essential clang bison flex \
      libreadline-dev gawk tcl-dev libffi-dev git make \
      graphviz xdot pkg-config python3 libboost-system-dev \
      libboost-python-dev libboost-filesystem-dev zlib1g-dev

Installing all prerequisites for macOS 11 (with Homebrew):

.. code:: console

   brew install bison flex gawk libffi git graphviz \
      pkg-config python3 tcl-tk xdot bash boost-python3

Running the build system
^^^^^^^^^^^^^^^^^^^^^^^^

From the root `yosys` directory, call the following commands:

.. code:: console
   
   make
   sudo make install

This will build and then install Yosys, making it available on the command line
as `yosys`.  Note that this also downloads, builds, and installs `ABC`_ (using
:program:`yosys-abc` as the executable name).

.. _ABC: https://github.com/berkeley-abc/abc

The default compiler is ``clang``, to change between ``clang`` and ``gcc``, use
one of the following:

.. code:: console

   make config-clang
   make config-gcc

To use a compiler different than the default, use:

.. code:: console

   make CXX="g++-11"

.. seealso:: 

   Refer to :doc:`/test_suites` for details on testing Yosys once compiled.

Source tree and build system
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The Yosys source tree is organized into the following top-level
directories:

``backends/``
   This directory contains a subdirectory for each of the backend modules.

``docs/``
   Contains the source for this documentation, including images and sample code.

``examples/``
   Contains example code for using Yosys with some other tools including a demo
   of the Yosys Python api, and synthesizing for various toolchains such as
   Intel and Anlogic.

``frontends/``
   This directory contains a subdirectory for each of the frontend modules.

``guidelines/``
   Contains developer guidelines, including the code of conduct and coding style
   guide.

``kernel/``
   This directory contains all the core functionality of Yosys. This includes
   the functions and definitions for working with the RTLIL data structures
   (:file:`rtlil.{h|cc}`), the ``main()`` function (:file:`driver.cc`), the
   internal framework for generating log messages (:file:`log.{h|cc}`), the
   internal framework for registering and calling passes
   (:file:`register.{h|cc}`), some core commands that are not really passes
   (:file:`select.cc`, :file:`show.cc`, …) and a couple of other small utility
   libraries.

``libs/``
   Libraries packaged with Yosys builds are contained in this folder.  See
   :doc:`/appendix/auxlibs`.

``misc/``
   Other miscellany which doesn't fit anywhere else.

``passes/``
   This directory contains a subdirectory for each pass or group of passes. For
   example as of this writing the directory :file:`passes/hierarchy/` contains the
   code for three passes: :cmd:ref:`hierarchy`, :cmd:ref:`submod`, and
   :cmd:ref:`uniquify`.

``techlibs/``
   This directory contains simulation models and standard implementations for
   the cells from the internal cell library.

``tests/``
   This directory contains the suite of unit tests and regression tests used by
   Yosys.  See :doc:`/test_suites`.

The top-level Makefile includes :file:`frontends/{*}/Makefile.inc`,
:file:`passes/{*}/Makefile.inc` and :file:`backends/{*}/Makefile.inc`. So when
extending Yosys it is enough to create a new directory in :file:`frontends/`,
:file:`passes/` or :file:`backends/` with your sources and a
:file:`Makefile.inc`. The Yosys kernel automatically detects all commands linked
with Yosys. So it is not needed to add additional commands to a central list of
commands.

Good starting points for reading example source code to learn how to write
passes are :file:`passes/opt/opt_dff.cc` and :file:`passes/opt/opt_merge.cc`.

See the top-level README file for a quick Getting Started guide and build
instructions. The Yosys build is based solely on Makefiles.

Users of the Qt Creator IDE can generate a QT Creator project file using make
qtcreator. Users of the Eclipse IDE can use the "Makefile Project with Existing
Code" project type in the Eclipse "New Project" dialog (only available after the
CDT plugin has been installed) to create an Eclipse project in order to
programming extensions to Yosys or just browse the Yosys code base.



getting_started/scripting_intro.rst
--------------------------------------
Scripting in Yosys
------------------

On the previous page we went through a synthesis script, running each command in
the interactive Yosys shell.  On this page, we will be introducing the script
file format and how you can make your own synthesis scripts.

Yosys script files typically use the :file:`.ys` extension and contain a set of
commands for Yosys to run sequentially.  These commands are the same ones we
were using on the previous page like :cmd:ref:`read_verilog` and
:cmd:ref:`hierarchy`.

Script parsing
~~~~~~~~~~~~~~

As with the interactive shell, each command consists of the command name, and an
optional whitespace separated list of arguments. Commands are terminated with
the newline character, and anything after a hash sign ``#`` is a comment (i.e.
it is ignored).

It is also possible to terminate commands with a semicolon ``;``.  This is
particularly useful in conjunction with the ``-p <command>`` command line
option, where ``<command>`` can be a string with multiple commands separated by
semicolon. In-line comments can also be made with the colon ``:``, where the end
of the comment is a semicolon ``;`` or a new line.

.. code-block::
   :caption: Using the ``-p`` option

   $ yosys -p "read_verilog fifo.v; :this is a comment; prep"

.. warning::

   The space after the semicolon is required for correct parsing. ``log a;log
   b;`` for example will display ``a;log b`` instead of ``a`` and ``b`` as might
   be expected.

Another special character that can be used in Yosys scripts is the bang ``!``.
Anything after the bang will be executed as a shell command.  This can only be
terminated with a new line.  Any semicolons, hashes, or other special characters
will be passed to the shell.  If an error code is returned from the shell it
will be raised by Yosys.  :cmd:ref:`exec` provides a much more flexible way of
executing commands, allowing the output to be logged and more control over when
to generate errors.

The synthesis starter script
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. role:: yoscrypt(code)
   :language: yoscrypt

All of the images and console output used in
:doc:`/getting_started/example_synth` were generated by Yosys, using Yosys
script files found in :file:`docs/source/code_examples/fifo`. If you haven't
already, let's take a look at some of those script files now.

.. literalinclude:: /code_examples/fifo/fifo.ys
   :language: yoscrypt
   :lineno-match:
   :start-at: echo on
   :end-before: design -reset
   :caption: A section of :file:`fifo.ys`, generating the images used for :ref:`addr_gen_example`
   :name: fifo-ys

The first command there, :yoscrypt:`echo on`, uses :cmd:ref:`echo` to enable
command echoes on.  This is how we generated the code listing for
:ref:`hierarchy_output`.  Turning command echoes on prints the ``yosys>
hierarchy -top addr_gen`` line, making the output look the same as if it were an
interactive terminal.  :yoscrypt:`hierarchy -top addr_gen` is of course the
command we were demonstrating, including the output text and an image of the
design schematic after running it.

We briefly touched on :cmd:ref:`select` when it came up in
:cmd:ref:`synth_ice40`, but let's look at it more now.

.. _select_intro:

Selections intro
^^^^^^^^^^^^^^^^

The :cmd:ref:`select` command is used to modify and view the list of selected
objects:

.. literalinclude:: /code_examples/fifo/fifo.out
   :language: doscon
   :start-at: yosys> select
   :end-before: yosys> show

When we call :yoscrypt:`select -module addr_gen` we are changing the currently
active selection from the whole design, to just the ``addr_gen`` module.  Notice
how this changes the ``yosys`` at the start of each command to ``yosys
[addr_gen]``?  This indicates that any commands we run at this point will *only*
operate on the ``addr_gen`` module.  When we then call :yoscrypt:`select -list`
we get a list of all objects in the ``addr_gen`` module, including the module
itself, as well as all of the wires, inputs, outputs, processes, and cells.

Next we perform another selection, :yoscrypt:`select t:*`.  The ``t:`` part
signifies we are matching on the *cell type*, and the ``*`` means to match
anything.  For this (very simple) selection, we are trying to find all of the
cells, regardless of their type.  The active selection is now shown as
``[addr_gen]*``, indicating some sub-selection of the ``addr_gen`` module.  This
gives us the ``$add`` and ``$eq`` cells, which we want to highlight for the
:ref:`addr_gen_hier` image.

.. _select_new_cells:

We can assign a name to a selection with :yoscrypt:`select -set`.  In our case
we are using the name ``new_cells``, and telling it to use the current
selection, indicated by the ``%`` symbol.  We can then use this named selection
by referring to it as ``@new_cells``, which we will see later.  Then we clear
the selection so that the following commands can operate on the full design.
While we split that out for this document, we could have done the same thing in
a single line by calling :yoscrypt:`select -set new_cells addr_gen/t:*`.  If we
know we only have the one module in our design, we can even skip the `addr_gen/`
part.  Looking further down :ref:`the fifo.ys code <fifo-ys>` we can see this
with :yoscrypt:`select -set new_cells t:$mux t:*dff`.  We can also see in that
command that selections don't have to be limited to a single statement.

Many commands also support an optional ``[selection]`` argument which can be
used to override the currently selected objects.  We could, for example, call
:yoscrypt:`clean addr_gen` to have :cmd:ref:`clean` operate on *just* the
``addr_gen`` module.

Detailed documentation of the select framework can be found under
:doc:`/using_yosys/more_scripting/selections` or in the command reference at
:doc:`/cmd/select`.

.. _show_intro:

Displaying schematics
^^^^^^^^^^^^^^^^^^^^^

While the :cmd:ref:`select` command is very useful, sometimes nothing beats
being able to see a design for yourself.  This is where :cmd:ref:`show` comes
in.  Note that this document is just an introduction to the :cmd:ref:`show`
command, only covering the basics.  For more information, including a guide on
what the different symbols represent, see :ref:`interactive_show` and the
:doc:`/using_yosys/more_scripting/interactive_investigation` page.

.. figure:: /_images/code_examples/fifo/addr_gen_show.*
   :class: width-helper invert-helper
   :name: addr_gen_show

   Calling :yoscrypt:`show addr_gen` after :cmd:ref:`hierarchy`

.. note:: 

   The :cmd:ref:`show` command requires a working installation of `GraphViz`_
   and `xdot`_ for displaying the actual circuit diagrams.
   
.. _GraphViz: http://www.graphviz.org/
.. _xdot: https://github.com/jrfonseca/xdot.py

This is the first :yoscrypt:`show` command we called in :file:`fifo.ys`,
:ref:`as we saw above <fifo-ys>`.  If we look at the log output for this image
we see the following:

.. literalinclude:: /code_examples/fifo/fifo.out
   :language: doscon
   :start-at: -prefix addr_gen_show
   :end-before: yosys> show

Calling :cmd:ref:`show` with :yoscrypt:`-format dot` tells it we want to output
a :file:`.dot` file rather than opening it for display.  The :yoscrypt:`-prefix
addr_gen_show` option indicates we want the file to be called
:file:`addr_gen_show.{*}`. Remember, we do this in :file:`fifo.ys` because we
need to store the image for displaying in the documentation you're reading.  But
if you just want to display the images locally you can skip these two options.
The ``-format`` option internally calls the ``dot`` command line program from
GraphViz to convert to formats other than :file:`.dot`.  Check `GraphViz output
docs`_ for more on available formats.

.. _GraphViz output docs: https://graphviz.org/docs/outputs/

.. note::

   If you are using a POSIX based version of Yosys (such as for Mac or Linux),
   xdot will be opened in the background and Yosys can continue to be used. If
   it it still open, future calls to :yoscrypt:`show` will use the same xdot
   instance.

The ``addr_gen`` at the end tells it we only want the ``addr_gen`` module, just
like when we called :yoscrypt:`select -module addr_gen` in :ref:`select_intro`.
That last parameter doesn't have to be a module name, it can be any valid
selection string.  Remember when we :ref:`assigned a name to a
selection<select_new_cells>` and called it ``new_cells``?  We saw in the
:yoscrypt:`select -list` output that it contained two cells, an ``$add`` and an
``$eq``.  We can call :cmd:ref:`show` on that selection just as easily:

.. figure:: /_images/code_examples/fifo/new_cells_show.*
   :class: width-helper invert-helper
   :name: new_cells_show

   Calling :yoscrypt:`show -notitle @new_cells`

We could have gotten the same output with :yoscrypt:`show -notitle t:$add t:$eq`
if we didn't have the named selection. By adding the :yoscrypt:`-notitle` flag
there we can also get rid of the ``addr_gen`` title that would have been
automatically added.  The last two images were both added for this introduction.
The next image is the first one we saw in :doc:`/getting_started/example_synth`:
showing the full ``addr_gen`` module while also highlighting ``@new_cells`` and
the two ``PROC`` blocks.  To achieve this highlight, we make use of the
:yoscrypt:`-color` option:

.. figure:: /_images/code_examples/fifo/addr_gen_hier.*
   :class: width-helper invert-helper

   Calling :yoscrypt:`show -color maroon3 @new_cells -color cornflowerblue p:* -notitle`

As described in the the :cmd:ref:`help` output for :cmd:ref:`show` (or by
clicking on the :cmd:ref:`show` link), colors are specified as :yoscrypt:`-color
<color> <object>`.  Color names for the ``<color>`` portion can be found on the
`GraphViz color docs`_.  Unlike the final :cmd:ref:`show` parameter which can
have be any selection string, the ``<object>`` part must be a single selection
expression or named selection.  That means while we can use ``@new_cells``, we
couldn't use ``t:$eq t:$add``.  In general, if a command lists ``[selection]``
as its final parameter it can be any selection string.  Any selections that are
not the final parameter, such as those used in options, must be a single
expression instead.

.. _GraphViz color docs: https://graphviz.org/doc/info/colors

For all of the options available to :cmd:ref:`show`, check the command reference
at :doc:`/cmd/show`.

.. seealso:: :ref:`interactive_show` on the
   :doc:`/using_yosys/more_scripting/interactive_investigation` page.



using_yosys/index.rst
--------------------------------------
Using Yosys (advanced)
======================

While much of Yosys is focused around synthesis, there are also a number of
other useful things that can be accomplished with Yosys scripts or in an
interactive shell.  As such this section is broken into two parts:
:doc:`/using_yosys/synthesis/index` expands on the
:doc:`/getting_started/example_synth` and goes into further detail on the major
commands used in synthesis; :doc:`/using_yosys/more_scripting/index` covers the
ways Yosys can interact with designs for a deeper investigation.

.. toctree::
   :maxdepth: 2
   :hidden:

   synthesis/index
   more_scripting/index



more_scripting/index.rst
--------------------------------------
More scripting
--------------

.. todo:: brief overview for the more scripting index

.. toctree::
   :maxdepth: 3

   load_design
   selections
   interactive_investigation
   model_checking

.. troubleshooting



more_scripting/interactive_investigation.rst
--------------------------------------
Interactive design investigation
--------------------------------

.. todo:: interactive design opening text

.. role:: yoscrypt(code)
   :language: yoscrypt

.. _interactive_show:

A look at the show command
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. TODO:: merge into :doc:`/getting_started/scripting_intro` show section

This section explores the :cmd:ref:`show` command and explains the symbols used
in the circuit diagrams generated by it. The code used is included in the Yosys
code base under |code_examples/show|_.

.. |code_examples/show| replace:: :file:`docs/source/code_examples/show`
.. _code_examples/show: https://github.com/YosysHQ/yosys/tree/main/docs/source/code_examples/show

A simple circuit
^^^^^^^^^^^^^^^^

:ref:`example_v` below provides the Verilog code for a simple circuit which we
will use to demonstrate the usage of :cmd:ref:`show` in a simple setting.

.. literalinclude:: /code_examples/show/example.v
   :language: Verilog
   :caption: :file:`example.v`
   :name: example_v

The Yosys synthesis script we will be running is included as
:numref:`example_ys`. Note that :cmd:ref:`show` is called with the ``-pause``
option, that halts execution of the Yosys script until the user presses the
Enter key. Using :yoscrypt:`show -pause` also allows the user to enter an
interactive shell to further investigate the circuit before continuing
synthesis.

.. literalinclude:: /code_examples/show/example_show.ys
   :language: yoscrypt
   :caption: :file:`example_show.ys`
   :name: example_ys

This script, when executed, will show the design after each of the three
synthesis commands. We will now look at each of these diagrams and explain what
is shown.

.. note::

   The images uses in this document are generated from the :file:`example.ys`
   file, rather than :file:`example_show.ys`.  :file:`example.ys` outputs the
   schematics as :file:`.dot` files rather than displaying them directly.  You
   can view these images yourself by running :file:`yosys example.ys` and then
   ``xdot example_first.dot`` etc.

.. figure:: /_images/code_examples/show/example_first.*
   :class: width-helper invert-helper
   
   Output of the first :cmd:ref:`show` command in :numref:`example_ys`

The first output shows the design directly after being read by the Verilog
front-end. Input and output ports are displayed as octagonal shapes. Cells are
displayed as rectangles with inputs on the left and outputs on the right side.
The cell labels are two lines long: The first line contains a unique identifier
for the cell and the second line contains the cell type. Internal cell types are
prefixed with a dollar sign. For more details on the internal cell library, see
:doc:`/yosys_internals/formats/cell_library`.

Constants are shown as ellipses with the constant value as label. The syntax
``<bit_width>'<bits>`` is used for constants that are not 32-bit wide and/or
contain bits that are not 0 or 1 (i.e. ``x`` or ``z``). Ordinary 32-bit
constants are written using decimal numbers.

Single-bit signals are shown as thin arrows pointing from the driver to the
load. Signals that are multiple bits wide are shown as think arrows.

Finally *processes* are shown in boxes with round corners. Processes are Yosys'
internal representation of the decision-trees and synchronization events
modelled in a Verilog ``always``-block. The label reads ``PROC`` followed by a
unique identifier in the first line and contains the source code location of the
original ``always``-block in the second line. Note how the multiplexer from the
``?:``-expression is represented as a ``$mux`` cell but the multiplexer from the
``if``-statement is yet still hidden within the process.

The :cmd:ref:`proc` command transforms the process from the first diagram into a
multiplexer and a d-type flip-flop, which brings us to the second diagram:

.. figure:: /_images/code_examples/show/example_second.*
   :class: width-helper invert-helper
   
   Output of the second :cmd:ref:`show` command in :numref:`example_ys`

The Rhombus shape to the right is a dangling wire. (Wire nodes are only shown if
they are dangling or have "public" names, for example names assigned from the
Verilog input.) Also note that the design now contains two instances of a
``BUF``-node. These are artefacts left behind by the :cmd:ref:`proc` command. It
is quite usual to see such artefacts after calling commands that perform changes
in the design, as most commands only care about doing the transformation in the
least complicated way, not about cleaning up after them. The next call to
:cmd:ref:`clean` (or :cmd:ref:`opt`, which includes :cmd:ref:`clean` as one of
its operations) will clean up these artefacts. This operation is so common in
Yosys scripts that it can simply be abbreviated with the ``;;`` token, which
doubles as separator for commands. Unless one wants to specifically analyze this
artefacts left behind some operations, it is therefore recommended to always
call :cmd:ref:`clean` before calling :cmd:ref:`show`.

In this script we directly call :cmd:ref:`opt` as the next step, which finally
leads us to the third diagram: 

.. figure:: /_images/code_examples/show/example_third.*
   :class: width-helper invert-helper
   :name: example_out
   
   Output of the third :cmd:ref:`show` command in :ref:`example_ys`
   
Here we see that the :cmd:ref:`proc` command not only has removed the artifacts
left behind by :cmd:ref:`proc`, but also determined correctly that it can remove
the first ``$mux`` cell without changing the behavior of the circuit.

Break-out boxes for signal vectors
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The code listing below shows a simple circuit which uses a lot of spliced signal
accesses.

.. literalinclude:: /code_examples/show/splice.v
   :caption: :file:`splice.v`
   :name: splice_src

Notice how the output for this circuit from the :cmd:ref:`show` command
(:numref:`splice_dia`) appears quite complex. This is an unfortunate side effect
of the way Yosys handles signal vectors (aka. multi-bit wires or buses) as
native objects. While this provides great advantages when analyzing circuits
that operate on wide integers, it also introduces some additional complexity
when the individual bits of of a signal vector are accessed.

.. figure:: /_images/code_examples/show/splice.*
   :class: width-helper invert-helper
   :name: splice_dia

   Output of ``yosys -p 'prep -top splice_demo; show' splice.v``

The key elements in understanding this circuit diagram are of course the boxes
with round corners and rows labeled ``<MSB_LEFT>:<LSB_LEFT> -
<MSB_RIGHT>:<LSB_RIGHT>``. Each of these boxes have one signal per row on one
side and a common signal for all rows on the other side. The ``<MSB>:<LSB>``
tuples specify which bits of the signals are broken out and connected. So the
top row of the box connecting the signals ``a`` and ``x`` indicates that the bit
0 (i.e. the range 0:0) from signal ``a`` is connected to bit 1 (i.e. the range
1:1) of signal ``x``.

Lines connecting such boxes together and lines connecting such boxes to cell
ports have a slightly different look to emphasise that they are not actual
signal wires but a necessity of the graphical representation. This distinction
seems like a technicality, until one wants to debug a problem related to the way
Yosys internally represents signal vectors, for example when writing custom
Yosys commands.

Gate level netlists
^^^^^^^^^^^^^^^^^^^

:numref:`first_pitfall` shows two common pitfalls when working with designs
mapped to a cell library:

.. figure:: /_images/code_examples/show/cmos_00.*
   :class: width-helper invert-helper
   :name: first_pitfall

   A half-adder built from simple CMOS gates, demonstrating common pitfalls when 
   using :cmd:ref:`show`

.. literalinclude:: /code_examples/show/cmos.ys
   :language: yoscrypt
   :start-after: pitfall
   :end-at: cmos_00
   :name: pitfall_code
   :caption: Generating :numref:`first_pitfall`
   
First, Yosys did not have access to the cell library when this diagram was
generated, resulting in all cell ports defaulting to being inputs. This is why
all ports are drawn on the left side the cells are awkwardly arranged in a large
column. Secondly the two-bit vector ``y`` requires breakout-boxes for its
individual bits, resulting in an unnecessary complex diagram.

.. figure:: /_images/code_examples/show/cmos_01.*
   :class: width-helper invert-helper
   :name: second_pitfall

   Effects of :cmd:ref:`splitnets` command and of providing a cell library on 
   design in :numref:`first_pitfall`

.. literalinclude:: /code_examples/show/cmos.ys
   :language: yoscrypt
   :start-after: fixed
   :end-at: cmos_01
   :name: pitfall_avoided
   :caption: Generating :numref:`second_pitfall`

For :numref:`second_pitfall`, Yosys has been given a description of the cell
library as Verilog file containing blackbox modules. There are two ways to load
cell descriptions into Yosys: First the Verilog file for the cell library can be
passed directly to the :cmd:ref:`show` command using the ``-lib <filename>``
option. Secondly it is possible to load cell libraries into the design with the
:yoscrypt:`read_verilog -lib <filename>` command. The second method has the
great advantage that the library only needs to be loaded once and can then be
used in all subsequent calls to the :cmd:ref:`show` command.

In addition to that, :numref:`second_pitfall` was generated after
:yoscrypt:`splitnet -ports` was run on the design. This command splits all
signal vectors into individual signal bits, which is often desirable when
looking at gate-level circuits. The ``-ports`` option is required to also split
module ports. Per default the command only operates on interior signals.

Miscellaneous notes
^^^^^^^^^^^^^^^^^^^

Per default the :cmd:ref:`show` command outputs a temporary dot file and
launches ``xdot`` to display it. The options ``-format``, ``-viewer`` and
``-prefix`` can be used to change format, viewer and filename prefix. Note that
the ``pdf`` and ``ps`` format are the only formats that support plotting
multiple modules in one run.  The ``dot`` format can be used to output multiple
modules, however ``xdot`` will raise an error when trying to read them.

In densely connected circuits it is sometimes hard to keep track of the
individual signal wires. For these cases it can be useful to call
:cmd:ref:`show` with the ``-colors <integer>`` argument, which randomly assigns
colors to the nets. The integer (> 0) is used as seed value for the random color
assignments. Sometimes it is necessary it try some values to find an assignment
of colors that looks good.

The command :yoscrypt:`help show` prints a complete listing of all options
supported by the :cmd:ref:`show` command.

Navigating the design
~~~~~~~~~~~~~~~~~~~~~

Plotting circuit diagrams for entire modules in the design brings us only helps
in simple cases. For complex modules the generated circuit diagrams are just
stupidly big and are no help at all. In such cases one first has to select the
relevant portions of the circuit.

In addition to *what* to display one also needs to carefully decide *when* to
display it, with respect to the synthesis flow. In general it is a good idea to
troubleshoot a circuit in the earliest state in which a problem can be
reproduced. So if, for example, the internal state before calling the
:cmd:ref:`techmap` command already fails to verify, it is better to troubleshoot
the coarse-grain version of the circuit before :cmd:ref:`techmap` than the
gate-level circuit after :cmd:ref:`techmap`.

.. Note:: 

   It is generally recommended to verify the internal state of a design by
   writing it to a Verilog file using :yoscrypt:`write_verilog -noexpr` and
   using the simulation models from :file:`simlib.v` and :file:`simcells.v` from
   the Yosys data directory (as printed by ``yosys-config --datdir``).

Interactive navigation
^^^^^^^^^^^^^^^^^^^^^^

Once the right state within the synthesis flow for debugging the circuit has
been identified, it is recommended to simply add the :cmd:ref:`shell` command to
the matching place in the synthesis script. This command will stop the synthesis
at the specified moment and go to shell mode, where the user can interactively
enter commands.

For most cases, the shell will start with the whole design selected (i.e. when
the synthesis script does not already narrow the selection). The command
:cmd:ref:`ls` can now be used to create a list of all modules. The command
:cmd:ref:`cd` can be used to switch to one of the modules (type ``cd ..`` to
switch back). Now the :cmd:ref:`ls` command lists the objects within that
module. This is demonstrated below using :file:`example.v` from `A simple
circuit`_:

.. literalinclude:: /code_examples/show/example.out
   :language: doscon
   :start-at: yosys> ls
   :end-before: yosys [example]> dump
   :caption: Output of :cmd:ref:`ls` and :cmd:ref:`cd` after running :file:`yosys example.v`
   :name: lscd

When a module is selected using the :cmd:ref:`cd` command, all commands (with a
few exceptions, such as the ``read_`` and ``write_`` commands) operate only on
the selected module. This can also be useful for synthesis scripts where
different synthesis strategies should be applied to different modules in the
design.

We can see that the cell names from :numref:`example_out` are just abbreviations
of the actual cell names, namely the part after the last dollar-sign. Most
auto-generated names (the ones starting with a dollar sign) are rather long and
contains some additional information on the origin of the named object. But in
most cases those names can simply be abbreviated using the last part.

Usually all interactive work is done with one module selected using the
:cmd:ref:`cd` command. But it is also possible to work from the design-context
(``cd ..``). In this case all object names must be prefixed with
``<module_name>/``. For example ``a*/b*`` would refer to all objects whose names
start with ``b`` from all modules whose names start with ``a``.

The :cmd:ref:`dump` command can be used to print all information about an
object. For example, calling :yoscrypt:`dump $2` after the :yoscrypt:`cd
example` above:

.. literalinclude:: /code_examples/show/example.out
   :language: RTLIL
   :start-after: yosys [example]> dump
   :end-before: yosys [example]> cd
   :dedent:
   :caption: Output of :yoscrypt:`dump $2` after :numref:`lscd`
   :name: dump2

This can for example be useful to determine the names of nets connected to
cells, as the net-names are usually suppressed in the circuit diagram if they
are auto-generated.  Note that the output is in the RTLIL representation,
described in :doc:`/yosys_internals/formats/rtlil_rep`.

Interactive Design Investigation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Yosys can also be used to investigate designs (or netlists created from other
tools).

- The selection mechanism, especially patterns such as ``%ci`` and ``%co``, can
  be used to figure out how parts of the design are connected.
- Commands such as :cmd:ref:`submod`, :cmd:ref:`expose`, and :cmd:ref:`splice`
  can be used to transform the design into an equivalent design that is easier
  to analyse.
- Commands such as :cmd:ref:`eval` and :cmd:ref:`sat` can be used to investigate
  the behavior of the circuit.
- :doc:`/cmd/show`.
- :doc:`/cmd/dump`.
- :doc:`/cmd/add` and :doc:`/cmd/delete` can be used to modify and reorganize a
  design dynamically.
  
The code used is included in the Yosys code base under
|code_examples/scrambler|_.

.. |code_examples/scrambler| replace:: :file:`docs/source/code_examples/scrambler`
.. _code_examples/scrambler: https://github.com/YosysHQ/yosys/tree/main/docs/source/code_examples/scrambler

Changing design hierarchy
^^^^^^^^^^^^^^^^^^^^^^^^^

Commands such as :cmd:ref:`flatten` and :cmd:ref:`submod` can be used to change
the design hierarchy, i.e. flatten the hierarchy or moving parts of a module to
a submodule. This has applications in synthesis scripts as well as in reverse
engineering and analysis.  An example using :cmd:ref:`submod` is shown below for
reorganizing a module in Yosys and checking the resulting circuit.

.. literalinclude:: /code_examples/scrambler/scrambler.v
   :language: verilog
   :caption: :file:`scrambler.v`

.. literalinclude:: /code_examples/scrambler/scrambler.ys
   :language: yoscrypt
   :caption: :file:`scrambler.ys`
   :end-before: cd ..

.. figure:: /_images/code_examples/scrambler/scrambler_p01.*
    :class: width-helper invert-helper

.. figure:: /_images/code_examples/scrambler/scrambler_p02.*
    :class: width-helper invert-helper

Analyzing the resulting circuit with :doc:`/cmd/eval`:

.. todo:: replace inline code

.. code:: text

    > cd xorshift32
    > rename n2 in
    > rename n1 out

    > eval -set in 1 -show out
    Eval result: \out = 270369.

    > eval -set in 270369 -show out
    Eval result: \out = 67634689.

    > sat -set out 632435482
    Signal Name                 Dec        Hex                                   Bin
    -------------------- ---------- ---------- -------------------------------------
    \in                   745495504   2c6f5bd0      00101100011011110101101111010000
    \out                  632435482   25b2331a      00100101101100100011001100011010

Behavioral changes
^^^^^^^^^^^^^^^^^^

Commands such as :cmd:ref:`techmap` can be used to make behavioral changes to
the design, for example changing asynchronous resets to synchronous resets. This
has applications in design space exploration (evaluation of various
architectures for one circuit).

The following techmap map file replaces all positive-edge async reset flip-flops
with positive-edge sync reset flip-flops. The code is taken from the example
Yosys script for ASIC synthesis of the Amber ARMv2 CPU.

.. todo:: replace inline code

.. code:: verilog

    (* techmap_celltype = "$adff" *)
    module adff2dff (CLK, ARST, D, Q);

        parameter WIDTH = 1;
        parameter CLK_POLARITY = 1;
        parameter ARST_POLARITY = 1;
        parameter ARST_VALUE = 0;

        input CLK, ARST;
        input [WIDTH-1:0] D;
        output reg [WIDTH-1:0] Q;

        wire [1023:0] _TECHMAP_DO_ = "proc";

        wire _TECHMAP_FAIL_ = !CLK_POLARITY || !ARST_POLARITY;

        always @(posedge CLK)
            if (ARST)
                Q <= ARST_VALUE;
            else
                Q <= D;

    endmodule

For more on the :cmd:ref:`techmap` command, see the page on
:doc:`/yosys_internals/techmap`.

Advanced investigation techniques
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When working with very large modules, it is often not enough to just select the
interesting part of the module. Instead it can be useful to extract the
interesting part of the circuit into a separate module. This can for example be
useful if one wants to run a series of synthesis commands on the critical part
of the module and wants to carefully read all the debug output created by the
commands in order to spot a problem. This kind of troubleshooting is much easier
if the circuit under investigation is encapsulated in a separate module.

Recall the ``memdemo`` design from :ref:`advanced_logic_cones`:

.. figure:: /_images/code_examples/selections/memdemo_00.*
   :class: width-helper invert-helper
   
   ``memdemo``

Because this produces a rather large circuit, it can be useful to split it into
smaller parts for viewing and working with.  :numref:`submod` does exactly that,
utilising the :cmd:ref:`submod` command to split the circuit into three
sections: ``outstage``, ``selstage``, and ``scramble``.

.. literalinclude:: /code_examples/selections/submod.ys
   :language: yoscrypt
   :caption: Using :cmd:ref:`submod` to break up the circuit from :file:`memdemo.v`
   :start-after: cd memdemo
   :end-before: cd ..
   :name: submod

The ``-name`` option is used to specify the name of the new module and also the
name of the new cell in the current module. The resulting circuits are shown
below.

.. figure:: /_images/code_examples/selections/submod_02.*
   :class: width-helper invert-helper
   
   ``outstage``

.. figure:: /_images/code_examples/selections/submod_03.*
   :class: width-helper invert-helper
   :name: selstage
   
   ``selstage``

.. figure:: /_images/code_examples/selections/submod_01.*
   :class: width-helper invert-helper
   
   ``scramble``

Evaluation of combinatorial circuits
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The :cmd:ref:`eval` command can be used to evaluate combinatorial circuits. As
an example, we will use the ``selstage`` subnet of ``memdemo`` which we found
above and is shown in :numref:`selstage`.

.. todo:: replace inline code

::

      yosys [selstage]> eval -set s2,s1 4'b1001 -set d 4'hc -show n2 -show n1

      1. Executing EVAL pass (evaluate the circuit given an input).
      Full command line: eval -set s2,s1 4'b1001 -set d 4'hc -show n2 -show n1
      Eval result: \n2 = 2'10.
      Eval result: \n1 = 2'10.

So the ``-set`` option is used to set input values and the ``-show`` option is
used to specify the nets to evaluate. If no ``-show`` option is specified, all
selected output ports are used per default.

If a necessary input value is not given, an error is produced. The option
``-set-undef`` can be used to instead set all unspecified input nets to undef
(``x``).

The ``-table`` option can be used to create a truth table. For example:

::

      yosys [selstage]> eval -set-undef -set d[3:1] 0 -table s1,d[0]

      10. Executing EVAL pass (evaluate the circuit given an input).
      Full command line: eval -set-undef -set d[3:1] 0 -table s1,d[0]

        \s1 \d [0] |  \n1  \n2
       ---- ------ | ---- ----
       2'00    1'0 | 2'00 2'00
       2'00    1'1 | 2'xx 2'00
       2'01    1'0 | 2'00 2'00
       2'01    1'1 | 2'xx 2'01
       2'10    1'0 | 2'00 2'00
       2'10    1'1 | 2'xx 2'10
       2'11    1'0 | 2'00 2'00
       2'11    1'1 | 2'xx 2'11

      Assumed undef (x) value for the following signals: \s2

Note that the :cmd:ref:`eval` command (as well as the :cmd:ref:`sat` command
discussed in the next sections) does only operate on flattened modules. It can
not analyze signals that are passed through design hierarchy levels. So the
:cmd:ref:`flatten` command must be used on modules that instantiate other
modules before these commands can be applied.

Solving combinatorial SAT problems
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Often the opposite of the :cmd:ref:`eval` command is needed, i.e. the circuits
output is given and we want to find the matching input signals. For small
circuits with only a few input bits this can be accomplished by trying all
possible input combinations, as it is done by the ``eval -table`` command. For
larger circuits however, Yosys provides the :cmd:ref:`sat` command that uses a
`SAT`_ solver, `MiniSAT`_, to solve this kind of problems.

.. _SAT: http://en.wikipedia.org/wiki/Circuit_satisfiability

.. _MiniSAT: http://minisat.se/

.. note:: 
    
    While it is possible to perform model checking directly in Yosys, it 
    is highly recommended to use SBY or EQY for formal hardware verification.

The :cmd:ref:`sat` command works very similar to the :cmd:ref:`eval` command.
The main difference is that it is now also possible to set output values and
find the corresponding input values. For Example:

.. todo:: replace inline code

::

      yosys [selstage]> sat -show s1,s2,d -set s1 s2 -set n2,n1 4'b1001

      11. Executing SAT pass (solving SAT problems in the circuit).
      Full command line: sat -show s1,s2,d -set s1 s2 -set n2,n1 4'b1001

      Setting up SAT problem:
      Import set-constraint: \s1 = \s2
      Import set-constraint: { \n2 \n1 } = 4'1001
      Final constraint equation: { \n2 \n1 \s1 } = { 4'1001 \s2 }
      Imported 3 cells to SAT database.
      Import show expression: { \s1 \s2 \d }

      Solving problem with 81 variables and 207 clauses..
      SAT solving finished - model found:

        Signal Name                 Dec        Hex             Bin
        -------------------- ---------- ---------- ---------------
        \d                            9          9            1001
        \s1                           0          0              00
        \s2                           0          0              00

Note that the :cmd:ref:`sat` command supports signal names in both arguments to
the ``-set`` option. In the above example we used ``-set s1 s2`` to constraint
``s1`` and ``s2`` to be equal. When more complex constraints are needed, a
wrapper circuit must be constructed that checks the constraints and signals if
the constraint was met using an extra output port, which then can be forced to a
value using the ``-set`` option. (Such a circuit that contains the circuit under
test plus additional constraint checking circuitry is called a ``miter``
circuit.)

.. literalinclude:: /code_examples/primetest.v
   :language: verilog
   :caption: :file:`primetest.v`, a simple miter circuit for testing if a number is
             prime. But it has a problem.
   :name: primetest

:numref:`primetest` shows a miter circuit that is supposed to be used as a prime
number test. If ``ok`` is 1 for all input values ``a`` and ``b`` for a given
``p``, then ``p`` is prime, or at least that is the idea.

.. todo:: replace inline code

.. code-block::
   :caption: Experiments with the miter circuit from :file:`primetest.v`.
   :name: prime_shell

   yosys [primetest]> sat -prove ok 1 -set p 31

   1. Executing SAT pass (solving SAT problems in the circuit).
   Full command line: sat -prove ok 1 -set p 31

   Setting up SAT problem:
   Import set-constraint: \p = 16'0000000000011111
   Final constraint equation: \p = 16'0000000000011111
   Imported 6 cells to SAT database.
   Import proof-constraint: \ok = 1'1
   Final proof equation: \ok = 1'1

   Solving problem with 2790 variables and 8241 clauses..
   SAT proof finished - model found: FAIL!

      ______                   ___       ___       _ _            _ _
     (_____ \                 / __)     / __)     (_) |          | | |
      _____) )___ ___   ___ _| |__    _| |__ _____ _| | _____  __| | |
     |  ____/ ___) _ \ / _ (_   __)  (_   __|____ | | || ___ |/ _  |_|
     | |   | |  | |_| | |_| || |       | |  / ___ | | || ____( (_| |_
     |_|   |_|   \___/ \___/ |_|       |_|  \_____|_|\_)_____)\____|_|


     Signal Name                 Dec        Hex                   Bin
     -------------------- ---------- ---------- ---------------------
     \a                        15029       3ab5      0011101010110101
     \b                         4099       1003      0001000000000011
     \ok                           0          0                     0
     \p                           31         1f      0000000000011111

The Yosys shell session shown in :numref:`prime_shell` demonstrates that SAT
solvers can even find the unexpected solutions to a problem: Using integer
overflow there actually is a way of "factorizing" 31. The clean solution would
of course be to perform the test in 32 bits, for example by replacing ``p !=
a*b`` in the miter with ``p != {16'd0,a}b``, or by using a temporary variable
for the 32 bit product ``a*b``. But as 31 fits well into 8 bits (and as the
purpose of this document is to show off Yosys features) we can also simply force
the upper 8 bits of ``a`` and ``b`` to zero for the :cmd:ref:`sat` call, as is
done below.

.. todo:: replace inline code

.. code-block::
   :caption: Miter circuit from :file:`primetest.v`, with the upper 8 bits of ``a``
             and ``b`` constrained to prevent overflow.
   :name: prime_fixed

   yosys [primetest]> sat -prove ok 1 -set p 31 -set a[15:8],b[15:8] 0

   1. Executing SAT pass (solving SAT problems in the circuit).
   Full command line: sat -prove ok 1 -set p 31 -set a[15:8],b[15:8] 0

   Setting up SAT problem:
   Import set-constraint: \p = 16'0000000000011111
   Import set-constraint: { \a [15:8] \b [15:8] } = 16'0000000000000000
   Final constraint equation: { \a [15:8] \b [15:8] \p } = { 16'0000000000000000 16'0000000000011111 }
   Imported 6 cells to SAT database.
   Import proof-constraint: \ok = 1'1
   Final proof equation: \ok = 1'1

   Solving problem with 2790 variables and 8257 clauses..
   SAT proof finished - no model found: SUCCESS!

                     /$$$$$$      /$$$$$$$$     /$$$$$$$
                    /$$__  $$    | $$_____/    | $$__  $$
                   | $$  \ $$    | $$          | $$  \ $$
                   | $$  | $$    | $$$$$       | $$  | $$
                   | $$  | $$    | $$__/       | $$  | $$
                   | $$/$$ $$    | $$          | $$  | $$
                   |  $$$$$$/ /$$| $$$$$$$$ /$$| $$$$$$$//$$
                    \____ $$$|__/|________/|__/|_______/|__/
                          \__/

The ``-prove`` option used in :numref:`prime_fixed` works similar to ``-set``,
but tries to find a case in which the two arguments are not equal. If such a
case is not found, the property is proven to hold for all inputs that satisfy
the other constraints.

It might be worth noting, that SAT solvers are not particularly efficient at
factorizing large numbers. But if a small factorization problem occurs as part
of a larger circuit problem, the Yosys SAT solver is perfectly capable of
solving it.

Solving sequential SAT problems
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The SAT solver functionality in Yosys can not only be used to solve
combinatorial problems, but can also solve sequential problems. Let's consider
the ``memdemo`` design from :ref:`advanced_logic_cones` again, and suppose we
want to know which sequence of input values for ``d`` will cause the output y to
produce the sequence 1, 2, 3 from any initial state. Let's use the following
command:

.. todo:: replace inline code?

.. code-block:: yoscrypt

      sat -seq 6 -show y -show d -set-init-undef \
        -max_undef -set-at 4 y 1 -set-at 5 y 2 -set-at 6 y 3

The ``-seq 6`` option instructs the :cmd:ref:`sat` command to solve a sequential
problem in 6 time steps. (Experiments with lower number of steps have show that
at least 3 cycles are necessary to bring the circuit in a state from which the
sequence 1, 2, 3 can be produced.)

The ``-set-init-undef`` option tells the :cmd:ref:`sat` command to initialize
all registers to the undef (``x``) state. The way the ``x`` state is treated in
Verilog will ensure that the solution will work for any initial state.

The ``-max_undef`` option instructs the :cmd:ref:`sat` command to find a
solution with a maximum number of undefs. This way we can see clearly which
inputs bits are relevant to the solution.

Finally the three ``-set-at`` options add constraints for the ``y`` signal to
play the 1, 2, 3 sequence, starting with time step 4.

This produces the following output:

.. todo:: replace inline code

.. code-block::
   :caption: Solving a sequential SAT problem in the ``memdemo`` module.
   :name: memdemo_sat

   yosys [memdemo]> sat -seq 6 -show y -show d -set-init-undef \
       -max_undef -set-at 4 y 1 -set-at 5 y 2 -set-at 6 y 3

   1. Executing SAT pass (solving SAT problems in the circuit).
   Full command line: sat -seq 6 -show y -show d -set-init-undef
       -max_undef -set-at 4 y 1 -set-at 5 y 2 -set-at 6 y 3

   Setting up time step 1:
   Final constraint equation: { } = { }
   Imported 29 cells to SAT database.

   Setting up time step 2:
   Final constraint equation: { } = { }
   Imported 29 cells to SAT database.

   Setting up time step 3:
   Final constraint equation: { } = { }
   Imported 29 cells to SAT database.

   Setting up time step 4:
   Import set-constraint for timestep: \y = 4'0001
   Final constraint equation: \y = 4'0001
   Imported 29 cells to SAT database.

   Setting up time step 5:
   Import set-constraint for timestep: \y = 4'0010
   Final constraint equation: \y = 4'0010
   Imported 29 cells to SAT database.

   Setting up time step 6:
   Import set-constraint for timestep: \y = 4'0011
   Final constraint equation: \y = 4'0011
   Imported 29 cells to SAT database.

   Setting up initial state:
   Final constraint equation: { \y \s2 \s1 \mem[3] \mem[2] \mem[1]
               \mem[0] } = 24'xxxxxxxxxxxxxxxxxxxxxxxx

   Import show expression: \y
   Import show expression: \d

   Solving problem with 10322 variables and 27881 clauses..
   SAT model found. maximizing number of undefs.
   SAT solving finished - model found:

     Time Signal Name                 Dec        Hex             Bin
     ---- -------------------- ---------- ---------- ---------------
     init \mem[0]                      --         --            xxxx
     init \mem[1]                      --         --            xxxx
     init \mem[2]                      --         --            xxxx
     init \mem[3]                      --         --            xxxx
     init \s1                          --         --              xx
     init \s2                          --         --              xx
     init \y                           --         --            xxxx
     ---- -------------------- ---------- ---------- ---------------
        1 \d                            0          0            0000
        1 \y                           --         --            xxxx
     ---- -------------------- ---------- ---------- ---------------
        2 \d                            1          1            0001
        2 \y                           --         --            xxxx
     ---- -------------------- ---------- ---------- ---------------
        3 \d                            2          2            0010
        3 \y                            0          0            0000
     ---- -------------------- ---------- ---------- ---------------
        4 \d                            3          3            0011
        4 \y                            1          1            0001
     ---- -------------------- ---------- ---------- ---------------
        5 \d                           --         --            001x
        5 \y                            2          2            0010
     ---- -------------------- ---------- ---------- ---------------
        6 \d                           --         --            xxxx
        6 \y                            3          3            0011

It is not surprising that the solution sets ``d = 0`` in the first step, as this
is the only way of setting the ``s1`` and ``s2`` registers to a known value. The
input values for the other steps are a bit harder to work out manually, but the
SAT solver finds the correct solution in an instant.

There is much more to write about the :cmd:ref:`sat` command. For example, there
is a set of options that can be used to performs sequential proofs using
temporal induction :cite:p:`een2003temporal`. The command ``help sat`` can be
used to print a list of all options with short descriptions of their functions.



more_scripting/load_design.rst
--------------------------------------
Loading a design
~~~~~~~~~~~~~~~~

keyword: Frontends

- :doc:`/cmd/read_verilog`

.. todo:: include ``read_verilog <<EOF``, also other methods of loading designs

.. code-block:: yoscrypt

    read_verilog file1.v
    read_verilog -I include_dir -D enable_foo -D WIDTH=12 file2.v
    read_verilog -lib cell_library.v

    verilog_defaults -add -I include_dir
    read_verilog file3.v
    read_verilog file4.v
    verilog_defaults -clear

    verilog_defaults -push
    verilog_defaults -add -I include_dir
    read_verilog file5.v
    read_verilog file6.v
    verilog_defaults -pop

.. todo:: more info on other ``read_*`` commands, also is this the first time we
   mention verific?

Others:

- :doc:`/cmd/read`
- `GHDL plugin`_ for VHDL
- :doc:`/cmd/read_rtlil` (direct textual representation of Yosys internal state)
- :doc:`/cmd/read_aiger`
- :doc:`/cmd/read_blif`
- :doc:`/cmd/read_json`
- :doc:`/cmd/read_liberty`

.. _GHDL plugin: https://github.com/ghdl/ghdl-yosys-plugin



more_scripting/model_checking.rst
--------------------------------------
Symbolic model checking
-----------------------

.. todo:: check text context

.. note:: 
    
    While it is possible to perform model checking directly in Yosys, it 
    is highly recommended to use SBY or EQY for formal hardware verification.

Symbolic Model Checking (SMC) is used to formally prove that a circuit has (or
has not) a given property.

One application is Formal Equivalence Checking: Proving that two circuits are
identical. For example this is a very useful feature when debugging custom
passes in Yosys.

Other applications include checking if a module conforms to interface standards.

The :cmd:ref:`sat` command in Yosys can be used to perform Symbolic Model
Checking.

Checking techmap
~~~~~~~~~~~~~~~~

.. todo:: add/expand supporting text

Let's take a look at an example included in the Yosys code base under
|code_examples/synth_flow|_:

.. |code_examples/synth_flow| replace:: :file:`docs/source/code_examples/synth_flow`
.. _code_examples/synth_flow: https://github.com/YosysHQ/yosys/tree/main/docs/source/code_examples/synth_flow

.. literalinclude:: /code_examples/synth_flow/techmap_01_map.v
   :language: verilog
   :caption: :file:`techmap_01_map.v`

.. literalinclude:: /code_examples/synth_flow/techmap_01.v
   :language: verilog
   :caption: :file:`techmap_01.v`

.. literalinclude:: /code_examples/synth_flow/techmap_01.ys
   :language: yoscrypt
   :caption: :file:`techmap_01.ys`

To see if it is correct we can use the following code:

.. todo:: replace inline code

.. code:: yoscrypt

    # read test design
    read_verilog techmap_01.v
    hierarchy -top test

    # create two version of the design: test_orig and test_mapped
    copy test test_orig
    rename test test_mapped

    # apply the techmap only to test_mapped
    techmap -map techmap_01_map.v test_mapped

    # create a miter circuit to test equivalence
    miter -equiv -make_assert -make_outputs test_orig test_mapped miter
    flatten miter

    # run equivalence check
    sat -verify -prove-asserts -show-inputs -show-outputs miter

Result:

.. code::

    Solving problem with 945 variables and 2505 clauses..
    SAT proof finished - no model found: SUCCESS!

AXI4 Stream Master
~~~~~~~~~~~~~~~~~~

The code used in this section is included in the Yosys code base under
|code_examples/axis|_.

.. |code_examples/axis| replace:: :file:`docs/source/code_examples/axis`
.. _code_examples/axis: https://github.com/YosysHQ/yosys/tree/main/docs/source/code_examples/axis

The following AXI4 Stream Master has a bug. But the bug is not exposed if the
slave keeps ``tready`` asserted all the time. (Something a test bench might do.)

Symbolic Model Checking can be used to expose the bug and find a sequence of
values for ``tready`` that yield the incorrect behavior.

.. todo:: add/expand supporting text

.. literalinclude:: /code_examples/axis/axis_master.v
   :language: verilog
   :caption: :file:`axis_master.v`

.. literalinclude:: /code_examples/axis/axis_test.v
   :language: verilog
   :caption: :file:`axis_test.v`

.. literalinclude:: /code_examples/axis/axis_test.ys
   :language: yoscrypt
   :caption: :file:`test.ys`

Result with unmodified :file:`axis_master.v`:

.. todo:: replace inline code

.. code::

    Solving problem with 159344 variables and 442126 clauses..
    SAT proof finished - model found: FAIL!

Result with fixed :file:`axis_master.v`:

.. code::

    Solving problem with 159144 variables and 441626 clauses..
    SAT proof finished - no model found: SUCCESS!



more_scripting/selections.rst
--------------------------------------
Selections
----------

.. role:: yoscrypt(code)
   :language: yoscrypt

The selection framework
~~~~~~~~~~~~~~~~~~~~~~~

.. todo:: reduce overlap with :doc:`/getting_started/scripting_intro` select section

The :cmd:ref:`select` command can be used to create a selection for subsequent
commands. For example:

.. code:: yoscrypt

    select foobar         # select the module foobar
    delete                # delete selected objects

Normally the :cmd:ref:`select` command overwrites a previous selection. The
commands :yoscrypt:`select -add` and :yoscrypt:`select -del` can be used to add
or remove objects from the current selection.

The command :yoscrypt:`select -clear` can be used to reset the selection to the
default, which is a complete selection of everything in the current module.

This selection framework can also be used directly in many other commands.
Whenever a command has ``[selection]`` as last argument in its usage help, this
means that it will use the engine behind the :cmd:ref:`select` command to
evaluate additional arguments and use the resulting selection instead of the
selection created by the last :cmd:ref:`select` command.

For example, the command :cmd:ref:`delete` will delete everything in the current
selection; while :yoscrypt:`delete foobar` will only delete the module foobar.
If no :cmd:ref:`select` command has been made, then the "current selection" will
be the whole design.

.. note:: Many of the examples on this page make use of the :cmd:ref:`show` 
   command to visually demonstrate the effect of selections.  For a more 
   detailed look at this command, refer to :ref:`interactive_show`.

How to make a selection
~~~~~~~~~~~~~~~~~~~~~~~

Selection by object name
^^^^^^^^^^^^^^^^^^^^^^^^

The easiest way to select objects is by object name. This is usually only done
in synthesis scripts that are hand-tailored for a specific design.

.. code:: yoscrypt

    select foobar         # select module foobar
    select foo*           # select all modules whose names start with foo
    select foo*/bar*      # select all objects matching bar* from modules matching foo*
    select */clk          # select objects named clk from all modules

Module and design context
^^^^^^^^^^^^^^^^^^^^^^^^^

Commands can be executed in *module/* or *design/* context. Until now, all
commands have been executed in design context. The :cmd:ref:`cd` command can be
used to switch to module context.

In module context, all commands only effect the active module. Objects in the
module are selected without the ``<module_name>/`` prefix. For example:

.. code:: yoscrypt

    cd foo                # switch to module foo
    delete bar            # delete object foo/bar

    cd mycpu              # switch to module mycpu
    dump reg_*            # print details on all objects whose names start with reg_

    cd ..                 # switch back to design

Note: Most synthesis scripts never switch to module context. But it is a very
powerful tool which we explore more in
:doc:`/using_yosys/more_scripting/interactive_investigation`.

Selecting by object property or type
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Special patterns can be used to select by object property or type. For example:

- select all wires whose names start with ``reg_``: :yoscrypt:`select w:reg_*`
- select all objects with the attribute ``foobar`` set: :yoscrypt:`select
  a:foobar`
- select all objects with the attribute ``foobar`` set to 42: :yoscrypt:`select
  a:foobar=42`
- select all modules with the attribute ``blabla`` set: :yoscrypt:`select
  A:blabla`
- select all $add cells from the module foo: :yoscrypt:`select foo/t:$add`

A complete list of pattern expressions can be found in :doc:`/cmd/select`.

Operations on selections
~~~~~~~~~~~~~~~~~~~~~~~~

Combining selections
^^^^^^^^^^^^^^^^^^^^

The :cmd:ref:`select` command is actually much more powerful than it might seem
at first glance. When it is called with multiple arguments, each argument is
evaluated and pushed separately on a stack. After all arguments have been
processed it simply creates the union of all elements on the stack. So
:yoscrypt:`select t:$add a:foo` will select all ``$add`` cells and all objects
with the ``foo`` attribute set:   

.. literalinclude:: /code_examples/selections/foobaraddsub.v
   :caption: Test module for operations on selections
   :name: foobaraddsub
   :language: verilog

.. code-block::
   :caption: Output for command ``select t:$add a:foo -list`` on :numref:`foobaraddsub`

   yosys> select t:$add a:foo -list
   foobaraddsub/$add$foobaraddsub.v:6$3
   foobaraddsub/$sub$foobaraddsub.v:5$2
   foobaraddsub/$add$foobaraddsub.v:4$1

In many cases simply adding more and more stuff to the selection is an
ineffective way of selecting the interesting part of the design. Special
arguments can be used to combine the elements on the stack. For example the
``%i`` arguments pops the last two elements from the stack, intersects them, and
pushes the result back on the stack. So :yoscrypt:`select t:$add a:foo %i` will
select all ``$add`` cells that have the ``foo`` attribute set:

.. code-block::
   :caption: Output for command ``select t:$add a:foo %i -list`` on :numref:`foobaraddsub`
   
   yosys> select t:$add a:foo %i -list
   foobaraddsub/$add$foobaraddsub.v:4$1

Some of the special ``%``-codes:

- ``%u``: union of top two elements on stack -- pop 2, push 1
- ``%d``: difference of top two elements on stack -- pop 2, push 1
- ``%i``: intersection of top two elements on stack -- pop 2, push 1
- ``%n``: inverse of top element on stack -- pop 1, push 1

See :doc:`/cmd/select` for the full list.

Expanding selections
^^^^^^^^^^^^^^^^^^^^

:numref:`sumprod` uses the Yosys non-standard ``{... *}`` syntax to set the
attribute ``sumstuff`` on all cells generated by the first assign statement.
(This works on arbitrary large blocks of Verilog code and can be used to mark
portions of code for analysis.)

.. literalinclude:: /code_examples/selections/sumprod.v
   :caption: Another test module for operations on selections
   :name: sumprod
   :language: verilog

Selecting ``a:sumstuff`` in this module will yield the following circuit
diagram:

.. figure:: /_images/code_examples/selections/sumprod_00.*
   :class: width-helper invert-helper
   :name: sumprod_00

   Output of ``show a:sumstuff`` on :numref:`sumprod`

As only the cells themselves are selected, but not the temporary wire ``$1_Y``,
the two adders are shown as two disjunct parts. This can be very useful for
global signals like clock and reset signals: just unselect them using a command
such as :yoscrypt:`select -del clk rst` and each cell using them will get its
own net label.

In this case however we would like to see the cells connected properly. This can
be achieved using the ``%x`` action, that broadens the selection, i.e. for each
selected wire it selects all cells connected to the wire and vice versa. So
:yoscrypt:`show a:sumstuff %x` yields the diagram shown in :numref:`sumprod_01`:

.. figure:: /_images/code_examples/selections/sumprod_01.*
   :class: width-helper invert-helper
   :name: sumprod_01

   Output of ``show a:sumstuff %x`` on :numref:`sumprod`

.. _selecting_logic_cones:

Selecting logic cones
^^^^^^^^^^^^^^^^^^^^^

:numref:`sumprod_01` shows what is called the ``input cone`` of ``sum``, i.e.
all cells and signals that are used to generate the signal ``sum``. The ``%ci``
action can be used to select the input cones of all object in the top selection
in the stack maintained by the :cmd:ref:`select` command.

As with the ``%x`` action, these commands broaden the selection by one "step".
But this time the operation only works against the direction of data flow. That
means, wires only select cells via output ports and cells only select wires via
input ports.

The following sequence of diagrams demonstrates this step-wise expansion:

.. figure:: /_images/code_examples/selections/sumprod_02.*
   :class: width-helper invert-helper

   Output of :yoscrypt:`show prod` on :numref:`sumprod`

.. figure:: /_images/code_examples/selections/sumprod_03.*
   :class: width-helper invert-helper

   Output of :yoscrypt:`show prod %ci` on :numref:`sumprod`

.. figure:: /_images/code_examples/selections/sumprod_04.*
   :class: width-helper invert-helper

   Output of :yoscrypt:`show prod %ci %ci` on :numref:`sumprod`

.. figure:: /_images/code_examples/selections/sumprod_05.*
   :class: width-helper invert-helper

   Output of :yoscrypt:`show prod %ci %ci %ci` on :numref:`sumprod`

Notice the subtle difference between :yoscrypt:`show prod %ci` and
:yoscrypt:`show prod %ci %ci`.  Both images show the ``$mul`` cell driven by
some inputs ``$3_Y`` and ``c``.  However it is not until the second image,
having called ``%ci`` the second time, that :cmd:ref:`show` is able to
distinguish between ``$3_Y`` being a wire and ``c`` being an input.  We can see
this better with the :cmd:ref:`dump` command instead:

.. literalinclude:: /code_examples/selections/sumprod.out
   :language: RTLIL
   :end-at: end
   :caption: Output of :yoscrypt:`dump prod %ci`

.. literalinclude:: /code_examples/selections/sumprod.out
   :language: RTLIL
   :start-after: end
   :caption: Output of :yoscrypt:`dump prod %ci %ci`

When selecting many levels of logic, repeating ``%ci`` over and over again can
be a bit dull. So there is a shortcut for that: the number of iterations can be
appended to the action. So for example the action ``%ci3`` is identical to
performing the ``%ci`` action three times.

The action ``%ci*`` performs the ``%ci`` action over and over again until it
has no effect anymore.

.. _advanced_logic_cones:

Advanced logic cone selection
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In most cases there are certain cell types and/or ports that should not be
considered for the ``%ci`` action, or we only want to follow certain cell types
and/or ports. This can be achieved using additional patterns that can be
appended to the ``%ci`` action.

Lets consider :numref:`memdemo_src`. It serves no purpose other than being a
non-trivial circuit for demonstrating some of the advanced Yosys features. This
code is available in ``docs/source/code_examples/selections`` of the Yosys
source repository.

.. literalinclude:: /code_examples/selections/memdemo.v
   :caption: :file:`memdemo.v`
   :name: memdemo_src
   :language: verilog

The script :file:`memdemo.ys` is used to generate the images included here. Let's
look at the first section:

.. literalinclude:: /code_examples/selections/memdemo.ys
   :caption: Synthesizing :ref:`memdemo_src`
   :name: memdemo_ys
   :language: yoscrypt
   :end-at: opt

This loads :numref:`memdemo_src` and synthesizes the included module. Note that
this code can be copied and run directly in a Yosys command line session,
provided :file:`memdemo.v` is in the same directory. We can now change to the
``memdemo`` module with ``cd memdemo``, and call :cmd:ref:`show` to see the
diagram in :numref:`memdemo_00`.

.. figure:: /_images/code_examples/selections/memdemo_00.*
   :class: width-helper invert-helper
   :name: memdemo_00
   
   Complete circuit diagram for the design shown in :numref:`memdemo_src`

There's a lot going on there, but maybe we are only interested in the tree of
multiplexers that select the output value. Let's start by just showing the
output signal, ``y``, and its immediate predecessors. Remember `Selecting logic
cones`_ from above, we can use :yoscrypt:`show y %ci2`:

.. figure:: /_images/code_examples/selections/memdemo_01.*
   :class: width-helper invert-helper
   :name: memdemo_01
   
   Output of :yoscrypt:`show y %ci2`

From this we would learn that ``y`` is driven by a ``$dff cell``, that ``y`` is
connected to the output port ``Q``, that the ``clk`` signal goes into the
``CLK`` input port of the cell, and that the data comes from an auto-generated
wire into the input ``D`` of the flip-flop cell (indicated by the ``$`` at the
start of the name).  Let's go a bit further now and try :yoscrypt:`show y %ci5`:

.. figure:: /_images/code_examples/selections/memdemo_02.*
   :class: width-helper invert-helper
   :name: memdemo_02
   
   Output of :yoscrypt:`show y %ci5`

That's starting to get a bit messy, so maybe we want to ignore the mux select
inputs. To add a pattern we add a colon followed by the pattern to the ``%ci``
action. The pattern itself starts with ``-`` or ``+``, indicating if it is an
include or exclude pattern, followed by an optional comma separated list of cell
types, followed by an optional comma separated list of port names in square
brackets.  In this case, we want to exclude the ``S`` port of the ``$mux`` cell
type with :yoscrypt:`show y %ci5:-$mux[S]`:

.. figure:: /_images/code_examples/selections/memdemo_03.*
   :class: width-helper invert-helper
   :name: memdemo_03
   
   Output of :yoscrypt:`show y %ci5:-$mux[S]`

We could use a command such as :yoscrypt:`show y %ci2:+$dff[Q,D]
%ci*:-$mux[S]:-$dff` in which the first ``%ci`` jumps over the initial d-type
flip-flop and the 2nd action selects the entire input cone without going over
multiplexer select inputs and flip-flop cells:

.. figure:: /_images/code_examples/selections/memdemo_05.*
   :class: width-helper invert-helper
   :name: memdemo_05
   
   Output of ``show y %ci2:+$dff[Q,D] %ci*:-$mux[S]:-$dff``

Or we could use :yoscrypt:`show y %ci*:-[CLK,S]:+$dff:+$mux` instead, following
the input cone all the way but only following ``$dff`` and ``$mux`` cells, and
ignoring any ports named ``CLK`` or ``S``:

.. TODO:: pending discussion on whether rule ordering is a bug or a feature

.. figure:: /_images/code_examples/selections/memdemo_04.*
   :class: width-helper invert-helper
   :name: memdemo_04
   
   Output of :yoscrypt:`show y %ci*:-[CLK,S]:+$dff,$mux`

Similar to ``%ci`` exists an action ``%co`` to select output cones that accepts
the same syntax for pattern and repetition. The ``%x`` action mentioned
previously also accepts this advanced syntax.

These actions for traversing the circuit graph, combined with the actions for
boolean operations such as intersection (``%i``) and difference (``%d``) are
powerful tools for extracting the relevant portions of the circuit under
investigation.

Again, see :doc:`/cmd/select` for full documentation of these expressions.

Incremental selection
^^^^^^^^^^^^^^^^^^^^^

Sometimes a selection can most easily be described by a series of add/delete
operations. As mentioned previously, the commands :yoscrypt:`select -add` and
:yoscrypt:`select -del` respectively add or remove objects from the current
selection instead of overwriting it.

.. code:: yoscrypt

    select -none            # start with an empty selection
    select -add reg_*       # select a bunch of objects
    select -del reg_42      # but not this one
    select -add state %ci   # and add more stuff

Within a select expression the token ``%`` can be used to push the previous selection
on the stack.

.. code:: yoscrypt

    select t:$add t:$sub    # select all $add and $sub cells
    select % %ci % %d       # select only the input wires to those cells

Storing and recalling selections
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. todo:: reflow for not presentation

The current selection can be stored in memory with the command ``select -set
<name>``. It can later be recalled using ``select @<name>``. In fact, the
``@<name>`` expression pushes the stored selection on the stack maintained by
the :cmd:ref:`select` command. So for example :yoscrypt:`select @foo @bar %i`
will select the intersection between the stored selections ``foo`` and ``bar``.

In larger investigation efforts it is highly recommended to maintain a script
that sets up relevant selections, so they can easily be recalled, for example
when Yosys needs to be re-run after a design or source code change.

The :cmd:ref:`history` command can be used to list all recent interactive
commands. This feature can be useful for creating such a script from the
commands used in an interactive session.

Remember that select expressions can also be used directly as arguments to most
commands. Some commands also accept a single select argument to some options. In
those cases selection variables must be used to capture more complex selections.

Example code from |code_examples/selections|_:

.. |code_examples/selections| replace:: :file:`docs/source/code_examples/selections`
.. _code_examples/selections: https://github.com/YosysHQ/yosys/tree/main/docs/source/code_examples/selections

.. literalinclude:: /code_examples/selections/select.v
   :language: verilog
   :caption: :file:`select.v`

.. literalinclude:: /code_examples/selections/select.ys
   :language: yoscrypt
   :caption: :file:`select.ys`
   :name: select_ys

.. figure:: /_images/code_examples/selections/select.*
    :class: width-helper invert-helper

    Circuit diagram produced by :numref:`select_ys`



more_scripting/troubleshooting.rst
--------------------------------------
Troubleshooting
~~~~~~~~~~~~~~~

.. todo:: troubleshooting document(?)

See :doc:`/cmd/bugpoint`



synthesis/abc.rst
--------------------------------------
The ABC toolbox
---------------

.. role:: yoscrypt(code)
   :language: yoscrypt

ABC_, from the University of California, Berkeley, is a logic toolbox used for
fine-grained optimisation and LUT mapping.

Yosys has two different commands, which both use this logic toolbox, but use it
in different ways.

The :cmd:ref:`abc` pass can be used for both ASIC (e.g. :yoscrypt:`abc
-liberty`) and FPGA (:yoscrypt:`abc -lut`) mapping, but this page will focus on
FPGA mapping.

The :cmd:ref:`abc9` pass generally provides superior mapping quality due to
being aware of combination boxes and DFF and LUT timings, giving it a more
global view of the mapping problem.

.. _ABC: https://github.com/berkeley-abc/abc

ABC: the unit delay model, simple and efficient
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :cmd:ref:`abc` pass uses a highly simplified view of an FPGA:

- An FPGA is made up of a network of inputs that connect through LUTs to a
  network of outputs. These inputs may actually be I/O pins, D flip-flops,
  memory blocks or DSPs, but ABC is unaware of this.
- Each LUT has 1 unit of delay between an input and its output, and this applies
  for all inputs of a LUT, and for all sizes of LUT up to the maximum LUT size
  allowed; e.g. the delay between the input of a LUT2 and its output is the same
  as the delay between the input of a LUT6 and its output.
- A LUT may take up a variable number of area units. This is constant for each
  size of LUT; e.g. a LUT4 may take up 1 unit of area, but a LUT5 may take up 2
  units of area, but this applies for all LUT4s and LUT5s.

This is known as the "unit delay model", because each LUT uses one unit of
delay.

From this view, the problem ABC has to solve is finding a mapping of the network
to LUTs that has the lowest delay, and then optimising the mapping for size
while maintaining this delay.

This approach has advantages:

- It is simple and easy to implement.
- Working with unit delays is fast to manipulate.
- It reflects *some* FPGA families, for example, the iCE40HX/LP fits the
  assumptions of the unit delay model quite well (almost all synchronous blocks,
  except for adders).

But this approach has drawbacks, too:

- The network of inputs and outputs with only LUTs means that a lot of
  combinational cells (multipliers and LUTRAM) are invisible to the unit delay
  model, meaning the critical path it optimises for is not necessarily the
  actual critical path.
- LUTs are implemented as multiplexer trees, so there is a delay caused by the
  result propagating through the remaining multiplexers. This means the
  assumption of delay being equal isn't true in physical hardware, and is
  proportionally larger for larger LUTs.
- Even synchronous blocks have arrival times (propagation delay between clock
  edge to output changing) and setup times (requirement for input to be stable
  before clock edge) which affect the delay of a path.

ABC9: the generalised delay model, realistic and flexible
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ABC9 uses a more detailed and accurate model of an FPGA:

- An FPGA is made up of a network of inputs that connect through LUTs and
  combinational boxes to a network of outputs. These boxes have specified delays
  between inputs and outputs, and may have an associated network ("white boxes")
  or not ("black boxes"), but must be treated as a whole.
- Each LUT has a specified delay between an input and its output in arbitrary
  delay units, and this varies for all inputs of a LUT and for all sizes of LUT,
  but each size of LUT has the same associated delay; e.g. the delay between
  input A and output is different between a LUT2 and a LUT6, but is constant for
  all LUT6s.
- A LUT may take up a variable number of area units. This is constant for each
  size of LUT; e.g. a LUT4 may take up 1 unit of area, but a LUT5 may take up 2
  units of area, but this applies for all LUT4s and LUT5s.

This is known as the "generalised delay model", because it has been generalised
to arbitrary delay units. ABC9 doesn't actually care what units you use here,
but the Yosys convention is picoseconds. Note the introduction of boxes as a
concept. While the generalised delay model does not require boxes, they
naturally fit into it to represent combinational delays. Even synchronous delays
like arrival and setup can be emulated with combinational boxes that act as a
delay. This is further extended to white boxes, where the mapper is able to see
inside a box, and remove orphan boxes with no outputs, such as adders.

Again, ABC9 finds a mapping of the network to LUTs that has the lowest delay,
and then minimises it to find the lowest area, but it has a lot more information
to work with about the network.

The result here is that ABC9 can remove boxes (like adders) to reduce area,
optimise better around those boxes, and also permute inputs to give the critical
path the fastest inputs.

.. todo:: more about logic minimization & register balancing et al with ABC



synthesis/cell_libs.rst
--------------------------------------
Mapping to cell libraries
-------------------------

.. role:: yoscrypt(code)
   :language: yoscrypt

While much of this documentation focuses on the use of Yosys with FPGAs, it is
also possible to map to cell libraries which can be used in designing ASICs.
This section will cover a brief `example project`_, available in the Yosys
source code under :file:`docs/source/code_examples/intro/`.  The project
contains a simple ASIC synthesis script (:file:`counter.ys`), a digital design
written in Verilog (:file:`counter.v`), and a simple CMOS cell library
(:file:`mycells.lib`).  Many of the early steps here are already covered in more
detail in the :doc:`/getting_started/example_synth` document.

.. note::

   The :file:`counter.ys` script includes the commands used to generate the
   images in this document.  Code snippets in this document skip these commands;
   including line numbers to allow the reader to follow along with the source.
   
   To learn more about these commands, check out :ref:`interactive_show`.

.. _example project: https://github.com/YosysHQ/yosys/tree/main/docs/source/code_examples/intro

A simple counter
~~~~~~~~~~~~~~~~

First, let's quickly look at the design:

.. literalinclude:: /code_examples/intro/counter.v
   :language: Verilog
   :linenos:
   :name: counter-v
   :caption: :file:`counter.v`

This is a simple counter with reset and enable.  If the reset signal, ``rst``,
is high then the counter will reset to 0.  Otherwise, if the enable signal,
``en``, is high then the ``count`` register will increment by 1 each rising edge
of the clock, ``clk``.  

Loading the design
~~~~~~~~~~~~~~~~~~

.. literalinclude:: /code_examples/intro/counter.ys
   :language: yoscrypt
   :lines: 1-3
   :lineno-match:
   :caption: :file:`counter.ys` - read design

Our circuit now looks like this:

.. figure:: /_images/code_examples/intro/counter_00.*
   :class: width-helper invert-helper
   :name: counter-hierarchy

   ``counter`` after :cmd:ref:`hierarchy`

Coarse-grain representation
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. literalinclude:: /code_examples/intro/counter.ys
   :language: yoscrypt
   :lines: 7-10
   :lineno-match:
   :caption: :file:`counter.ys` - the high-level stuff

.. figure:: /_images/code_examples/intro/counter_01.*
   :class: width-helper invert-helper

   Coarse-grain representation of the ``counter`` module

Logic gate mapping
~~~~~~~~~~~~~~~~~~

.. literalinclude:: /code_examples/intro/counter.ys
   :language: yoscrypt
   :lines: 14-15
   :lineno-match:
   :caption: :file:`counter.ys` - mapping to internal cell library

.. figure:: /_images/code_examples/intro/counter_02.*
   :class: width-helper invert-helper

   ``counter`` after :cmd:ref:`techmap`

Mapping to hardware
~~~~~~~~~~~~~~~~~~~

For this example, we are using a Liberty file to describe a cell library which
our internal cell library will be mapped to:

.. literalinclude:: /code_examples/intro/mycells.lib
   :language: Liberty
   :linenos:
   :name: mycells-lib
   :caption: :file:`mycells.lib`

Recall that the Yosys built-in logic gate types are ``$_NOT_``, ``$_AND_``,
``$_OR_``, ``$_XOR_``, and ``$_MUX_`` with an assortment of dff memory types.
:ref:`mycells-lib` defines our target cells as ``BUF``, ``NOT``, ``NAND``,
``NOR``, and ``DFF``.  Mapping between these is performed with the commands
:cmd:ref:`dfflibmap` and :cmd:ref:`abc` as follows:

.. literalinclude:: /code_examples/intro/counter.ys
   :language: yoscrypt
   :lines: 20-27
   :lineno-match:
   :caption: :file:`counter.ys` - mapping to hardware

The final version of our ``counter`` module looks like this:

.. figure:: /_images/code_examples/intro/counter_03.*
   :class: width-helper invert-helper

   ``counter`` after hardware cell mapping

Before finally being output as a verilog file with :cmd:ref:`write_verilog`,
which can then be loaded into another tool:

.. literalinclude:: /code_examples/intro/counter.ys
   :language: yoscrypt
   :lines: 30-31
   :lineno-match:
   :caption: :file:`counter.ys` - write synthesized design



synthesis/extract.rst
--------------------------------------
The extract pass
----------------

- Like the :cmd:ref:`techmap` pass, the :cmd:ref:`extract` pass is called with a
  map file. It compares the circuits inside the modules of the map file with the
  design and looks for sub-circuits in the design that match any of the modules
  in the map file.
- If a match is found, the :cmd:ref:`extract` pass will replace the matching
  subcircuit with an instance of the module from the map file.
- In a way the :cmd:ref:`extract` pass is the inverse of the techmap pass.

.. todo:: add/expand supporting text, also mention custom pattern matching and
   pmgen

Example code can be found in |code_examples/macc|_.

.. |code_examples/macc| replace:: :file:`docs/source/code_examples/macc`
.. _code_examples/macc: https://github.com/YosysHQ/yosys/tree/main/docs/source/code_examples/macc


.. literalinclude:: /code_examples/macc/macc_simple_test.ys
    :language: yoscrypt
    :lines: 1-2

.. figure:: /_images/code_examples/macc/macc_simple_test_00a.*
    :class: width-helper invert-helper
    
    before :cmd:ref:`extract`

.. literalinclude:: /code_examples/macc/macc_simple_test.ys
    :language: yoscrypt
    :lines: 6

.. figure:: /_images/code_examples/macc/macc_simple_test_00b.*
    :class: width-helper invert-helper
    
    after :cmd:ref:`extract`

.. literalinclude:: /code_examples/macc/macc_simple_test.v
   :language: verilog
   :caption: :file:`macc_simple_test.v`

.. literalinclude:: /code_examples/macc/macc_simple_xmap.v
   :language: verilog
   :caption: :file:`macc_simple_xmap.v`

.. literalinclude:: /code_examples/macc/macc_simple_test_01.v
   :language: verilog
   :caption: :file:`macc_simple_test_01.v`

.. figure:: /_images/code_examples/macc/macc_simple_test_01a.*
    :class: width-helper invert-helper

.. figure:: /_images/code_examples/macc/macc_simple_test_01b.*
    :class: width-helper invert-helper

.. literalinclude:: /code_examples/macc/macc_simple_test_02.v
   :language: verilog
   :caption: :file:`macc_simple_test_02.v`

.. figure:: /_images/code_examples/macc/macc_simple_test_02a.*
    :class: width-helper invert-helper

.. figure:: /_images/code_examples/macc/macc_simple_test_02b.*
    :class: width-helper invert-helper

The wrap-extract-unwrap method
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Often a coarse-grain element has a constant bit-width, but can be used to
implement operations with a smaller bit-width. For example, a 18x25-bit multiplier
can also be used to implement 16x20-bit multiplication.

A way of mapping such elements in coarse grain synthesis is the
wrap-extract-unwrap method:

wrap
  Identify candidate-cells in the circuit and wrap them in a cell with a
  constant wider bit-width using :cmd:ref:`techmap`. The wrappers use the same
  parameters as the original cell, so the information about the original width
  of the ports is preserved. Then use the :cmd:ref:`connwrappers` command to
  connect up the bit-extended in- and outputs of the wrapper cells.

extract
  Now all operations are encoded using the same bit-width as the coarse grain
  element. The :cmd:ref:`extract` command can be used to replace circuits with
  cells of the target architecture.

unwrap
  The remaining wrapper cell can be unwrapped using :cmd:ref:`techmap`.

Example: DSP48_MACC
~~~~~~~~~~~~~~~~~~~

This section details an example that shows how to map MACC operations of
arbitrary size to MACC cells with a 18x25-bit multiplier and a 48-bit adder
(such as the Xilinx DSP48 cells).

Preconditioning: :file:`macc_xilinx_swap_map.v`

Make sure ``A`` is the smaller port on all multipliers

.. todo:: add/expand supporting text

.. literalinclude:: /code_examples/macc/macc_xilinx_swap_map.v
   :language: verilog
   :caption: :file:`macc_xilinx_swap_map.v`

Wrapping multipliers: :file:`macc_xilinx_wrap_map.v`

.. literalinclude:: /code_examples/macc/macc_xilinx_wrap_map.v
   :language: verilog
   :lines: 1-46
   :caption: :file:`macc_xilinx_wrap_map.v`

Wrapping adders: :file:`macc_xilinx_wrap_map.v`

.. literalinclude:: /code_examples/macc/macc_xilinx_wrap_map.v
   :language: verilog
   :lines: 48-89
   :caption: :file:`macc_xilinx_wrap_map.v`

Extract: :file:`macc_xilinx_xmap.v`

.. literalinclude:: /code_examples/macc/macc_xilinx_xmap.v
   :language: verilog
   :caption: :file:`macc_xilinx_xmap.v`

... simply use the same wrapping commands on this module as on the design to
create a template for the :cmd:ref:`extract` command.

Unwrapping multipliers: :file:`macc_xilinx_unwrap_map.v`

.. literalinclude:: /code_examples/macc/macc_xilinx_unwrap_map.v
   :language: verilog
   :lines: 1-30
   :caption: ``$__mul_wrapper`` module in :file:`macc_xilinx_unwrap_map.v`

Unwrapping adders: :file:`macc_xilinx_unwrap_map.v`

.. literalinclude:: /code_examples/macc/macc_xilinx_unwrap_map.v
   :language: verilog
   :lines: 32-61
   :caption: ``$__add_wrapper`` module in :file:`macc_xilinx_unwrap_map.v`

.. literalinclude:: /code_examples/macc/macc_xilinx_test.v
   :language: verilog
   :lines: 1-6
   :caption: ``test1`` of :file:`macc_xilinx_test.v`

.. figure:: /_images/code_examples/macc/macc_xilinx_test1a.*
    :class: width-helper invert-helper

.. figure:: /_images/code_examples/macc/macc_xilinx_test1b.*
    :class: width-helper invert-helper

.. literalinclude:: /code_examples/macc/macc_xilinx_test.v
   :language: verilog
   :lines: 8-13
   :caption: ``test2`` of :file:`macc_xilinx_test.v`

.. figure:: /_images/code_examples/macc/macc_xilinx_test2a.*
    :class: width-helper invert-helper

.. figure:: /_images/code_examples/macc/macc_xilinx_test2b.*
    :class: width-helper invert-helper

Wrapping in ``test1``:

.. figure:: /_images/code_examples/macc/macc_xilinx_test1b.*
    :class: width-helper invert-helper

.. literalinclude:: /code_examples/macc/macc_xilinx_test.ys
    :language: yoscrypt
    :start-after: part c
    :end-before: end part c

.. figure:: /_images/code_examples/macc/macc_xilinx_test1c.*
    :class: width-helper invert-helper

Wrapping in ``test2``:

.. figure:: /_images/code_examples/macc/macc_xilinx_test2b.*
    :class: width-helper invert-helper

.. literalinclude:: /code_examples/macc/macc_xilinx_test.ys
    :language: yoscrypt
    :start-after: part c
    :end-before: end part c

.. figure:: /_images/code_examples/macc/macc_xilinx_test2c.*
    :class: width-helper invert-helper

Extract in ``test1``:

.. figure:: /_images/code_examples/macc/macc_xilinx_test1c.*
    :class: width-helper invert-helper

.. literalinclude:: /code_examples/macc/macc_xilinx_test.ys
    :language: yoscrypt
    :start-after: part d
    :end-before: end part d

.. figure:: /_images/code_examples/macc/macc_xilinx_test1d.*
    :class: width-helper invert-helper

Extract in ``test2``:

.. figure:: /_images/code_examples/macc/macc_xilinx_test2c.*
    :class: width-helper invert-helper

.. literalinclude:: /code_examples/macc/macc_xilinx_test.ys
    :language: yoscrypt
    :start-after: part d
    :end-before: end part d

.. figure:: /_images/code_examples/macc/macc_xilinx_test2d.*
    :class: width-helper invert-helper

Unwrap in ``test2``:

.. figure:: /_images/code_examples/macc/macc_xilinx_test2d.*
    :class: width-helper invert-helper

.. literalinclude:: /code_examples/macc/macc_xilinx_test.ys
    :language: yoscrypt
    :start-after: part e
    :end-before: end part e

.. figure:: /_images/code_examples/macc/macc_xilinx_test2e.*
    :class: width-helper invert-helper


synthesis/fsm.rst
--------------------------------------
FSM handling
============

The :cmd:ref:`fsm` command identifies, extracts, optimizes (re-encodes), and
re-synthesizes finite state machines. It again is a macro that calls a series of
other commands:

.. literalinclude:: /code_examples/macro_commands/fsm.ys
   :language: yoscrypt
   :start-after: #end:
   :caption: Passes called by :cmd:ref:`fsm`

See also :doc:`/cmd/fsm`.

The algorithms used for FSM detection and extraction are influenced by a more
general reported technique :cite:p:`fsmextract`.

FSM detection
~~~~~~~~~~~~~

The :cmd:ref:`fsm_detect` pass identifies FSM state registers. It sets the
``\fsm_encoding = "auto"`` attribute on any (multi-bit) wire that matches the
following description:

-  Does not already have the ``\fsm_encoding`` attribute.
-  Is not an output of the containing module.
-  Is driven by single ``$dff`` or ``$adff`` cell.
-  The ``\D``-Input of this ``$dff`` or ``$adff`` cell is driven by a
   multiplexer tree that only has constants or the old state value on its
   leaves.
-  The state value is only used in the said multiplexer tree or by simple
   relational cells that compare the state value to a constant (usually ``$eq``
   cells).

This heuristic has proven to work very well. It is possible to overwrite it by
setting ``\fsm_encoding = "auto"`` on registers that should be considered FSM
state registers and setting ``\fsm_encoding = "none"`` on registers that match
the above criteria but should not be considered FSM state registers.

Note however that marking state registers with ``\fsm_encoding`` that are not
suitable for FSM recoding can cause synthesis to fail or produce invalid
results.

FSM extraction
~~~~~~~~~~~~~~

The :cmd:ref:`fsm_extract` pass operates on all state signals marked with the
(``\fsm_encoding != "none"``) attribute. For each state signal the following
information is determined:

-  The state registers

-  The asynchronous reset state if the state registers use asynchronous reset

-  All states and the control input signals used in the state transition
   functions

-  The control output signals calculated from the state signals and control
   inputs

-  A table of all state transitions and corresponding control inputs- and
   outputs

The state registers (and asynchronous reset state, if applicable) is simply
determined by identifying the driver for the state signal.

From there the ``$mux-tree`` driving the state register inputs is recursively
traversed. All select inputs are control signals and the leaves of the
``$mux-tree`` are the states. The algorithm fails if a non-constant leaf that is
not the state signal itself is found.

The list of control outputs is initialized with the bits from the state signal.
It is then extended by adding all values that are calculated by cells that
compare the state signal with a constant value.

In most cases this will cover all uses of the state register, thus rendering the
state encoding arbitrary. If however a design uses e.g. a single bit of the
state value to drive a control output directly, this bit of the state signal
will be transformed to a control output of the same value.

Finally, a transition table for the FSM is generated. This is done by using the
ConstEval C++ helper class (defined in kernel/consteval.h) that can be used to
evaluate parts of the design. The ConstEval class can be asked to calculate a
given set of result signals using a set of signal-value assignments. It can also
be passed a list of stop-signals that abort the ConstEval algorithm if the value
of a stop-signal is needed in order to calculate the result signals.

The :cmd:ref:`fsm_extract` pass uses the ConstEval class in the following way to
create a transition table. For each state:

1. Create a ConstEval object for the module containing the FSM
2. Add all control inputs to the list of stop signals
3. Set the state signal to the current state
4. Try to evaluate the next state and control output
5. If step 4 was not successful:
   
   -  Recursively goto step 4 with the offending stop-signal set to 0.
   -  Recursively goto step 4 with the offending stop-signal set to 1.

6. If step 4 was successful: Emit transition

Finally a ``$fsm`` cell is created with the generated transition table and added
to the module. This new cell is connected to the control signals and the old
drivers for the control outputs are disconnected.

FSM optimization
~~~~~~~~~~~~~~~~

The :cmd:ref:`fsm_opt` pass performs basic optimizations on ``$fsm`` cells (not
including state recoding). The following optimizations are performed (in this
order):

-  Unused control outputs are removed from the ``$fsm`` cell. The attribute
   ``\unused_bits`` (that is usually set by the :cmd:ref:`opt_clean` pass) is
   used to determine which control outputs are unused.

-  Control inputs that are connected to the same driver are merged.

-  When a control input is driven by a control output, the control input is
   removed and the transition table altered to give the same performance without
   the external feedback path.

-  Entries in the transition table that yield the same output and only differ in
   the value of a single control input bit are merged and the different bit is
   removed from the sensitivity list (turned into a don't-care bit).

-  Constant inputs are removed and the transition table is altered to give an
   unchanged behaviour.

-  Unused inputs are removed.

FSM recoding
~~~~~~~~~~~~

The :cmd:ref:`fsm_recode` pass assigns new bit pattern to the states. Usually
this also implies a change in the width of the state signal. At the moment of
this writing only one-hot encoding with all-zero for the reset state is
supported.

The :cmd:ref:`fsm_recode` pass can also write a text file with the changes
performed by it that can be used when verifying designs synthesized by Yosys
using Synopsys Formality.



synthesis/index.rst
--------------------------------------
Synthesis in detail
-------------------

Synthesis can generally be broken down into coarse-grain synthesis, and
fine-grain synthesis.  We saw this in :doc:`/getting_started/example_synth`
where a design was loaded and elaborated and then went through a series of
coarse-grain optimizations before being mapped to hard blocks and fine-grain
cells.  Most commands in Yosys will target either coarse-grain representation or
fine-grain representation, with only a select few compatible with both states.

Commands such as :cmd:ref:`proc`, :cmd:ref:`fsm`, and :cmd:ref:`memory` rely on
the additional information in the coarse-grain representation, along with a
number of optimizations such as :cmd:ref:`wreduce`, :cmd:ref:`share`, and
:cmd:ref:`alumacc`.  :cmd:ref:`opt` provides optimizations which are useful in
both states, while :cmd:ref:`techmap` is used to convert coarse-grain cells
to the corresponding fine-grain representation.

Single-bit cells (logic gates, FFs) as well as LUTs, half-adders, and
full-adders make up the bulk of the fine-grain representation and are necessary
for commands such as :cmd:ref:`abc`\ /:cmd:ref:`abc9`, :cmd:ref:`simplemap`,
:cmd:ref:`dfflegalize`, and :cmd:ref:`memory_map`.

.. toctree::
   :maxdepth: 3

   synth
   proc
   fsm
   memory
   opt
   techmap_synth
   extract
   abc
   cell_libs




synthesis/memory.rst
--------------------------------------
Memory handling
===============

The :cmd:ref:`memory` command
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In the RTL netlist, memory reads and writes are individual cells. This makes
consolidating the number of ports for a memory easier. The :cmd:ref:`memory`
pass transforms memories to an implementation. Per default that is logic for
address decoders and registers. It also is a macro command that calls the other
common ``memory_*`` passes in a sensible order:

.. literalinclude:: /code_examples/macro_commands/memory.ys
   :language: yoscrypt
   :start-after: #end:
   :caption: Passes called by :cmd:ref:`memory`

.. todo:: Make ``memory_*`` notes less quick

Some quick notes:

-  :cmd:ref:`memory_dff` merges registers into the memory read- and write cells.
-  :cmd:ref:`memory_collect` collects all read and write cells for a memory and
   transforms them into one multi-port memory cell.
-  :cmd:ref:`memory_map` takes the multi-port memory cell and transforms it to
   address decoder logic and registers.

For more information about :cmd:ref:`memory`, such as disabling certain sub
commands, see :doc:`/cmd/memory`.

Example
-------

.. todo:: describe ``memory`` images

|code_examples/synth_flow|_.

.. |code_examples/synth_flow| replace:: :file:`docs/source/code_examples/synth_flow`
.. _code_examples/synth_flow: https://github.com/YosysHQ/yosys/tree/main/docs/source/code_examples/synth_flow

.. figure:: /_images/code_examples/synth_flow/memory_01.*
   :class: width-helper invert-helper

.. literalinclude:: /code_examples/synth_flow/memory_01.ys
   :language: yoscrypt
   :caption: :file:`memory_01.ys`

.. literalinclude:: /code_examples/synth_flow/memory_01.v
   :language: verilog
   :caption: :file:`memory_01.v`

.. figure:: /_images/code_examples/synth_flow/memory_02.*
   :class: width-helper invert-helper

.. literalinclude:: /code_examples/synth_flow/memory_02.v
   :language: verilog
   :caption: :file:`memory_02.v`

.. literalinclude:: /code_examples/synth_flow/memory_02.ys
   :language: yoscrypt
   :caption: :file:`memory_02.ys`

.. _memory_map:

Memory mapping
^^^^^^^^^^^^^^

Usually it is preferred to use architecture-specific RAM resources for memory.
For example:

.. code-block:: yoscrypt

    memory -nomap
    memory_libmap -lib my_memory_map.txt
    techmap -map my_memory_map.v
    memory_map

:cmd:ref:`memory_libmap` attempts to convert memory cells (``$mem_v2`` etc) into
hardware supported memory using a provided library (:file:`my_memory_map.txt` in the
example above).  Where necessary, emulation logic is added to ensure functional
equivalence before and after this conversion. :yoscrypt:`techmap -map
my_memory_map.v` then uses :cmd:ref:`techmap` to map to hardware primitives. Any
leftover memory cells unable to be converted are then picked up by
:cmd:ref:`memory_map` and mapped to DFFs and address decoders.

.. note::

   More information about what mapping options are available and associated
   costs of each can be found by enabling debug outputs.  This can be done with
   the :cmd:ref:`debug` command, or by using the ``-g`` flag when calling Yosys
   to globally enable debug messages.

For more on the lib format for :cmd:ref:`memory_libmap`, see
`passes/memory/memlib.md
<https://github.com/YosysHQ/yosys/blob/main/passes/memory/memlib.md>`_

Supported memory patterns
^^^^^^^^^^^^^^^^^^^^^^^^^

Note that not all supported patterns are included in this document, of
particular note is that combinations of multiple patterns should generally work.
For example, `wbe`_ could be used in conjunction with any of the simple dual
port (SDP) models.  In general if a hardware memory definition does not support
a given configuration, additional logic will be instantiated to guarantee
behaviour is consistent with simulation.

Notes
-----

Memory kind selection
~~~~~~~~~~~~~~~~~~~~~

The memory inference code will automatically pick target memory primitive based on memory geometry
and features used.  Depending on the target, there can be up to four memory primitive classes
available for selection:

- FF RAM (aka logic): no hardware primitive used, memory lowered to a bunch of FFs and multiplexers

  - Can handle arbitrary number of write ports, as long as all write ports are in the same clock domain
  - Can handle arbitrary number and kind of read ports

- LUT RAM (aka distributed RAM): uses LUT storage as RAM
  
  - Supported on most FPGAs (with notable exception of ice40)
  - Usually has one synchronous write port, one or more asynchronous read ports
  - Small
  - Will never be used for ROMs (lowering to plain LUTs is always better)

- Block RAM: dedicated memory tiles

  - Supported on basically all FPGAs
  - Supports only synchronous reads
  - Two ports with separate clocks
  - Usually supports true dual port (with notable exception of ice40 that only supports SDP)
  - Usually supports asymmetric memories and per-byte write enables
  - Several kilobits in size

- Huge RAM:

  - Only supported on several targets:
    
    - Some Xilinx UltraScale devices (UltraRAM)

      - Two ports, both with mutually exclusive synchronous read and write
      - Single clock
      - Initial data must be all-0

    - Some ice40 devices (SPRAM)

      - Single port with mutually exclusive synchronous read and write
      - Does not support initial data

    - Nexus (large RAM)
      
      - Two ports, both with mutually exclusive synchronous read and write
      - Single clock

  - Will not be automatically selected by memory inference code, needs explicit opt-in via
    ram_style attribute

In general, you can expect the automatic selection process to work roughly like this:

- If any read port is asynchronous, only LUT RAM (or FF RAM) can be used.
- If there is more than one write port, only block RAM can be used, and this needs to be a
  hardware-supported true dual port pattern

  - … unless all write ports are in the same clock domain, in which case FF RAM can also be used,
    but this is generally not what you want for anything but really small memories

- Otherwise, either FF RAM, LUT RAM, or block RAM will be used, depending on memory size

This process can be overridden by attaching a ram_style attribute to the memory:

- `(* ram_style = "logic" *)` selects FF RAM
- `(* ram_style = "distributed" *)` selects LUT RAM
- `(* ram_style = "block" *)` selects block RAM
- `(* ram_style = "huge" *)` selects huge RAM

It is an error if this override cannot be realized for the given target.

Many alternate spellings of the attribute are also accepted, for compatibility with other software.

Initial data
~~~~~~~~~~~~

Most FPGA targets support initializing all kinds of memory to user-provided values.  If explicit
initialization is not used the initial memory value is undefined.  Initial data can be provided by
either initial statements writing memory cells one by one of ``$readmemh`` or ``$readmemb`` system
tasks.  For an example pattern, see `sr_init`_.

.. _wbe:

Write port with byte enables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Byte enables can be used with any supported pattern
- To ensure that multiple writes will be merged into one port, they need to have disjoint bit
  ranges, have the same address, and the same clock
- Any write enable granularity will be accepted (down to per-bit write enables), but using smaller
  granularity than natively supported by the target is very likely to be inefficient (eg. using
  4-bit bytes on ECP5 will result in either padding the bytes with 5 dummy bits to native 9-bit
  units or splitting the RAM into two block RAMs)

.. code:: verilog

	reg [31 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge clk) begin
		if (write_enable[0])
			mem[write_addr][7:0] <= write_data[7:0];
		if (write_enable[1])
			mem[write_addr][15:8] <= write_data[15:8];
		if (write_enable[2])
			mem[write_addr][23:16] <= write_data[23:16];
		if (write_enable[3])
			mem[write_addr][31:24] <= write_data[31:24];
		if (read_enable)
			read_data <= mem[read_addr];
	end

Simple dual port (SDP) memory patterns
--------------------------------------

.. todo:: assorted enables, e.g. cen, wen+ren

Asynchronous-read SDP
~~~~~~~~~~~~~~~~~~~~~

- This will result in LUT RAM on supported targets

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];
	always @(posedge clk)
		if (write_enable)
			mem[write_addr] <= write_data;
	assign read_data = mem[read_addr];

Synchronous SDP with clock domain crossing
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Will result in block RAM or LUT RAM depending on size
- No behavior guarantees in case of simultaneous read and write to the same address

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge write_clk) begin
		if (write_enable)
			mem[write_addr] <= write_data;
	end

	always @(posedge read_clk) begin
		if (read_enable)
			read_data <= mem[read_addr];
	end

Synchronous SDP read first
~~~~~~~~~~~~~~~~~~~~~~~~~~

- The read and write parts can be in the same or different processes.
- Will result in block RAM or LUT RAM depending on size
- As long as the same clock is used for both, yosys will ensure read-first behavior.  This may
  require extra circuitry on some targets for block RAM.  If this is not necessary, use one of the
  patterns below.

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge clk) begin
		if (write_enable)
			mem[write_addr] <= write_data;
		if (read_enable)
			read_data <= mem[read_addr];
	end

.. _no_rw_check:

Synchronous SDP with undefined collision behavior
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Like above, but the read value is undefined when read and write ports target the same address in
  the same cycle

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge clk) begin
		if (write_enable)
			mem[write_addr] <= write_data;

		if (read_enable) begin
			read_data <= mem[read_addr];
		
		if (write_enable && read_addr == write_addr)
			// this if block
			read_data <= 'x;
		end
	end

- Or below, using the no_rw_check attribute

.. code:: verilog

	(* no_rw_check *)
	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge clk) begin
		if (write_enable)
			mem[write_addr] <= write_data;

		if (read_enable) 
			read_data <= mem[read_addr];
	end

.. _sdp_wf:

Synchronous SDP with write-first behavior
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Will result in block RAM or LUT RAM depending on size
- May use additional circuitry for block RAM if write-first is not natively supported. Will always
  use additional circuitry for LUT RAM.

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge clk) begin
		if (write_enable)
			mem[write_addr] <= write_data;

		if (read_enable) begin
			read_data <= mem[read_addr];
			if (write_enable && read_addr == write_addr)
				read_data <= write_data;
		end
	end

Synchronous SDP with write-first behavior (alternate pattern)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- This pattern is supported for compatibility, but is much less flexible than the above

.. code:: verilog

	reg [ADDR_WIDTH - 1 : 0] read_addr_reg;
	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge clk) begin
		if (write_enable)
			mem[write_addr] <= write_data;
		read_addr_reg <= read_addr;
	end

	assign read_data = mem[read_addr_reg];

Single-port RAM memory patterns
-------------------------------

Asynchronous-read single-port RAM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Will result in single-port LUT RAM on supported targets

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];
	always @(posedge clk)
		if (write_enable)
			mem[addr] <= write_data;
	assign read_data = mem[addr];

Synchronous single-port RAM with mutually exclusive read/write
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Will result in single-port block RAM or LUT RAM depending on size
- This is the correct pattern to infer ice40 SPRAM (with manual ram_style selection)
- On targets that don't support read/write block RAM ports (eg. ice40), will result in SDP block RAM instead
- For block RAM, will use "NO_CHANGE" mode if available

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge clk) begin
		if (write_enable)
			mem[addr] <= write_data;
		else if (read_enable)
			read_data <= mem[addr];
	end

Synchronous single-port RAM with read-first behavior
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Will only result in single-port block RAM when read-first behavior is natively supported;
  otherwise, SDP RAM with additional circuitry will be used
- Many targets (Xilinx, ECP5, …) can only natively support read-first/write-first single-port RAM
  (or TDP RAM) where the write_enable signal implies the read_enable signal (ie. can never write
  without reading). The memory inference code will run a simple SAT solver on the control signals to
  determine if this is the case, and insert emulation circuitry if it cannot be easily proven.

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge clk) begin
		if (write_enable)
			mem[addr] <= write_data;
		if (read_enable)
			read_data <= mem[addr];
	end

Synchronous single-port RAM with write-first behavior
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Will result in single-port block RAM or LUT RAM when supported
- Block RAMs will require extra circuitry if write-first behavior not natively supported

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge clk) begin
		if (write_enable)
			mem[addr] <= write_data;
		if (read_enable)
			if (write_enable)
				read_data <= write_data;
			else 
				read_data <= mem[addr];
	end

.. _sr_init:

Synchronous read port with initial value
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Initial read port values can be combined with any other supported pattern
- If block RAM is used and initial read port values are not natively supported by the target, small
  emulation circuit will be inserted

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];
	reg [DATA_WIDTH - 1 : 0] read_data;
	initial read_data = 'h1234;

	always @(posedge clk) begin
		if (write_enable)
			mem[write_addr] <= write_data;
		if (read_enable)
			read_data <= mem[read_addr];
	end

Read register reset patterns
----------------------------

Resets can be combined with any other supported pattern (except that synchronous reset and
asynchronous reset cannot both be used on a single read port).  If block RAM is used and the
selected reset (synchronous or asynchronous) is used but not natively supported by the target, small
emulation circuitry will be inserted.

Synchronous reset, reset priority over enable
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge clk) begin
		if (write_enable)
			mem[write_addr] <= write_data;

		if (read_reset)
			read_data <= 'h1234;
		else if (read_enable)
			read_data <= mem[read_addr];
	end

Synchronous reset, enable priority over reset
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge clk) begin
		if (write_enable)
			mem[write_addr] <= write_data;
		if (read_enable)
			if (read_reset)
				read_data <= 'h1234;
			else
				read_data <= mem[read_addr];
	end

Synchronous read port with asynchronous reset
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge clk) begin
		if (write_enable)
			mem[write_addr] <= write_data;
	end

	always @(posedge clk, posedge read_reset) begin
		if (read_reset)
			read_data <= 'h1234;
		else if (read_enable)
			read_data <= mem[read_addr];
	end

Asymmetric memory patterns
--------------------------

To construct an asymmetric memory (memory with read/write ports of differing widths):

- Declare the memory with the width of the narrowest intended port
- Split all wide ports into multiple narrow ports
- To ensure the wide ports will be correctly merged:

  - For the address, use a concatenation of actual address in the high bits and a constant in the
    low bits
  - Ensure the actual address is identical for all ports belonging to the wide port
  - Ensure that clock is identical
  - For read ports, ensure that enable/reset signals are identical (for write ports, the enable
    signal may vary — this will result in using the byte enable functionality)

Asymmetric memory is supported on all targets, but may require emulation circuitry where not
natively supported.  Note that when the memory is larger than the underlying block RAM primitive,
hardware asymmetric memory support is likely not to be used even if present as it is more expensive.

.. _wide_sr:

Wide synchronous read port
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code:: verilog

	reg [7:0] mem [0:255];
	wire [7:0] write_addr;
	wire [5:0] read_addr;
	wire [7:0] write_data;
	reg [31:0] read_data;

	always @(posedge clk) begin
		if (write_enable)
			mem[write_addr] <= write_data;
		if (read_enable) begin
			read_data[7:0] <= mem[{read_addr, 2'b00}];
			read_data[15:8] <= mem[{read_addr, 2'b01}];
			read_data[23:16] <= mem[{read_addr, 2'b10}];
			read_data[31:24] <= mem[{read_addr, 2'b11}];
		end
	end

Wide asynchronous read port
~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Note: the only target natively supporting this pattern is Xilinx UltraScale

.. code:: verilog

	reg [7:0] mem [0:511];
	wire [8:0] write_addr;
	wire [5:0] read_addr;
	wire [7:0] write_data;
	wire [63:0] read_data;

	always @(posedge clk) begin
		if (write_enable)
			mem[write_addr] <= write_data;
	end

	assign read_data[7:0] = mem[{read_addr, 3'b000}];
	assign read_data[15:8] = mem[{read_addr, 3'b001}];
	assign read_data[23:16] = mem[{read_addr, 3'b010}];
	assign read_data[31:24] = mem[{read_addr, 3'b011}];
	assign read_data[39:32] = mem[{read_addr, 3'b100}];
	assign read_data[47:40] = mem[{read_addr, 3'b101}];
	assign read_data[55:48] = mem[{read_addr, 3'b110}];
	assign read_data[63:56] = mem[{read_addr, 3'b111}];

Wide write port
~~~~~~~~~~~~~~~

.. code:: verilog

	reg [7:0] mem [0:255];
	wire [5:0] write_addr;
	wire [7:0] read_addr;
	wire [31:0] write_data;
	reg [7:0] read_data;

	always @(posedge clk) begin
		if (write_enable[0])
			mem[{write_addr, 2'b00}] <= write_data[7:0];
		if (write_enable[1])
			mem[{write_addr, 2'b01}] <= write_data[15:8];
		if (write_enable[2])
			mem[{write_addr, 2'b10}] <= write_data[23:16];
		if (write_enable[3])
			mem[{write_addr, 2'b11}] <= write_data[31:24];
		if (read_enable)
			read_data <= mem[read_addr];
	end

True dual port (TDP) patterns
-----------------------------

- Many different variations of true dual port memory can be created by combining two single-port RAM
  patterns on the same memory
- When TDP memory is used, memory inference code has much less maneuver room to create requested
  semantics compared to individual single-port patterns (which can end up lowered to SDP memory
  where necessary) — supported patterns depend strongly on the target
- In particular, when both ports have the same clock, it's likely that "undefined collision" mode
  needs to be manually selected to enable TDP memory inference
- The examples below are non-exhaustive — many more combinations of port types are possible
- Note: if two write ports are in the same process, this defines a priority relation between them
  (if both ports are active in the same clock, the later one wins). On almost all targets, this will
  result in a bit of extra circuitry to ensure the priority semantics. If this is not what you want,
  put them in separate processes.

  - Priority is not supported when using the verific front end and any priority semantics are ignored.

TDP with different clocks, exclusive read/write
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge clk_a) begin
		if (write_enable_a)
			mem[addr_a] <= write_data_a;
		else if (read_enable_a)
			read_data_a <= mem[addr_a];
	end

	always @(posedge clk_b) begin
		if (write_enable_b)
			mem[addr_b] <= write_data_b;
		else if (read_enable_b)
			read_data_b <= mem[addr_b];
	end

TDP with same clock, read-first behavior
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- This requires hardware inter-port read-first behavior, and will only work on some targets (Xilinx, Nexus)

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge clk) begin
		if (write_enable_a)
			mem[addr_a] <= write_data_a;
		if (read_enable_a)
			read_data_a <= mem[addr_a];
	end

	always @(posedge clk) begin
		if (write_enable_b)
			mem[addr_b] <= write_data_b;
		if (read_enable_b)
			read_data_b <= mem[addr_b];
	end

TDP with multiple read ports
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- The combination of a single write port with an arbitrary amount of read ports is supported on all
  targets — if a multi-read port primitive is available (like Xilinx RAM64M), it'll be used as
  appropriate.  Otherwise, the memory will be automatically split into multiple primitives.

.. code:: verilog

	reg [31:0] mem [0:31];

	always @(posedge clk) begin
		if (write_enable)
			mem[write_addr] <= write_data;
	end

	assign read_data_a = mem[read_addr_a];
	assign read_data_b = mem[read_addr_b];
	assign read_data_c = mem[read_addr_c];

Patterns only supported with Verific
------------------------------------

Synchronous SDP with write-first behavior via blocking assignments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Use `sdp_wf`_ for compatibility with Yosys
  Verilog frontend.

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @(posedge clk) begin
		if (write_enable)
			mem[write_addr] = write_data;

		if (read_enable)
			read_data <= mem[read_addr];
	end

Asymmetric memories via part selection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Build wide ports out of narrow ports instead (see `wide_sr`_) for
  compatibility with Yosys Verilog frontend.

.. code:: verilog

	reg [31:0] mem [2**ADDR_WIDTH - 1 : 0];

	wire [1:0] byte_lane;
	wire [7:0] write_data;

	always @(posedge clk) begin
		if (write_enable)
			mem[write_addr][byte_lane * 8 +: 8] <= write_data;

		if (read_enable)
			read_data <= mem[read_addr];
	end


Undesired patterns
------------------

Asynchronous writes
~~~~~~~~~~~~~~~~~~~

- Not supported in modern FPGAs
- Not supported in yosys code anyhow

.. code:: verilog

	reg [DATA_WIDTH - 1 : 0] mem [2**ADDR_WIDTH - 1 : 0];

	always @* begin
		if (write_enable)
			mem[write_addr] = write_data;
	end

	assign read_data = mem[read_addr];




synthesis/opt.rst
--------------------------------------
Optimization passes
===================

Yosys employs a number of optimizations to generate better and cleaner results.
This chapter outlines these optimizations.

.. todo:: "outlines these optimizations" or "outlines *some*.."?

The :cmd:ref:`opt` macro command
--------------------------------

The Yosys pass :cmd:ref:`opt` runs a number of simple optimizations. This
includes removing unused signals and cells and const folding. It is recommended
to run this pass after each major step in the synthesis script.  As listed in
:doc:`/cmd/opt`, this macro command calls the following ``opt_*`` commands:

.. literalinclude:: /code_examples/macro_commands/opt.ys
   :language: yoscrypt
   :start-after: #end:
   :caption: Passes called by :cmd:ref:`opt`

.. _adv_opt_expr:

Constant folding and simple expression rewriting - :cmd:ref:`opt_expr`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. todo:: unsure if this is too much detail and should be in :doc:`/yosys_internals/index`

This pass performs constant folding on the internal combinational cell types
described in :doc:`/yosys_internals/formats/cell_library`. This means a cell
with all constant inputs is replaced with the constant value this cell drives.
In some cases this pass can also optimize cells with some constant inputs.

.. table:: Const folding rules for ``$_AND_`` cells as used in :cmd:ref:`opt_expr`.
   :name: tab:opt_expr_and
   :align: center

   ========= ========= ===========
   A-Input   B-Input   Replacement
   ========= ========= ===========
   any       0         0
   0         any       0
   1         1         1
   --------- --------- -----------
   X/Z       X/Z       X
   1         X/Z       X
   X/Z       1         X
   --------- --------- -----------
   any       X/Z       0
   X/Z       any       0
   --------- --------- -----------
   :math:`a` 1         :math:`a`
   1         :math:`b` :math:`b`
   ========= ========= ===========

:numref:`Table %s <tab:opt_expr_and>` shows the replacement rules used for
optimizing an ``$_AND_`` gate. The first three rules implement the obvious const
folding rules. Note that 'any' might include dynamic values calculated by other
parts of the circuit. The following three lines propagate undef (X) states.
These are the only three cases in which it is allowed to propagate an undef
according to Sec. 5.1.10 of IEEE Std. 1364-2005 :cite:p:`Verilog2005`.

The next two lines assume the value 0 for undef states. These two rules are only
used if no other substitutions are possible in the current module. If other
substitutions are possible they are performed first, in the hope that the 'any'
will change to an undef value or a 1 and therefore the output can be set to
undef.

The last two lines simply replace an ``$_AND_`` gate with one constant-1 input
with a buffer.

Besides this basic const folding the :cmd:ref:`opt_expr` pass can replace 1-bit
wide ``$eq`` and ``$ne`` cells with buffers or not-gates if one input is
constant.  Equality checks may also be reduced in size if there are redundant
bits in the arguments (i.e. bits which are constant on both inputs).  This can,
for example, result in a 32-bit wide constant like ``255`` being reduced to the
8-bit value of ``8'11111111`` if the signal being compared is only 8-bit as in
:ref:`addr_gen_clean` of :doc:`/getting_started/example_synth`.

The :cmd:ref:`opt_expr` pass is very conservative regarding optimizing ``$mux``
cells, as these cells are often used to model decision-trees and breaking these
trees can interfere with other optimizations.

.. literalinclude:: /code_examples/opt/opt_expr.ys
   :language: Verilog
   :start-after: read_verilog <<EOT
   :end-before: EOT
   :caption: example verilog for demonstrating :cmd:ref:`opt_expr`

.. figure:: /_images/code_examples/opt/opt_expr.*
   :class: width-helper invert-helper

   Before and after :cmd:ref:`opt_expr`

Merging identical cells - :cmd:ref:`opt_merge`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This pass performs trivial resource sharing. This means that this pass
identifies cells with identical inputs and replaces them with a single instance
of the cell.

The option ``-nomux`` can be used to disable resource sharing for multiplexer
cells (``$mux`` and ``$pmux``.) This can be useful as it prevents multiplexer
trees to be merged, which might prevent :cmd:ref:`opt_muxtree` to identify
possible optimizations.

.. literalinclude:: /code_examples/opt/opt_merge.ys
   :language: Verilog
   :start-after: read_verilog <<EOT
   :end-before: EOT
   :caption: example verilog for demonstrating :cmd:ref:`opt_merge`

.. figure:: /_images/code_examples/opt/opt_merge.*
   :class: width-helper invert-helper

   Before and after :cmd:ref:`opt_merge`

Removing never-active branches from multiplexer tree - :cmd:ref:`opt_muxtree`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This pass optimizes trees of multiplexer cells by analyzing the select inputs.
Consider the following simple example:

.. literalinclude:: /code_examples/opt/opt_muxtree.ys
   :language: Verilog
   :start-after: read_verilog <<EOT
   :end-before: EOT
   :caption: example verilog for demonstrating :cmd:ref:`opt_muxtree`

The output can never be ``c``, as this would require ``a`` to be 1 for the outer
multiplexer and 0 for the inner multiplexer. The :cmd:ref:`opt_muxtree` pass
detects this contradiction and replaces the inner multiplexer with a constant 1,
yielding the logic for ``y = a ? b : d``.

.. figure:: /_images/code_examples/opt/opt_muxtree.*
   :class: width-helper invert-helper

   Before and after :cmd:ref:`opt_muxtree`

Simplifying large MUXes and AND/OR gates - :cmd:ref:`opt_reduce`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This is a simple optimization pass that identifies and consolidates identical
input bits to ``$reduce_and`` and ``$reduce_or`` cells. It also sorts the input
bits to ease identification of shareable ``$reduce_and`` and ``$reduce_or``
cells in other passes.

This pass also identifies and consolidates identical inputs to multiplexer
cells. In this case the new shared select bit is driven using a ``$reduce_or``
cell that combines the original select bits.

Lastly this pass consolidates trees of ``$reduce_and`` cells and trees of
``$reduce_or`` cells to single large ``$reduce_and`` or ``$reduce_or`` cells.

These three simple optimizations are performed in a loop until a stable result
is produced.

Merging mutually exclusive cells with shared inputs - :cmd:ref:`opt_share`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This pass identifies mutually exclusive cells of the same type that:
   a. share an input signal, and
   b. drive the same ``$mux``, ``$_MUX_``, or ``$pmux`` multiplexing cell,

allowing the cell to be merged and the multiplexer to be moved from
multiplexing its output to multiplexing the non-shared input signals.

.. literalinclude:: /code_examples/opt/opt_share.ys
   :language: Verilog
   :start-after: read_verilog <<EOT
   :end-before: EOT
   :caption: example verilog for demonstrating :cmd:ref:`opt_share`

.. figure:: /_images/code_examples/opt/opt_share.*
   :class: width-helper invert-helper

   Before and after :cmd:ref:`opt_share`

When running :cmd:ref:`opt` in full, the original ``$mux`` (labeled ``$3``) is
optimized away by :cmd:ref:`opt_expr`.

Performing DFF optimizations - :cmd:ref:`opt_dff`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This pass identifies single-bit d-type flip-flops (``$_DFF_``, ``$dff``, and
``$adff`` cells) with a constant data input and replaces them with a constant
driver.  It can also merge clock enables and synchronous reset multiplexers,
removing unused control inputs.

Called with ``-nodffe`` and ``-nosdff``, this pass is used to prepare a design
for :doc:`/using_yosys/synthesis/fsm`.

Removing unused cells and wires - :cmd:ref:`opt_clean` pass
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This pass identifies unused signals and cells and removes them from the design.
It also creates an ``\unused_bits`` attribute on wires with unused bits. This
attribute can be used for debugging or by other optimization passes.

When to use :cmd:ref:`opt` or :cmd:ref:`clean`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Usually it does not hurt to call :cmd:ref:`opt` after each regular command in
the synthesis script. But it increases the synthesis time, so it is favourable
to only call :cmd:ref:`opt` when an improvement can be achieved.

It is generally a good idea to call :cmd:ref:`opt` before inherently expensive
commands such as :cmd:ref:`sat` or :cmd:ref:`freduce`, as the possible gain is
much higher in these cases as the possible loss.

The :cmd:ref:`clean` command, which is an alias for :cmd:ref:`opt_clean` with
fewer outputs, on the other hand is very fast and many commands leave a mess
(dangling signal wires, etc). For example, most commands do not remove any wires
or cells. They just change the connections and depend on a later call to clean
to get rid of the now unused objects. So the occasional ``;;``, which itself is
an alias for :cmd:ref:`clean`, is a good idea in every synthesis script, e.g:

.. code-block:: yoscrypt

   hierarchy; proc; opt; memory; opt_expr;; fsm;;

Other optimizations
-------------------

.. todo:: more on the other optimizations

- :doc:`/cmd/wreduce`
- :doc:`/cmd/peepopt`
- :doc:`/cmd/share`
- :cmd:ref:`abc` and :cmd:ref:`abc9`, see also: :doc:`abc`.



synthesis/proc.rst
--------------------------------------
Converting process blocks
~~~~~~~~~~~~~~~~~~~~~~~~~

.. role:: yoscrypt(code)
   :language: yoscrypt

The Verilog frontend converts ``always``-blocks to RTL netlists for the
expressions and "processess" for the control- and memory elements. The
:cmd:ref:`proc` command then transforms these "processess" to netlists of RTL
multiplexer and register cells. It also is a macro command that calls the other
``proc_*`` commands in a sensible order:

.. literalinclude:: /code_examples/macro_commands/proc.ys
   :language: yoscrypt
   :start-after: #end:
   :caption: Passes called by :cmd:ref:`proc`

After all the ``proc_*`` commands, :cmd:ref:`opt_expr` is called. This can be
disabled by calling :yoscrypt:`proc -noopt`.  For more information about
:cmd:ref:`proc`, such as disabling certain sub commands, see :doc:`/cmd/proc`.

Many commands can not operate on modules with "processess" in them. Usually a
call to :cmd:ref:`proc` is the first command in the actual synthesis procedure
after design elaboration.

Example
^^^^^^^

.. todo:: describe ``proc`` images

|code_examples/synth_flow|_.

.. |code_examples/synth_flow| replace:: :file:`docs/source/code_examples/synth_flow`
.. _code_examples/synth_flow: https://github.com/YosysHQ/yosys/tree/main/docs/source/code_examples/synth_flow

.. literalinclude:: /code_examples/synth_flow/proc_01.v
   :language: verilog
   :caption: :file:`proc_01.v`

.. literalinclude:: /code_examples/synth_flow/proc_01.ys
   :language: yoscrypt
   :caption: :file:`proc_01.ys`

.. figure:: /_images/code_examples/synth_flow/proc_01.*
   :class: width-helper invert-helper

.. figure:: /_images/code_examples/synth_flow/proc_02.*
   :class: width-helper invert-helper

.. literalinclude:: /code_examples/synth_flow/proc_02.v
   :language: verilog
   :caption: :file:`proc_02.v`

.. literalinclude:: /code_examples/synth_flow/proc_02.ys
   :language: yoscrypt
   :caption: :file:`proc_02.ys`

.. figure:: /_images/code_examples/synth_flow/proc_03.*
   :class: width-helper invert-helper

.. literalinclude:: /code_examples/synth_flow/proc_03.ys
   :language: yoscrypt
   :caption: :file:`proc_03.ys`

.. literalinclude:: /code_examples/synth_flow/proc_03.v
   :language: verilog
   :caption: :file:`proc_03.v`



synthesis/synth.rst
--------------------------------------
Synth commands
--------------

.. todo:: comment on common ``synth_*`` options, like ``-run``

Packaged ``synth_*`` commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following is a list of all synth commands included in Yosys for different
platforms.  Each command runs a script of sub commands specific to the platform
being targeted.  Note that not all of these scripts are actively maintained and
may not be up-to-date.

- :doc:`/cmd/synth_achronix`
- :doc:`/cmd/synth_anlogic`
- :doc:`/cmd/synth_coolrunner2`
- :doc:`/cmd/synth_easic`
- :doc:`/cmd/synth_ecp5`
- :doc:`/cmd/synth_efinix`
- :doc:`/cmd/synth_fabulous`
- :doc:`/cmd/synth_gatemate`
- :doc:`/cmd/synth_gowin`
- :doc:`/cmd/synth_greenpak4`
- :doc:`/cmd/synth_ice40`
- :doc:`/cmd/synth_intel` (MAX10, Cyclone IV)
- :doc:`/cmd/synth_intel_alm` (Cyclone V, Arria V, Cyclone 10 GX)
- :doc:`/cmd/synth_lattice`
- :doc:`/cmd/synth_nexus`
- :doc:`/cmd/synth_quicklogic`
- :doc:`/cmd/synth_sf2`
- :doc:`/cmd/synth_xilinx`

General synthesis
~~~~~~~~~~~~~~~~~

In addition to the above hardware-specific synth commands, there is also
:doc:`/cmd/prep`.  This command is limited to coarse-grain synthesis, without
getting into any architecture-specific mappings or optimizations.  Among other
things, this is useful for design verification.

The following commands are executed by the :cmd:ref:`prep` command:

.. literalinclude:: /cmd/prep.rst
   :start-at: begin:
   :end-before: .. raw:: latex
   :dedent:

:doc:`/getting_started/example_synth` covers most of these commands and what
they do.



synthesis/techmap_synth.rst
--------------------------------------
Technology mapping 
==================

.. todo:: less academic, check text is coherent

Previous chapters outlined how HDL code is transformed into an RTL netlist. The
RTL netlist is still based on abstract coarse-grain cell types like arbitrary
width adders and even multipliers. This chapter covers how an RTL netlist is
transformed into a functionally equivalent netlist utilizing the cell types
available in the target architecture.

Technology mapping is often performed in two phases. In the first phase RTL
cells are mapped to an internal library of single-bit cells (see
:ref:`sec:celllib_gates`). In the second phase this netlist of internal gate
types is transformed to a netlist of gates from the target technology library.

When the target architecture provides coarse-grain cells (such as block ram or
ALUs), these must be mapped to directly form the RTL netlist, as information on
the coarse-grain structure of the design is lost when it is mapped to bit-width
gate types.

Cell substitution
-----------------

The simplest form of technology mapping is cell substitution, as performed by
the techmap pass. This pass, when provided with a Verilog file that implements
the RTL cell types using simpler cells, simply replaces the RTL cells with the
provided implementation.

When no map file is provided, techmap uses a built-in map file that maps the
Yosys RTL cell types to the internal gate library used by Yosys. The curious
reader may find this map file as `techlibs/common/techmap.v` in the Yosys source
tree.

Additional features have been added to techmap to allow for conditional mapping
of cells (see :doc:`/cmd/techmap`). This can for example be useful if the target
architecture supports hardware multipliers for certain bit-widths but not for
others.

A usual synthesis flow would first use the techmap pass to directly map some RTL
cells to coarse-grain cells provided by the target architecture (if any) and
then use techmap with the built-in default file to map the remaining RTL cells
to gate logic.

Subcircuit substitution
-----------------------

Sometimes the target architecture provides cells that are more powerful than the
RTL cells used by Yosys. For example a cell in the target architecture that can
calculate the absolute-difference of two numbers does not match any single RTL
cell type but only combinations of cells.

For these cases Yosys provides the extract pass that can match a given set of
modules against a design and identify the portions of the design that are
identical (i.e. isomorphic subcircuits) to any of the given modules. These
matched subcircuits are then replaced by instances of the given modules.

The extract pass also finds basic variations of the given modules, such as
swapped inputs on commutative cell types.

In addition to this the extract pass also has limited support for frequent
subcircuit mining, i.e. the process of finding recurring subcircuits in the
design. This has a few applications, including the design of new coarse-grain
architectures :cite:p:`intersynthFdlBookChapter`.

The hard algorithmic work done by the extract pass (solving the isomorphic
subcircuit problem and frequent subcircuit mining) is performed using the
SubCircuit library that can also be used stand-alone without Yosys (see
:ref:`sec:SubCircuit`).

.. _sec:techmap_extern:

Gate-level technology mapping
-----------------------------

.. todo:: newer techmap libraries appear to be largely ``.v`` instead of ``.lib``

On the gate-level the target architecture is usually described by a "Liberty
file". The Liberty file format is an industry standard format that can be used
to describe the behaviour and other properties of standard library cells .

Mapping a design utilizing the Yosys internal gate library (e.g. as a result of
mapping it to this representation using the techmap pass) is performed in two
phases.

First the register cells must be mapped to the registers that are available on
the target architectures. The target architecture might not provide all
variations of d-type flip-flops with positive and negative clock edge,
high-active and low-active asynchronous set and/or reset, etc. Therefore the
process of mapping the registers might add additional inverters to the design
and thus it is important to map the register cells first.

Mapping of the register cells may be performed by using the dfflibmap pass. This
pass expects a Liberty file as argument (using the -liberty option) and only
uses the register cells from the Liberty file.

Secondly the combinational logic must be mapped to the target architecture. This
is done using the external program ABC via the abc pass by using the -liberty
option to the pass. Note that in this case only the combinatorial cells are used
from the cell library.

Occasionally Liberty files contain trade secrets (such as sensitive timing
information) that cannot be shared freely. This complicates processes such as
reporting bugs in the tools involved. When the information in the Liberty file
used by Yosys and ABC are not part of the sensitive information, the additional
tool yosys-filterlib (see :ref:`sec:filterlib`) can be used to strip the
sensitive information from the Liberty file.



yosys_internals/index.rst
--------------------------------------
.. _chapter:overview:

Yosys internals
===============

.. todo:: less academic

Yosys is an extensible open source hardware synthesis tool. It is aimed at
designers who are looking for an easily accessible, universal, and
vendor-independent synthesis tool, as well as scientists who do research in
electronic design automation (EDA) and are looking for an open synthesis
framework that can be used to test algorithms on complex real-world designs.

Yosys can synthesize a large subset of Verilog 2005 and has been tested with a
wide range of real-world designs, including the `OpenRISC 1200 CPU`_, the
`openMSP430 CPU`_, the `OpenCores I2C master`_, and the `k68 CPU`_.

.. todo:: add RISC-V core example

.. _OpenRISC 1200 CPU: https://github.com/openrisc/or1200

.. _openMSP430 CPU: http://opencores.org/projects/openmsp430

.. _OpenCores I2C master: http://opencores.org/projects/i2c

.. _k68 CPU: http://opencores.org/projects/k68

Yosys is written in C++, targeting C++17 at minimum. This chapter describes some
of the fundamental Yosys data structures. For the sake of simplicity the C++
type names used in the Yosys implementation are used in this chapter, even
though the chapter only explains the conceptual idea behind it and can be used
as reference to implement a similar system in any language.

.. toctree::
   :maxdepth: 3

   flow/index
   formats/index
   extending_yosys/index
   techmap



yosys_internals/techmap.rst
--------------------------------------
Techmap by example
------------------

As a quick recap, the :cmd:ref:`techmap` command replaces cells in the design
with implementations given as Verilog code (called "map files"). It can replace
Yosys' internal cell types (such as ``$or``) as well as user-defined cell types.

- Verilog parameters are used extensively to customize the internal cell types.
- Additional special parameters are used by techmap to communicate meta-data to
  the map files.
- Special wires are used to instruct techmap how to handle a module in the map
  file.
- Generate blocks and recursion are powerful tools for writing map files.

Code examples used in this document are included in the Yosys code base under
|code_examples/techmap|_.

.. |code_examples/techmap| replace:: :file:`docs/source/code_examples/techmap`
.. _code_examples/techmap: https://github.com/YosysHQ/yosys/tree/main/docs/source/code_examples/techmap


Mapping OR3X1
~~~~~~~~~~~~~

.. todo:: add/expand supporting text

.. note::

    This is a simple example for demonstration only.  Techmap shouldn't be used
    to implement basic logic optimization.

.. literalinclude:: /code_examples/techmap/red_or3x1_map.v
   :language: verilog
   :caption: :file:`red_or3x1_map.v`

.. figure:: /_images/code_examples/techmap/red_or3x1.*
    :class: width-helper invert-helper

.. literalinclude:: /code_examples/techmap/red_or3x1_test.ys
   :language: yoscrypt
   :caption: :file:`red_or3x1_test.ys`

.. literalinclude:: /code_examples/techmap/red_or3x1_test.v
   :language: verilog
   :caption: :file:`red_or3x1_test.v`

Conditional techmap
~~~~~~~~~~~~~~~~~~~

- In some cases only cells with certain properties should be substituted.
- The special wire ``_TECHMAP_FAIL_`` can be used to disable a module in the map
  file for a certain set of parameters.
- The wire ``_TECHMAP_FAIL_`` must be set to a constant value. If it is non-zero
  then the module is disabled for this set of parameters.
- Example use-cases:

    - coarse-grain cell types that only operate on certain bit widths
    - memory resources for different memory geometries (width, depth, ports,
      etc.)

Example:

.. figure:: /_images/code_examples/techmap/sym_mul.*
    :class: width-helper invert-helper

.. literalinclude:: /code_examples/techmap/sym_mul_map.v
   :language: verilog
   :caption: :file:`sym_mul_map.v`

.. literalinclude:: /code_examples/techmap/sym_mul_test.v
   :language: verilog
   :caption: :file:`sym_mul_test.v`

.. literalinclude:: /code_examples/techmap/sym_mul_test.ys
   :language: yoscrypt
   :caption: :file:`sym_mul_test.ys`


Scripting in map modules
~~~~~~~~~~~~~~~~~~~~~~~~

- The special wires ``_TECHMAP_DO_*`` can be used to run Yosys scripts in the
  context of the replacement module.
- The wire that comes first in alphabetical oder is interpreted as string (must
  be connected to constants) that is executed as script. Then the wire is
  removed. Repeat.
- You can even call techmap recursively!
- Example use-cases:

    - Using always blocks in map module: call :cmd:ref:`proc`
    - Perform expensive optimizations (such as :cmd:ref:`freduce`) on cells
      where this is known to work well.
    - Interacting with custom commands.

.. note:: PROTIP:

    Commands such as :cmd:ref:`shell`, ``show -pause``, and :cmd:ref:`dump` can
    be used in the ``_TECHMAP_DO_*`` scripts for debugging map modules.

Example:

.. figure:: /_images/code_examples/techmap/mymul.*
    :class: width-helper invert-helper

.. literalinclude:: /code_examples/techmap/mymul_map.v
   :language: verilog
   :caption: :file:`mymul_map.v`

.. literalinclude:: /code_examples/techmap/mymul_test.v
   :language: verilog
   :caption: :file:`mymul_test.v`

.. literalinclude:: /code_examples/techmap/mymul_test.ys
   :language: yoscrypt
   :caption: :file:`mymul_test.ys`

Handling constant inputs
~~~~~~~~~~~~~~~~~~~~~~~~

- The special parameters ``_TECHMAP_CONSTMSK_<port-name>_`` and
  ``_TECHMAP_CONSTVAL_<port-name>_`` can be used to handle constant input values
  to cells.
- The former contains 1-bits for all constant input bits on the port.
- The latter contains the constant bits or undef (x) for non-constant bits.
- Example use-cases:

    - Converting arithmetic (for example multiply to shift).
    - Identify constant addresses or enable bits in memory interfaces.

Example:

.. figure:: /_images/code_examples/techmap/mulshift.*
    :class: width-helper invert-helper

.. literalinclude:: /code_examples/techmap/mulshift_map.v
   :language: verilog
   :caption: :file:`mulshift_map.v`

.. literalinclude:: /code_examples/techmap/mulshift_test.v
   :language: verilog
   :caption: :file:`mulshift_test.v`

.. literalinclude:: /code_examples/techmap/mulshift_test.ys
   :language: yoscrypt
   :caption: :file:`mulshift_test.ys`

Handling shorted inputs
~~~~~~~~~~~~~~~~~~~~~~~

- The special parameters ``_TECHMAP_BITS_CONNMAP_`` and
  ``_TECHMAP_CONNMAP_<port-name>_`` can be used to handle shorted inputs.
- Each bit of the port correlates to an ``_TECHMAP_BITS_CONNMAP_`` bits wide
  number in ``_TECHMAP_CONNMAP_<port-name>_``.
- Each unique signal bit is assigned its own number. Identical fields in the
  ``_TECHMAP_CONNMAP_<port-name>_`` parameters mean shorted signal bits.
- The numbers 0-3 are reserved for ``0``, ``1``, ``x``, and ``z`` respectively.
- Example use-cases:

    - Detecting shared clock or control signals in memory interfaces.
    - In some cases this can be used for for optimization.

Example:

.. figure:: /_images/code_examples/techmap/addshift.*
    :class: width-helper invert-helper

.. literalinclude:: /code_examples/techmap/addshift_map.v
   :language: verilog
   :caption: :file:`addshift_map.v`

.. literalinclude:: /code_examples/techmap/addshift_test.v
   :language: verilog
   :caption: :file:`addshift_test.v`

.. literalinclude:: /code_examples/techmap/addshift_test.ys
   :language: yoscrypt
   :caption: :file:`addshift_test.ys`

Notes on using techmap
~~~~~~~~~~~~~~~~~~~~~~

- Don't use positional cell parameters in map modules.
- You can use the ``$__``-prefix for internal cell types to avoid collisions
  with the user-namespace. But always use two underscores or the internal
  consistency checker will trigger on these cells.
- Techmap has two major use cases:

    - Creating good logic-level representation of arithmetic functions. This
      also means using dedicated hardware resources such as half- and full-adder
      cells in ASICS or dedicated carry logic in FPGAs.
    - Mapping of coarse-grain resources such as block memory or DSP cells.



extending_yosys/abc_flow.rst
--------------------------------------
Setting up a flow for ABC9
--------------------------

Much of the configuration comes from attributes and ``specify`` blocks in
Verilog simulation models.

``specify`` syntax
~~~~~~~~~~~~~~~~~~

Since ``specify`` is a relatively obscure part of the Verilog standard, a quick
guide to the syntax:

.. code-block:: verilog

   specify                           // begins a specify block
     (A => B) = 123;                 // simple combinational path from A to B with a delay of 123.
     (A *> B) = 123;                 // simple combinational path from A to all bits of B with a delay of 123 for all.
     if (FOO) (A => B) = 123;        // paths may apply under specific conditions.
     (posedge CLK => (Q : D)) = 123; // combinational path triggered on the positive edge of CLK; used for clock-to-Q arrival paths.
     $setup(A, posedge CLK, 123);    // setup constraint for an input relative to a clock.
   endspecify                        // ends a specify block

By convention, all delays in ``specify`` blocks are in integer picoseconds.
Files containing ``specify`` blocks should be read with the ``-specify`` option
to :cmd:ref:`read_verilog` so that they aren't skipped.

LUTs
^^^^

LUTs need to be annotated with an ``(* abc9_lut=N *)`` attribute, where ``N`` is
the relative area of that LUT model. For example, if an architecture can combine
LUTs to produce larger LUTs, then the combined LUTs would have increasingly
larger ``N``. Conversely, if an architecture can split larger LUTs into smaller
LUTs, then the smaller LUTs would have smaller ``N``.

LUTs are generally specified with simple combinational paths from the LUT inputs
to the LUT output.

DFFs
^^^^

DFFs should be annotated with an ``(* abc9_flop *)`` attribute, however ABC9 has
some specific requirements for this to be valid: - the DFF must initialise to
zero (consider using :cmd:ref:`dfflegalize` to ensure this). - the DFF cannot
have any asynchronous resets/sets (see the simplification idiom and the Boxes
section for what to do here).

It is worth noting that in pure ``abc9`` mode, only the setup and arrival times
are passed to ABC9 (specifically, they are modelled as buffers with the given
delay). In ``abc9 -dff``, the flop itself is passed to ABC9, permitting
sequential optimisations.

Some vendors have universal DFF models which include async sets/resets even when
they're unused. Therefore *the simplification idiom* exists to handle this: by
using a ``techmap`` file to discover flops which have a constant driver to those
asynchronous controls, they can be mapped into an intermediate, simplified flop
which qualifies as an ``(* abc9_flop *)``, ran through :cmd:ref:`abc9`, and then
mapped back to the original flop. This is used in :cmd:ref:`synth_intel_alm` and
:cmd:ref:`synth_quicklogic` for the PolarPro3.

DFFs are usually specified to have setup constraints against the clock on the
input signals, and an arrival time for the ``Q`` output.

Boxes
^^^^^

A "box" is a purely-combinational piece of hard logic. If the logic is exposed
to ABC9, it's a "whitebox", otherwise it's a "blackbox". Carry chains would be
best implemented as whiteboxes, but a DSP would be best implemented as a
blackbox (multipliers are too complex to easily work with). LUT RAMs can be
implemented as whiteboxes too.

Boxes are arguably the biggest advantage that ABC9 has over ABC: by being aware
of carry chains and DSPs, it avoids optimising for a path that isn't the actual
critical path, while the generally-longer paths result in ABC9 being able to
reduce design area by mapping other logic to larger-but-slower cells.



extending_yosys/extensions.rst
--------------------------------------
Writing extensions
==================

.. role:: yoscrypt(code)
   :language: yoscrypt

.. todo:: check text is coherent

.. todo:: update to use :file:`/code_examples/extensions/test*.log`

This chapter contains some bits and pieces of information about programming
yosys extensions. Don't be afraid to ask questions on the YosysHQ Slack.

The `guidelines/` directory of the Yosys source code contains notes on various
aspects of Yosys development. In particular, the files GettingStarted and
CodingStyle may be of interest.

.. todo:: what's in guidelines/GettingStarted that's missing from the manual?

Quick guide
-----------

Code examples from this section are included in the
|code_examples/extensions|_ directory of the Yosys source code.

.. |code_examples/extensions| replace:: :file:`docs/source/code_examples/extensions`
.. _code_examples/extensions: https://github.com/YosysHQ/yosys/tree/main/docs/source/code_examples/extensions


Program components and data formats
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

See :doc:`/yosys_internals/formats/rtlil_rep` document for more information
about the internal data storage format used in Yosys and the classes that it
provides.

This document will focus on the much simpler version of RTLIL left after the
commands :cmd:ref:`proc` and :cmd:ref:`memory` (or :yoscrypt:`memory -nomap`):

.. figure:: /_images/internals/simplified_rtlil.*
    :class: width-helper invert-helper
    :name: fig:Simplified_RTLIL

    Simplified RTLIL entity-relationship diagram without memories and processes

It is possible to only work on this simpler version:

.. todo:: consider replacing inline code

.. code:: c++

    for (RTLIL::Module *module : design->selected_modules() {
        if (module->has_memories_warn() || module->has_processes_warn())
            continue;
        ....
    }

When trying to understand what a command does, creating a small test case to
look at the output of :cmd:ref:`dump` and :cmd:ref:`show` before and after the
command has been executed can be helpful.
:doc:`/using_yosys/more_scripting/selections` has more information on using
these commands.

Creating a command
~~~~~~~~~~~~~~~~~~

.. todo:: add/expand supporting text

Let's create a very simple test command which prints the arguments we called it
with, and lists off the current design's modules.

.. literalinclude:: /code_examples/extensions/my_cmd.cc
   :language: c++
   :lines: 1, 4, 6, 7-20
   :caption: Example command :yoscrypt:`my_cmd` from :file:`my_cmd.cc`
   
Note that we are making a global instance of a class derived from
``Yosys::Pass``, which we get by including :file:`kernel/yosys.h`.

Compiling to a plugin
~~~~~~~~~~~~~~~~~~~~~

Yosys can be extended by adding additional C++ code to the Yosys code base, or
by loading plugins into Yosys.  For maintainability it is generally recommended
to create plugins.

The following command compiles our example :yoscrypt:`my_cmd` to a Yosys plugin:

.. todo:: replace inline code

.. code:: shell

   yosys-config --exec --cxx --cxxflags --ldflags \
   -o my_cmd.so -shared my_cmd.cc --ldlibs

Or shorter:

.. code:: shell

   yosys-config --build my_cmd.so my_cmd.cc

Running Yosys with the ``-m`` option allows the plugin to be used.  Here's a
quick example that also uses the ``-p`` option to run :yoscrypt:`my_cmd foo
bar`.

.. code:: shell-session

   $ yosys -m ./my_cmd.so -p 'my_cmd foo bar'

   -- Running command `my_cmd foo bar' --
   Arguments to my_cmd:
     my_cmd
     foo
     bar
   Modules in current design:

Creating modules from scratch
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's create the following module using the RTLIL API:

.. literalinclude:: /code_examples/extensions/absval_ref.v
   :language: Verilog
   :caption: absval_ref.v

We'll do the same as before and format it as a a ``Yosys::Pass``.

.. literalinclude:: /code_examples/extensions/my_cmd.cc
   :language: c++
   :lines: 23-47
   :caption: :yoscrypt:`test1` - creating the absval module, from :file:`my_cmd.cc`

.. code:: shell-session

   $ yosys -m ./my_cmd.so -p 'test1' -Q

   -- Running command `test1' --
   Name of this module: absval

And if we look at the schematic for this new module we see the following:

.. figure:: /_images/code_examples/extensions/test1.*
   :class: width-helper invert-helper

   Output of ``yosys -m ./my_cmd.so -p 'test1; show'``

Modifying modules
~~~~~~~~~~~~~~~~~

Most commands modify existing modules, not create new ones.

When modifying existing modules, stick to the following DOs and DON'Ts:

- Do not remove wires. Simply disconnect them and let a successive
  :cmd:ref:`clean` command worry about removing it.
- Use ``module->fixup_ports()`` after changing the ``port_*`` properties of
  wires.
- You can safely remove cells or change the ``connections`` property of a cell,
  but be careful when changing the size of the ``SigSpec`` connected to a cell
  port.
- Use the ``SigMap`` helper class (see next section) when you need a unique
  handle for each signal bit.

Using the SigMap helper class
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider the following module:

.. literalinclude:: /code_examples/extensions/sigmap_test.v
   :language: Verilog
   :caption: :file:`sigmap_test.v`

In this case ``a``, ``x``, and ``y`` are all different names for the same
signal. However:

.. todo:: use my_cmd.cc literalincludes

.. code:: C++

    RTLIL::SigSpec a(module->wire("\\a")), x(module->wire("\\x")),
                                           y(module->wire("\\y"));
    log("%d %d %d\n", a == x, x == y, y == a); // will print "0 0 0"

The ``SigMap`` helper class can be used to map all such aliasing signals to a
unique signal from the group (usually the wire that is directly driven by a cell
or port).

.. code:: C++

    SigMap sigmap(module);
    log("%d %d %d\n", sigmap(a) == sigmap(x), sigmap(x) == sigmap(y),
                      sigmap(y) == sigmap(a)); // will print "1 1 1"

Printing log messages
~~~~~~~~~~~~~~~~~~~~~

The ``log()`` function is a ``printf()``-like function that can be used to
create log messages.

Use ``log_signal()`` to create a C-string for a SigSpec object:

.. code:: C++

    log("Mapped signal x: %s\n", log_signal(sigmap(x)));

The pointer returned by ``log_signal()`` is automatically freed by the log
framework at a later time.

Use ``log_id()`` to create a C-string for an ``RTLIL::IdString``:

.. code:: C++

    log("Name of this module: %s\n", log_id(module->name));

Use ``log_header()`` and ``log_push()``/\ ``log_pop()`` to structure log
messages:

.. todo:: replace inline code

.. code:: C++

    log_header(design, "Doing important stuff!\n");
    log_push();
    for (int i = 0; i < 10; i++)
        log("Log message #%d.\n", i);
    log_pop();

Error handling
~~~~~~~~~~~~~~

Use ``log_error()`` to report a non-recoverable error:

.. todo:: replace inline code

.. code:: C++

    if (design->modules.count(module->name) != 0)
        log_error("A module with the name %s already exists!\n",
                   RTLIL::id2cstr(module->name));

Use ``log_cmd_error()`` to report a recoverable error:

.. code:: C++

    if (design->selection_stack.back().empty())
        log_cmd_error("This command can't operator on an empty selection!\n");

Use ``log_assert()`` and ``log_abort()`` instead of ``assert()`` and ``abort()``.

The "stubnets" example module
------------------------------

The following is the complete code of the "stubnets" example module. It is
included in the Yosys source distribution under |code_examples/stubnets|_.

.. |code_examples/stubnets| replace:: :file:`docs/source/code_examples/stubnets`
.. _code_examples/stubnets: https://github.com/YosysHQ/yosys/tree/main/docs/source/code_examples/stubnets

.. literalinclude:: /code_examples/stubnets/stubnets.cc
    :language: c++
    :linenos:
    :caption: :file:`stubnets.cc`

.. literalinclude:: /code_examples/stubnets/Makefile
    :language: makefile
    :linenos:
    :caption: :file:`Makefile`

.. literalinclude:: /code_examples/stubnets/test.v
    :language: verilog
    :linenos:
    :caption: :file:`test.v`



extending_yosys/index.rst
--------------------------------------
Extending Yosys
---------------

.. todo:: brief overview for the extending Yosys index

.. toctree::
   :maxdepth: 3

   extensions
   abc_flow




flow/control_and_data.rst
--------------------------------------
Control and data flow
=====================

.. todo:: less academic

The data- and control-flow of a typical synthesis tool is very similar to the
data- and control-flow of a typical compiler: different subsystems are called in
a predetermined order, each consuming the data generated by the last subsystem
and generating the data for the next subsystem (see :numref:`Fig. %s
<fig:approach_flow>`).

.. figure:: /_images/internals/approach_flow.*
	:class: width-helper invert-helper
	:name: fig:approach_flow

	General data- and control-flow of a synthesis tool

The first subsystem to be called is usually called a frontend. It does not
process the data generated by another subsystem but instead reads the user
input—in the case of a HDL synthesis tool, the behavioural HDL code.

The subsystems that consume data from previous subsystems and produce data for
the next subsystems (usually in the same or a similar format) are called passes.

The last subsystem that is executed transforms the data generated by the last
pass into a suitable output format and writes it to a disk file. This subsystem
is usually called the backend.

In Yosys all frontends, passes and backends are directly available as commands
in the synthesis script. Thus the user can easily create a custom synthesis flow
just by calling passes in the right order in a synthesis script.



flow/index.rst
--------------------------------------
Internal flow
=============

A (usually short) synthesis script controls Yosys.

These scripts contain three types of commands:

- **Frontends**, that read input files (usually Verilog);
- **Passes**, that perform transformations on the design in memory;
- **Backends**, that write the design in memory to a file (various formats are
  available: Verilog, BLIF, EDIF, SPICE, BTOR, . . .).

.. toctree:: 
	:maxdepth: 3

	overview
	control_and_data
	verilog_frontend




flow/overview.rst
--------------------------------------
Flow overview
=============

.. todo:: less academic

:numref:`Figure %s <fig:Overview_flow>` shows the simplified data flow within
Yosys. Rectangles in the figure represent program modules and ellipses internal
data structures that are used to exchange design data between the program
modules.

Design data is read in using one of the frontend modules. The high-level HDL
frontends for Verilog and VHDL code generate an abstract syntax tree (AST) that
is then passed to the AST frontend. Note that both HDL frontends use the same
AST representation that is powerful enough to cover the Verilog HDL and VHDL
language.

The AST Frontend then compiles the AST to Yosys's main internal data format, the
RTL Intermediate Language (RTLIL). A more detailed description of this format is
given in :doc:`/yosys_internals/formats/rtlil_rep`.

There is also a text representation of the RTLIL data structure that can be
parsed using the RTLIL Frontend which is described in
:doc:`/yosys_internals/formats/rtlil_text`.

The design data may then be transformed using a series of passes that all
operate on the RTLIL representation of the design.

Finally the design in RTLIL representation is converted back to text by one of
the backends, namely the Verilog Backend for generating Verilog netlists and the
RTLIL Backend for writing the RTLIL data in the same format that is understood
by the RTLIL Frontend.

With the exception of the AST Frontend, which is called by the high-level HDL
frontends and can't be called directly by the user, all program modules are
called by the user (usually using a synthesis script that contains text commands
for Yosys).

By combining passes in different ways and/or adding additional passes to Yosys
it is possible to adapt Yosys to a wide range of applications. For this to be
possible it is key that (1) all passes operate on the same data structure
(RTLIL) and (2) that this data structure is powerful enough to represent the
design in different stages of the synthesis.

.. figure:: /_images/internals/overview_flow.*
	:class: width-helper invert-helper
	:name: fig:Overview_flow

	Yosys simplified data flow (ellipses: data structures, rectangles:
	program modules)



flow/verilog_frontend.rst
--------------------------------------
.. _chapter:verilog:

The Verilog and AST frontends
=============================

This chapter provides an overview of the implementation of the Yosys Verilog and
AST frontends. The Verilog frontend reads Verilog-2005 code and creates an
abstract syntax tree (AST) representation of the input. This AST representation
is then passed to the AST frontend that converts it to RTLIL data, as
illustrated in :numref:`Fig. %s <fig:Verilog_flow>`.

.. figure:: /_images/internals/verilog_flow.*
	:class: width-helper invert-helper
	:name: fig:Verilog_flow

	Simplified Verilog to RTLIL data flow

Transforming Verilog to AST
---------------------------

The Verilog frontend converts the Verilog sources to an internal AST
representation that closely resembles the structure of the original Verilog
code. The Verilog frontend consists of three components, the Preprocessor, the
Lexer and the Parser.

The source code to the Verilog frontend can be found in
:file:`frontends/verilog/` in the Yosys source tree.

The Verilog preprocessor
~~~~~~~~~~~~~~~~~~~~~~~~

The Verilog preprocessor scans over the Verilog source code and interprets some
of the Verilog compiler directives such as :literal:`\`include`,
:literal:`\`define` and :literal:`\`ifdef`.

It is implemented as a C++ function that is passed a file descriptor as input
and returns the pre-processed Verilog code as a ``std::string``.

The source code to the Verilog Preprocessor can be found in
:file:`frontends/verilog/preproc.cc` in the Yosys source tree.

The Verilog lexer
~~~~~~~~~~~~~~~~~

The Verilog Lexer is written using the lexer generator flex. Its source code can
be found in :file:`frontends/verilog/verilog_lexer.l` in the Yosys source tree.
The lexer does little more than identifying all keywords and literals recognised
by the Yosys Verilog frontend.

The lexer keeps track of the current location in the Verilog source code using
some global variables. These variables are used by the constructor of AST nodes
to annotate each node with the source code location it originated from.

Finally the lexer identifies and handles special comments such as "``// synopsys
translate_off``" and "``// synopsys full_case``". (It is recommended to use
:literal:`\`ifdef` constructs instead of the Synsopsys translate_on/off comments
and attributes such as ``(* full_case *)`` over "``// synopsys full_case``"
whenever possible.)

The Verilog parser
~~~~~~~~~~~~~~~~~~

The Verilog Parser is written using the parser generator bison. Its source code
can be found in :file:`frontends/verilog/verilog_parser.y` in the Yosys source
tree.

It generates an AST using the ``AST::AstNode`` data structure defined in
:file:`frontends/ast/ast.h`. An ``AST::AstNode`` object has the following
properties:

.. list-table:: AST node types with their corresponding Verilog constructs.
    :name: tab:Verilog_AstNodeType
    :widths: 50 50

    * - AST Node Type
      - Corresponding Verilog Construct
    * - AST_NONE
      - This Node type should never be used.
    * - AST_DESIGN
      - This node type is used for the top node of the AST tree. It has no
        corresponding Verilog construct.
    * - AST_MODULE, AST_TASK, AST_FUNCTION
      - ``module``, ``task`` and ``function``
    * - AST_WIRE
      - ``input``, ``output``, ``wire``, ``reg`` and ``integer``
    * - AST_MEMORY
      - Verilog Arrays
    * - AST_AUTOWIRE
      - Created by the simplifier when an undeclared signal name is used.
    * - AST_PARAMETER, AST_LOCALPARAM
      - ``parameter`` and ``localparam``
    * - AST_PARASET
      - Parameter set in cell instantiation
    * - AST_ARGUMENT
      - Port connection in cell instantiation
    * - AST_RANGE
      - Bit-Index in a signal or element index in array
    * - AST_CONSTANT
      - A literal value
    * - AST_CELLTYPE
      - The type of cell in cell instantiation
    * - AST_IDENTIFIER
      - An Identifier (signal name in expression or cell/task/etc. name in other
        contexts)
    * - AST_PREFIX
      - Construct an identifier in the form <prefix>[<index>].<suffix> (used
        only in advanced generate constructs)
    * - AST_FCALL, AST_TCALL
      - Call to function or task
    * - AST_TO_SIGNED, AST_TO_UNSIGNED
      - The ``$signed()`` and ``$unsigned()`` functions
    * - AST_CONCAT, AST_REPLICATE
      - The ``{...}`` and ``{...{...}}`` operators
    * - AST_BIT_NOT, AST_BIT_AND, AST_BIT_OR, AST_BIT_XOR, AST_BIT_XNOR
      - The bitwise operators ``~``, ``&``, ``|``, ``^`` and ``~^``
    * - AST_REDUCE_AND, AST_REDUCE_OR, AST_REDUCE_XOR, AST_REDUCE_XNOR
      - The unary reduction operators ``~``, ``&``, ``|``, ``^`` and ``~^``
    * - AST_REDUCE_BOOL
      - Conversion from multi-bit value to boolean value (equivalent to
        AST_REDUCE_OR)
    * - AST_SHIFT_LEFT, AST_SHIFT_RIGHT, AST_SHIFT_SLEFT, AST_SHIFT_SRIGHT
      - The shift operators ``<<``, ``>>``, ``<<<`` and ``>>>``
    * - AST_LT, AST_LE, AST_EQ, AST_NE, AST_GE, AST_GT
      - The relational operators ``<``, ``<=``, ``==``, ``!=``, ``>=`` and ``>``
    * - AST_ADD, AST_SUB, AST_MUL, AST_DIV, AST_MOD, AST_POW
      - The binary operators ``+``, ``-``, ``*``, ``/``, ``%`` and ``**``
    * - AST_POS, AST_NEG
      - The prefix operators ``+`` and ``-``
    * - AST_LOGIC_AND, AST_LOGIC_OR, AST_LOGIC_NOT
      - The logic operators ``&&``, ``||`` and ``!``
    * - AST_TERNARY
      - The ternary ``?:``-operator
    * - AST_MEMRD AST_MEMWR
      - Read and write memories. These nodes are generated by the AST simplifier
        for writes/reads to/from Verilog arrays.
    * - AST_ASSIGN
      - An ``assign`` statement
    * - AST_CELL
      - A cell instantiation
    * - AST_PRIMITIVE
      - A primitive cell (``and``, ``nand``, ``or``, etc.)
    * - AST_ALWAYS, AST_INITIAL
      - Verilog ``always``- and ``initial``-blocks
    * - AST_BLOCK
      - A ``begin``-``end``-block
    * - AST_ASSIGN_EQ. AST_ASSIGN_LE
      - Blocking (``=``) and nonblocking (``<=``) assignments within an
        ``always``- or ``initial``-block
    * - AST_CASE. AST_COND, AST_DEFAULT
      - The ``case`` (``if``) statements, conditions within a case and the
        default case respectively
    * - AST_FOR
      - A ``for``-loop with an ``always``- or ``initial``-block
    * - AST_GENVAR, AST_GENBLOCK, AST_GENFOR, AST_GENIF
      - The ``genvar`` and ``generate`` keywords and ``for`` and ``if`` within a
        generate block.
    * - AST_POSEDGE, AST_NEGEDGE, AST_EDGE
      - Event conditions for ``always`` blocks.

-  | The node type
   | This enum (``AST::AstNodeType``) specifies the role of the node.
     :numref:`Table %s <tab:Verilog_AstNodeType>` contains a list of all node
     types.

-  | The child nodes
   | This is a list of pointers to all children in the abstract syntax tree.

-  | Attributes
   | As almost every AST node might have Verilog attributes assigned to it, the
     ``AST::AstNode`` has direct support for attributes. Note that the attribute
     values are again AST nodes.

-  | Node content
   | Each node might have additional content data. A series of member variables
     exist to hold such data. For example the member ``std::string str`` can
     hold a string value and is used e.g. in the ``AST_IDENTIFIER`` node type to
     store the identifier name.

-  | Source code location
   | Each ``AST::AstNode`` is automatically annotated with the current source
     code location by the ``AST::AstNode`` constructor. It is stored in the
     ``std::string filename`` and ``int linenum`` member variables.

The ``AST::AstNode`` constructor can be called with up to two child nodes that
are automatically added to the list of child nodes for the new object. This
simplifies the creation of AST nodes for simple expressions a bit. For example
the bison code for parsing multiplications:

.. code:: none
   	:number-lines:

	basic_expr '*' attr basic_expr {
		$$ = new AstNode(AST_MUL, $1, $4);
		append_attr($$, $3);
	} |

The generated AST data structure is then passed directly to the AST frontend
that performs the actual conversion to RTLIL.

Note that the Yosys command ``read_verilog`` provides the options ``-yydebug``
and ``-dump_ast`` that can be used to print the parse tree or abstract syntax
tree respectively.

Transforming AST to RTLIL
-------------------------

The AST Frontend converts a set of modules in AST representation to modules in
RTLIL representation and adds them to the current design. This is done in two
steps: simplification and RTLIL generation.

The source code to the AST frontend can be found in ``frontends/ast/`` in the
Yosys source tree.

AST simplification
~~~~~~~~~~~~~~~~~~

A full-featured AST is too complex to be transformed into RTLIL directly.
Therefore it must first be brought into a simpler form. This is done by calling
the ``AST::AstNode::simplify()`` method of all ``AST_MODULE`` nodes in the AST.
This initiates a recursive process that performs the following transformations
on the AST data structure:

-  Inline all task and function calls.

-  Evaluate all ``generate``-statements and unroll all ``for``-loops.

-  Perform const folding where it is necessary (e.g. in the value part of
   ``AST_PARAMETER``, ``AST_LOCALPARAM``, ``AST_PARASET`` and ``AST_RANGE``
   nodes).

-  Replace ``AST_PRIMITIVE`` nodes with appropriate ``AST_ASSIGN`` nodes.

-  Replace dynamic bit ranges in the left-hand-side of assignments with
   ``AST_CASE`` nodes with ``AST_COND`` children for each possible case.

-  Detect array access patterns that are too complicated for the
   ``RTLIL::Memory`` abstraction and replace them with a set of signals and
   cases for all reads and/or writes.

-  Otherwise replace array accesses with ``AST_MEMRD`` and ``AST_MEMWR`` nodes.

In addition to these transformations, the simplifier also annotates the
AST with additional information that is needed for the RTLIL generator,
namely:

-  All ranges (width of signals and bit selections) are not only const
   folded but (when a constant value is found) are also written to
   member variables in the AST_RANGE node.

-  All identifiers are resolved and all ``AST_IDENTIFIER`` nodes are annotated
   with a pointer to the AST node that contains the declaration of the
   identifier. If no declaration has been found, an ``AST_AUTOWIRE`` node is
   created and used for the annotation.

This produces an AST that is fairly easy to convert to the RTLIL format.

Generating RTLIL
~~~~~~~~~~~~~~~~

After AST simplification, the ``AST::AstNode::genRTLIL()`` method of each
``AST_MODULE`` node in the AST is called. This initiates a recursive process
that generates equivalent RTLIL data for the AST data.

The ``AST::AstNode::genRTLIL()`` method returns an ``RTLIL::SigSpec`` structure.
For nodes that represent expressions (operators, constants, signals, etc.), the
cells needed to implement the calculation described by the expression are
created and the resulting signal is returned. That way it is easy to generate
the circuits for large expressions using depth-first recursion. For nodes that
do not represent an expression (such as ``AST_CELL``), the corresponding circuit
is generated and an empty ``RTLIL::SigSpec`` is returned.

Synthesizing Verilog always blocks
--------------------------------------

For behavioural Verilog code (code utilizing ``always``- and ``initial``-blocks)
it is necessary to also generate ``RTLIL::Process`` objects. This is done in the
following way:

Whenever ``AST::AstNode::genRTLIL()`` encounters an ``always``- or
``initial``-block, it creates an instance of ``AST_INTERNAL::ProcessGenerator``.
This object then generates the ``RTLIL::Process`` object for the block. It also
calls ``AST::AstNode::genRTLIL()`` for all right-hand-side expressions contained
within the block.

First the ``AST_INTERNAL::ProcessGenerator`` creates a list of all signals
assigned within the block. It then creates a set of temporary signals using the
naming scheme ``$ <number> \ <original_name>`` for each of the assigned signals.

Then an ``RTLIL::Process`` is created that assigns all intermediate values for
each left-hand-side signal to the temporary signal in its
``RTLIL::CaseRule``/``RTLIL::SwitchRule`` tree.

Finally a ``RTLIL::SyncRule`` is created for the ``RTLIL::Process`` that assigns
the temporary signals for the final values to the actual signals.

A process may also contain memory writes. A ``RTLIL::MemWriteAction`` is created
for each of them.

Calls to ``AST::AstNode::genRTLIL()`` are generated for right hand sides as
needed. When blocking assignments are used, ``AST::AstNode::genRTLIL()`` is
configured using global variables to use the temporary signals that hold the
correct intermediate values whenever one of the previously assigned signals is
used in an expression.

Unfortunately the generation of a correct
``RTLIL::CaseRule``/\ ``RTLIL::SwitchRule`` tree for behavioural code is a
non-trivial task. The AST frontend solves the problem using the approach
described on the following pages. The following example illustrates what the
algorithm is supposed to do. Consider the following Verilog code:

.. code:: verilog
   :number-lines:

   always @(posedge clock) begin
       out1 = in1;
       if (in2)
           out1 = !out1;
       out2 <= out1;
       if (in3)
           out2 <= out2;
       if (in4)
           if (in5)
               out3 <= in6;
           else
               out3 <= in7;
       out1 = out1 ^ out2;
   end

This is translated by the Verilog and AST frontends into the following RTLIL
code (attributes, cell parameters and wire declarations not included):

.. code:: RTLIL
   :number-lines:

   cell $logic_not $logic_not$<input>:4$2
     connect \A \in1
     connect \Y $logic_not$<input>:4$2_Y
   end
   cell $xor $xor$<input>:13$3
     connect \A $1\out1[0:0]
     connect \B \out2
     connect \Y $xor$<input>:13$3_Y
   end
   process $proc$<input>:1$1
     assign $0\out3[0:0] \out3
     assign $0\out2[0:0] $1\out1[0:0]
     assign $0\out1[0:0] $xor$<input>:13$3_Y
     switch \in2
       case 1'1
         assign $1\out1[0:0] $logic_not$<input>:4$2_Y
       case
         assign $1\out1[0:0] \in1
     end
     switch \in3
       case 1'1
         assign $0\out2[0:0] \out2
       case
     end
     switch \in4
       case 1'1
         switch \in5
           case 1'1
             assign $0\out3[0:0] \in6
           case
             assign $0\out3[0:0] \in7
         end
       case
     end
     sync posedge \clock
       update \out1 $0\out1[0:0]
       update \out2 $0\out2[0:0]
       update \out3 $0\out3[0:0]
   end

Note that the two operators are translated into separate cells outside the
generated process. The signal ``out1`` is assigned using blocking assignments
and therefore ``out1`` has been replaced with a different signal in all
expressions after the initial assignment. The signal ``out2`` is assigned using
nonblocking assignments and therefore is not substituted on the right-hand-side
expressions.

The ``RTLIL::CaseRule``/\ ``RTLIL::SwitchRule`` tree must be interpreted the
following way:

-  On each case level (the body of the process is the root case), first the
   actions on this level are evaluated and then the switches within the case are
   evaluated. (Note that the last assignment on line 13 of the Verilog code has
   been moved to the beginning of the RTLIL process to line 13 of the RTLIL
   listing.)

   I.e. the special cases deeper in the switch hierarchy override the defaults
   on the upper levels. The assignments in lines 12 and 22 of the RTLIL code
   serve as an example for this.

   Note that in contrast to this, the order within the ``RTLIL::SwitchRule``
   objects within a ``RTLIL::CaseRule`` is preserved with respect to the
   original AST and Verilog code.

-  The whole ``RTLIL::CaseRule``/\ ``RTLIL::SwitchRule`` tree describes an
   asynchronous circuit. I.e. the decision tree formed by the switches can be
   seen independently for each assigned signal. Whenever one assigned signal
   changes, all signals that depend on the changed signals are to be updated.
   For example the assignments in lines 16 and 18 in the RTLIL code in fact
   influence the assignment in line 12, even though they are in the "wrong
   order".

The only synchronous part of the process is in the ``RTLIL::SyncRule`` object
generated at line 35 in the RTLIL code. The sync rule is the only part of the
process where the original signals are assigned. The synchronization event from
the original Verilog code has been translated into the synchronization type
(posedge) and signal (``\clock``) for the ``RTLIL::SyncRule`` object. In the
case of this simple example the ``RTLIL::SyncRule`` object is later simply
transformed into a set of d-type flip-flops and the
``RTLIL::CaseRule``/\ ``RTLIL::SwitchRule`` tree to a decision tree using
multiplexers.

In more complex examples (e.g. asynchronous resets) the part of the
``RTLIL::CaseRule``/\ ``RTLIL::SwitchRule`` tree that describes the asynchronous
reset must first be transformed to the correct ``RTLIL::SyncRule`` objects. This
is done by the ``proc_arst`` pass.

The ProcessGenerator algorithm
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ``AST_INTERNAL::ProcessGenerator`` uses the following internal state
variables:

-  | ``subst_rvalue_from`` and ``subst_rvalue_to``
   | These two variables hold the replacement pattern that should be used by
     ``AST::AstNode::genRTLIL()`` for signals with blocking assignments. After
    initialization of ``AST_INTERNAL::ProcessGenerator`` these two variables are
    empty.

-  | ``subst_lvalue_from`` and ``subst_lvalue_to`` 
   | These two variables contain the mapping from left-hand-side signals (``\
     <name>``) to the current temporary signal for the same thing (initially
     ``$0\ <name>``).

-  | ``current_case`` 
   | A pointer to a ``RTLIL::CaseRule`` object. Initially this is the root case
     of the generated ``RTLIL::Process``.

As the algorithm runs these variables are continuously modified as well as
pushed to the stack and later restored to their earlier values by popping from
the stack.

On startup the ProcessGenerator generates a new ``RTLIL::Process`` object with
an empty root case and initializes its state variables as described above. Then
the ``RTLIL::SyncRule`` objects are created using the synchronization events
from the AST_ALWAYS node and the initial values of ``subst_lvalue_from`` and
``subst_lvalue_to``. Then the AST for this process is evaluated recursively.

During this recursive evaluation, three different relevant types of AST nodes
can be discovered: ``AST_ASSIGN_LE`` (nonblocking assignments),
``AST_ASSIGN_EQ`` (blocking assignments) and ``AST_CASE`` (``if`` or ``case``
statement).

Handling of nonblocking assignments
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When an ``AST_ASSIGN_LE`` node is discovered, the following actions are
performed by the ProcessGenerator:

-  The left-hand-side is evaluated using ``AST::AstNode::genRTLIL()`` and mapped
   to a temporary signal name using ``subst_lvalue_from`` and
   ``subst_lvalue_to``.

-  The right-hand-side is evaluated using ``AST::AstNode::genRTLIL()``. For this
   call, the values of ``subst_rvalue_from`` and ``subst_rvalue_to`` are used to
   map blocking-assigned signals correctly.

-  Remove all assignments to the same left-hand-side as this assignment from the
   ``current_case`` and all cases within it.

-  Add the new assignment to the ``current_case``.

Handling of blocking assignments
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When an ``AST_ASSIGN_EQ`` node is discovered, the following actions are
performed by the ProcessGenerator:

-  Perform all the steps that would be performed for a nonblocking assignment
   (see above).

-  Remove the found left-hand-side (before lvalue mapping) from
   ``subst_rvalue_from`` and also remove the respective bits from
   ``subst_rvalue_to``.

-  Append the found left-hand-side (before lvalue mapping) to
   ``subst_rvalue_from`` and append the found right-hand-side to
   ``subst_rvalue_to``.

Handling of cases and if-statements
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When an ``AST_CASE`` node is discovered, the following actions are performed by
the ProcessGenerator:

-  The values of ``subst_rvalue_from``, ``subst_rvalue_to``,
   ``subst_lvalue_from`` and ``subst_lvalue_to`` are pushed to the stack.

-  A new ``RTLIL::SwitchRule`` object is generated, the selection expression is
   evaluated using ``AST::AstNode::genRTLIL()`` (with the use of
   ``subst_rvalue_from`` and ``subst_rvalue_to``) and added to the
   ``RTLIL::SwitchRule`` object and the object is added to the ``current_case``.

-  All lvalues assigned to within the ``AST_CASE`` node using blocking
   assignments are collected and saved in the local variable
   ``this_case_eq_lvalue``.

-  New temporary signals are generated for all signals in
   ``this_case_eq_lvalue`` and stored in ``this_case_eq_ltemp``.

-  The signals in ``this_case_eq_lvalue`` are mapped using ``subst_rvalue_from``
   and ``subst_rvalue_to`` and the resulting set of signals is stored in
   ``this_case_eq_rvalue``.

Then the following steps are performed for each ``AST_COND`` node within the
``AST_CASE`` node:

-  Set ``subst_rvalue_from``, ``subst_rvalue_to``, ``subst_lvalue_from`` and
   ``subst_lvalue_to`` to the values that have been pushed to the stack.

-  Remove ``this_case_eq_lvalue`` from
   ``subst_lvalue_from``/``subst_lvalue_to``.

-  Append ``this_case_eq_lvalue`` to ``subst_lvalue_from`` and append
   ``this_case_eq_ltemp`` to ``subst_lvalue_to``.

-  Push the value of ``current_case``.

-  Create a new ``RTLIL::CaseRule``. Set ``current_case`` to the new object and
   add the new object to the ``RTLIL::SwitchRule`` created above.

-  Add an assignment from ``this_case_eq_rvalue`` to ``this_case_eq_ltemp`` to
   the new ``current_case``.

-  Evaluate the compare value for this case using
   ``AST::AstNode::genRTLIL()`` (with the use of ``subst_rvalue_from``
   and ``subst_rvalue_to``) modify the new ``current_case`` accordingly.

-  Recursion into the children of the ``AST_COND`` node.

-  Restore ``current_case`` by popping the old value from the stack.

Finally the following steps are performed:

-  The values of ``subst_rvalue_from``, ``subst_rvalue_to``,
   ``subst_lvalue_from`` and ``subst_lvalue_to`` are popped from the stack.

-  The signals from ``this_case_eq_lvalue`` are removed from the
   ``subst_rvalue_from``/``subst_rvalue_to``-pair.

-  The value of ``this_case_eq_lvalue`` is appended to ``subst_rvalue_from`` and
   the value of ``this_case_eq_ltemp`` is appended to ``subst_rvalue_to``.

-  Map the signals in ``this_case_eq_lvalue`` using
   ``subst_lvalue_from``/``subst_lvalue_to``.

-  Remove all assignments to signals in ``this_case_eq_lvalue`` in
   ``current_case`` and all cases within it.

-  Add an assignment from ``this_case_eq_ltemp`` to ``this_case_eq_lvalue`` to
   ``current_case``.

Further analysis of the algorithm for cases and if-statements
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

With respect to nonblocking assignments the algorithm is easy: later assignments
invalidate earlier assignments. For each signal assigned using nonblocking
assignments exactly one temporary variable is generated (with the ``$0``-prefix)
and this variable is used for all assignments of the variable.

Note how all the ``_eq_``-variables become empty when no blocking assignments
are used and many of the steps in the algorithm can then be ignored as a result
of this.

For a variable with blocking assignments the algorithm shows the following
behaviour: First a new temporary variable is created. This new temporary
variable is then registered as the assignment target for all assignments for
this variable within the cases for this ``AST_CASE`` node. Then for each case
the new temporary variable is first assigned the old temporary variable. This
assignment is overwritten if the variable is actually assigned in this case and
is kept as a default value otherwise.

This yields an ``RTLIL::CaseRule`` that assigns the new temporary variable in
all branches. So when all cases have been processed a final assignment is added
to the containing block that assigns the new temporary variable to the old one.
Note how this step always overrides a previous assignment to the old temporary
variable. Other than nonblocking assignments, the old assignment could still
have an effect somewhere in the design, as there have been calls to
``AST::AstNode::genRTLIL()`` with a
``subst_rvalue_from``/\ ``subst_rvalue_to``-tuple that contained the
right-hand-side of the old assignment.

The proc pass
~~~~~~~~~~~~~

The ProcessGenerator converts a behavioural model in AST representation to a
behavioural model in ``RTLIL::Process`` representation. The actual conversion
from a behavioural model to an RTL representation is performed by the
:cmd:ref:`proc` pass and the passes it launches:

-  | :cmd:ref:`proc_clean` and :cmd:ref:`proc_rmdead` 
   | These two passes just clean up the ``RTLIL::Process`` structure. The
     :cmd:ref:`proc_clean` pass removes empty parts (eg. empty assignments) from
     the process and :cmd:ref:`proc_rmdead` detects and removes unreachable
     branches from the process's decision trees.

-  | :cmd:ref:`proc_arst` 
   | This pass detects processes that describe d-type flip-flops with
     asynchronous resets and rewrites the process to better reflect what they
     are modelling: Before this pass, an asynchronous reset has two
     edge-sensitive sync rules and one top-level ``RTLIL::SwitchRule`` for the
     reset path. After this pass the sync rule for the reset is level-sensitive
     and the top-level ``RTLIL::SwitchRule`` has been removed.

-  | :cmd:ref:`proc_mux` 
   | This pass converts the ``RTLIL::CaseRule``/\ ``RTLIL::SwitchRule``-tree to a
     tree of multiplexers per written signal. After this, the ``RTLIL::Process``
     structure only contains the ``RTLIL::SyncRule`` s that describe the output
     registers.

-  | :cmd:ref:`proc_dff`
   | This pass replaces the ``RTLIL::SyncRule``\ s to d-type flip-flops (with
     asynchronous resets if necessary).

-  | :cmd:ref:`proc_dff`
   | This pass replaces the ``RTLIL::MemWriteAction``\ s with ``$memwr`` cells.

-  | :cmd:ref:`proc_clean`
   | A final call to :cmd:ref:`proc_clean` removes the now empty
     ``RTLIL::Process`` objects.

Performing these last processing steps in passes instead of in the Verilog
frontend has two important benefits:

First it improves the transparency of the process. Everything that happens in a
separate pass is easier to debug, as the RTLIL data structures can be easily
investigated before and after each of the steps.

Second it improves flexibility. This scheme can easily be extended to support
other types of storage-elements, such as sr-latches or d-latches, without having
to extend the actual Verilog frontend.

.. todo:: Synthesizing Verilog arrays

  Add some information on the generation of ``$memrd`` and ``$memwr`` cells and
  how they are processed in the memory pass.


.. todo:: Synthesizing parametric designs

  Add some information on the ``RTLIL::Module::derive()`` method and how it is
  used to synthesize parametric modules via the hierarchy pass.



formats/cell_library.rst
--------------------------------------
.. role:: verilog(code)
	:language: Verilog

.. _chapter:celllib:

Internal cell library
=====================

.. todo:: less academic, also check formatting consistency

Most of the passes in Yosys operate on netlists, i.e. they only care about the
``RTLIL::Wire`` and ``RTLIL::Cell`` objects in an ``RTLIL::Module``. This
chapter discusses the cell types used by Yosys to represent a behavioural design
internally.

.. TODO:: is this chapter split preserved

This chapter is split in two parts. In the first part the internal RTL cells are
covered. These cells are used to represent the design on a coarse grain level.
Like in the original HDL code on this level the cells operate on vectors of
signals and complex cells like adders exist. In the second part the internal
gate cells are covered. These cells are used to represent the design on a
fine-grain gate-level. All cells from this category operate on single bit
signals.

RTL cells
---------

Most of the RTL cells closely resemble the operators available in HDLs such as
Verilog or VHDL. Therefore Verilog operators are used in the following sections
to define the behaviour of the RTL cells.

Note that all RTL cells have parameters indicating the size of inputs and
outputs. When passes modify RTL cells they must always keep the values of these
parameters in sync with the size of the signals connected to the inputs and
outputs.

Simulation models for the RTL cells can be found in the file
:file:`techlibs/common/simlib.v` in the Yosys source tree.

Unary operators
~~~~~~~~~~~~~~~

All unary RTL cells have one input port ``\A`` and one output port ``\Y``. They
also have the following parameters:

``\A_SIGNED``
	Set to a non-zero value if the input ``\A`` is signed and therefore should be
	sign-extended when needed.

``\A_WIDTH``
	The width of the input port ``\A``.

``\Y_WIDTH``
	The width of the output port ``\Y``.

:numref:`tab:CellLib_unary` lists all cells for unary RTL operators.

.. table:: Cell types for unary operators with their corresponding Verilog expressions.
	:name: tab:CellLib_unary

	================== ============
	Verilog            Cell Type
	================== ============
	:verilog:`Y =  ~A` $not
	:verilog:`Y =  +A` $pos
	:verilog:`Y =  -A` $neg
	:verilog:`Y =  &A` $reduce_and
	:verilog:`Y =  |A` $reduce_or
	:verilog:`Y =  ^A` $reduce_xor
	:verilog:`Y = ~^A` $reduce_xnor
	:verilog:`Y =  |A` $reduce_bool
	:verilog:`Y =  !A` $logic_not
	================== ============

For the unary cells that output a logical value (``$reduce_and``,
``$reduce_or``, ``$reduce_xor``, ``$reduce_xnor``, ``$reduce_bool``,
``$logic_not``), when the ``\Y_WIDTH`` parameter is greater than 1, the output
is zero-extended, and only the least significant bit varies.

Note that ``$reduce_or`` and ``$reduce_bool`` actually represent the same logic
function. But the HDL frontends generate them in different situations. A
``$reduce_or`` cell is generated when the prefix ``|`` operator is being used. A
``$reduce_bool`` cell is generated when a bit vector is used as a condition in
an ``if``-statement or ``?:``-expression.

Binary operators
~~~~~~~~~~~~~~~~

All binary RTL cells have two input ports ``\A`` and ``\B`` and one output port
``\Y``. They also have the following parameters:

``\A_SIGNED``
	Set to a non-zero value if the input ``\A`` is signed and therefore
	should be sign-extended when needed.

``\A_WIDTH``
	The width of the input port ``\A``.

``\B_SIGNED``
	Set to a non-zero value if the input ``\B`` is signed and therefore
	should be sign-extended when needed.

``\B_WIDTH``
	The width of the input port ``\B``.

``\Y_WIDTH``
	The width of the output port ``\Y``.

:numref:`tab:CellLib_binary` lists all cells for binary RTL operators.

.. table:: Cell types for binary operators with their corresponding Verilog expressions.
	:name: tab:CellLib_binary

	======================= ============= ======================= =========
	Verilog                 Cell Type     Verilog                 Cell Type
	======================= ============= ======================= =========
	:verilog:`Y = A  & B`   $and          :verilog:`Y = A <  B`   $lt
	:verilog:`Y = A  | B`   $or           :verilog:`Y = A <= B`   $le
	:verilog:`Y = A  ^ B`   $xor          :verilog:`Y = A == B`   $eq
	:verilog:`Y = A ~^ B`   $xnor         :verilog:`Y = A != B`   $ne
	:verilog:`Y = A << B`   $shl          :verilog:`Y = A >= B`   $ge
	:verilog:`Y = A >> B`   $shr          :verilog:`Y = A >  B`   $gt
	:verilog:`Y = A <<< B`  $sshl         :verilog:`Y = A  + B`   $add
	:verilog:`Y = A >>> B`  $sshr         :verilog:`Y = A  - B`   $sub
	:verilog:`Y = A && B`   $logic_and    :verilog:`Y = A  * B`   $mul
	:verilog:`Y = A || B`   $logic_or     :verilog:`Y = A  / B`   $div
	:verilog:`Y = A === B`  $eqx          :verilog:`Y = A  % B`   $mod
	:verilog:`Y = A !== B`  $nex          ``N/A``                 $divfloor
	:verilog:`Y = A ** B`   $pow          ``N/A``                 $modfloor
	======================= ============= ======================= =========

The ``$shl`` and ``$shr`` cells implement logical shifts, whereas the ``$sshl``
and ``$sshr`` cells implement arithmetic shifts. The ``$shl`` and ``$sshl``
cells implement the same operation. All four of these cells interpret the second
operand as unsigned, and require ``\B_SIGNED`` to be zero.

Two additional shift operator cells are available that do not directly
correspond to any operator in Verilog, ``$shift`` and ``$shiftx``. The
``$shift`` cell performs a right logical shift if the second operand is positive
(or unsigned), and a left logical shift if it is negative. The ``$shiftx`` cell
performs the same operation as the ``$shift`` cell, but the vacated bit
positions are filled with undef (x) bits, and corresponds to the Verilog indexed
part-select expression.

For the binary cells that output a logical value (``$logic_and``, ``$logic_or``,
``$eqx``, ``$nex``, ``$lt``, ``$le``, ``$eq``, ``$ne``, ``$ge``, ``$gt``), when
the ``\Y_WIDTH`` parameter is greater than 1, the output is zero-extended, and
only the least significant bit varies.

Division and modulo cells are available in two rounding modes. The original
``$div`` and ``$mod`` cells are based on truncating division, and correspond to
the semantics of the verilog ``/`` and ``%`` operators. The ``$divfloor`` and
``$modfloor`` cells represent flooring division and flooring modulo, the latter
of which is also known as "remainder" in several languages. See
:numref:`tab:CellLib_divmod` for a side-by-side comparison between the different
semantics.

.. table:: Comparison between different rounding modes for division and modulo cells.
	:name: tab:CellLib_divmod

	+-----------+--------+-----------+-----------+-----------+-----------+
	| Division  | Result |      Truncating       |        Flooring       |
	+-----------+--------+-----------+-----------+-----------+-----------+
	|           |        | $div      | $mod      | $divfloor | $modfloor |
	+===========+========+===========+===========+===========+===========+
	| -10 / 3   | -3.3   | -3        |        -1 | -4        |  2        |
	+-----------+--------+-----------+-----------+-----------+-----------+
	| 10 / -3   | -3.3   | -3        |         1 | -4        | -2        |
	+-----------+--------+-----------+-----------+-----------+-----------+
	| -10 / -3  |  3.3   |  3        |        -1 |  3        | -1        |
	+-----------+--------+-----------+-----------+-----------+-----------+
	| 10 / 3    |  3.3   |  3        |         1 |  3        |  1        |
	+-----------+--------+-----------+-----------+-----------+-----------+

Multiplexers
~~~~~~~~~~~~

Multiplexers are generated by the Verilog HDL frontend for ``?:``-expressions.
Multiplexers are also generated by the proc pass to map the decision trees from
RTLIL::Process objects to logic.

The simplest multiplexer cell type is ``$mux``. Cells of this type have a
``\WITDH`` parameter and data inputs ``\A`` and ``\B`` and a data output ``\Y``,
all of the specified width. This cell also has a single bit control input
``\S``. If ``\S`` is 0 the value from the input ``\A`` is sent to the output, if
it is 1 the value from the ``\B`` input is sent to the output. So the ``$mux``
cell implements the function :verilog:`Y = S ? B : A`.

The ``$pmux`` cell is used to multiplex between many inputs using a one-hot
select signal. Cells of this type have a ``\WIDTH`` and a ``\S_WIDTH`` parameter
and inputs ``\A``, ``\B``, and ``\S`` and an output ``\Y``. The ``\S`` input is
``\S_WIDTH`` bits wide. The ``\A`` input and the output are both ``\WIDTH`` bits
wide and the ``\B`` input is ``\WIDTH*\S_WIDTH`` bits wide. When all bits of
``\S`` are zero, the value from ``\A`` input is sent to the output. If the
:math:`n`\ 'th bit from ``\S`` is set, the value :math:`n`\ 'th ``\WIDTH`` bits
wide slice of the ``\B`` input is sent to the output. When more than one bit
from ``\S`` is set the output is undefined. Cells of this type are used to model
"parallel cases" (defined by using the ``parallel_case`` attribute or detected
by an optimization).

The ``$tribuf`` cell is used to implement tristate logic. Cells of this type
have a ``\WIDTH`` parameter and inputs ``\A`` and ``\EN`` and an output ``\Y``. The
``\A`` input and ``\Y`` output are ``\WIDTH`` bits wide, and the ``\EN`` input
is one bit wide. When ``\EN`` is 0, the output is not driven. When ``\EN`` is 1,
the value from ``\A`` input is sent to the ``\Y`` output. Therefore, the
``$tribuf`` cell implements the function :verilog:`Y = EN ? A : 'bz`.

Behavioural code with cascaded if-then-else- and case-statements usually results
in trees of multiplexer cells. Many passes (from various optimizations to FSM
extraction) heavily depend on these multiplexer trees to understand dependencies
between signals. Therefore optimizations should not break these multiplexer
trees (e.g. by replacing a multiplexer between a calculated signal and a
constant zero with an ``$and`` gate).

Registers
~~~~~~~~~

SR-type latches are represented by ``$sr`` cells. These cells have input ports
``\SET`` and ``\CLR`` and an output port ``\Q``. They have the following
parameters:

``\WIDTH``
	The width of inputs ``\SET`` and ``\CLR`` and output ``\Q``.

``\SET_POLARITY``
	The set input bits are active-high if this parameter has the value
	``1'b1`` and active-low if this parameter is ``1'b0``.

``\CLR_POLARITY``
	The reset input bits are active-high if this parameter has the value
	``1'b1`` and active-low if this parameter is ``1'b0``.

Both set and reset inputs have separate bits for every output bit. When both the
set and reset inputs of an ``$sr`` cell are active for a given bit index, the
reset input takes precedence.

D-type flip-flops are represented by ``$dff`` cells. These cells have a clock
port ``\CLK``, an input port ``\D`` and an output port ``\Q``. The following
parameters are available for ``$dff`` cells:

``\WIDTH``
	The width of input ``\D`` and output ``\Q``.

``\CLK_POLARITY``
	Clock is active on the positive edge if this parameter has the value
	``1'b1`` and on the negative edge if this parameter is ``1'b0``.

D-type flip-flops with asynchronous reset are represented by ``$adff`` cells. As
the ``$dff`` cells they have ``\CLK``, ``\D`` and ``\Q`` ports. In addition they
also have a single-bit ``\ARST`` input port for the reset pin and the following
additional two parameters:

``\ARST_POLARITY``
	The asynchronous reset is active-high if this parameter has the value
	``1'b1`` and active-low if this parameter is ``1'b0``.

``\ARST_VALUE``
   	The state of ``\Q`` will be set to this value when the reset is active.

Usually these cells are generated by the :cmd:ref:`proc` pass using the
information in the designs RTLIL::Process objects.

D-type flip-flops with synchronous reset are represented by ``$sdff`` cells. As
the ``$dff`` cells they have ``\CLK``, ``\D`` and ``\Q`` ports. In addition they
also have a single-bit ``\SRST`` input port for the reset pin and the following
additional two parameters:

``\SRST_POLARITY``
	The synchronous reset is active-high if this parameter has the value
	``1'b1`` and active-low if this parameter is ``1'b0``.

``\SRST_VALUE``
	The state of ``\Q`` will be set to this value when the reset is active.

Note that the ``$adff`` and ``$sdff`` cells can only be used when the reset
value is constant.

D-type flip-flops with asynchronous load are represented by ``$aldff`` cells. As
the ``$dff`` cells they have ``\CLK``, ``\D`` and ``\Q`` ports. In addition they
also have a single-bit ``\ALOAD`` input port for the async load enable pin, a
``\AD`` input port with the same width as data for the async load data, and the
following additional parameter:

``\ALOAD_POLARITY``
	The asynchronous load is active-high if this parameter has the value
	``1'b1`` and active-low if this parameter is ``1'b0``.

D-type flip-flops with asynchronous set and reset are represented by ``$dffsr``
cells. As the ``$dff`` cells they have ``\CLK``, ``\D`` and ``\Q`` ports. In
addition they also have multi-bit ``\SET`` and ``\CLR`` input ports and the
corresponding polarity parameters, like ``$sr`` cells.

D-type flip-flops with enable are represented by ``$dffe``, ``$adffe``,
``$aldffe``, ``$dffsre``, ``$sdffe``, and ``$sdffce`` cells, which are enhanced
variants of ``$dff``, ``$adff``, ``$aldff``, ``$dffsr``, ``$sdff`` (with reset
over enable) and ``$sdff`` (with enable over reset) cells, respectively.  They
have the same ports and parameters as their base cell. In addition they also
have a single-bit ``\EN`` input port for the enable pin and the following
parameter:

``\EN_POLARITY``
	The enable input is active-high if this parameter has the value ``1'b1``
	and active-low if this parameter is ``1'b0``.

D-type latches are represented by ``$dlatch`` cells.  These cells have an enable
port ``\EN``, an input port ``\D``, and an output port ``\Q``.  The following
parameters are available for ``$dlatch`` cells:

``\WIDTH``
	The width of input ``\D`` and output ``\Q``.

``\EN_POLARITY``
	The enable input is active-high if this parameter has the value ``1'b1``
	and active-low if this parameter is ``1'b0``.

The latch is transparent when the ``\EN`` input is active.

D-type latches with reset are represented by ``$adlatch`` cells.  In addition to
``$dlatch`` ports and parameters, they also have a single-bit ``\ARST`` input
port for the reset pin and the following additional parameters:

``\ARST_POLARITY``
	The asynchronous reset is active-high if this parameter has the value
	``1'b1`` and active-low if this parameter is ``1'b0``.

``\ARST_VALUE``
	The state of ``\Q`` will be set to this value when the reset is active.

D-type latches with set and reset are represented by ``$dlatchsr`` cells. In
addition to ``$dlatch`` ports and parameters, they also have multi-bit ``\SET``
and ``\CLR`` input ports and the corresponding polarity parameters, like ``$sr``
cells.

.. _sec:memcells:

Memories
~~~~~~~~

Memories are either represented using ``RTLIL::Memory`` objects, ``$memrd_v2``,
``$memwr_v2``, and ``$meminit_v2`` cells, or by ``$mem_v2`` cells alone.

In the first alternative the ``RTLIL::Memory`` objects hold the general metadata
for the memory (bit width, size in number of words, etc.) and for each port a
``$memrd_v2`` (read port) or ``$memwr_v2`` (write port) cell is created. Having
individual cells for read and write ports has the advantage that they can be
consolidated using resource sharing passes. In some cases this drastically
reduces the number of required ports on the memory cell. In this alternative,
memory initialization data is represented by ``$meminit_v2`` cells, which allow
delaying constant folding for initialization addresses and data until after the
frontend finishes.

The ``$memrd_v2`` cells have a clock input ``\CLK``, an enable input ``\EN``, an
address input ``\ADDR``, a data output ``\DATA``, an asynchronous reset input
``\ARST``, and a synchronous reset input ``\SRST``. They also have the following
parameters:

``\MEMID``
	The name of the ``RTLIL::Memory`` object that is associated with this read
	port.

``\ABITS``
	The number of address bits (width of the ``\ADDR`` input port).

``\WIDTH``
	The number of data bits (width of the ``\DATA`` output port).  Note that
	this may be a power-of-two multiple of the underlying memory's width --
	such ports are called wide ports and access an aligned group of cells at
	once.  In this case, the corresponding low bits of ``\ADDR`` must be
	tied to 0.

``\CLK_ENABLE``
	When this parameter is non-zero, the clock is used. Otherwise this read
	port is asynchronous and the ``\CLK`` input is not used.

``\CLK_POLARITY``
	Clock is active on the positive edge if this parameter has the value
	``1'b1`` and on the negative edge if this parameter is ``1'b0``.

``\TRANSPARENCY_MASK``
	This parameter is a bitmask of write ports that this read port is
	transparent with. The bits of this parameter are indexed by the write
	port's ``\PORTID`` parameter. Transparency can only be enabled between
	synchronous ports sharing a clock domain. When transparency is enabled
	for a given port pair, a read and write to the same address in the same
	cycle will return the new value. Otherwise the old value is returned.

``\COLLISION_X_MASK``
	This parameter is a bitmask of write ports that have undefined collision
	behavior with this port. The bits of this parameter are indexed by the
	write port's ``\PORTID`` parameter. This behavior can only be enabled
	between synchronous ports sharing a clock domain. When undefined
	collision is enabled for a given port pair, a read and write to the same
	address in the same cycle will return the undefined (all-X) value.This
	option is exclusive (for a given port pair) with the transparency
	option.

``\ARST_VALUE``
	Whenever the ``\ARST`` input is asserted, the data output will be reset
	to this value. Only used for synchronous ports.

``\SRST_VALUE``
	Whenever the ``\SRST`` input is synchronously asserted, the data output
	will be reset to this value. Only used for synchronous ports.

``\INIT_VALUE``
	The initial value of the data output, for synchronous ports.

``\CE_OVER_SRST``
	If this parameter is non-zero, the ``\SRST`` input is only recognized
	when ``\EN`` is true. Otherwise, ``\SRST`` is recognized regardless of
	``\EN``.

The ``$memwr_v2`` cells have a clock input ``\CLK``, an enable input ``\EN``
(one enable bit for each data bit), an address input ``\ADDR`` and a data input
``\DATA``. They also have the following parameters:

``\MEMID``
	The name of the ``RTLIL::Memory`` object that is associated with this write
	port.

``\ABITS``
	The number of address bits (width of the ``\ADDR`` input port).

``\WIDTH``
	The number of data bits (width of the ``\DATA`` output port). Like with
	``$memrd_v2`` cells, the width is allowed to be any power-of-two
	multiple of memory width, with the corresponding restriction on address.

``\CLK_ENABLE``
	When this parameter is non-zero, the clock is used. Otherwise this write
	port is asynchronous and the ``\CLK`` input is not used.

``\CLK_POLARITY``
	Clock is active on positive edge if this parameter has the value
	``1'b1`` and on the negative edge if this parameter is ``1'b0``.

``\PORTID``
	An identifier for this write port, used to index write port bit mask
	parameters.

``\PRIORITY_MASK``
	This parameter is a bitmask of write ports that this write port has priority
	over in case of writing to the same address.  The bits of this parameter are
	indexed by the other write port's ``\PORTID`` parameter. Write ports can
	only have priority over write ports with lower port ID. When two ports write
	to the same address and neither has priority over the other, the result is
	undefined.  Priority can only be set between two synchronous ports sharing
	the same clock domain.

The ``$meminit_v2`` cells have an address input ``\ADDR``, a data input
``\DATA``, with the width of the ``\DATA`` port equal to ``\WIDTH`` parameter
times ``\WORDS`` parameter, and a bit enable mask input ``\EN`` with width equal
to ``\WIDTH`` parameter. All three of the inputs must resolve to a constant for
synthesis to succeed.

``\MEMID``
	The name of the ``RTLIL::Memory`` object that is associated with this
	initialization cell.

``\ABITS``
	The number of address bits (width of the ``\ADDR`` input port).

``\WIDTH``
	The number of data bits per memory location.

``\WORDS``
	The number of consecutive memory locations initialized by this cell.

``\PRIORITY``
	The cell with the higher integer value in this parameter wins an
	initialization conflict.

The HDL frontend models a memory using ``RTLIL::Memory`` objects and
asynchronous ``$memrd_v2`` and ``$memwr_v2`` cells. The :cmd:ref:`memory` pass
(i.e. its various sub-passes) migrates ``$dff`` cells into the ``$memrd_v2`` and
``$memwr_v2`` cells making them synchronous, then converts them to a single
``$mem_v2`` cell and (optionally) maps this cell type to ``$dff`` cells for the
individual words and multiplexer-based address decoders for the read and write
interfaces. When the last step is disabled or not possible, a ``$mem_v2`` cell
is left in the design.

The ``$mem_v2`` cell provides the following parameters:

``\MEMID``
	The name of the original ``RTLIL::Memory`` object that became this
	``$mem_v2`` cell.

``\SIZE``
	The number of words in the memory.

``\ABITS``
	The number of address bits.

``\WIDTH``
	The number of data bits per word.

``\INIT``
	The initial memory contents.

``\RD_PORTS``
	The number of read ports on this memory cell.

``\RD_WIDE_CONTINUATION``
	This parameter is ``\RD_PORTS`` bits wide, containing a bitmask of
	"wide continuation" read ports. Such ports are used to represent the
	extra data bits of wide ports in the combined cell, and must have all
	control signals identical with the preceding port, except for address,
	which must have the proper sub-cell address encoded in the low bits.

``\RD_CLK_ENABLE``
	This parameter is ``\RD_PORTS`` bits wide, containing a clock enable bit
	for each read port.

``\RD_CLK_POLARITY``
	This parameter is ``\RD_PORTS`` bits wide, containing a clock polarity
	bit for each read port.

``\RD_TRANSPARENCY_MASK``
	This parameter is ``\RD_PORTS*\WR_PORTS`` bits wide, containing a
	concatenation of all ``\TRANSPARENCY_MASK`` values of the original
	``$memrd_v2`` cells.

``\RD_COLLISION_X_MASK``
	This parameter is ``\RD_PORTS*\WR_PORTS`` bits wide, containing a
	concatenation of all ``\COLLISION_X_MASK`` values of the original
	``$memrd_v2`` cells.

``\RD_CE_OVER_SRST``
	This parameter is ``\RD_PORTS`` bits wide, determining relative
	synchronous reset and enable priority for each read port.

``\RD_INIT_VALUE``
	This parameter is ``\RD_PORTS*\WIDTH`` bits wide, containing the initial
	value for each synchronous read port.

``\RD_ARST_VALUE``
	This parameter is ``\RD_PORTS*\WIDTH`` bits wide, containing the
	asynchronous reset value for each synchronous read port.

``\RD_SRST_VALUE``
	This parameter is ``\RD_PORTS*\WIDTH`` bits wide, containing the
	synchronous reset value for each synchronous read port.

``\WR_PORTS``
	The number of write ports on this memory cell.

``\WR_WIDE_CONTINUATION``
	This parameter is ``\WR_PORTS`` bits wide, containing a bitmask of
	"wide continuation" write ports.

``\WR_CLK_ENABLE``
	This parameter is ``\WR_PORTS`` bits wide, containing a clock enable bit
	for each write port.

``\WR_CLK_POLARITY``
	This parameter is ``\WR_PORTS`` bits wide, containing a clock polarity
	bit for each write port.

``\WR_PRIORITY_MASK``
	This parameter is ``\WR_PORTS*\WR_PORTS`` bits wide, containing a
	concatenation of all ``\PRIORITY_MASK`` values of the original
	``$memwr_v2`` cells.

The ``$mem_v2`` cell has the following ports:

``\RD_CLK``
	This input is ``\RD_PORTS`` bits wide, containing all clock signals for
	the read ports.

``\RD_EN``
	This input is ``\RD_PORTS`` bits wide, containing all enable signals for
	the read ports.

``\RD_ADDR``
	This input is ``\RD_PORTS*\ABITS`` bits wide, containing all address
	signals for the read ports.

``\RD_DATA``
	This output is ``\RD_PORTS*\WIDTH`` bits wide, containing all data
	signals for the read ports.

``\RD_ARST``
	This input is ``\RD_PORTS`` bits wide, containing all asynchronous reset
	signals for the read ports.

``\RD_SRST``
	This input is ``\RD_PORTS`` bits wide, containing all synchronous reset
	signals for the read ports.

``\WR_CLK``
	This input is ``\WR_PORTS`` bits wide, containing all clock signals for
	the write ports.

``\WR_EN``
	This input is ``\WR_PORTS*\WIDTH`` bits wide, containing all enable
	signals for the write ports.

``\WR_ADDR``
	This input is ``\WR_PORTS*\ABITS`` bits wide, containing all address
	signals for the write ports.

``\WR_DATA``
	This input is ``\WR_PORTS*\WIDTH`` bits wide, containing all data
	signals for the write ports.

The :cmd:ref:`memory_collect` pass can be used to convert discrete
``$memrd_v2``, ``$memwr_v2``, and ``$meminit_v2`` cells belonging to the same
memory to a single ``$mem_v2`` cell, whereas the :cmd:ref:`memory_unpack` pass
performs the inverse operation. The :cmd:ref:`memory_dff` pass can combine
asynchronous memory ports that are fed by or feeding registers into synchronous
memory ports. The :cmd:ref:`memory_bram` pass can be used to recognize
``$mem_v2`` cells that can be implemented with a block RAM resource on an FPGA.
The :cmd:ref:`memory_map` pass can be used to implement ``$mem_v2`` cells as
basic logic: word-wide DFFs and address decoders.

Finite state machines
~~~~~~~~~~~~~~~~~~~~~

Add a brief description of the ``$fsm`` cell type.

Coarse arithmetics
~~~~~~~~~~~~~~~~~~~~~

The ``$macc`` cell type represents a generalized multiply and accumulate operation. The cell is purely combinational. It outputs the result of summing up a sequence of products and other injected summands.

.. code-block::

	Y = 0 +- a0factor1 * a0factor2 +- a1factor1 * a1factor2 +- ...
	     + B[0] + B[1] + ...

The A port consists of concatenated pairs of multiplier inputs ("factors").
A zero length factor2 acts as a constant 1, turning factor1 into a simple summand.

In this pseudocode, ``u(foo)`` means an unsigned int that's foo bits long.

.. code-block::

	struct A {
		u(CONFIG.mul_info[0].factor1_len) a0factor1;
		u(CONFIG.mul_info[0].factor2_len) a0factor2;
		u(CONFIG.mul_info[1].factor1_len) a1factor1;
		u(CONFIG.mul_info[1].factor2_len) a1factor2;
		...
	};

The cell's ``CONFIG`` parameter determines the layout of cell port ``A``.
The CONFIG parameter carries the following information:

.. code-block::

	struct CONFIG {
		u4 num_bits;
		struct mul_info {
			bool is_signed;
			bool is_subtract;
			u(num_bits) factor1_len;
			u(num_bits) factor2_len;
		}[num_ports];
	};

B is an array of concatenated 1-bit-wide unsigned integers to also be summed up.

Arbitrary logic functions
~~~~~~~~~~~~~~~~~~~~~~~~~

The ``$lut`` cell type implements a single-output LUT (lookup table).
It implements an arbitrary logic function with its ``\LUT`` parameter to map
input port ``\A`` to values of ``\Y`` output port values.
In psuedocode: ``Y = \LUT[A]``.
``\A`` has width set by parameter ``\WIDTH`` and ``\Y`` has a width of 1.
Every logic function with a single bit output has a unique ``$lut``
representation.

The ``$sop`` cell type implements a sum-of-products expression, also known
as disjunctive normal form (DNF). It implements an arbitrary logic function.
Its structure mimics a programmable logic array (PLA).
Output port ``\Y`` is the sum of products of the bits of the input port ``\A``
as defined by parameter ``\TABLE``. ``\A`` is ``\WIDTH`` bits wide.
The number of products in the sum is set by parameter ``\DEPTH``, and each
product has two bits for each input bit - for the presence of the
unnegated and negated version of said input bit in the product.
Therefore the ``\TABLE`` parameter holds ``2 * \WIDTH * \DEPTH`` bits.

For example:

Let ``\WIDTH`` be 3. We would like to represent ``\Y =~\A[0] + \A[1]~\A[2]``.
There are 2 products to be summed, so ``\DEPTH`` shall be 2.

.. code-block::

    ~A[2]-----+
     A[2]----+|
    ~A[1]---+||
     A[1]--+|||
    ~A[0]-+||||
     A[0]+||||| 
         |||||| product formula
         010000 ~\A[0]
         001001 \A[1]~\A[2]

So the value of ``\TABLE`` will become ``010000001001``.

Any logic function with a single bit output can be represented with
``$sop`` but may have variously minimized or ordered summands represented
in the ``\TABLE`` values.

Specify rules
~~~~~~~~~~~~~

Add information about ``$specify2``, ``$specify3``, and ``$specrule`` cells.

Formal verification cells
~~~~~~~~~~~~~~~~~~~~~~~~~

Add information about ``$check``, ``$assert``, ``$assume``, ``$live``, ``$fair``,
``$cover``, ``$equiv``, ``$initstate``, ``$anyconst``, ``$anyseq``,
``$anyinit``, ``$allconst``, ``$allseq`` cells.

Add information about ``$ff`` and ``$_FF_`` cells.

Debugging cells
~~~~~~~~~~~~~~~

The ``$print`` cell is used to log the values of signals, akin to (and
translatable to) the ``$display`` and ``$write`` family of tasks in Verilog.  It
has the following parameters:

``\FORMAT``
	The internal format string.  The syntax is described below.

``\ARGS_WIDTH``
	The width (in bits) of the signal on the ``\ARGS`` port.

``\TRG_ENABLE``
	True if triggered on specific signals defined in ``\TRG``; false if
	triggered whenever ``\ARGS`` or ``\EN`` change and ``\EN`` is 1.

If ``\TRG_ENABLE`` is true, the following parameters also apply:

``\TRG_WIDTH``
	The number of bits in the ``\TRG`` port.

``\TRG_POLARITY``
	For each bit in ``\TRG``, 1 if that signal is positive-edge triggered, 0 if
	negative-edge triggered.

``\PRIORITY``
	When multiple ``$print`` or ``$$check`` cells fire on the same trigger, they\
	execute in descending priority order.

Ports:

``\TRG``
	The signals that control when this ``$print`` cell is triggered.
	If the width of this port is zero and ``\TRG_ENABLE`` is true, the cell is
	triggered during initial evaluation (time zero) only.

``\EN``
	Enable signal for the whole cell.

``\ARGS``
	The values to be displayed, in format string order.

Format string syntax
^^^^^^^^^^^^^^^^^^^^

The format string syntax resembles Python f-strings.  Regular text is passed
through unchanged until a format specifier is reached, starting with a ``{``.

Format specifiers have the following syntax.  Unless noted, all items are
required:

``{``
	Denotes the start of the format specifier.

size
	Signal size in bits; this many bits are consumed from the ``\ARGS`` port by
	this specifier.

``:``
	Separates the size from the remaining items.

justify
	``>`` for right-justified, ``<`` for left-justified.

padding
	``0`` for zero-padding, or a space for space-padding.

width\ *?*
	(optional) The number of characters wide to pad to.

base
	* ``b`` for base-2 integers (binary)
	* ``o`` for base-8 integers	(octal)
	* ``d`` for base-10 integers (decimal)
	* ``h`` for base-16	integers (hexadecimal)
	* ``c`` for ASCII characters/strings
	* ``t`` and ``r`` for simulation time (corresponding to :verilog:`$time` and :verilog:`$realtime`)

For integers, this item may follow:

``+``\ *?*
	(optional, decimals only) Include a leading plus for non-negative numbers.
	This can assist with symmetry with negatives in tabulated output.

signedness
	``u`` for unsigned, ``s`` for signed.  This distinction is only respected
	when rendering decimals.

ASCII characters/strings have no special options, but the signal size must be
divisible by 8.

For simulation time, the signal size must be zero.

Finally:

``}``
	Denotes the end of the format specifier.

Some example format specifiers:

+ ``{8:>02hu}`` - 8-bit unsigned integer rendered as hexadecimal,
  right-justified, zero-padded to 2 characters wide.
+ ``{32:< 15d+s}`` - 32-bit signed integer rendered as decimal, left-justified,
  space-padded to 15 characters wide, positive values prefixed with ``+``.
+ ``{16:< 10hu}`` - 16-bit unsigned integer rendered as hexadecimal,
  left-justified, space-padded to 10 characters wide.
+ ``{0:>010t}`` - simulation time, right-justified, zero-padded to 10 characters
  wide.

To include literal ``{`` and ``}`` characters in your format string, use ``{{``
and ``}}`` respectively.

It is an error for a format string to consume more or less bits from ``\ARGS``
than the port width.

Values are never truncated, regardless of the specified width.

Note that further restrictions on allowable combinations of options may apply
depending on the backend used.

For example, Verilog does not have a format specifier that allows zero-padding a
string (i.e. more than 1 ASCII character), though zero-padding a single
character is permitted.

Thus, while the RTLIL format specifier ``{8:>02c}`` translates to ``%02c``,
``{16:>02c}`` cannot be represented in Verilog and will fail to emit.  In this
case, ``{16:> 02c}`` must be used, which translates to ``%2s``.

.. _sec:celllib_gates:

Gates
-----

For gate level logic networks, fixed function single bit cells are used that do
not provide any parameters.

Simulation models for these cells can be found in the file
techlibs/common/simcells.v in the Yosys source tree.

.. table:: Cell types for gate level logic networks (main list)
	:name: tab:CellLib_gates

	======================================= ============
	Verilog                                 Cell Type
	======================================= ============
	:verilog:`Y = A`                        $_BUF_
	:verilog:`Y = ~A`                       $_NOT_
	:verilog:`Y = A & B`                    $_AND_
	:verilog:`Y = ~(A & B)`                 $_NAND_
	:verilog:`Y = A & ~B`                   $_ANDNOT_
	:verilog:`Y = A | B`                    $_OR_
	:verilog:`Y = ~(A | B)`                 $_NOR_
	:verilog:`Y = A | ~B`                   $_ORNOT_
	:verilog:`Y = A ^ B`                    $_XOR_
	:verilog:`Y = ~(A ^ B)`                 $_XNOR_
	:verilog:`Y = ~((A & B) | C)`           $_AOI3_
	:verilog:`Y = ~((A | B) & C)`           $_OAI3_
	:verilog:`Y = ~((A & B) | (C & D))`     $_AOI4_
	:verilog:`Y = ~((A | B) & (C | D))`     $_OAI4_
	:verilog:`Y = S ? B : A`                $_MUX_
	:verilog:`Y = ~(S ? B : A)`             $_NMUX_
	(see below)                             $_MUX4_
	(see below)                             $_MUX8_
	(see below)                             $_MUX16_
	:verilog:`Y = EN ? A : 1'bz`            $_TBUF_
	:verilog:`always @(negedge C) Q <= D`   $_DFF_N_
	:verilog:`always @(posedge C) Q <= D`   $_DFF_P_
	:verilog:`always @* if (!E) Q <= D`     $_DLATCH_N_
	:verilog:`always @* if (E)  Q <= D`     $_DLATCH_P_
	======================================= ============

.. table:: Cell types for gate level logic networks (FFs with reset)
	:name: tab:CellLib_gates_adff

	================== ============== ============== =======================
	:math:`ClkEdge`    :math:`RstLvl` :math:`RstVal` Cell Type
	================== ============== ============== =======================
	:verilog:`negedge` ``0``          ``0``          $_DFF_NN0_, $_SDFF_NN0_
	:verilog:`negedge` ``0``          ``1``          $_DFF_NN1_, $_SDFF_NN1_
	:verilog:`negedge` ``1``          ``0``          $_DFF_NP0_, $_SDFF_NP0_
	:verilog:`negedge` ``1``          ``1``          $_DFF_NP1_, $_SDFF_NP1_
	:verilog:`posedge` ``0``          ``0``          $_DFF_PN0_, $_SDFF_PN0_
	:verilog:`posedge` ``0``          ``1``          $_DFF_PN1_, $_SDFF_PN1_
	:verilog:`posedge` ``1``          ``0``          $_DFF_PP0_, $_SDFF_PP0_
	:verilog:`posedge` ``1``          ``1``          $_DFF_PP1_, $_SDFF_PP1_
	================== ============== ============== =======================


.. table:: Cell types for gate level logic networks (FFs with enable)
	:name: tab:CellLib_gates_dffe

	================== ============= ===========
	:math:`ClkEdge`    :math:`EnLvl` Cell Type
	================== ============= ===========
	:verilog:`negedge` ``0``         $_DFFE_NN_
	:verilog:`negedge` ``1``         $_DFFE_NP_
	:verilog:`posedge` ``0``         $_DFFE_PN_
	:verilog:`posedge` ``1``         $_DFFE_PP_
	================== ============= ===========


.. table:: Cell types for gate level logic networks (FFs with reset and enable)
	:name: tab:CellLib_gates_adffe

	================== ============== ============== ============= ===========================================
	:math:`ClkEdge`    :math:`RstLvl` :math:`RstVal` :math:`EnLvl` Cell Type
	================== ============== ============== ============= ===========================================
	:verilog:`negedge` ``0``          ``0``          ``0``         $_DFFE_NN0N_, $_SDFFE_NN0N_, $_SDFFCE_NN0N_
	:verilog:`negedge` ``0``          ``0``          ``1``         $_DFFE_NN0P_, $_SDFFE_NN0P_, $_SDFFCE_NN0P_
	:verilog:`negedge` ``0``          ``1``          ``0``         $_DFFE_NN1N_, $_SDFFE_NN1N_, $_SDFFCE_NN1N_
	:verilog:`negedge` ``0``          ``1``          ``1``         $_DFFE_NN1P_, $_SDFFE_NN1P_, $_SDFFCE_NN1P_
	:verilog:`negedge` ``1``          ``0``          ``0``         $_DFFE_NP0N_, $_SDFFE_NP0N_, $_SDFFCE_NP0N_
	:verilog:`negedge` ``1``          ``0``          ``1``         $_DFFE_NP0P_, $_SDFFE_NP0P_, $_SDFFCE_NP0P_
	:verilog:`negedge` ``1``          ``1``          ``0``         $_DFFE_NP1N_, $_SDFFE_NP1N_, $_SDFFCE_NP1N_
	:verilog:`negedge` ``1``          ``1``          ``1``         $_DFFE_NP1P_, $_SDFFE_NP1P_, $_SDFFCE_NP1P_
	:verilog:`posedge` ``0``          ``0``          ``0``         $_DFFE_PN0N_, $_SDFFE_PN0N_, $_SDFFCE_PN0N_
	:verilog:`posedge` ``0``          ``0``          ``1``         $_DFFE_PN0P_, $_SDFFE_PN0P_, $_SDFFCE_PN0P_
	:verilog:`posedge` ``0``          ``1``          ``0``         $_DFFE_PN1N_, $_SDFFE_PN1N_, $_SDFFCE_PN1N_
	:verilog:`posedge` ``0``          ``1``          ``1``         $_DFFE_PN1P_, $_SDFFE_PN1P_, $_SDFFCE_PN1P_
	:verilog:`posedge` ``1``          ``0``          ``0``         $_DFFE_PP0N_, $_SDFFE_PP0N_, $_SDFFCE_PP0N_
	:verilog:`posedge` ``1``          ``0``          ``1``         $_DFFE_PP0P_, $_SDFFE_PP0P_, $_SDFFCE_PP0P_
	:verilog:`posedge` ``1``          ``1``          ``0``         $_DFFE_PP1N_, $_SDFFE_PP1N_, $_SDFFCE_PP1N_
	:verilog:`posedge` ``1``          ``1``          ``1``         $_DFFE_PP1P_, $_SDFFE_PP1P_, $_SDFFCE_PP1P_
	================== ============== ============== ============= ===========================================

.. table:: Cell types for gate level logic networks (FFs with set and reset)
	:name: tab:CellLib_gates_dffsr

	================== ============== ============== ============
	:math:`ClkEdge`    :math:`SetLvl` :math:`RstLvl` Cell Type
	================== ============== ============== ============
	:verilog:`negedge` ``0``          ``0``          $_DFFSR_NNN_
	:verilog:`negedge` ``0``          ``1``          $_DFFSR_NNP_
	:verilog:`negedge` ``1``          ``0``          $_DFFSR_NPN_
	:verilog:`negedge` ``1``          ``1``          $_DFFSR_NPP_
	:verilog:`posedge` ``0``          ``0``          $_DFFSR_PNN_
	:verilog:`posedge` ``0``          ``1``          $_DFFSR_PNP_
	:verilog:`posedge` ``1``          ``0``          $_DFFSR_PPN_
	:verilog:`posedge` ``1``          ``1``          $_DFFSR_PPP_
	================== ============== ============== ============


.. table:: Cell types for gate level logic networks (FFs with set and reset and enable)
	:name: tab:CellLib_gates_dffsre

	================== ============== ============== ============= ==============
	:math:`ClkEdge`    :math:`SetLvl` :math:`RstLvl` :math:`EnLvl` Cell Type
	================== ============== ============== ============= ==============
	:verilog:`negedge` ``0``          ``0``          ``0``         $_DFFSRE_NNNN_
	:verilog:`negedge` ``0``          ``0``          ``1``         $_DFFSRE_NNNP_
	:verilog:`negedge` ``0``          ``1``          ``0``         $_DFFSRE_NNPN_
	:verilog:`negedge` ``0``          ``1``          ``1``         $_DFFSRE_NNPP_
	:verilog:`negedge` ``1``          ``0``          ``0``         $_DFFSRE_NPNN_
	:verilog:`negedge` ``1``          ``0``          ``1``         $_DFFSRE_NPNP_
	:verilog:`negedge` ``1``          ``1``          ``0``         $_DFFSRE_NPPN_
	:verilog:`negedge` ``1``          ``1``          ``1``         $_DFFSRE_NPPP_
	:verilog:`posedge` ``0``          ``0``          ``0``         $_DFFSRE_PNNN_
	:verilog:`posedge` ``0``          ``0``          ``1``         $_DFFSRE_PNNP_
	:verilog:`posedge` ``0``          ``1``          ``0``         $_DFFSRE_PNPN_
	:verilog:`posedge` ``0``          ``1``          ``1``         $_DFFSRE_PNPP_
	:verilog:`posedge` ``1``          ``0``          ``0``         $_DFFSRE_PPNN_
	:verilog:`posedge` ``1``          ``0``          ``1``         $_DFFSRE_PPNP_
	:verilog:`posedge` ``1``          ``1``          ``0``         $_DFFSRE_PPPN_
	:verilog:`posedge` ``1``          ``1``          ``1``         $_DFFSRE_PPPP_
	================== ============== ============== ============= ==============


.. table:: Cell types for gate level logic networks (latches with reset)
	:name: tab:CellLib_gates_adlatch

	============= ============== ============== =============
	:math:`EnLvl` :math:`RstLvl` :math:`RstVal` Cell Type
	============= ============== ============== =============
	``0``         ``0``          ``0``          $_DLATCH_NN0_
	``0``         ``0``          ``1``          $_DLATCH_NN1_
	``0``         ``1``          ``0``          $_DLATCH_NP0_
	``0``         ``1``          ``1``          $_DLATCH_NP1_
	``1``         ``0``          ``0``          $_DLATCH_PN0_
	``1``         ``0``          ``1``          $_DLATCH_PN1_
	``1``         ``1``          ``0``          $_DLATCH_PP0_
	``1``         ``1``          ``1``          $_DLATCH_PP1_
	============= ============== ============== =============


.. table:: Cell types for gate level logic networks (latches with set and reset)
	:name: tab:CellLib_gates_dlatchsr

	============= ============== ============== ===============
	:math:`EnLvl` :math:`SetLvl` :math:`RstLvl` Cell Type
	============= ============== ============== ===============
	``0``         ``0``          ``0``          $_DLATCHSR_NNN_
	``0``         ``0``          ``1``          $_DLATCHSR_NNP_
	``0``         ``1``          ``0``          $_DLATCHSR_NPN_
	``0``         ``1``          ``1``          $_DLATCHSR_NPP_
	``1``         ``0``          ``0``          $_DLATCHSR_PNN_
	``1``         ``0``          ``1``          $_DLATCHSR_PNP_
	``1``         ``1``          ``0``          $_DLATCHSR_PPN_
	``1``         ``1``          ``1``          $_DLATCHSR_PPP_
	============= ============== ============== ===============



.. table:: Cell types for gate level logic networks (SR latches)
	:name: tab:CellLib_gates_sr

	============== ============== =========
	:math:`SetLvl` :math:`RstLvl` Cell Type
	============== ============== =========
	``0``          ``0``          $_SR_NN_
	``0``          ``1``          $_SR_NP_
	``1``          ``0``          $_SR_PN_
	``1``          ``1``          $_SR_PP_
	============== ============== =========


Tables :numref:`%s <tab:CellLib_gates>`, :numref:`%s <tab:CellLib_gates_dffe>`,
:numref:`%s <tab:CellLib_gates_adff>`, :numref:`%s <tab:CellLib_gates_adffe>`,
:numref:`%s <tab:CellLib_gates_dffsr>`, :numref:`%s <tab:CellLib_gates_dffsre>`,
:numref:`%s <tab:CellLib_gates_adlatch>`, :numref:`%s
<tab:CellLib_gates_dlatchsr>` and :numref:`%s <tab:CellLib_gates_sr>` list all
cell types used for gate level logic. The cell types ``$_BUF_``, ``$_NOT_``,
``$_AND_``, ``$_NAND_``, ``$_ANDNOT_``, ``$_OR_``, ``$_NOR_``, ``$_ORNOT_``,
``$_XOR_``, ``$_XNOR_``, ``$_AOI3_``, ``$_OAI3_``, ``$_AOI4_``, ``$_OAI4_``,
``$_MUX_``, ``$_MUX4_``, ``$_MUX8_``, ``$_MUX16_`` and ``$_NMUX_`` are used to
model combinatorial logic. The cell type ``$_TBUF_`` is used to model tristate
logic.

The ``$_MUX4_``, ``$_MUX8_`` and ``$_MUX16_`` cells are used to model wide
muxes, and correspond to the following Verilog code:

.. code-block:: verilog
	:force:

	// $_MUX4_
	assign Y = T ? (S ? D : C) :
	               (S ? B : A);
	// $_MUX8_
	assign Y = U ? T ? (S ? H : G) :
	                   (S ? F : E) :
	               T ? (S ? D : C) :
	                   (S ? B : A);
	// $_MUX16_
	assign Y = V ? U ? T ? (S ? P : O) :
	                       (S ? N : M) :
	                   T ? (S ? L : K) :
	                       (S ? J : I) :
	               U ? T ? (S ? H : G) :
	                       (S ? F : E) :
	                   T ? (S ? D : C) :
	                       (S ? B : A);

The cell types ``$_DFF_N_`` and ``$_DFF_P_`` represent d-type flip-flops.

The cell types ``$_DFFE_[NP][NP]_`` implement d-type flip-flops with enable. The
values in the table for these cell types relate to the following Verilog code
template.

.. code-block:: verilog
	:force:

	always @(CLK_EDGE C)
		if (EN == EN_LVL)
			Q <= D;

The cell types ``$_DFF_[NP][NP][01]_`` implement d-type flip-flops with
asynchronous reset. The values in the table for these cell types relate to the
following Verilog code template, where ``RST_EDGE`` is ``posedge`` if
``RST_LVL`` if ``1``, and ``negedge`` otherwise.

.. code-block:: verilog
	:force:

	always @(CLK_EDGE C, RST_EDGE R)
		if (R == RST_LVL)
			Q <= RST_VAL;
		else
			Q <= D;

The cell types ``$_SDFF_[NP][NP][01]_`` implement d-type flip-flops with
synchronous reset. The values in the table for these cell types relate to the
following Verilog code template:

.. code-block:: verilog
	:force:

	always @(CLK_EDGE C)
		if (R == RST_LVL)
			Q <= RST_VAL;
		else
			Q <= D;

The cell types ``$_DFFE_[NP][NP][01][NP]_`` implement d-type flip-flops with
asynchronous reset and enable. The values in the table for these cell types
relate to the following Verilog code template, where ``RST_EDGE`` is
``posedge`` if ``RST_LVL`` if ``1``, and ``negedge`` otherwise.

.. code-block:: verilog
	:force:

	always @(CLK_EDGE C, RST_EDGE R)
		if (R == RST_LVL)
			Q <= RST_VAL;
		else if (EN == EN_LVL)
			Q <= D;

The cell types ``$_SDFFE_[NP][NP][01][NP]_`` implement d-type flip-flops with
synchronous reset and enable, with reset having priority over enable. The values
in the table for these cell types relate to the following Verilog code template:

.. code-block:: verilog
	:force:

	always @(CLK_EDGE C)
		if (R == RST_LVL)
			Q <= RST_VAL;
		else if (EN == EN_LVL)
			Q <= D;

The cell types ``$_SDFFCE_[NP][NP][01][NP]_`` implement d-type flip-flops with
synchronous reset and enable, with enable having priority over reset. The values
in the table for these cell types relate to the following Verilog code template:

.. code-block:: verilog
	:force:

	always @(CLK_EDGE C)
		if (EN == EN_LVL)
			if (R == RST_LVL)
				Q <= RST_VAL;
			else
				Q <= D;

The cell types ``$_DFFSR_[NP][NP][NP]_`` implement d-type flip-flops with
asynchronous set and reset. The values in the table for these cell types relate
to the following Verilog code template, where ``RST_EDGE`` is ``posedge`` if
``RST_LVL`` if ``1``, ``negedge`` otherwise, and ``SET_EDGE`` is ``posedge``
if ``SET_LVL`` if ``1``, ``negedge`` otherwise.

.. code-block:: verilog
	:force:

	always @(CLK_EDGE C, RST_EDGE R, SET_EDGE S)
		if (R == RST_LVL)
			Q <= 0;
		else if (S == SET_LVL)
			Q <= 1;
		else
			Q <= D;

The cell types ``$_DFFSRE_[NP][NP][NP][NP]_`` implement d-type flip-flops with
asynchronous set and reset and enable. The values in the table for these cell
types relate to the following Verilog code template, where ``RST_EDGE`` is
``posedge`` if ``RST_LVL`` if ``1``, ``negedge`` otherwise, and ``SET_EDGE``
is ``posedge`` if ``SET_LVL`` if ``1``, ``negedge`` otherwise.

.. code-block:: verilog
	:force:

	always @(CLK_EDGE C, RST_EDGE R, SET_EDGE S)
		if (R == RST_LVL)
			Q <= 0;
		else if (S == SET_LVL)
			Q <= 1;
		else if (E == EN_LVL)
			Q <= D;

The cell types ``$_DLATCH_N_`` and ``$_DLATCH_P_`` represent d-type latches.

The cell types ``$_DLATCH_[NP][NP][01]_`` implement d-type latches with reset.
The values in the table for these cell types relate to the following Verilog
code template:

.. code-block:: verilog
	:force:

	always @*
		if (R == RST_LVL)
			Q <= RST_VAL;
		else if (E == EN_LVL)
			Q <= D;

The cell types ``$_DLATCHSR_[NP][NP][NP]_`` implement d-type latches with set
and reset. The values in the table for these cell types relate to the following
Verilog code template:

.. code-block:: verilog
	:force:

	always @*
		if (R == RST_LVL)
			Q <= 0;
		else if (S == SET_LVL)
			Q <= 1;
		else if (E == EN_LVL)
			Q <= D;

The cell types ``$_SR_[NP][NP]_`` implement sr-type latches. The values in the
table for these cell types relate to the following Verilog code template:

.. code-block:: verilog
	:force:

	always @*
		if (R == RST_LVL)
			Q <= 0;
		else if (S == SET_LVL)
			Q <= 1;

In most cases gate level logic networks are created from RTL networks using the
techmap pass. The flip-flop cells from the gate level logic network can be
mapped to physical flip-flop cells from a Liberty file using the dfflibmap pass.
The combinatorial logic cells can be mapped to physical cells from a Liberty
file via ABC using the abc pass.

.. todo:: Add information about ``$slice`` and ``$concat`` cells.

.. todo:: Add information about ``$alu``, ``$fa``, and ``$lcu`` cells.

.. todo:: Add information about ``$demux`` cell.


formats/index.rst
--------------------------------------
Internal formats
================

.. todo:: brief overview for the internal formats index

.. toctree:: 
	:maxdepth: 3

	overview
	rtlil_rep
	rtlil_text
	cell_library




formats/overview.rst
--------------------------------------
Format overview
===============

Yosys uses two different internal formats. The first is used to store an
abstract syntax tree (AST) of a Verilog input file. This format is simply called
AST and is generated by the Verilog Frontend. This data structure is consumed by
a subsystem called AST Frontend [1]_. This AST Frontend then generates a design
in Yosys' main internal format, the
Register-Transfer-Level-Intermediate-Language (RTLIL) representation. It does
that by first performing a number of simplifications within the AST
representation and then generating RTLIL from the simplified AST data structure.

The RTLIL representation is used by all passes as input and outputs. This has
the following advantages over using different representational formats between
different passes:

-  The passes can be rearranged in a different order and passes can be removed
   or inserted.

-  Passes can simply pass-thru the parts of the design they don't change without
   the need to convert between formats. In fact Yosys passes output the same
   data structure they received as input and performs all changes in place.

-  All passes use the same interface, thus reducing the effort required to
   understand a pass when reading the Yosys source code, e.g. when adding
   additional features.

The RTLIL representation is basically a netlist representation with the
following additional features:

-  An internal cell library with fixed-function cells to represent RTL datapath
   and register cells as well as logical gate-level cells (single-bit gates and
   registers).

-  Support for multi-bit values that can use individual bits from wires as well
   as constant bits to represent coarse-grain netlists.

-  Support for basic behavioural constructs (if-then-else structures and
   multi-case switches with a sensitivity list for updating the outputs).

-  Support for multi-port memories.

The use of RTLIL also has the disadvantage of having a very powerful format
between all passes, even when doing gate-level synthesis where the more advanced
features are not needed. In order to reduce complexity for passes that operate
on a low-level representation, these passes check the features used in the input
RTLIL and fail to run when unsupported high-level constructs are used. In such
cases a pass that transforms the higher-level constructs to lower-level
constructs must be called from the synthesis script first.

.. [1]
   In Yosys the term pass is only used to refer to commands that operate on the
   RTLIL data structure.


formats/rtlil_rep.rst
--------------------------------------
The RTL Intermediate Language (RTLIL)
=====================================

All frontends, passes and backends in Yosys operate on a design in RTLIL
representation. The only exception are the high-level frontends that use the AST
representation as an intermediate step before generating RTLIL data.

In order to avoid reinventing names for the RTLIL classes, they are simply
referred to by their full C++ name, i.e. including the ``RTLIL::`` namespace
prefix, in this document.

:numref:`Figure %s <fig:Overview_RTLIL>` shows a simplified Entity-Relationship
Diagram (ER Diagram) of RTLIL. In :math:`1:N` relationships the arrow points
from the :math:`N` side to the :math:`1`. For example one ``RTLIL::Design``
contains :math:`N` (zero to many) instances of ``RTLIL::Module`` . A two-pointed
arrow indicates a :math:`1:1` relationship.

The ``RTLIL::Design`` is the root object of the RTLIL data structure. There is
always one "current design" in memory which passes operate on, frontends add
data to and backends convert to exportable formats. But in some cases passes
internally generate additional ``RTLIL::Design`` objects. For example when a
pass is reading an auxiliary Verilog file such as a cell library, it might
create an additional ``RTLIL::Design`` object and call the Verilog frontend with
this other object to parse the cell library.

.. figure:: /_images/internals/overview_rtlil.*
	:class: width-helper invert-helper
	:name: fig:Overview_RTLIL

	Simplified RTLIL Entity-Relationship Diagram

There is only one active ``RTLIL::Design`` object that is used by all frontends,
passes and backends called by the user, e.g. using a synthesis script. The
``RTLIL::Design`` then contains zero to many ``RTLIL::Module`` objects. This
corresponds to modules in Verilog or entities in VHDL. Each module in turn
contains objects from three different categories:

-  ``RTLIL::Cell`` and ``RTLIL::Wire`` objects represent classical netlist data.

-  ``RTLIL::Process`` objects represent the decision trees (if-then-else statements,
   etc.) and synchronization declarations (clock signals and sensitivity) from
   Verilog always and VHDL process blocks.

-  ``RTLIL::Memory`` objects represent addressable memories (arrays).

Usually the output of the synthesis procedure is a netlist, i.e. all
``RTLIL::Process`` and ``RTLIL::Memory`` objects must be replaced by
``RTLIL::Cell`` and ``RTLIL::Wire`` objects by synthesis passes.

All features of the HDL that cannot be mapped directly to these RTLIL classes
must be transformed to an RTLIL-compatible representation by the HDL frontend.
This includes Verilog-features such as generate-blocks, loops and parameters.

The following sections contain a more detailed description of the different
parts of RTLIL and rationale behind some of the design decisions.

RTLIL identifiers
-----------------

All identifiers in RTLIL (such as module names, port names, signal names, cell
types, etc.) follow the following naming convention: they must either start with
a backslash (``\``) or a dollar sign (``$``).

Identifiers starting with a backslash are public visible identifiers. Usually
they originate from one of the HDL input files. For example the signal name
``\sig42`` is most likely a signal that was declared using the name ``sig42`` in
an HDL input file. On the other hand the signal name ``$sig42`` is an
auto-generated signal name. The backends convert all identifiers that start with
a dollar sign to identifiers that do not collide with identifiers that start
with a backslash.

This has three advantages:

-  First, it is impossible that an auto-generated identifier collides with an
   identifier that was provided by the user.

-  Second, the information about which identifiers were originally provided by
   the user is always available which can help guide some optimizations. For
   example, :cmd:ref:`opt_clean` tries to preserve signals with a user-provided
   name but doesn't hesitate to delete signals that have auto-generated names
   when they just duplicate other signals.  Note that this can be overridden
   with the `-purge` option to also delete internal nets with user-provided
   names.

-  Third, the delicate job of finding suitable auto-generated public visible
   names is deferred to one central location. Internally auto-generated names
   that may hold important information for Yosys developers can be used without
   disturbing external tools. For example the Verilog backend assigns names in
   the form ``_123_``.

Whitespace and control characters (any character with an ASCII code 32 or less)
are not allowed in RTLIL identifiers; most frontends and backends cannot support
these characters in identifiers.

In order to avoid programming errors, the RTLIL data structures check if all
identifiers start with either a backslash or a dollar sign, and contain no
whitespace or control characters. Violating these rules results in a runtime
error.

All RTLIL identifiers are case sensitive.

Some transformations, such as flattening, may have to change identifiers
provided by the user to avoid name collisions. When that happens, attribute
``hdlname`` is attached to the object with the changed identifier. This
attribute contains one name (if emitted directly by the frontend, or is a result
of disambiguation) or multiple names separated by spaces (if a result of
flattening). All names specified in the ``hdlname`` attribute are public and do
not include the leading ``\``.

RTLIL::Design and RTLIL::Module
-------------------------------

The ``RTLIL::Design`` object is basically just a container for ``RTLIL::Module``
objects. In addition to a list of ``RTLIL::Module`` objects the
``RTLIL::Design`` also keeps a list of selected objects, i.e. the objects that
passes should operate on. In most cases the whole design is selected and
therefore passes operate on the whole design. But this mechanism can be useful
for more complex synthesis jobs in which only parts of the design should be
affected by certain passes.

Besides the objects shown in the :ref:`ER diagram <fig:Overview_RTLIL>` above,
an ``RTLIL::Module`` object contains the following additional properties:

-  The module name
-  A list of attributes
-  A list of connections between wires
-  An optional frontend callback used to derive parametrized variations of the
   module

The attributes can be Verilog attributes imported by the Verilog frontend or
attributes assigned by passes. They can be used to store additional metadata
about modules or just mark them to be used by certain part of the synthesis
script but not by others.

Verilog and VHDL both support parametric modules (known as "generic entities" in
VHDL). The RTLIL format does not support parametric modules itself. Instead each
module contains a callback function into the AST frontend to generate a
parametrized variation of the ``RTLIL::Module`` as needed. This callback then
returns the auto-generated name of the parametrized variation of the module. (A
hash over the parameters and the module name is used to prohibit the same
parametrized variation from being generated twice. For modules with only a few
parameters, a name directly containing all parameters is generated instead of a
hash string.)

.. _sec:rtlil_cell_wire:

RTLIL::Cell and RTLIL::Wire
---------------------------

A module contains zero to many ``RTLIL::Cell`` and ``RTLIL::Wire`` objects.
Objects of these types are used to model netlists. Usually the goal of all
synthesis efforts is to convert all modules to a state where the functionality
of the module is implemented only by cells from a given cell library and wires
to connect these cells with each other. Note that module ports are just wires
with a special property.

An ``RTLIL::Wire`` object has the following properties:

-  The wire name
-  A list of attributes
-  A width (buses are just wires with a width more than 1)
-  Bus direction (MSB to LSB or vice versa)
-  Lowest valid bit index (LSB or MSB depending on bus direction)
-  If the wire is a port: port number and direction (input/output/inout)

As with modules, the attributes can be Verilog attributes imported by the
Verilog frontend or attributes assigned by passes.

In Yosys, busses (signal vectors) are represented using a single wire object
with a width more than 1. So Yosys does not convert signal vectors to individual
signals. This makes some aspects of RTLIL more complex but enables Yosys to be
used for coarse grain synthesis where the cells of the target architecture
operate on entire signal vectors instead of single bit wires.

In Verilog and VHDL, busses may have arbitrary bounds, and LSB can have either
the lowest or the highest bit index. In RTLIL, bit 0 always corresponds to LSB;
however, information from the HDL frontend is preserved so that the bus will be
correctly indexed in error messages, backend output, constraint files, etc.

An ``RTLIL::Cell`` object has the following properties:

-  The cell name and type
-  A list of attributes
-  A list of parameters (for parametric cells)
-  Cell ports and the connections of ports to wires and constants

The connections of ports to wires are coded by assigning an ``RTLIL::SigSpec``
to each cell port. The ``RTLIL::SigSpec`` data type is described in the next
section.

.. _sec:rtlil_sigspec:

RTLIL::SigSpec
--------------

A "signal" is everything that can be applied to a cell port. I.e.

-  | Any constant value of arbitrary bit-width
   | 1em For example: ``1337, 16'b0000010100111001, 1'b1, 1'bx``

-  | All bits of a wire or a selection of bits from a wire
   | 1em For example: ``mywire, mywire[24], mywire[15:8]``

-  | Concatenations of the above
   | 1em For example: ``{16'd1337, mywire[15:8]}``

The ``RTLIL::SigSpec`` data type is used to represent signals. The ``RTLIL::Cell``
object contains one ``RTLIL::SigSpec`` for each cell port.

In addition, connections between wires are represented using a pair of
``RTLIL::SigSpec`` objects. Such pairs are needed in different locations.
Therefore the type name ``RTLIL::SigSig`` was defined for such a pair.

.. _sec:rtlil_process:

RTLIL::Process
--------------

When a high-level HDL frontend processes behavioural code it splits it up into
data path logic (e.g. the expression ``a + b`` is replaced by the output of an
adder that takes a and b as inputs) and an ``RTLIL::Process`` that models the
control logic of the behavioural code. Let's consider a simple example:

.. code:: verilog
   :number-lines:

   module ff_with_en_and_async_reset(clock, reset, enable, d, q);
   input clock, reset, enable, d;
   output reg q;
   always @(posedge clock, posedge reset)
       if (reset)
           q <= 0;
       else if (enable)
           q <= d;
   endmodule

In this example there is no data path and therefore the ``RTLIL::Module`` generated
by the frontend only contains a few ``RTLIL::Wire`` objects and an ``RTLIL::Process`` .
The ``RTLIL::Process`` in RTLIL syntax:

.. code:: RTLIL
   :number-lines:

   process $proc$ff_with_en_and_async_reset.v:4$1
       assign $0\q[0:0] \q
       switch \reset
           case 1'1
               assign $0\q[0:0] 1'0
           case
               switch \enable
                   case 1'1
                       assign $0\q[0:0] \d
                   case
               end
       end
       sync posedge \clock
           update \q $0\q[0:0]
       sync posedge \reset
           update \q $0\q[0:0]
   end

This ``RTLIL::Process`` contains two ``RTLIL::SyncRule`` objects, two
``RTLIL::SwitchRule`` objects and five ``RTLIL::CaseRule`` objects. The wire
``$0\q[0:0]`` is an automatically created wire that holds the next value of
``\q``. The lines 2..12 describe how ``$0\q[0:0]`` should be calculated. The
lines 13..16 describe how the value of ``$0\q[0:0]`` is used to update ``\q``.

An ``RTLIL::Process`` is a container for zero or more ``RTLIL::SyncRule``
objects and exactly one ``RTLIL::CaseRule`` object, which is called the root
case.

An ``RTLIL::SyncRule`` object contains an (optional) synchronization condition
(signal and edge-type), zero or more assignments (``RTLIL::SigSig``), and zero
or more memory writes (``RTLIL::MemWriteAction``). The always synchronization
condition is used to break combinatorial loops when a latch should be inferred
instead.

An ``RTLIL::CaseRule`` is a container for zero or more assignments
(``RTLIL::SigSig``) and zero or more ``RTLIL::SwitchRule`` objects. An
``RTLIL::SwitchRule`` objects is a container for zero or more
``RTLIL::CaseRule`` objects.

In the above example the lines 2..12 are the root case. Here ``$0\q[0:0]`` is
first assigned the old value ``\q`` as default value (line 2). The root case
also contains an ``RTLIL::SwitchRule`` object (lines 3..12). Such an object is
very similar to the C switch statement as it uses a control signal (``\reset``
in this case) to determine which of its cases should be active. The
``RTLIL::SwitchRule`` object then contains one ``RTLIL::CaseRule`` object per
case. In this example there is a case [1]_ for ``\reset == 1`` that causes
``$0\q[0:0]`` to be set (lines 4 and 5) and a default case that in turn contains
a switch that sets ``$0\q[0:0]`` to the value of ``\d`` if ``\enable`` is active
(lines 6..11).

A case can specify zero or more compare values that will determine whether it
matches. Each of the compare values must be the exact same width as the control
signal. When more than one compare value is specified, the case matches if any
of them matches the control signal; when zero compare values are specified, the
case always matches (i.e. it is the default case).

A switch prioritizes cases from first to last: multiple cases can match, but
only the first matched case becomes active. This normally synthesizes to a
priority encoder. The parallel_case attribute allows passes to assume that no
more than one case will match, and full_case attribute allows passes to assume
that exactly one case will match; if these invariants are ever dynamically
violated, the behavior is undefined. These attributes are useful when an
invariant invisible to the synthesizer causes the control signal to never take
certain bit patterns.

The lines 13..16 then cause ``\q`` to be updated whenever there is a positive
clock edge on ``\clock`` or ``\reset``.

In order to generate such a representation, the language frontend must be able
to handle blocking and nonblocking assignments correctly. However, the language
frontend does not need to identify the correct type of storage element for the
output signal or generate multiplexers for the decision tree. This is done by
passes that work on the RTLIL representation. Therefore it is relatively easy to
substitute these steps with other algorithms that target different target
architectures or perform optimizations or other transformations on the decision
trees before further processing them.

One of the first actions performed on a design in RTLIL representation in most
synthesis scripts is identifying asynchronous resets. This is usually done using
the :cmd:ref:`proc_arst` pass. This pass transforms the above example to the
following ``RTLIL::Process``:

.. code:: RTLIL
   :number-lines:

   process $proc$ff_with_en_and_async_reset.v:4$1
       assign $0\q[0:0] \q
       switch \enable
           case 1'1
               assign $0\q[0:0] \d
           case
       end
       sync posedge \clock
           update \q $0\q[0:0]
       sync high \reset
           update \q 1'0
   end

This pass has transformed the outer ``RTLIL::SwitchRule`` into a modified
``RTLIL::SyncRule`` object for the ``\reset`` signal. Further processing converts the
``RTLIL::Process`` into e.g. a d-type flip-flop with asynchronous reset and a
multiplexer for the enable signal:

.. code:: RTLIL
   :number-lines:

   cell $adff $procdff$6
       parameter \ARST_POLARITY 1'1
       parameter \ARST_VALUE 1'0
       parameter \CLK_POLARITY 1'1
       parameter \WIDTH 1
       connect \ARST \reset
       connect \CLK \clock
       connect \D $0\q[0:0]
       connect \Q \q
   end
   cell $mux $procmux$3
       parameter \WIDTH 1
       connect \A \q
       connect \B \d
       connect \S \enable
       connect \Y $0\q[0:0]
   end

Different combinations of passes may yield different results. Note that
``$adff`` and ``$mux`` are internal cell types that still need to be mapped to
cell types from the target cell library.

Some passes refuse to operate on modules that still contain ``RTLIL::Process`` 
objects as the presence of these objects in a module increases the complexity.
Therefore the passes to translate processes to a netlist of cells are usually
called early in a synthesis script. The proc pass calls a series of other passes
that together perform this conversion in a way that is suitable for most
synthesis tasks.

.. _sec:rtlil_memory:

RTLIL::Memory
-------------

For every array (memory) in the HDL code an ``RTLIL::Memory`` object is created.
A memory object has the following properties:

-  The memory name
-  A list of attributes
-  The width of an addressable word
-  The size of the memory in number of words

All read accesses to the memory are transformed to ``$memrd`` cells and all
write accesses to ``$memwr`` cells by the language frontend. These cells consist
of independent read- and write-ports to the memory. Memory initialization is
transformed to ``$meminit`` cells by the language frontend. The ``\MEMID``
parameter on these cells is used to link them together and to the
``RTLIL::Memory`` object they belong to.

The rationale behind using separate cells for the individual ports versus
creating a large multiport memory cell right in the language frontend is that
the separate ``$memrd`` and ``$memwr`` cells can be consolidated using resource
sharing. As resource sharing is a non-trivial optimization problem where
different synthesis tasks can have different requirements it lends itself to do
the optimisation in separate passes and merge the ``RTLIL::Memory`` objects and
``$memrd`` and ``$memwr`` cells to multiport memory blocks after resource
sharing is completed.

The memory pass performs this conversion and can (depending on the options
passed to it) transform the memories directly to d-type flip-flops and address
logic or yield multiport memory blocks (represented using ``$mem`` cells).

See :ref:`sec:memcells` for details about the memory cell types.

.. [1]
   The syntax ``1'1`` in the RTLIL code specifies a constant with a length of
   one bit (the first ``1``), and this bit is a one (the second ``1``).



formats/rtlil_text.rst
--------------------------------------
.. _chapter:textrtlil:

RTLIL text representation
-------------------------

This appendix documents the text representation of RTLIL in extended Backus-Naur
form (EBNF).

The grammar is not meant to represent semantic limitations. That is, the grammar
is "permissive", and later stages of processing perform more rigorous checks.

The grammar is also not meant to represent the exact grammar used in the RTLIL
frontend, since that grammar is specific to processing by lex and yacc, is even
more permissive, and is somewhat less understandable than simple EBNF notation.

Finally, note that all statements (rules ending in ``-stmt``) terminate in an
end-of-line. Because of this, a statement cannot be broken into multiple lines.

Lexical elements
~~~~~~~~~~~~~~~~

Characters
^^^^^^^^^^

An RTLIL file is a stream of bytes. Strictly speaking, a "character" in an RTLIL
file is a single byte. The lexer treats multi-byte encoded characters as
consecutive single-byte characters. While other encodings *may* work, UTF-8 is
known to be safe to use. Byte order marks at the beginning of the file will
cause an error.

ASCII spaces (32) and tabs (9) separate lexer tokens.

A ``nonws`` character, used in identifiers, is any character whose encoding
consists solely of bytes above ASCII space (32).

An ``eol`` is one or more consecutive ASCII newlines (10) and carriage returns
(13).

Identifiers
^^^^^^^^^^^

There are two types of identifiers in RTLIL:

-  Publically visible identifiers
-  Auto-generated identifiers

.. code:: BNF

    <id>            ::= <public-id> | <autogen-id>
    <public-id>     ::= \ <nonws>+
    <autogen-id>    ::= $ <nonws>+

Values
^^^^^^

A *value* consists of a width in bits and a bit representation, most
significant bit first. Bits may be any of:

-  ``0``: A logic zero value
-  ``1``: A logic one value
-  ``x``: An unknown logic value (or don't care in case patterns)
-  ``z``: A high-impedance value (or don't care in case patterns)
-  ``m``: A marked bit (internal use only)
-  ``-``: A don't care value

An *integer* is simply a signed integer value in decimal format. **Warning:**
Integer constants are limited to 32 bits. That is, they may only be in the range
:math:`[-2147483648, 2147483648)`. Integers outside this range will result in an
error.

.. code:: BNF

    <value>         ::= <decimal-digit>+ ' <binary-digit>*
    <decimal-digit> ::= 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
    <binary-digit>  ::= 0 | 1 | x | z | m | -
    <integer>       ::= -? <decimal-digit>+

Strings
^^^^^^^

A string is a series of characters delimited by double-quote characters. Within
a string, any character except ASCII NUL (0) may be used. In addition, certain
escapes can be used:

-  ``\n``: A newline
-  ``\t``: A tab
-  ``\ooo``: A character specified as a one, two, or three digit octal value

All other characters may be escaped by a backslash, and become the following
character. Thus:

-  ``\\``: A backslash
-  ``\"``: A double-quote
-  ``\r``: An 'r' character

Comments
^^^^^^^^

A comment starts with a ``#`` character and proceeds to the end of the line. All
comments are ignored.

File
~~~~

A file consists of an optional autoindex statement followed by zero or more
modules.

.. code:: BNF

    <file> ::= <autoidx-stmt>? <module>*

Autoindex statements
^^^^^^^^^^^^^^^^^^^^

The autoindex statement sets the global autoindex value used by Yosys when it
needs to generate a unique name, e.g. ``flattenN``. The N part is filled with
the value of the global autoindex value, which is subsequently incremented. This
global has to be dumped into RTLIL, otherwise e.g. dumping and running a pass
would have different properties than just running a pass on a warm design.

.. code:: BNF

    <autoidx-stmt> ::= autoidx <integer> <eol>

Modules
^^^^^^^

Declares a module, with zero or more attributes, consisting of zero or more
wires, memories, cells, processes, and connections.

.. code:: BNF

    <module>            ::= <attr-stmt>* <module-stmt> <module-body> <module-end-stmt>
    <module-stmt>       ::= module <id> <eol>
    <module-body>       ::= (<param-stmt> 
                         |   <wire> 
                         |   <memory> 
                         |   <cell> 
                         |   <process>)*
    <param-stmt>        ::= parameter <id> <constant>? <eol>
    <constant>          ::= <value> | <integer> | <string>
    <module-end-stmt>   ::= end <eol>

Attribute statements
^^^^^^^^^^^^^^^^^^^^

Declares an attribute with the given identifier and value.

.. code:: BNF

    <attr-stmt> ::= attribute <id> <constant> <eol>

Signal specifications
^^^^^^^^^^^^^^^^^^^^^

A signal is anything that can be applied to a cell port, i.e. a constant value,
all bits or a selection of bits from a wire, or concatenations of those.

**Warning:** When an integer constant is a sigspec, it is always 32 bits wide,
2's complement. For example, a constant of :math:`-1` is the same as
``32'11111111111111111111111111111111``, while a constant of :math:`1` is the
same as ``32'1``.

See :ref:`sec:rtlil_sigspec` for an overview of signal specifications.

.. code:: BNF

    <sigspec> ::= <constant> 
               |  <wire-id>
               |  <sigspec> [ <integer> (:<integer>)? ] 
               |  { <sigspec>* }

Connections
^^^^^^^^^^^

Declares a connection between the given signals.

.. code:: BNF

    <conn-stmt> ::= connect <sigspec> <sigspec> <eol>

Wires
^^^^^

Declares a wire, with zero or more attributes, with the given identifier and
options in the enclosing module.

See :ref:`sec:rtlil_cell_wire` for an overview of wires.

.. code:: BNF

    <wire>          ::= <attr-stmt>* <wire-stmt>
    <wire-stmt>     ::= wire <wire-option>* <wire-id> <eol>
    <wire-id>       ::= <id>
    <wire-option>   ::= width <integer> 
                     |  offset <integer> 
                     |  input <integer> 
                     |  output <integer> 
                     |  inout <integer> 
                     |  upto 
                     |  signed

Memories
^^^^^^^^

Declares a memory, with zero or more attributes, with the given identifier and
options in the enclosing module.

See :ref:`sec:rtlil_memory` for an overview of memory cells, and
:ref:`sec:memcells` for details about memory cell types.

.. code:: BNF

    <memory>        ::= <attr-stmt>* <memory-stmt>
    <memory-stmt>   ::= memory <memory-option>* <id> <eol>
    <memory-option> ::= width <integer> 
                     |  size <integer> 
                     |  offset <integer>

Cells
^^^^^

Declares a cell, with zero or more attributes, with the given identifier and
type in the enclosing module.

Cells perform functions on input signals. See
:doc:`/yosys_internals/formats/cell_library` for a detailed list of cell types.

.. code:: BNF

    <cell>              ::= <attr-stmt>* <cell-stmt> <cell-body-stmt>* <cell-end-stmt>
    <cell-stmt>         ::= cell <cell-type> <cell-id> <eol>
    <cell-id>           ::= <id>
    <cell-type>         ::= <id>
    <cell-body-stmt>    ::= parameter (signed | real)? <id> <constant> <eol>
                         |  connect <id> <sigspec> <eol>
    <cell-end-stmt>     ::= end <eol>


Processes
^^^^^^^^^

Declares a process, with zero or more attributes, with the given identifier in
the enclosing module. The body of a process consists of zero or more
assignments, exactly one switch, and zero or more syncs.

See :ref:`sec:rtlil_process` for an overview of processes.

.. code:: BNF

    <process>       ::= <attr-stmt>* <proc-stmt> <process-body> <proc-end-stmt>
    <proc-stmt>     ::= process <id> <eol>
    <process-body>  ::= <assign-stmt>* <switch>? <assign-stmt>* <sync>*
    <assign-stmt>   ::= assign <dest-sigspec> <src-sigspec> <eol>
    <dest-sigspec>  ::= <sigspec>
    <src-sigspec>   ::= <sigspec>
    <proc-end-stmt> ::= end <eol>

Switches
^^^^^^^^

Switches test a signal for equality against a list of cases. Each case specifies
a comma-separated list of signals to check against. If there are no signals in
the list, then the case is the default case. The body of a case consists of zero
or more switches and assignments. Both switches and cases may have zero or more
attributes.

.. code:: BNF

    <switch>            ::= <switch-stmt> <case>* <switch-end-stmt>
    <switch-stmt>        := <attr-stmt>* switch <sigspec> <eol>
    <case>              ::= <attr-stmt>* <case-stmt> <case-body>
    <case-stmt>         ::= case <compare>? <eol>
    <compare>           ::= <sigspec> (, <sigspec>)*
    <case-body>         ::= (<switch> | <assign-stmt>)*
    <switch-end-stmt>   ::= end <eol>

Syncs
^^^^^

Syncs update signals with other signals when an event happens. Such an event may
be:

-  An edge or level on a signal
-  Global clock ticks
-  Initialization
-  Always

.. code:: BNF

    <sync>          ::= <sync-stmt> <update-stmt>*
    <sync-stmt>     ::= sync <sync-type> <sigspec> <eol> 
                     |  sync global <eol>
                     |  sync init <eol> 
                     |  sync always <eol>
    <sync-type>     ::= low | high | posedge | negedge | edge
    <update-stmt>   ::= update <dest-sigspec> <src-sigspec> <eol>



memory/memlib.md
--------------------------------------
# The `memory_libmap` pass

The `memory_libmap` pass is used to map memories to hardware primitives.  To work,
it needs a description of available target memories in a custom format.


## Basic structure

A basic library could look like this:

    # A distributed-class RAM called $__RAM16X4SDP_
    ram distributed $__RAM16X4SDP_ {
        # Has 4 address bits (ie. 16 rows).
        abits 4;
        # Has 4 data bits.
        width 4;
        # Cost for the selection heuristic.
        cost 4;
        # Can be initialized to any value on startup.
        init any;
        # Has a synchronous write port called "W"...
        port sw "W" {
            # ... with a positive edge clock.
            clock posedge;
        }
        # Has an asynchronous read port called "R".
        port ar "R" {
        }
    }

    # A block-class RAM called $__RAMB9K_
    ram block $__RAMB9K_ {
        # Has 13 address bits in the base (most narrow) data width.
        abits 13;
        # The available widths are:
        # - 1 (13 address bits)
        # - 2 (12 address bits)
        # - 4 (11 address bits)
        # - 9 (10 address bits)
        # - 18 (9 address bits)
        # The width selection is per-port.
        widths 1 2 4 9 18 per_port;
        # Has a write enable signal with 1 bit for every 9 data bits.
        byte 9;
        cost 64;
        init any;
        # Has two synchronous read+write ports, called "A" and "B".
        port srsw "A" "B" {
            clock posedge;
            # Has a clock enable signal (gates both read and write).
            clken;
            # Has three per-port selectable options for handling read+write behavior:
            portoption "RDWR" "NO_CHANGE" {
                # When port is writing, reading is not done (output register keeps
                # its value).
                rdwr no_change;
            }
            portoption "RDWR" "OLD" {
                # When port is writing, the data read is the old value (before the
                # write).
                rdwr old;
            }
            portoption "RDWR" "NEW" {
                # When port is writing, the data read is the new value.
                rdwr new;
            }
        }
    }

The pass will automatically select between the two available cells and
the logic fallback (mapping the whole memory to LUTs+FFs) based on required
capabilities and cost function.  The selected memories will be transformed
to intermediate `$__RAM16X4SDP_` and `$__RAMB9K_` cells that need to be mapped
to actual hardware cells by a `techmap` pass, while memories selected for logic
fallback will be left unmapped and will be later mopped up by `memory_map` pass.

## RAM definition blocks

The syntax for a RAM definition is:

    ram <kind: distributed|block|huge> <name> {
        <ram properties>
        <ports>
    }

The `<name>` is used as the type of the mapped cell that will be passed to `techmap`.
The memory kind is one of `distributed`, `block`, or `huge`.  It describes the general
class of the memory and can be matched on by manual selection attributes.

The available ram properties are:

- `abits <address bits>;`
- `width <width>;`
- `widths <width 1> <width 2> ... <width n> <global|per_port>;`
- `byte <width>;`
- `cost <cost>;`
- `widthscale [<factor>];`
- `resource <name> <count>;`
- `init <none|zero|any|no_undef>;`
- `style "<name 1>" "<name 2>" "<name 3>" ...;`
- `prune_rom;`

### RAM dimensions

The memory dimensions are described by `abits` and `width` or `widths` properties.

For a simple memory cell with a fixed width, use `abits` and `width` like this:

    abits 4;
    width 4;

This will result in a `2**abits × width` memory cell.

Multiple-width memories are also possible, and use the `widths` property instead.
The rules for multiple-width memories are:

- the widths are given in `widths` property in increasing order
- the value in the `abits` property corresponds to the most narrow width
- every width in the list needs to be greater than or equal to twice
  the previous width (ie. `1 2 4 9 18` is valid, `1 2 4 7 14` is not)
- it is assumed that, for every width in progression, the word in memory
  is made of two smaller words, plus optionally some extra bits (eg. in the above
  list, the 9-bit word is made of two 4-bit words and 1 extra bit), and thus
  each sequential width in the list corresponds to one fewer usable address bit
- all addresses connected to memory ports are always `abits` bits wide, with const
  zero wired to the unused bits corresponding to wide ports

When multiple widths are specified, they can be `per_port` or `global`.
For the `global` version, the pass has to pick one width for the whole cell,
and it is set on the resulting cell as the `WIDTH` parameter.  For the `per_port`
version, the selection is made on per-port basis, and passed using `PORT_*_WIDTH`
parameters.  When the mode is `per_port`, the width selection can be fine-tuned
with the port `width` property.

Specifying dimensions is mandatory.


### Byte width

If the memory cell has per-byte write enables, the `byte` property can be used
to define the byte size (ie. how many data bits correspond to one write enable
bit).

The property is optional.  If not used, it is assumed that there is a single
write enable signal for each writable port.

The rules for this property are as follows:

- for every available width, the width needs to be a multiple of the byte size,
  or the byte size needs to be larger than the width
- if the byte size is larger than the width, the byte enable signel is assumed
  to be one bit wide and cover the whole port
- otherwise, the byte enable signal has one bit for every `byte` bits of the
  data port

The exact kind of byte enable signal is determined by the presence or absence
of the per-port `wrbe_separate` property.


### Cost properties

The `cost` property is used to estimate the cost of using a given mapping.
This is the cost of using one cell, and will be scaled as appropriate if
the mapping requires multiple cells.

If the `widthscale` property is specified, the mapping is assumed to be flexible,
with cost scaling with the percentage of data width actually used.  The value
of the `widthscale` property is how much of the cost is scalable as such.
If the value is omitted, all of the cost is assumed to scale.
Eg. for the following properties:

    width 14;
    cost 8;
    widthscale 7;

The cost of a given cell will be assumed to be `(8 - 7) + 7 * (used_bits / 14)`.

If `widthscale` is used, The pass will attach a `BITS_USED` parameter to mapped
calls, with a bitmask of which data bits of the memory are actually in use.
The parameter width will be the widest width in the `widths` property, and
the bit correspondence is defined accordingly.

The `cost` property is mandatory.


### `init` property

This property describes the state of the memory at initialization time.  Can have
one of the following values:

- `none`: the memory contents are unpredictable, memories requiring any sort
  of initialization will not be mapped to this cell
- `zero`: the memory contents are zero, memories can be mapped to this cell iff
  their initialization value is entirely zero or undef
- `any`: the memory contents can be arbitrarily selected, and the initialization
  will be passes as the `INIT` parameter to the mapped cell
- `no_undef`: like `any`, but only 0 and 1 bit values are supported (the pass will
  convert any x bits to 0)

The `INIT` parameter is always constructed as a concatenation of words corresponding
to the widest available `widths` setting, so that all available memory cell bits
are covered.

This property is optional and assumed to be `none` when not present.


### `style` property

Provides a name (or names) for this definition that can be passed to the `ram_style`
or similar attribute to manually select it.  Optional and can be used multiple times.


### `prune_rom` property

Specifying this property disqualifies the definition from consideration for source
memories that have no write ports (ie. ROMs).  Use this on definitions that have
an obviously superior read-only alternative (eg. LUTRAMs) to make the pass skip
them over quickly.


## Port definition blocks

The syntax for a port group definition is:

    port <ar|sr|sw|arsw|srsw> "NAME 1" "NAME 2" ... {
        <port properties>
    }

A port group definition defines a group of ports with identical properties.
There are as many ports in a group as there are names given.

Ports come in 5 kinds:

- `ar`: asynchronous read port
- `sr`: synchronous read port
- `sw`: synchronous write port
- `arsw`: simultanous synchronous write + asynchronous read with common address (commonly found in LUT RAMs)
- `srsw`: synchronous write + synchronous read with common address

The port properties available are:

- `width <tied|mix>;`
- `width <width 1> <width 2> ...;`
- `width <tied|mix> <width 1> <width 2> ...;`
- `width rd <width 1> <width 2> ... wr <width 1> <width 2> ...;`
- `clock <posedge|negedge|anyedge> ["SHARED_NAME"];`
- `clken;`
- `rden;`
- `wrbe_separate;`
- `rdwr <undefined|no_change|new|old|new_only>;`
- `rdinit <none|zero|any|no_undef>;`
- `rdarst <none|zero|any|no_undef|init>;`
- `rdsrst <none|zero|any|no_undef|init> <ungated|gatec_clken|gated_rden> [block_wr];`
- `wrprio "NAME" "NAME" ...;`
- `wrtrans <"NAME"|all> <old|new>;`
- `optional;`
- `optional_rw;`

The base signals connected to the mapped cell for ports are:

- `PORT_<name>_ADDR`: the address
- `PORT_<name>_WR_DATA`: the write data (for `sw`/`arsw`/`srsw` ports only)
- `PORT_<name>_RD_DATA`: the read data (for `ar`/`sr`/`arsw`/`srsw` ports only)
- `PORT_<name>_WR_EN`: the write enable or enables (for `sw`/`arsw`/`srsw` ports only)

The address is always `abits` wide.  If a non-narrowest width is used, the appropriate low
bits will be tied to 0.


### Port `width` property

If the RAM has `per_port` widths, the available width selection can be further described
on per-port basis, by using one of the following properties:

- `width tied;`: any width from the master `widths` list is acceptable, and
  (for read+write ports) the read and write width has to be the same
- `width tied <width 1> <width 2> ...;`: like above, but limits the width
  selection to the given list; the list has to be a contiguous sublist of the
  master `widths` list
- `width <width 1> <width 2> ...;`: alias for the above, to be used for read-only
  or write-only ports
- `width mix;`: any width from the master `widths` list is acceptable, and
  read width can be different than write width (only usable for read+write ports)
- `width mix <width 1> <width 2> ...;`: like above, but limits the width
  selection to the given list; the list has to be a contiguous sublist of the
  master `widths` list
- `width rd <width 1> <width 2> ... wr <width 1> <width 2> ...;`: like above,
  but the limitted selection can be different for read and write widths

If `per_port` widths are in use and this property is not specified, `width tied;` is assumed.

The parameters attached to the cell in `per_port` widths mode are:

- `PORT_<name>_WIDTH`: the selected width (for `tied` ports)
- `PORT_<name>_RD_WIDTH`: the selected read width (for `mix` ports)
- `PORT_<name>_WR_WIDTH`: the selected write width (for `mix` ports)


### `clock` property

The `clock` property is used with synchronous ports (and synchronous ports only).
It is mandatory for them and describes the clock polarity and clock sharing.
`anyedge` means that both polarities are supported.

If a shared clock name is provided, the port is assumed to have a shared clock signal
with all other ports using the same shared name.  Otherwise, the port is assumed to
have its own clock signal.

The port clock is always provided on the memory cell as `PORT_<name>_CLK` signal
(even if it is also shared).  Shared clocks are also provided as `CLK_<shared_name>`
signals.

For `anyedge` clocks, the cell gets a `PORT_<name>_CLKPOL` parameter that is set
to 1 for `posedge` clocks and 0 for `negedge` clocks.  If the clock is shared,
the same information will also be provided as `CLK_<shared_name>_POL` parameter.


### `clken` and `rden`

The `clken` property, if present, means that the port has a clock enable signal
gating both reads and writes.  Such signal will be provided to the mapped cell
as `PORT_<name>_CLK_EN`.  It is only applicable to synchronous ports.

The `rden` property, if present, means that the port has a read clock enable signal.
Such signal will be provided to the mapped cell as `PORT_<name>_RD_EN`.  It is only
applicable to synchronous read ports (`sr` and `srsw`).

For `sr` ports, both of these options are effectively equivalent.


### `wrbe_separate` and the write enables

The `wrbe_separate` property specifies that the write byte enables are provided
as a separate signal from the main write enable.  It can only be used when the
RAM-level `byte` property is also specified.

The rules are as follows:

If no `byte` is specified:

- `wrbe_separate` is not allowed
- `PORT_<name>_WR_EN` signal is single bit

If `byte` is specified, but `wrbe_separate` is not:

- `PORT_<name>_WR_EN` signal has one bit for every data byte
- `PORT_<name>_WR_EN_WIDTH` parameter is the width of the above (only present for multiple-width cells)

If `byte` is specified and `wrbe_separate` is present:

- `PORT_<name>_WR_EN` signal is single bit
- `PORT_<name>_WR_BE` signal has one bit for every data byte
- `PORT_<name>_WR_BE_WIDTH` parameter is the width of the above (only present for multiple-width cells)
- a given byte is written iff all of `CLK_EN` (if present), `WR_EN`, and the corresponding `WR_BE` bit are one

This property can only be used on write ports.


### `rdwr` property

This property is allowed only on `srsw` ports and describes read-write interactions.

The possible values are:

- `no_change`: if write is being performed (any bit of `WR_EN` is set),
  reading is not performed and the `RD_DATA` keeps its old value
- `undefined`: all `RD_DATA` bits corresponding to enabled `WR_DATA` bits
  have undefined value, remaining bits read from memory
- `old`: all `RD_DATA` bits get the previous value in memory
- `new`: all `RD_DATA` bits get the new value in memory (transparent write)
- `new_only`: all `RD_DATA` bits corresponding to enabled `WR_DATA` bits
  get the new value, all others are undefined

If this property is not found on an `srsw` port, `undefined` is assumed.


### Read data initial value and resets

The `rdinit`, `rdarst`, and `rdsrst` are applicable only to synchronous read
ports.

`rdinit` describes the initial value of the read port data, and can be set to
one of the following:

- `none`: initial data is indeterminate
- `zero`: initial data is all-0
- `any`: initial data is arbitrarily configurable, and the selected value
  will be attached to the cell as `PORT_<name>_RD_INIT_VALUE` parameter
- `no_undef`: like `any`, but only 0 and 1 bits are allowed

`rdarst` and `rdsrst` describe the asynchronous and synchronous reset capabilities.
The values are similar to `rdinit`:

- `none`: no reset
- `zero`: reset to all-0 data
- `any`: reset to arbitrary value, the selected value
  will be attached to the cell as `PORT_<name>_RD_ARST_VALUE` or
  `PORT_<name>_RD_SRST_VALUE` parameter
- `no_undef`: like `any`, but only 0 and 1 bits are allowed
- `init`: reset to the initial value, as specified by `rdinit` (which must be `any`
  or `no_undef` itself)

If the capability is anything other than `none`, the reset signal
will be provided as `PORT_<name>_RD_ARST` or `PORT_<name>_RD_SRST`.

For `rdsrst`, the priority must be additionally specified, as one of:

- `ungated`: `RD_SRST` has priority over both `CLK_EN` and `RD_EN` (if present)
- `gated_clken`: `CLK_EN` has priority over `RD_SRST`; `RD_SRST` has priority over `RD_EN` if present
- `gated_rden`: `RD_EN` and `CLK_EN` (if present) both have priority over `RD_SRST`

Also, `rdsrst` can optionally have `block_wr` specified, which means that sync reset
cannot be performed in the same cycle as a write.

If not provided, `none` is assumed for all three properties.


### Write priority

The `wrprio` property is only allowed on write ports and defines a priority relationship
between port — when `wrprio "B";` is used in definition of port `"A"`, and both ports
simultanously write to the same memory cell, the value written by port `"A"` will have
precedence.

This property is optional, and can be used multiple times as necessary.  If no relationship
is described for a pair of write ports, no priority will be assumed.


### Write transparency

The `wrtrans` property is only allowed on write ports and defines behavior when
another synchronous read port reads from the memory cell at the same time as the
given port writes it.  The values are:

- `old`: the read port will get the old value of the cell
- `new`: the read port will get the new value of the cell

This property is optional, and can be used multiple times as necessary.  If no relationship
is described for a pair of ports, the value read is assumed to be indeterminate.

Note that this property is not used to describe the read value on the port itself for `srsw`
ports — for that purpose, the `rdwr` property is used instead.


### Optional ports

The `optional;` property will make the pass attach a `PORT_<name>_USED` parameter
with a boolean value specifying whether a given port was meaningfully used in
mapping a given cell.  Likewise, `optional_rw;` will attach `PORT_<name>_RD_USED`
and `PORT_<name>_WR_USED` the specify whether the read / write part in particular
was used.  These can be useful if the mapping has some meaningful optimization
to apply for unused ports, but doesn't otherwise influence the selection process.


## Options

For highly configurable cells, multiple variants may be described in one cell description.
All properties and port definitions within a RAM or port definition can be put inside
an `option` block as follows:

    option "NAME" <value> {
        <properties, ports, ...>
    }

The value and name of an option are arbitrary, and the selected option value
will be provided to the cell as `OPTION_<name>` parameter.  Values can be
strings or integers.


Likewise, for per-port options, a `portoption` block can be used:

    portoption "NAME" <value> {
        <properties, ...>
    }

These options will be provided as `PORT_<pname>_OPTION_<oname>` parameters.

The library parser will simply expand the RAM definition for every possible combination
of option values mentioned in the RAM body, and likewise for port definitions.
This can lead to a combinatorial explosion.

If some option values cannot be used together, a `forbid` pseudo-property can be used
to discard a given combination, eg:

    option "ABC" 1 {
        portoption "DEF" "GHI" {
            forbid;
        }
    }

will disallow combining the RAM option `ABC = 2` with port option `DEF = "GHI"`.


## Ifdefs

To allow reusing a library for multiple FPGA families with slighly differing
capabilities, `ifdef` (and `ifndef`) blocks are provided:

    ifdef IS_FANCY_FPGA_WITH_CONFIGURABLE_ASYNC_RESET {
        rdarst any;
    } else {
        rdarst zero;
    }

Such blocks can be enabled by passing the `-D` option to the pass.



pmgen/README.md
--------------------------------------
Pattern Matcher Generator
=========================

The program `pmgen.py` reads a `.pmg` (Pattern Matcher Generator) file and
writes a header-only C++ library that implements that pattern matcher.

The "patterns" in this context are subgraphs in a Yosys RTLIL netlist.

The algorithm used in the generated pattern matcher is a simple recursive
search with backtracking. It is left to the author of the `.pmg` file to
determine an efficient cell order for the search that allows for maximum
use of indices and early backtracking.


API of Generated Matcher
========================

When `pmgen.py` reads a `foobar.pmg` file, it writes `foobar_pm.h` containing
a class `foobar_pm`. That class is instantiated with an RTLIL module and a
list of cells from that module:

    foobar_pm pm(module, module->selected_cells());

The caller must make sure that none of the cells in the 2nd argument are
deleted for as long as the patter matcher instance is used.

At any time it is possible to disable cells, preventing them from showing
up in any future matches:

    pm.blacklist(some_cell);

The `.run_<pattern_name>(callback_function)` method searches for all matches
for the pattern`<pattern_name>` and calls the callback function for each found
match:

    pm.run_foobar([&](){
        log("found matching 'foo' cell: %s\n", log_id(pm.st.foo));
        log("          with 'bar' cell: %s\n", log_id(pm.st.bar));
    });

The `.pmg` file declares matcher state variables that are accessible via the
`.st_<pattern_name>.<state_name>` members. (The `.st_<pattern_name>` member is
of type `foobar_pm::state_<pattern_name>_t`.)

Similarly the `.pmg` file declares user data variables that become members of
`.ud_<pattern_name>`, a struct of type `foobar_pm::udata_<pattern_name>_t`.

There are three versions of the `run_<pattern_name>()` method: Without callback,
callback without arguments, and callback with reference to `pm`. All versions
of the `run_<pattern_name>()` method return the number of found matches.


The .pmg File Format
====================

The `.pmg` file format is a simple line-based file format. For the most part
lines consist of whitespace-separated tokens.

Lines in `.pmg` files starting with `//` are comments.

Declaring a pattern
-------------------

A `.pmg` file contains one or more patterns. Each pattern starts with a line
with the `pattern` keyword followed by the name of the pattern.

Declaring state variables
-------------------------

One or more state variables can be declared using the `state` statement,
followed by a C++ type in angle brackets, followed by a whitespace separated
list of variable names. For example:

    state <bool> flag1 flag2 happy big
    state <SigSpec> sigA sigB sigY

State variables are automatically managed by the generated backtracking algorithm
and saved and restored as needed.

They are automatically initialized to the default constructed value of their type
when `.run_<pattern_name>(callback_function)` is called.

Declaring udata variables
-------------------------

Udata (user-data) variables can be used for example to configure the matcher or
the callback function used to perform actions on found matches.

There is no automatic management of udata variables. For this reason it is
recommended that the user-supplied matcher code treats them as read-only
variables.

They are declared like state variables, just using the `udata` statement:

    udata <int> min_data_width max_data_width
    udata <IdString> data_port_name

They are automatically initialized to the default constructed value of their type
when the pattern matcher object is constructed.

Embedded C++ code
-----------------

Many statements in a `.pmg` file contain C++ code. However, there are some
slight additions to regular C++/Yosys/RTLIL code that make it a bit easier to
write matchers:

- Identifiers starting with a dollar sign or backslash are automatically
  converted to special IdString variables that are initialized when the
  matcher object is constructed.

- The `port(<cell>, <portname>)` function is a handy alias for
  `sigmap(<cell>->getPort(<portname>))`.

- Similarly `param(<cell>, <paramname>)` looks up a parameter on a cell.

- The function `nusers(<sigspec>)` returns the number of different cells
  connected to any of the given signal bits, plus one if any of the signal
  bits is also a primary input or primary output.

- In `code..endcode` blocks there exist `accept`, `reject`, `branch`,
  `finish`, and `subpattern` statements.

- In `index` statements there is a special `===` operator for the index
  lookup.

Matching cells
--------------

Cells are matched using `match..endmatch` blocks. For example:

    match mul
        if ff
        select mul->type == $mul
        select nusers(port(mul, \Y) == 2
        index <SigSpec> port(mul, \Y) === port(ff, \D)
        filter some_weird_function(mul) < other_weird_function(ff)
        optional
    endmatch

A `match` block starts with `match <statevar>` and implicitly generates
a state variable `<statevar>` of type `RTLIL::Cell*`.

All statements in the match block are optional. (An empty match block
would simply match each and every cell in the module.)

The `if <expression>` statement makes the match block conditional. If
`<expression>` evaluates to `false` then the match block will be ignored
and the corresponding state variable is set to `nullptr`. In our example
we only try to match the `mul` cell if the `ff` state variable points
to a cell. (Presumably `ff` is provided by a prior `match` block.)

The `select` lines are evaluated once for each cell when the matcher is
initialized. A `match` block will only consider cells for which all `select`
expressions evaluated to `true`. Note that the state variable corresponding to
the match (in the example `mul`) is the only state variable that may be used
in `select` lines.

Index lines are using the `index <type> expr1 === expr2` syntax.  `expr1` is
evaluated during matcher initialization and the same restrictions apply as for
`select` expressions. `expr2` is evaluated when the match is calulated. It is a
function of any state variables assigned to by previous blocks. Both expression
are converted to the given type and compared for equality. Only cells for which
all `index` statements in the block pass are considered by the match.

Note that `select` and `index` are fast operations. Thus `select` and `index`
should be used whenever possible to create efficient matchers.

Finally, `filter <expression>` narrows down the remaining list of cells. For
performance reasons `filter` statements should only be used for things that
can't be done using `select` and `index`.

The `optional` statement marks optional matches. That is, the matcher will also
explore the case where `mul` is set to `nullptr`. Without the `optional`
statement a match may only be assigned nullptr when one of the `if` expressions
evaluates to `false`.

The `semioptional` statement marks matches that must match if at least one
matching cell exists, but if no matching cell exists it is set to `nullptr`.

Slices and choices
------------------

Cell matches can contain "slices" and "choices". Slices can be used to
create matches for different sections of a cell. For example:

    state <int> pmux_slice

    match pmux
        select pmux->type == $pmux
        slice idx GetSize(port(pmux, \S))
        index <SigBit> port(pmux, \S)[idx] === port(eq, \Y)
        set pmux_slice idx
    endmatch

The first argument to `slice` is the local variable name used to identify the
slice. The second argument is the number of slices that should be created for
this cell. The `set` statement can be used to copy that index into a state
variable so that later matches and/or code blocks can refer to it.

A similar mechanism is "choices", where a list of options is given as
second argument, and the matcher will iterate over those options:

    state <SigSpec> foo bar
    state <IdString> eq_ab eq_ba

    match eq
        select eq->type == $eq
        choice <IdString> AB {\A, \B}
        define <IdString> BA (AB == \A ? \B : \A)
        index <SigSpec> port(eq, AB) === foo
        index <SigSpec> port(eq, BA) === bar
        set eq_ab AB
        set eq_ba BA
    endmatch

Notice how `define` can be used to define additional local variables similar
to the loop variables defined by `slice` and `choice`.

Additional code
---------------

Interleaved with `match..endmatch` blocks there may be `code..endcode` blocks.
Such a block starts with the keyword `code` followed by a list of state variables
that the block may modify. For example:

    code addAB sigS
        if (addA) {
            addAB = addA;
            sigS = port(addA, \B);
        }
        if (addB) {
            addAB = addB;
            sigS = port(addB, \A);
        }
    endcode

The special keyword `reject` can be used to reject the current state and
backtrack. For example:

    code
        if (ffA && ffB) {
            if (port(ffA, \CLK) != port(ffB, \CLK))
                reject;
            if (param(ffA, \CLK_POLARITY) != param(ffB, \CLK_POLARITY))
                reject;
        }
    endcode

Similarly, the special keyword `accept` can be used to accept the current
state. (`accept` will not backtrack. This means it continues with the current
branch and may accept a larger match later.)

The special keyword `branch` can be used to explore different cases. Note that
each code block has an implicit `branch` at the end. So most use-cases of the
`branch` keyword need to end the block with `reject` to avoid the implicit
branch at the end. For example:

    state <int> mode

    code mode
        for (mode = 0; mode < 8; mode++)
            branch;
        reject;
    endcode

But in some cases it is more natural to utilize the implicit branch statement:

    state <IdString> portAB

    code portAB
        portAB = \A;
        branch;
        portAB = \B;
    endcode

There is an implicit `code..endcode` block at the end of each (sub)pattern
that just rejects.

A `code..finally..endcode` block executes the code after `finally` during
back-tracking. This is useful for maintaining user data state or printing
debug messages. For example:

    udata <vector<Cell*>> stack

    code
        stack.push_back(addAB);
        ...
    finally
        stack.pop_back();
    endcode

`accept` and `finish` statements can be used inside the `finally` section,
but not `reject`, `branch`, or `subpattern`.

Declaring a subpattern
----------------------

A subpattern starts with a line containing the `subpattern` keyword followed
by the name of the subpattern. Subpatterns can be called from a `code` block
using a `subpattern(<subpattern_name>);` C statement.

Arguments may be passed to subpattern via state variables. The `subpattern`
line must be followed by a `arg <arg1> <arg2> ...` line that lists the
state variables used to pass arguments.

    state <IdString> foobar_type
    state <bool> foobar_state

    code foobar_type foobar_state
        foobar_state = false;
        foobar_type = $add;
        subpattern(foo);
        foobar_type = $sub;
        subpattern(bar);
    endcode

    subpattern foo
    arg foobar_type foobar_state

    match addsub
        index <IdString> addsub->type === foobar_type
        ...
    endmatch

    code
        if (foobar_state) {
            subpattern(tail);
        } else {
            foobar_state = true;
            subpattern(bar);
        }
    endcode

    subpattern bar
    arg foobar_type foobar_state

    match addsub
        index <IdString> addsub->type === foobar_type
        ...
    endmatch

    code
        if (foobar_state) {
            subpattern(tail);
        } else {
            foobar_state = true;
            subpattern(foo);
        }
    endcode

    subpattern tail
    ...

Subpatterns can be called recursively.

If a `subpattern` statement is preceded by a `fallthrough` statement, this is
equivalent to calling the subpattern at the end of the preceding block.

Generate Blocks
---------------

Match blocks may contain an optional `generate` section that is used for automatic
test-case generation. For example:

    match mul
        ...
    generate 10 0
        SigSpec Y = port(ff, \D);
        SigSpec A = module->addWire(NEW_ID, GetSize(Y) - rng(GetSize(Y)/2));
        SigSpec B = module->addWire(NEW_ID, GetSize(Y) - rng(GetSize(Y)/2));
        module->addMul(NEW_ID, A, B, Y, rng(2));
    endmatch

The expression `rng(n)` returns a non-negative integer less than `n`.

The first argument to `generate` is the chance of this generate block being
executed when the match block did not match anything, in percent.

The second argument to `generate` is the chance of this generate block being
executed when the match block did match something, in percent.

The special statement `finish` can be used within generate blocks to terminate
the current pattern matcher run.



usage: vpr architecture circuit [--pack] [--legalize] [--place] 
       [--analytical_place] [--route] [--analysis] [--disp {on, off}] 
       [--save_graphics {on, off}] [--graphics_commands GRAPHICS_COMMANDS] [-h] 
       [--version] [--device DEVICE_NAME] [-j NUM_WORKERS] 
       [--timing_analysis {on, off}] [--disable_errors DISABLE_ERRORS] 
       [--suppress_warnings SUPPRESS_WARNINGS] 
       [--terminate_if_timing_fails {on, off}] 
       [--route_chan_width CHANNEL_WIDTH] [--server] [OTHER_OPTIONS ...]

Implements the specified circuit onto the target FPGA architecture by performing 
packing/placement/routing, and analyzes the result.

Attempts to find the minimum routable channel width, unless a fixed channel 
width is specified with --route_chan_width.

positional arguments:
  architecture      FPGA Architecture description file
                       - XML: this is the default frontend format
                       - FPGA Interchange: device architecture file in the FPGA 
                    Interchange format
  circuit           Circuit file (or circuit name if --circuit_file specified)

stage options:
  --pack            Run packing (Default: off)
  --legalize        Legalize a flat placement, i.e. reconstruct and place 
                    clusters based on a flat placement file, which lists cluster 
                    and intra-cluster placement coordinates for each primitive. (Default: off)
  --place           Run placement (Default: off)
  --analytical_place
                    Run analytical placement. Analytical Placement uses an 
                    integrated packing and placement algorithm, using 
                    information from the primitive level to improve clustering 
                    and placement. (Default: off)
  --route           Run routing (Default: off)
  --analysis        Run analysis (Default: off)

  If none of the stage options are specified, all stages are run.
  Analysis is always run after routing, unless the implementation
  is illegal.
  
  If the implementation is illegal analysis can be forced by explicitly
  specifying the --analysis option.

graphics options:
  --disp {on, off}  Enable or disable interactive graphics (Default: off)
  --auto {0, 1, 2}  Controls how often VPR pauses for interactive graphics 
                    (requiring Proceed to be clicked). Higher values pause less 
                    frequently (Default: 1)
  --save_graphics {on, off}
                    Save all graphical contents to PDF files (Default: off)
  --graphics_commands GRAPHICS_COMMANDS
                    A set of semi-colon seperated graphics commands. 
                    Commands must be surrounded by quotation marks (e.g. 
                    --graphics_commands "save_graphics place.png")
                       Commands:
                          * save_graphics <file>
                               Saves graphics to the specified file (.png/.pdf/
                               .svg). If <file> contains '{i}', it will be
                               replaced with an integer which increments
                               each time graphics is invoked.
                          * set_macros <int>
                               Sets the placement macro drawing state
                          * set_nets <int>
                               Sets the net drawing state
                          * set_cpd <int>
                               Sets the critical path delay drawing state
                          * set_routing_util <int>
                               Sets the routing utilization drawing state
                          * set_clip_routing_util <int>
                               Sets whether routing utilization values are
                               clipped to [0., 1.]. Useful when a consistent
                               scale is needed across images
                          * set_draw_block_outlines <int>
                               Sets whether blocks have an outline drawn around
                               them
                          * set_draw_block_text <int>
                               Sets whether blocks have label text drawn on them
                          * set_draw_block_internals <int>
                               Sets the level to which block internals are drawn
                          * set_draw_net_max_fanout <int>
                               Sets the maximum fanout for nets to be drawn (if
                               fanout is beyond this value the net will not be
                               drawn)
                          * set_congestion <int>
                               Sets the routing congestion drawing state
                          * exit <int>
                               Exits VPR with specified exit code
                    
                       Example:
                         'save_graphics place.png; \
                          set_nets 1; save_graphics nets1.png;\
                          set_nets 2; save_graphics nets2.png; set_nets 0;\
                          set_cpd 1; save_graphics cpd1.png; \
                          set_cpd 3; save_graphics cpd3.png; set_cpd 0; \
                          set_routing_util 5; save_graphics routing_util5.png; \
                          set_routing_util 0; \
                          set_congestion 1; save_graphics congestion1.png;'
                    
                       The above toggles various graphics settings (e.g. drawing
                       nets, drawing critical path) and then saves the results 
                    to
                       .png files.
                    
                       Note that drawing state is reset to its previous state 
                    after
                       these commands are invoked.
                    
                       Like the interactive graphics --disp option, the --auto
                       option controls how often the commands specified with
                       this option are invoked.
                    

general options:
  -h, --help        Show this help message then exit
  --version         Show version information then exit
  --device DEVICE_NAME
                    Controls which device layout/floorplan is used from the 
                    architecture file. 'auto' uses the smallest device which 
                    satisfies the circuit's resource requirements. (Default: auto)
  -j NUM_WORKERS, --num_workers NUM_WORKERS
                    Controls how many parallel workers VPR may use:
                     *  1 implies VPR will execute serially,
                     * >1 implies VPR may execute in parallel with up to the
                          specified concurrency, and
                     *  0 implies VPR may execute in parallel with up to the
                          maximum concurrency supported by the host machine.
                    If this option is not specified it may be set from the 
                    'VPR_NUM_WORKERS' environment variable; otherwise the 
                    default is used. (Default: 1)
  --timing_analysis {on, off}
                    Controls whether timing analysis (and timing driven 
                    optimizations) are enabled. (Default: on)
  --timing_update_type {auto, full, incremental}
                    Controls how timing analysis updates are performed:
                     * auto: VPR decides
                     * full: Full timing updates are performed (may be faster 
                             if circuit timing has changed significantly)
                     * incr: Incremental timing updates are performed (may be 
                             faster in the face of smaller circuit timing 
                    changes)
                     (Default: auto)
  --echo_file {on, off}
                    Generate echo files of key internal data structures. Useful 
                    for debugging VPR, and typically end in .echo (Default: off)
  --verify_file_digests {on, off}
                    Verify that files loaded by VPR (e.g. architecture, netlist, 
                    previous packing/placement/routing) are consistent (Default: on)
  --target_utilization TARGET_UTILIZATION
                    Sets the target device utilization. This corresponds to the 
                    maximum target fraction of device grid-tiles to be used. A 
                    value of 1.0 means the smallest device (which fits the 
                    circuit) will be used. (Default: 1.0)
  --constant_net_method {global, route}
                    Specifies how constant nets (i.e. those driven to a constant
                    value) are handled:
                     * global: Treat constant nets as globals (not routed)
                     * route : Treat constant nets as normal nets (routed)
                     (Default: global)
  --clock_modeling {ideal, route, dedicated_network}
                    Specifies how clock nets are handled
                     * ideal: Treat clock pins as ideal
                              (i.e. no routing delays on clocks)
                     * route: Treat the clock pins as normal nets
                              (i.e. routed using inter-block routing)
                     * dedicated_network : Build a dedicated clock network based 
                    on the
                                           clock network specified in the 
                    architecture file
                     (Default: ideal)
  --two_stage_clock_routing
                    Routes clock nets in two stages if using a dedicated clock 
                    network.
                     * First stage: From the Net source to a dedicated clock 
                    network source
                     * Second stage: From the clock network source to net sinks
                     (Default: off)
  --exit_before_pack {on, off}
                    Causes VPR to exit before packing starts (useful for 
                    statistics collection) (Default: off)
  --strict_checks {on, off}
                    Controls whether VPR enforces some consistency checks 
                    strictly (as errors) or treats them as warnings. Usually 
                    these checks indicate an issue with either the targeted 
                    architecture, or consistency issues with VPR's internal data 
                    structures/algorithms (possibly harming optimization 
                    quality). In specific circumstances on specific 
                    architectures these checks may be too restrictive and can be 
                    turned off. However exercise extreme caution when turning 
                    this option off -- be sure you completely understand why the 
                    issue is being flagged, and why it is OK to treat as a 
                    warning instead of an error. (Default: on)
  --disable_errors DISABLE_ERRORS
                    Parses a list of functions for which the errors are going to 
                    be treated as warnings.
                    Each function in the list is delimited by `:`
                    This option should be only used for development purposes.
  --suppress_warnings SUPPRESS_WARNINGS
                    Parses a list of functions for which the warnings will be 
                    suppressed on stdout.
                    The first element of the list is the name of the output log 
                    file with the suppressed warnings.
                    The output log file can be omitted to completely suppress 
                    warnings.
                    The file name and the list of functions is separated by `,`. 
                    If no output log file is specified,
                    the comma is not needed.
                    Each function in the list is delimited by `:`
                    This option should be only used for development purposes.
  --allow_dangling_combinational_nodes {on, off}
                    Option to allow dangling combinational nodes in the timing 
                    graph.
                    This option should normally be off, as dangling 
                    combinational nodes are unusual
                    in the timing graph and may indicate a problem in the 
                    circuit or architecture.
                    Unless you understand why your architecture/circuit can have 
                    valid dangling combinational nodes, this option should be 
                    off.
                    In general this is a dev-only option and should not be 
                    turned on by the end-user. (Default: off)
  --terminate_if_timing_fails {on, off}
                    During final timing analysis after routing, if a negative 
                    slack anywhere is returned and this option is set, 
                    VPR_FATAL_ERROR is called and processing ends. (Default: off)

file options:
  --arch_format {vtr, fpga-interchange}
                    File format for the input atom-level circuit/netlist.
                     * vtr: Architecture expressed in the explicit VTR format * 
                    fpga-interchage: Architecture expressed in the FPGA 
                    Interchange schema format
                     (Default: vtr)
  --circuit_file CIRCUIT_FILE
                    Path to technology mapped circuit
  --circuit_format {auto, blif, eblif, fpga-interchange}
                    File format for the input atom-level circuit/netlist.
                     * auto: infer from file extension
                     * blif: Strict structural BLIF format
                     * eblif: Structural BLIF format with the extensions:
                               .conn  - Connection between two wires
                               .cname - Custom name for atom primitive
                               .param - Parameter on atom primitive
                               .attr  - Attribute on atom primitive
                     * fpga-interchage: Logical netlist in FPGA Interchange 
                    schema format
                     (Default: auto)
  --net_file NET_FILE
                    Path to packed netlist file
  --flat_place_file FLAT_PLACE_FILE
                    Path to input flat placement file
  --place_file PLACE_FILE
                    Path to placement file
  --route_file ROUTE_FILE
                    Path to routing file
  --sdc_file SDC_FILE
                    Path to timing constraints file in SDC format
  --read_rr_graph RR_GRAPH_FILE
                    The routing resource graph file to load. The loaded routing 
                    resource graph overrides any routing architecture specified 
                    in the architecture file.
  --write_rr_graph RR_GRAPH_FILE
                    Writes the routing resource graph to the specified file
  --write_initial_place_file INITIAL_PLACE_FILE
                    Writes out the the placement chosen by the initial placement 
                    algorithm to the specified file
  --read_initial_place_file INITIAL_PLACE_FILE
                    Reads the initial placement and continues the rest of the 
                    placement process from there.
  --read_vpr_constraints READ_VPR_CONSTRAINTS
                    Reads the floorplanning constraints that packing and 
                    placement must respect from the specified XML file.
  --write_vpr_constraints WRITE_VPR_CONSTRAINTS
                    Writes out new floorplanning constraints based on current 
                    placement to the specified XML file.
  --write_fix_clusters WRITE_FIX_CLUSTERS
                    Output file containing fixed locations of legalized input 
                    clusters - does not include clusters without placement 
                    coordinates; this file is used during post-legalization 
                    placement in order to hold input placement coordinates fixed 
                    while VPR places legalizer-generated orphan clusters. (Default: fix_clusters.out)
  --write_flat_place WRITE_FLAT_PLACE
                    VPR's (or reconstructed external) placement solution in flat 
                    placement file format; this file lists cluster and 
                    intra-cluster placement coordinates for each atom and can be 
                    used to reconstruct a clustering and placement solution.
  --read_router_lookahead READ_ROUTER_LOOKAHEAD
                    Reads the lookahead data from the specified file instead of 
                    computing it.
  --read_intra_cluster_router_lookahead READ_INTRA_CLUSTER_ROUTER_LOOKAHEAD
                    Reads the intra-cluster lookahead data from the specified 
                    file.
  --write_router_lookahead WRITE_ROUTER_LOOKAHEAD
                    Writes the lookahead data to the specified file.
  --write_intra_cluster_router_lookahead WRITE_INTRA_CLUSTER_ROUTER_LOOKAHEAD
                    Writes the intra-cluster lookahead data to the specified 
                    file.
  --read_placement_delay_lookup READ_PLACEMENT_DELAY_LOOKUP
                    Reads the placement delay lookup from the specified file 
                    instead of computing it.
  --write_placement_delay_lookup WRITE_PLACEMENT_DELAY_LOOKUP
                    Writes the placement delay lookup to the specified file.
  --outfile_prefix OUTFILE_PREFIX
                    Prefix for output files
  --write_block_usage WRITE_BLOCK_USAGE
                    Writes the cluster-level block types usage summary to the 
                    specified JSON, XML or TXT file.

netlist options:
  --absorb_buffer_luts {on, off}
                    Controls whether LUTS programmed as buffers are absorbed by 
                    downstream logic (Default: on)
  --const_gen_inference {none, comb, comb_seq}
                    Controls how constant generators are detected
                     * none    : No constant generator inference is performed
                     * comb    : Only combinational primitives are considered
                                 for constant generator inference (always safe)
                     * comb_seq: Both combinational and sequential primitives
                                 are considered for constant generator inference
                                 (usually safe)
                     (Default: comb_seq)
  --sweep_dangling_primary_ios {on, off}
                    Controls whether dangling primary inputs and outputs are 
                    removed from the netlist (Default: on)
  --sweep_dangling_nets {on, off}
                    Controls whether dangling nets are removed from the netlist (Default: on)
  --sweep_dangling_blocks {on, off}
                    Controls whether dangling blocks are removed from the netlist (Default: on)
  --sweep_constant_primary_outputs {on, off}
                    Controls whether primary outputs driven by constant values 
                    are removed from the netlist (Default: off)
  --netlist_verbosity NETLIST_VERBOSITY
                    Controls how much detail netlist processing produces about 
                    detected netlist characteristics (e.g. constant generator 
                    detection) and applied netlist modifications (e.g. swept 
                    netlist components). Larger values produce more detail. (Default: 1)

packing options:
  --connection_driven_clustering {on, off}
                    Controls whether or not packing prioritizes the absorption 
                    of nets with fewer connections into a complex logic block 
                    over nets with more connections (Default: on)
  --allow_unrelated_clustering {on, off, auto}
                    Controls whether primitives with no attraction to a cluster 
                    can be packed into it.
                    Turning unrelated clustering on can increase packing density 
                    (fewer blocks are used), but at the cost of worse 
                    routability.
                     * on  : Unrelated clustering enabled
                     * off : Unrelated clustering disabled
                     * auto: Dynamically enabled/disabled (based on density)
                     (Default: auto)
  --alpha_clustering ALPHA_CLUSTERING
                    Parameter that weights the optimization of timing vs area. 
                    0.0 focuses solely on area, 1.0 solely on timing. (Default: 0.75)
  --beta_clustering BETA_CLUSTERING
                    Parameter that weights the absorption of small nets vs 
                    signal sharing. 0.0 focuses solely on sharing, 1.0 solely on 
                    small net absoprtion. Only meaningful if 
                    --connection_driven_clustering=on (Default: 0.9)
  --timing_driven_clustering {on, off}
                    Controls whether custering optimizes for timing (Default: on)
  --cluster_seed_type {timing, max_inputs, blend, max_pins, max_input_pins, blend2}
                    Controls how primitives are chosen as seeds. (Default: 
                    blend2 if timing driven, max_inputs otherwise)
  --clustering_pin_feasibility_filter {on, off}
                    Controls whether the pin counting feasibility filter is used 
                    during clustering. When enabled the clustering engine counts 
                    the number of available pins in groups/classes of mutually 
                    connected pins within a cluster. These counts are used to 
                    quickly filter out candidate primitives/atoms/molecules for 
                    which the cluster has insufficient pins to route (without 
                    performing a full routing). This reduces packer run-time (Default: on)
  --balance_block_type_utilization {on, off, auto}
                    If enabled, when a primitive can potentially be mapped to 
                    multiple block types the packer will
                    pick the block type which (currently) has the lowest 
                    utilization.
                     * on  : Try to balance block type utilization
                     * off : Do not try to balance block type utilization
                     * auto: Dynamically enabled/disabled (based on density)
                     (Default: auto)
  --target_ext_pin_util TARGET_EXT_PIN_UTIL [TARGET_EXT_PIN_UTIL ...]
                    Sets the external pin utilization target during clustering.
                    Value Ranges: [1.0, 0.0]
                    * 1.0 : The packer to pack as densely as possible (i.e. try
                            to use 100% of cluster external pins)
                    * 0.0 : The packer to pack as loosely as possible (i.e. each
                            block will contain a single mollecule).
                            Values in between trade-off pin usage and
                            packing density.
                    
                    Typically packing less densely improves routability, at
                    the cost of using more clusters. Note that these settings 
                    are
                    only guidelines, the packer will use up to 1.0 utilization 
                    if
                    a molecule would not otherwise pack into any cluster type.
                    
                    This option can take multiple specifications in several
                    formats:
                    * auto (i.e. 'auto'): VPR will determine the target pin
                                          utilizations automatically
                    * Single Value (e.g. '0.7'): the input pin utilization for
                                                 all block types (output pin
                                                 utilization defaults to 1.0)
                    * Double Value (e.g. '0.7,0.8'): the input and output pin
                                                 utilization for all block types
                    * Block Value (e.g. 'clb:0.7', 'clb:0.7,0.8'): the pin
                                                 utilization for a specific
                                                 block type
                    These can be used in combination. For example:
                       '--target_ext_pin_util 0.9 clb:0.7'
                    would set the input pin utilization of clb blocks to 0.7,
                    and all other blocks to 0.9.
                     (Default: auto)
  --pack_prioritize_transitive_connectivity {on, off}
                    Whether transitive connectivity is prioritized over 
                    high-fanout connectivity during packing (Default: on)
  --pack_high_fanout_threshold PACK_HIGH_FANOUT_THRESHOLD [PACK_HIGH_FANOUT_THRESHOLD ...]
                    Sets the high fanout threshold during clustering.
                    
                    Typically reducing the threshold reduces packing density
                    and improves routability.
                    This option can take multiple specifications in several
                    formats:
                    * auto (i.e. 'auto'): VPR will determine the target pin
                                          utilizations automatically
                    * Single Value (e.g. '256'): the high fanout threshold
                                                 for all block types
                    * Block Value (e.g. 'clb:16'): the high fanout threshold
                                                   for a specific block type
                    These can be used in combination. For example:
                       '--pack_high_fanout_threshold 256 clb:16'
                    would set the high fanout threshold for clb blocks to 16
                    and all other blocks to 256
                     (Default: auto)
  --pack_transitive_fanout_threshold PACK_TRANSITIVE_FANOUT_THRESHOLD
                    Packer transitive fanout threshold (Default: 4)
  --pack_feasible_block_array_size PACK_FEASIBLE_BLOCK_ARRAY_SIZE
                    This value is used to determine the max size of the
                    priority queue for candidates that pass the early filter
                    legality test but not the more detailed routing test
                     (Default: 30)
  --pack_verbosity PACK_VERBOSITY
                    Controls how verbose clustering's output is. Higher values 
                    produce more output (useful for debugging architecture 
                    packing problems) (Default: 2)
  --use_attraction_groups {on, off}
                    Whether attraction groups are used to make it easier to pack 
                    primitives in the same floorplan region together. (Default: on)
  --pack_num_moves PACK_NUM_MOVES
                    The number of moves that can be tried in packing stage (Default: 100000)
  --pack_move_type PACK_MOVE_TYPE
                    The move type used in packing.The available values are: 
                    randomSwap, semiDirectedSwap, semiDirectedSameTypeSwap (Default: semiDirectedSwap)

placement options:
  --seed SEED       Placement random number generator seed (Default: 1)
  --place_delta_delay_matrix_calculation_method {astar, dijkstra}
                    What algorithm should be used to compute the place delta 
                    matrix.
                    
                     * astar : Find delta delays between OPIN's and IPIN's using
                               the router with the current 
                    --router_profiler_astar_fac.
                     * dijkstra : Use Dijkstra's algorithm to find all shortest 
                    paths 
                                  from sampled OPIN's to all IPIN's.
                     (Default: astar)
  --inner_num INNER_NUM
                    Controls number of moves per temperature: inner_num * 
                    num_blocks ^ (4/3) (Default: 0.5)
  --place_effort_scaling {circuit, device_circuit}
                    Controls how the number of placer moves level scales with 
                    circuit
                     and device size:
                      * circuit: proportional to circuit size (num_blocks ^ 4/3)
                      * device_circuit: proportional to device and circuit size
                                        (grid_size ^ 2/3 * num_blocks ^ 2/3)
                     (Default: circuit)
  --init_t INIT_T   Initial temperature for manual annealing schedule (Default: 100.0)
  --exit_t EXIT_T   Temperature at which annealing which terminate for manual 
                    annealing schedule (Default: 0.01)
  --alpha_t ALPHA_T
                    Temperature scaling factor for manual annealing schedule. 
                    Old temperature is multiplied by alpha_t (Default: 0.8)
  --fix_pins {free, random}
                    Fixes I/O pad locations randomly during placement. Valid 
                    options:
                     * 'free' allows placement to optimize pad locations
                     * 'random' fixes pad locations to arbitrary locations
                    . (Default: free)
  --fix_clusters FIX_CLUSTERS
                    Fixes block locations during placement. Valid options:
                     * path to a file specifying block locations (.place format 
                    with block locations specified).
  --place_algorithm {bounding_box, criticality_timing, slack_timing}
                    Controls which placement algorithm is used. Valid options:
                     * bounding_box: Focuses purely on minimizing the bounding 
                    box wirelength of the circuit. Turns off timing analysis if 
                    specified.
                     * criticality_timing: Focuses on minimizing both the 
                    wirelength and the connection timing costs (criticality * 
                    delay).
                     * slack_timing: Focuses on improving the circuit slack 
                    values to reduce critical path delay.
                     (Default: criticality_timing)
  --place_quench_algorithm {bounding_box, criticality_timing, slack_timing}
                    Controls which placement algorithm is used during placement 
                    quench.
                    If specified, it overrides the option --place_algorithm 
                    during placement quench.
                    Valid options:
                     * bounding_box: Focuses purely on minimizing the bounding 
                    box wirelength of the circuit. Turns off timing analysis if 
                    specified.
                     * criticality_timing: Focuses on minimizing both the 
                    wirelength and the connection timing costs (criticality * 
                    delay).
                     * slack_timing: Focuses on improving the circuit slack 
                    values to reduce critical path delay.
                     (Default: criticality_timing)
  --place_chan_width PLACE_CHAN_WIDTH
                    Sets the assumed channel width during placement. If 
                    --place_chan_width is unspecified, but --route_chan_width is 
                    specified the --route_chan_width value will be used 
                    (otherwise the default value is used). (Default: 100)
  --place_rlim_escape PLACE_RLIM_ESCAPE
                    The fraction of moves which are allowed to ignore the region 
                    limit. For example, a value of 0.1 means 10%% of moves are 
                    allowed to ignore the region limit. (Default: 0.0)
  --place_move_stats PLACE_MOVE_STATS
                    File to write detailed placer move statistics to
  --save_placement_per_temperature SAVE_PLACEMENT_PER_TEMPERATURE
                    Controls how often VPR saves the current placement to a file 
                    per temperature (may be helpful for debugging). The value 
                    specifies how many times the placement should be saved 
                    (values less than 1 disable this feature). (Default: 0)
  --enable_analytic_placer {true, false}
                    Enables the analytic placer. Once analytic placement is 
                    done, the result is passed through the quench phase of the 
                    annealing placer for local improvement (Default: false)
  --place_static_move_prob PLACE_STATIC_MOVE_PROB [PLACE_STATIC_MOVE_PROB ...]
                    The percentage probabilities of different moves in Simulated 
                    Annealing placement. For non-timing-driven placement, only 
                    the first 3 probabilities should be provided. For 
                    timing-driven placement, all probabilities should be 
                    provided. When the number of provided probabilities is less 
                    then the number of move types, zero probability is 
                    assumed.The numbers listed are interpreted as the percentage 
                    probabilities of {UniformMove, MedianMove, CentroidMove, 
                    WeightedCentroid, WeightedMedian, Critical UniformMove, 
                    Timing feasible Region(TFR)}, in that order. (Default: 100)
  --place_high_fanout_net PLACE_HIGH_FANOUT_NET
                    Sets the assumed high fanout net during placement. Any net 
                    with higher fanout would be ignored while calculating some 
                    of the directed moves: Median and WeightedMedian (Default: 10)
  --place_bounding_box_mode {auto_bb, cube_bb, per_layer_bb}
                    Specifies the type of bounding box to be used in 3D 
                    architectures.
                    
                    MODE options:
                      auto_bb     : Automatically determine the appropriate 
                    bounding box based on the connections between layers.
                      cube_bb            : Use 3D bounding boxes.
                      per_layer_bb     : Use per-layer bounding boxes.
                    
                    Choose one of the available modes to define the behavior of 
                    bounding boxes in your 3D architecture. The default mode is 
                    'automatic'. (Default: auto_bb)
  --RL_agent_placement {on, off}
                    Uses a Reinforcement Learning (RL) agent in choosing the 
                    appropiate move type in placement.It activates the RL agent 
                    placement instead of using fixed probability for each move 
                    type. (Default: on)
  --place_agent_multistate {on, off}
                    Enable multistate agent. A second state will be activated 
                    late in the annealing and in the Quench that includes all 
                    the timing driven directed moves. (Default: on)
  --place_checkpointing {on, off}
                    Enable Placement checkpoints. This means saving the 
                    placement and restore it if it's better than later 
                    placements.Only effective if agnet's 2nd state is activated. (Default: on)
  --place_agent_epsilon PLACE_AGENT_EPSILON
                    Placement RL agent's epsilon for epsilon-greedy 
                    agent.Epsilon represents the percentage of exploration 
                    actions taken vs the exploitation ones. (Default: 0.3)
  --place_agent_gamma PLACE_AGENT_GAMMA
                    Controls how quickly the agent's memory decays. Values 
                    between [0., 1.] specify the fraction of weight in the 
                    exponentially weighted reward average applied to moves which 
                    occured greater than moves_per_temp moves ago.Values < 0 
                    cause the unweighted reward sample average to be used (all 
                    samples are weighted equally) (Default: 0.05)
  --place_dm_rlim PLACE_DM_RLIM
                    The maximum range limit of any directed move other than the 
                    uniform move. It also shrinks with the default rlim (Default: 3.0)
  --place_reward_fun PLACE_REWARD_FUN
                    The reward function used by placement RL agent.The available 
                    values are: basic, nonPenalizing_basic, runtime_aware, 
                    WLbiased_runtime_awareThe latter two are only available for 
                    timing-driven placement. (Default: WLbiased_runtime_aware)
  --place_crit_limit PLACE_CRIT_LIMIT
                    The criticality limit to count a block as a critical one (or 
                    have a critical connection). It used in some directed moves 
                    that only move critical blocks like critical uniform and 
                    feasible region. Its range equals to [0., 1.]. (Default: 0.7)
  --place_constraint_expand PLACE_CONSTRAINT_EXPAND
                    The value used to decide how much to expand the floorplan 
                    constraint region when writing a floorplan constraint XML 
                    file. Takes in an integer value from zero to infinity. If 
                    the value is zero, the block stays at the same x, y 
                    location. If it is greater than zero the constraint region 
                    expands by the specified value in each direction. For 
                    example, if 1 was specified, a block at the x, y location 
                    (1, 1) would have a constraint region of 2x2 centered around 
                    (1, 1), from (0, 0) to (2, 2). (Default: 0)
  --place_constraint_subtile {on, off}
                    The bool used to say whether to print subtile constraints 
                    when printing a floorplan constraints XML file. If it is 
                    off, no subtile locations are specified when printing the 
                    floorplan constraints. If it is on, the floorplan 
                    constraints are printed with the subtiles from current 
                    placement.  (Default: off)
  --floorplan_num_horizontal_partitions FLOORPLAN_NUM_HORIZONTAL_PARTITIONS
                    An argument used for generating test constraints files. 
                    Specifies how many partitions to make in the horizontal 
                    dimension. Must be used in conjunction with 
                    --floorplan_num_vertical_partitions (Default: 0)
  --floorplan_num_vertical_partitions FLOORPLAN_NUM_VERTICAL_PARTITIONS
                    An argument used for generating test constraints files. 
                    Specifies how many partitions to make in the vertical 
                    dimension. Must be used in conjunction with 
                    --floorplan_num_horizontal_partitions (Default: 0)
  --place_agent_algorithm {e_greedy, softmax}
                    Controls which placement RL agent is used (Default: softmax)
  --place_agent_space {move_type, move_block_type}
                    Agent exploration space can be either based on only move 
                    types or also consider different block types
                    The available values are: move_type, move_block_type (Default: move_block_type)
  --placer_debug_block PLACER_DEBUG_BLOCK
                     Controls when placer debugging is enabled for blocks.
                     * For values >= 0, the value is taken as the block ID for
                       which to enable placer debug output.
                     * For value == -1, placer debug output is enabled for
                       all blocks.
                     * For values < -1, all block-based placer debug output is 
                    disabled.
                    Note if VPR as compiled without debug logging enabled this 
                    will produce only limited output.
                     (Default: -2)
  --placer_debug_net PLACER_DEBUG_NET
                    Controls when placer debugging is enabled for nets.
                     * For values >= 0, the value is taken as the net ID for
                       which to enable placer debug output.
                     * For value == -1, placer debug output is enabled for
                       all nets.
                     * For values < -1, all net-based placer debug output is 
                    disabled.
                    Note if VPR as compiled without debug logging enabled this 
                    will produce only limited output.
                     (Default: -2)

timing-driven placement options:
  --timing_tradeoff TIMING_TRADEOFF
                    Trade-off control between delay and wirelength during 
                    placement. 0.0 focuses completely on wirelength, 1.0 
                    completely on timing (Default: 0.5)
  --recompute_crit_iter RECOMPUTE_CRIT_ITER
                    Controls how many temperature updates occur between timing 
                    analysis during placement (Default: 1)
  --inner_loop_recompute_divider INNER_LOOP_RECOMPUTE_DIVIDER
                    Controls how many timing analysies are perform per 
                    temperature during placement (Default: 0)
  --quench_recompute_divider QUENCH_RECOMPUTE_DIVIDER
                    Controls how many timing analysies are perform during the 
                    final placement quench (t=0). If unspecified, uses the value 
                    from --inner_loop_recompute_divider (Default: 0)
  --td_place_exp_first TD_PLACE_EXP_FIRST
                    Controls how critical a connection is as a function of slack 
                    at the start of placement. A value of zero treats all 
                    connections as equally critical (regardless of slack). 
                    Values larger than 1.0 cause low slack connections to be 
                    treated more critically. The value increases to 
                    --td_place_exp_last during placement. (Default: 1.0)
  --td_place_exp_last TD_PLACE_EXP_LAST
                    Controls how critical a connection is as a function of slack 
                    at the end of placement. (Default: 8.0)
  --place_delay_model {simple, delta, delta_override}
                    This option controls what information is considered and how 
                    the placement delay model is constructed.
                    Valid options:
                     * 'simple' uses map router lookahead
                     * 'delta' uses differences in position only
                     * 'delta_override' uses differences in position with 
                    overrides for direct connects
                     (Default: simple)
  --place_delay_model_reducer {min, max, median, arithmean, geomean}
                    When calculating delta delays for the placement delay model 
                    how are multiple values combined? (Default: min)
  --place_delay_offset PLACE_DELAY_OFFSET
                    A constant offset (in seconds) applied to the placer's delay 
                    model. (Default: 0.0)
  --place_delay_ramp_delta_threshold PLACE_DELAY_RAMP_DELTA_THRESHOLD
                    The delta distance beyond which --place_delay_ramp is 
                    applied. Negative values disable the placer delay ramp. (Default: -1)
  --place_delay_ramp_slope PLACE_DELAY_RAMP_SLOPE
                    The slope of the ramp (in seconds per grid tile) which is 
                    applied to the placer delay model for delta distance beyond 
                    --place_delay_ramp_delta_threshold (Default: 0.0e-9)
  --place_tsu_rel_margin PLACE_TSU_REL_MARGIN
                    Specifies the scaling factor for cell setup times used by 
                    the placer. This effectively controls whether the placer 
                    should try to achieve extra margin on setup paths. For 
                    example a value of 1.1 corresponds to requesting 10%% setup 
                    margin. (Default: 1.0)
  --place_tsu_abs_margin PLACE_TSU_ABS_MARGIN
                    Specifies an absolute offest added to cell setup times used 
                    by the placer. This effectively controls whether the placer 
                    should try to achieve extra margin on setup paths. For 
                    example a value of 500e-12 corresponds to requesting an 
                    extra 500ps of setup margin. (Default: 0.0)
  --post_place_timing_report POST_PLACE_TIMING_REPORT
                    Name of the post-placement timing report file (not generated 
                    if unspecfied)
  --allowed_tiles_for_delay_model ALLOWED_TILES_FOR_DELAY_MODEL
                    Names of allowed tile types that can be sampled during delay 
                    modelling.  Default is to allow all tiles. Can be used to 
                    exclude specialized tiles from placer delay sampling.

routing options:
  --max_router_iterations MAX_ROUTER_ITERATIONS
                    Maximum number of Pathfinder-based routing iterations before 
                    the circuit is declared unroutable at a given channel width (Default: 50)
  --first_iter_pres_fac FIRST_ITER_PRES_FAC
                    Sets the present overuse factor for the first routing 
                    iteration (Default: 0.0)
  --initial_pres_fac INITIAL_PRES_FAC
                    Sets the present overuse factor for the second routing 
                    iteration (Default: 0.5)
  --pres_fac_mult PRES_FAC_MULT
                    Sets the growth factor by which the present overuse penalty 
                    factor is multiplied after each routing iteration (Default: 1.3)
  -max_pres_fac MAX_PRES_FAC
                    Sets the maximum present overuse penalty factor (Default: 1000.0)
  --acc_fac ACC_FAC
                    Specifies the accumulated overuse factor (historical 
                    congestion cost factor) (Default: 1.0)
  --bb_factor BB_FACTOR
                    Sets the distance (in channels) outside a connection's 
                    bounding box which can be explored during routing (Default: 3)
  --base_cost_type {demand_only, demand_only_normalized_length, delay_normalized, delay_normalized_length, delay_normalized_length_bounded, delay_normalized_frequency, delay_normalized_length_frequency}
                    Sets the basic cost of routing resource nodes:
                     * demand_only: based on expected demand of node type
                     * demand_only_normalized_length: based on expected 
                          demand of node type normalized by length
                     * delay_normalized: like demand_only but normalized
                          to magnitude of typical routing resource delay
                     * delay_normalized_length: like delay_normalized but
                          scaled by routing resource length
                     * delay_normalized_length_bounded: like delay_normalized 
                    but
                          scaled by routing resource length.  Scaling is 
                    normalized
                          between 1 to 4, with min lengths getting scaled at 1,
                          and max lengths getting scaled at 4.
                     * delay_normalized_frequency: like delay_normalized
                          but scaled inversely by segment type frequency
                     * delay_normalized_length_frequency: like delay_normalized
                          but scaled by routing resource length, and inversely
                          by segment type frequency
                    (Default: delay_normalized_length)
  --bend_cost BEND_COST
                    The cost of a bend. (Default: 1.0 for global routing, 0.0 
                    for detailed routing)
  --route_type {global, detailed}
                    Specifies whether global, or combined global and detailed 
                    routing is performed. (Default: detailed)
  --route_chan_width CHANNEL_WIDTH
                    Specifies a fixed channel width to route at. A value of -1 
                    indicates that the minimum channel width should be determined (Default: -1)
  --min_route_chan_width_hint MIN_ROUTE_CHAN_WIDTH_HINT
                    Hint to the router what the minimum routable channel width 
                    is. Good hints can speed-up determining the minimum channel 
                    width.
  --verify_binary_search {on, off}
                    Force the router to verify the minimum channel width by 
                    routing at consecutively lower channel widths until two 
                    consecutive failures are observed. (Default: off)
  --router_algorithm {parallel, parallel_decomp, timing_driven}
                    Specifies the router algorithm to use.
                     * timing driven: focuses on routability and circuit speed 
                    [default]
                     * parallel: timing_driven with nets in different regions of 
                    the chip routed in parallel
                     * parallel_decomp: timing_driven with additional 
                    parallelism obtained by decomposing high-fanout nets, 
                    possibly reducing quality
                     (Default: timing_driven)
  --min_incremental_reroute_fanout MIN_INCREMENTAL_REROUTE_FANOUT
                    The net fanout threshold above which nets will be re-routed 
                    incrementally. (Default: 16)
  --exit_after_first_routing_iteration {on, off}
                    Causes VPR to exit after the first routing iteration (useful 
                    for saving graphics) (Default: off)
  --max_logged_overused_rr_nodes MAX_LOGGED_OVERUSED_RR_NODES
                    Maximum number of overused RR nodes logged each time the 
                    routing fails (Default: 20)
  --generate_rr_node_overuse_report {on, off}
                    Generate detailed reports on overused rr nodes and congested 
                    nets should the routing fails (Default: off)
  --reorder_rr_graph_nodes_algorithm {none, degree_bfs, random_shuffle}
                    Specifies the node reordering algorithm to use.
                     * none: don't reorder nodes
                     * degree_bfs: sort by degree and then by BFS
                     * random_shuffle: a random shuffle
                     (Default: none)
  --reorder_rr_graph_nodes_threshold REORDER_RR_GRAPH_NODES_THRESHOLD
                    Reorder rr_graph nodes to optimize memory layout above this 
                    number of nodes. (Default: 0)
  --reorder_rr_graph_nodes_seed REORDER_RR_GRAPH_NODES_SEED
                    Pseudo-random number generator seed used for the 
                    random_shuffle reordering algorithm (Default: 1)
  --flat_routing {on, off}
                    Enable VPR's flat routing (routing the nets from the source 
                    primitive to the destination primitive) (Default: off)
  --router_opt_choke_points {on, off}
                    Some FPGA architectures with limited fan-out options within 
                    a cluster (e.g. fracturable LUTs with shared pins) do not 
                    converge well in routing unless these fan-out choke points 
                    are discovered and optimized for during net routing. This 
                    option helps router convergence for such architectures. (Default: on)
  --route_verbosity ROUTE_VERBOSITY
                    Controls the verbosity of routing's output. Higher values 
                    produce more output (useful for debugging routing problems) (Default: 1)
  --custom_3d_sb_fanin_fanout CUSTOM_3D_SB_FANIN_FANOUT
                    Specifies the number of tracks that can drive a 3D switch 
                    block connectionand the number of tracks that can be driven 
                    by a 3D switch block connection (Default: 1)

timing-driven routing options:
  --astar_fac ASTAR_FAC
                    Controls the directedness of the timing-driven router's 
                    exploration. Values between 1 and 2 are resonable; higher 
                    values trade some quality for reduced run-time (Default: 1.2)
  --astar_offset ASTAR_OFFSET
                    Controls the directedness of the timing-driven router's 
                    exploration. It is a subtractive adjustment to the lookahead 
                    heuristic. Values between 0 and 1e-9 are resonable; higher 
                    values may increase quality at the expense of run-time. (Default: 0.0)
  --router_profiler_astar_fac ROUTER_PROFILER_ASTAR_FAC
                    Controls the directedness of the timing-driven router's 
                    exploration when doing router delay profiling of an 
                    architecture. The router delay profiling step is currently 
                    used to calculate the place delay matrix lookup. Values 
                    between 1 and 2 are resonable; higher values trade some 
                    quality for reduced run-time (Default: 1.2)
  --max_criticality MAX_CRITICALITY
                    Sets the maximum fraction of routing cost derived from delay 
                    (vs routability) for any net. 0.0 means no attention is paid 
                    to delay, 1.0 means nets on the critical path ignore 
                    congestion (Default: 0.99)
  --criticality_exp CRITICALITY_EXP
                    Controls the delay-routability trade-off for nets as a 
                    function of slack. 0.0 implies all nets treated equally 
                    regardless of slack. At large values (>> 1) only nets on the 
                    critical path will consider delay. (Default: 1.0)
  --router_init_wirelength_abort_threshold ROUTER_INIT_WIRELENGTH_ABORT_THRESHOLD
                    The first routing iteration wirelength abort threshold. If 
                    the first routing iteration uses more than this fraction of 
                    available wirelength routing is aborted. (Default: 0.85)
  --incremental_reroute_delay_ripup {on, off, auto}
                    Controls whether incremental net routing will rip-up (and 
                    re-route) a critical connection for delay, even if the 
                    routing is legal. (Default: auto)
  --routing_failure_predictor {safe, aggressive, off}
                    Controls how aggressively the router will predict a routing 
                    as unsuccessful and give up early. This can significantly 
                    reducing the run-time required to find the minimum channel 
                    width.
                     * safe: Only abort when it is extremely unlikely a routing 
                    will succeed
                     * aggressive: Further reduce run-time by giving up earlier. 
                    This may increase the reported minimum channel width
                     * off: Only abort when the maximum number of iterations is 
                    reached
                     (Default: safe)
  --routing_budgets_algorithm {minimax, scale_delay, yoyo, disable}
                    Controls how the routing budgets are created and applied.
                     * yoyo: Allocates budgets using minimax algorithm, and 
                    enables hold slack resolution in the router using the RCV 
                    algorithm. [EXPERIMENTAL]
                     * minimax: Sets the budgets depending on the amount slack 
                    between connections and the current delay values. 
                    [EXPERIMENTAL]
                     * scale_delay: Sets the minimum budgets to 0 and the 
                    maximum budgets as a function of delay and criticality (net 
                    delay/ pin criticality) [EXPERIMENTAL]
                     * disable: Removes the routing budgets, use the default VPR 
                    and ignore hold time constraints
                     (Default: disable)
  --save_routing_per_iteration {on, off}
                    Controls whether VPR saves the current routing to a file 
                    after each routing iteration. May be helpful for debugging. (Default: off)
  --congested_routing_iteration_threshold CONGESTED_ROUTING_ITERATION_THRESHOLD
                    Controls when the router enters a high effort mode to 
                    resolve lingering routing congestion. Value is the fraction 
                    of max_router_iterations beyond which the routing is deemed 
                    congested. (Default: 1.0)
  --route_bb_update {static, dynamic}
                    Controls how the router's net bounding boxes are updated:
                     * static : bounding boxes are never updated
                     * dynamic: bounding boxes are updated dynamically as 
                    routing progresses
                     (Default: dynamic)
  --router_high_fanout_threshold ROUTER_HIGH_FANOUT_THRESHOLD
                    Specifies the net fanout beyond which a net is considered 
                    high fanout. Values less than zero disable special behaviour 
                    for high fanout nets (Default: 64)
  --router_high_fanout_max_slope ROUTER_HIGH_FANOUT_MAX_SLOPE
                    Minimum routing progress where high fanout routing is 
                    enabled. This is a ratio of the actual congestion reduction 
                    to what is expected based in the history.
                     1.0 is normal progress, 0 is no progress. (Default: 0.1)
  --router_lookahead {classic, map, compressed_map, extended_map}
                    Controls what lookahead the router uses to calculate cost of 
                    completing a connection.
                     * classic: The classic VPR lookahead (may perform better on 
                    un-buffered routing
                                architectures)
                     * map: An advanced lookahead which accounts for diverse 
                    wire type
                     * compressed_map: The algorithm is similar to map lookahead 
                    with the exception of sparse sampling of the chip to reduce 
                    the run-time to build the router lookahead and also its 
                    memory footprint
                     * extended_map: A more advanced and extended lookahead 
                    which accounts for a more
                                     exhaustive node sampling method
                    
                     The extended map differs from the map lookahead in the 
                    lookahead computation.
                     It is better suited for architectures that have specialized 
                    routing for specific
                     kinds of connections, but note that the time and memory 
                    necessary to compute the
                     extended lookahead map are greater than the basic lookahead 
                    map.
                     (Default: map)
  --router_max_convergence_count ROUTER_MAX_CONVERGENCE_COUNT
                    Controls how many times the router is allowed to converge to 
                    a legal routing before halting. If multiple legal solutions 
                    are found the best quality implementation is used. (Default: 1)
  --router_reconvergence_cpd_threshold ROUTER_RECONVERGENCE_CPD_THRESHOLD
                    Specifies the minimum potential CPD improvement for which 
                    the router will continue to attempt re-convergent routing. 
                    For example, a value of 0.99 means the router will not give 
                    up on reconvergent routing if it thinks a > 1% CPD reduction 
                    is possible. (Default: 0.99)
  --router_initial_timing {all_critical, lookahead}
                    Controls how criticality is determined at the start of the 
                    first routing iteration.
                     * all_critical: All connections are considered timing
                                     critical.
                     * lookahead   : Connection criticalities are determined
                                     from timing analysis assuming best-case
                                     connection delays as estimated by the
                                     router's lookahead.
                    (Default: 'lookahead' if a non-classic router lookahead is
                               used, otherwise 'all_critical')
                     (Default: lookahead)
  --router_update_lower_bound_delays {on, off}
                    Controls whether the router updates lower bound connection 
                    delays after the 1st routing iteration. (Default: on)
  --router_heap {binary, four_ary, bucket}
                    Controls what type of heap to use for timing driven router.
                     * binary: A binary heap is used.
                     * four_ary: A four_ary heap is used.
                     * bucket: A bucket heap approximation is used. The bucket 
                    heap
                     *         is faster because it is only a heap 
                    approximation.
                     *         Testing has shown the approximation results in
                     *         similar QoR with less CPU work.
                     (Default: four_ary)
  --router_first_iter_timing_report ROUTER_FIRST_ITER_TIMING_REPORT
                    Name of the post first routing iteration timing report file 
                    (not generated if unspecified)
  --read_rr_edge_metadata {on, off}
                    Read RR edge metadata from --read_rr_graph.  RR edge 
                    metadata is not used in core VPR algorithms, and is 
                    typically not read to save runtime and memory. (Default: 
                    off). (Default: off)
  --check_route {off, quick, full}
                    Options to run check route in three different modes.
                     * off    : check route is completely disabled.
                     * quick  : runs check route with slow checks disabled.
                     * full   : runs the full check route step.
                     (Default: full)
  --router_debug_net ROUTER_DEBUG_NET
                    Controls when router debugging is enabled for nets.
                     * For values >= 0, the value is taken as the net ID for
                       which to enable router debug output.
                     * For value == -1, router debug output is enabled for
                       all nets.
                     * For values < -1, all net-based router debug output is 
                    disabled.
                    Note if VPR as compiled without debug logging enabled this 
                    will produce only limited output.
                     (Default: -2)
  --router_debug_sink_rr ROUTER_DEBUG_SINK_RR
                    Controls when router debugging is enabled for the specified 
                    sink RR.
                     * For values >= 0, the value is taken as the sink RR Node 
                    ID for
                       which to enable router debug output.
                     * For values < 0, sink-based router debug output is 
                    disabled.
                    Note if VPR as compiled without debug logging enabled this 
                    will produce only limited output.
                     (Default: -2)
  --router_debug_iteration ROUTER_DEBUG_ITERATION
                    Controls when router debugging is enabled for the specific 
                    router iteration.
                     * For values >= 0, the value is taken as the iteration 
                    number for
                       which to enable router debug output.
                     * For values < 0, all iteration-based router debug output 
                    is disabled.
                    Note if VPR as compiled without debug logging enabled this 
                    will produce only limited output.
                     (Default: -2)
  --check_rr_graph {on, off}
                    Controls whether to check the rr graph when reading from 
                    disk. (Default: on)

analysis options:
  --full_stats {on, off}
                    Print extra statistics about the circuit and it's routing 
                    (useful for wireability analysis) (Default: off)
  --gen_post_synthesis_netlist {on, off}
                    Generates the post-synthesis netlist (in BLIF and Verilog) 
                    along with delay information (in SDF). Used for 
                    post-implementation simulation and verification (Default: off)
  --gen_post_implementation_merged_netlist {on, off}
                    Generates the post-implementation netlist with merged top 
                    module ports Used for post-implementation simulation and 
                    verification (Default: off)
  --timing_report_npaths TIMING_REPORT_NPATHS
                    Controls how many timing paths are reported. (Default: 100)
  --timing_report_detail {netlist, aggregated, detailed, debug}
                    Controls how much detail is provided in timing reports.
                     * netlist: Shows only netlist pins
                     * aggregated: Like 'netlist', but also shows aggregated 
                    intra-block/inter-block delays
                     * detailed: Like 'aggregated' but shows detailed routing 
                    instead of aggregated inter-block delays
                     * debug: Like 'detailed' but shows additional tool internal 
                    debug information
                     (Default: netlist)
  --timing_report_skew {on, off}
                    Controls whether skew timing reports are generated
                     (Default: off)
  --echo_dot_timing_graph_node ECHO_DOT_TIMING_GRAPH_NODE
                    Controls how the timing graph echo file in DOT/GraphViz 
                    format is created when
                    '--echo_file on' is set:
                     * -1: All nodes are dumped into the DOT file
                     * >= 0: Only the transitive fanin/fanout of the node is 
                    dumped (easier to view)
                     * a string: Interpretted as a VPR pin name which is 
                    converted to a node id, and dumped as above
                     (Default: -1)
  --post_synth_netlist_unconn_inputs {unconnected, nets, gnd, vcc}
                    Controls how unconnected input cell ports are handled in the 
                    post-synthesis netlist
                     * unconnected: leave unconnected
                     * nets: connect each unconnected input pin to its own 
                    separate
                             undriven net named: __vpr__unconn<ID>, where <ID> 
                    is index
                             assigned to this occurrence of unconnected port in 
                    design
                     * gnd: tie all to ground (1'b0)
                     * vcc: tie all to VCC (1'b1)
                     (Default: unconnected)
  --post_synth_netlist_unconn_outputs {unconnected, nets}
                    Controls how unconnected output cell ports are handled in 
                    the post-synthesis netlist
                     * unconnected: leave unconnected
                     * nets: connect each unconnected input pin to its own 
                    separate
                             undriven net named: __vpr__unconn<ID>, where <ID> 
                    is index
                             assigned to this occurrence of unconnected port in 
                    design
                     (Default: unconnected)
  --write_timing_summary WRITE_TIMING_SUMMARY
                    Writes implemented design final timing summary to the 
                    specified JSON, XML or TXT file.

power analysis options:
  --power           Enable power estimation (Default: off)
  --tech_properties TECH_PROPERTIES
                    XML file containing CMOS technology properties (see 
                    documentation).
  --activity_file ACTIVITY_FILE
                    Signal activities file for all nets (see documentation).

noc options:
  --noc {on, off}   Enables a NoC-driven placer that optimizes the placement of 
                    routers on the NoC. Also enables an option in the graphical 
                    display that can be used to display the NoC on the FPGA. 
                    This should be on only when the FPGA device contains a NoC 
                    and the provided netlist connects to the NoC. (Default: off)
  --noc_flows_file NOC_FLOWS_FILE
                    XML file containing the list of traffic flows within the NoC 
                    (communication between routers).This is required if the 
                    --noc option is turned on.
  --noc_routing_algorithm {xy_routing, bfs_routing, west_first_routing, north_last_routing, negative_first_routing, odd_even_routing}
                    Controls the algorithm used by the NoC to route packets.
                    * xy_routing: Uses the direction oriented routing algorithm. 
                    This is recommended to be used with mesh NoC topologies.
                    * bfs_routing: Uses the breadth first search algorithm. The 
                    objective is to find a route that uses a minimum number of 
                    links.  This algorithm is not guaranteed to generate 
                    deadlock-free traffic flow routes, but can be used with any 
                    NoC topology
                    * west_first_routing: Uses the west-first routing algorithm. 
                    This is recommended to be used with mesh NoC topologies.
                    * north_last_routing: Uses the north-last routing algorithm. 
                    This is recommended to be used with mesh NoC topologies.
                    * negative_first_routing: Uses the negative-first routing 
                    algorithm. This is recommended to be used with mesh NoC 
                    topologies.
                    * odd_even_routing: Uses the odd-even routing algorithm. 
                    This is recommended to be used with mesh NoC topologies.
                     (Default: bfs_routing)
  --noc_placement_weighting NOC_PLACEMENT_WEIGHTING
                    Controls the importance of the NoC placement parameters 
                    relative to timing and wirelength of the design. This value 
                    can be >=0, where 0 would mean the placement is based solely 
                    on timing and wirelength. A value of 1 would mean noc 
                    placement is considered equal to timing and wirelength A 
                    value greater than 1 would mean the placement is 
                    increasingly dominated by NoC parameters. (Default: 5.0)
  --noc_aggregate_bandwidth_weighting NOC_AGGREGATE_BANDWIDTH_WEIGHTING
                    Controls the importance of minimizing the NoC aggregate 
                    bandwidth.
                    This value can be >=0, where 0 would mean the aggregate 
                    bandwidth has no relevance to placement.
                    Other positive numbers specify the importance of minimizing 
                    the NoC aggregate bandwidth to other NoC-related cost terms.
                    Weighting factors for NoC-related cost terms are normalized 
                    internally. Therefore, their absolute values are not 
                    important, and only their relative ratios determine the 
                    importance of each cost term. (Default: 0.38)
  --noc_latency_constraints_weighting NOC_LATENCY_CONSTRAINTS_WEIGHTING
                    Controls the importance of meeting all the NoC traffic flow 
                    latency constraints.
                    This value can be >=0, where 0 would mean the latency 
                    constraints have no relevance to placement.
                    Other positive numbers specify the importance of meeting 
                    latency constraints to other NoC-related cost terms.
                    Weighting factors for NoC-related cost terms are normalized 
                    internally. Therefore, their absolute values are not 
                    important, and only their relative ratios determine the 
                    importance of each cost term. (Default: 0.6)
  --noc_latency_weighting NOC_LATENCY_WEIGHTING
                    Controls the importance of reducing the latencies of the NoC 
                    traffic flows.
                    This value can be >=0, where 0 would mean the latencies have 
                    no relevance to placement.
                    Other positive numbers specify the importance of minimizing 
                    aggregate latency to other NoC-related cost terms.
                    Weighting factors for NoC-related cost terms are normalized 
                    internally. Therefore, their absolute values are not 
                    important, and only their relative ratios determine the 
                    importance of each cost term. (Default: 0.02)
  --noc_congestion_weighting NOC_CONGESTION_WEIGHTING
                    Controls the importance of reducing the congestion of the 
                    NoC links.
                    This value can be >=0, where 0 would mean the congestion has 
                    no relevance to placement.
                    Other positive numbers specify the importance of minimizing 
                    congestion to other NoC-related cost terms.
                    Weighting factors for NoC-related cost terms are normalized 
                    internally. Therefore, their absolute values are not 
                    important, and only their relative ratios determine the 
                    importance of each cost term. (Default: 0.25)
  --noc_centroid_weight NOC_CENTROID_WEIGHT
                    Sets the minimum fraction of swaps attempted by the placer 
                    that are NoC blocks.This value is an integer ranging from 
                    0-100. 0 means NoC blocks will be moved at the same rate as 
                    other blocks. 100 means all swaps attempted by the placer 
                    are NoC router blocks. (Default: 0)
  --noc_swap_percentage NOC_SWAP_PERCENTAGE
                    Sets the minimum fraction of swaps attempted by the placer 
                    that are NoC blocks. This value is an integer ranging from 
                    0-100. 0 means NoC blocks will be moved at the same rate as 
                    other blocks. 100 means all swaps attempted by the placer 
                    are NoC router blocks. (Default: 0)
  --noc_sat_routing_bandwidth_resolution NOC_SAT_ROUTING_BANDWIDTH_RESOLUTION
                    Specifies the resolution by which traffic flow bandwidths 
                    are converted into integers in SAT routing algorithm.
                    The higher this number is, the more accurate the congestion 
                    estimation and aggregate bandwidth minimization is.
                    Higher resolution for bandwidth conversion increases the 
                    number of variables in the SAT formulation. (Default: 128)
  --noc_sat_routing_latency_overrun_weighting_factor NOC_SAT_ROUTING_LATENCY_OVERRUN_WEIGHTING_FACTOR
                    Controls the importance of reducing traffic flow latency 
                    overrun in SAT routing. (Default: 1024)
  --noc_sat_routing_congestion_weighting_factor NOC_SAT_ROUTING_CONGESTION_WEIGHTING_FACTOR
                    Controls the importance of reducing the number of congested 
                    NoC links in SAT routing. (Default: 16384)
  --noc_sat_routing_num_workers NOC_SAT_ROUTING_NUM_WORKERS
                    The maximum number of parallel threads that the SAT solver 
                    can use to explore the solution space.
                    If not explicitly specified by the user, VPR will set the 
                    number parallel SAT solver workers to the value specified by 
                    -j command line option.
  --noc_sat_routing_log_search_progress {on, off}
                    Print the detailed log of the SAT solver's search progress. (Default: off)
  --noc_placement_file_name NOC_PLACEMENT_FILE_NAME
                    Name of the output file that contains the NoC placement 
                    information.The default name is 
                    'vpr_noc_placement_output.txt' (Default: vpr_noc_placement_output.txt)

server options:
  --server          Run in server mode.Accept client application connection and 
                    respond to requests. (Default: off)
  --port PORT       Server port number. (Default: 60555)

Usage Examples
--------------
   #Find the minimum routable channel width of my_circuit on my_arch
   vpr my_arch.xml my_circuit.blif

   #Show interactive graphics
   vpr my_arch.xml my_circuit.blif --disp on

   #Implement at a fixed channel width of 100
   vpr my_arch.xml my_circuit.blif --route_chan_width 100

   #Perform packing and placement only
   vpr my_arch.xml my_circuit.blif --pack --place

   #Generate post-implementation netlist
   vpr my_arch.xml my_circuit.blif --gen_post_synthesis_netlist on

   #Write routing-resource graph to a file
   vpr my_arch.xml my_circuit.blif --write_rr_graph my_rr_graph.xml
